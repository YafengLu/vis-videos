[{"created_time": "2019-12-27T17:13:25+00:00", "description": "Organizers: James Ahrens (Los Alamos National Laboratory, Los Alamos, New Mexico, USA), Soumya Dutta (Data Science at Scale, CCS-7, Los Alamos National Lab, Los Alamos, New Mexico, USA)\n\nReal-world decision makers, such as scientists and engineers are confronted with the needs to make critical decisions on a daily basis. Typically, this requires the understanding of the high-dimensional input parameter space and associated outcomes. To facilitate this understanding, one promising approach involves the definition and extractions of features of interest and the use of these features to drive the interactive exploration of the parameter space. Our application spotlight will focus on examples of these systems and synthesize common problems and themes.\nWe envision incorporating talks from leading visualization and application experts in this area including Dan Keefe (University of Minnesota) on the design of medical instruments, Valerio Pascucci/Jackie Chen (University of Utah/Sandia National Laboratory) on the design of combustion reactions and David Rogers/Richard Sandberg (Los Alamos National Laboratory) on the design of shock physics experiments. We will solicit lightning talks to gather other examples from the community. A discussion session will summarize common themes, methodological advances and next steps and turned into a journal paper for publication. In addition, a set of short papers describing each system will be archived. Potential contributions to basic research include high dimensional space representation, traversal methods and interactivity techniques.", "uri": "https://vimeo.com/381672933", "name": "VIS 2019 [Application Spotlight] Feature-based Visual Interactive Systems to Optimize Decision Making", "year": "2019", "event": ""}, {"created_time": "2019-12-09T18:32:56+00:00", "description": "Organizers: Madison Elliott, University of British Columbia; Zoya Bylinskii, Massachusetts Institute of Technology; Christine Nothelfer, Northwestern University; Cindy Xiong, Northwestern University; Danielle Albers Szafir, University of Colorado Boulder", "uri": "https://vimeo.com/378353877", "name": "VIS 2019: [Workshop] Vis X Vision: Novel Directions in Visual Science & Visualization Research", "year": "2019", "event": "WORKSHOP"}, {"created_time": "2019-12-09T18:21:56+00:00", "description": "Organizers: Nadia Boukhelifa, INRA; Anastasia Bezerianos, Universit\u00e9 Paris-Sud; Enrico Bertini, New York University; Christopher Collins, University of Ontario Institute of Technology; Steven Drucker, Microsoft Research; Alex Endert, Georgia Institute of Technology; Jessica Hullman, Northwestern University; Michael Sedlmair, University of Stuttgart", "uri": "https://vimeo.com/378351529", "name": "VIS 2019: [Workshop] Evaluation of Interactive Visual Machine Learning Systems (EVIVA-ML)", "year": "2019", "event": "WORKSHOP"}, {"created_time": "2019-12-09T18:19:03+00:00", "description": "Authors: Bao Nguyen, Ngan V. T. Nguyen, Vung Pham, Tommy Dang (Texas Tech University)", "uri": "https://vimeo.com/378350939", "name": "SciVis Contest 2019: Visualization of Data from HACC Simulations by Paraview", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-12-09T18:17:57+00:00", "description": "Authors: Clemens Hesse-Edenfeld, Malte Jonas Peter Steinke, Nikolaos Alexandros Santalidis, Karim Huesmann, Simon Leistikow, Lars Linsen (Westf\u00e4lische Wilhelms-Universit\u00e4t M\u00fcnster)", "uri": "https://vimeo.com/378350733", "name": "SciVis Contest 2019: Interactive Multi-level Visualization of Combined Particle and Multi-field Cosmology Data", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-12-09T18:13:12+00:00", "description": "Authors: Lea Fritschi, Irene Baeza Rojo, Tobias G\u00fcnther (ETH Zurich)", "uri": "https://vimeo.com/378349724", "name": "SciVis Contest 2019: Visualizing the Temporal Evolution of the Universe from Cosmology Simulations", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-12-09T17:44:54+00:00", "description": "Authors: Yueqi Hu, Qi Ma, Yang Chen, Haidong Chen, Weiqing Jin, and Fenjin Ye", "uri": "https://vimeo.com/378342967", "name": "VAST Challenge 2019: Visual Analysis of Multivariate Time Series of Static and Mobile Sensors", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:42:55+00:00", "description": "Authors: Shaobin Xu, Yiming Lin, Dezhan Qu. Ke Ren, and Huijie Zhang", "uri": "https://vimeo.com/378342501", "name": "VAST Challenge 2019: SUA: A Sensor Uncertainty Analysis Tool of Radiation Measurement Data", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:42:13+00:00", "description": "Authors: Mengyang Zhang, Kaokao Lv, Tang Liang, Chuanming Huang, Zhaokang Yuan, Xiaobo Luo, Zhengyan Yu, Lingjun He, and Zhuo Zhang", "uri": "https://vimeo.com/378342339", "name": "VAST Challenge 2019: Visual Analysis for Messages in Social Media", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:38:29+00:00", "description": "Authors: Chen Guo, Xiang Liu, Evie Cai, Yingjie Victor Chen, Zhenyu Cheryl Qian, and Rui Li", "uri": "https://vimeo.com/378341470", "name": "VAST Challenge 2019: TopicInk: Visualizing Disaster-related Textual Data using LDA Topic Modeling", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:37:34+00:00", "description": "Authors: Shuai Chen, Sihang Li, Liwenhan Xie, Yi Zhong, Yun Han, and Xiaoru Yuan", "uri": "https://vimeo.com/378341230", "name": "VAST Challenge 2019: EarthquakeAware: Visual Analytics for Understanding Human Impacts of Earthquakes from Social Media Data", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:35:02+00:00", "description": "Authors: Astrik Jeitler, Alpin T\u00fcrkoglu, Denis Makarov, Timo Jockers, Juri Buchmuller, Udo Schlegel, and Daniel Keim", "uri": "https://vimeo.com/378340610", "name": "VAST Challenge 2019: RescueMark: Visual Analytics of Social Media Data for Guiding Emergency Response in Disaster Situations", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:32:31+00:00", "description": "Authors: Wenjie Wu, Zheng Zhou, Yingjie Victor Chen, and Zhenyu Cheryl Qian", "uri": "https://vimeo.com/378339951", "name": "VAST Challenge 2019: HeatMosaic: Interactive uncertainty analysis of disaster events", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:30:47+00:00", "description": "Authors: Rodrigo Santos do Amor Divino Lima, Carlos Gustavo Resque dos Santos, and Bianchi Serique Meiguins", "uri": "https://vimeo.com/378339549", "name": "VAST Challenge 2019: IDUVis: An Interactive Dashboard for Uncertainty Visualization and Analysis", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:27:39+00:00", "description": "Authors: Shichao Jia, Jiaqi Wang, Zeyu Li, and Jiawan Zhang", "uri": "https://vimeo.com/378338876", "name": "VAST Challenge 2019: Interactive Ranking Uncertain Multivariate Ordinal Time Series: Citizen Science and Uncertainty", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:24:01+00:00", "description": "Author: Jo Wood", "uri": "https://vimeo.com/378338006", "name": "VAST Challenge 2019: Literate Visual Analytics for Crisis Management", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:20:40+00:00", "description": "Author: Natthawut Adulyanukosol", "uri": "https://vimeo.com/378337221", "name": "VAST Challenge 2019: Earthquake Damage Report Interactive Dashboard using Bayesian Structural Time Series and Value-Suppressing", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:17:21+00:00", "description": "Authors: Wolfgang Jentner, Juri Buchmuller, Fabian Sperrle, Rita Sevastjanova, Thilo Spinner, Udo Schlegel, Dirk Streeb, and Hanna Schaefer", "uri": "https://vimeo.com/378336444", "name": "VAST Challenge 2019: N.E.A.T.: Novel Emergency Management Tool", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:15:04+00:00", "description": "Authors: Riley Benson, Rajiv Ramarajan, Jon Nemargut, Biljana Belamaric Wilsey, Lisa Everdyke, Karl Prewo, Shaun Kurian, and Falko Shulz", "uri": "https://vimeo.com/378335939", "name": "VAST Challenge 2019: Earthquake at St. Himark", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:13:44+00:00", "description": "Authors: Datong Wei, Hanning Shao, Zijing Tan, Chenlu Li, Zhixian Lin, Xiaoju Dong, and Xiaoru Yuan", "uri": "https://vimeo.com/378335655", "name": "VAST Challenge 2019: RadiationMonitor: An Interactive System for Visualizing and Exploring Spatial-Temporal Data", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-09T17:10:21+00:00", "description": "Authors: Jochen Jankowai and Ingrid Hotz", "uri": "https://vimeo.com/378334896", "name": "SciVis 2019: [TVCG] Feature Level-Sets: Generalizing Iso-surfaces to Multi-variate Data", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-12-09T17:06:16+00:00", "description": "Authors: Rafael Ballester-Ripoll, Peter Lindstrom, Renato Pajarola", "uri": "https://vimeo.com/378333953", "name": "SciVis 2019: [TVCG] TTHRESH: Tensor Compression for Multidimensional Visual Data", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-12-04T05:10:52+00:00", "description": "Authors: Hua Guo and David H. Laidlaw", "uri": "https://vimeo.com/377231390", "name": "VAST 2019: [TVCG] Topic-based Exploration and Embedded Visualizations for Research Idea Generation", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-04T04:54:26+00:00", "description": "Authors: Maoyuan Sun, Jian Zhao, Hao Wu, Kurt Luther, Chris North, Naren Ramakrishnan", "uri": "https://vimeo.com/377229665", "name": "VAST 2019: [TVCG] The Effect of Edge Bundling and Seriation on Sensemaking of Biclusters in Bipartite Graphs", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-04T04:40:33+00:00", "description": null, "uri": "https://vimeo.com/377228089", "name": "InfoVis 2019: [TVCG] The Effect of Color Scales on Climate Scientists\u2019 Objective and Subjective Performance", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-12-04T04:11:14+00:00", "description": "Authors: Siming Chen, Jie Li, Gennady Andrienko, Natalia Andrienko, Yun Wang, Phong H. Nguyen, Cagatay Turkay", "uri": "https://vimeo.com/377224753", "name": "VAST 2019: [TVCG] Supporting Story Synthesis: Bridging the Gap between Visual Analytics and Storytelling", "year": "2019", "event": "VAST"}, {"created_time": "2019-12-04T03:59:12+00:00", "description": "Authors: Thom Castermans, Kevin Verbeek, Bettina Speckmann, Michel A. Westenberg, Rob Koopman, Shenghui Wang, Hein van den Berg, Arianna Betti", "uri": "https://vimeo.com/377223334", "name": "InfoVis 2019: [TVCG] SolarView: Low Distortion Radial Embedding with a Focus", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-12-03T23:37:23+00:00", "description": "Authors: Qianwen Wang, Jun Yuan, Shuxin Chen, Hang Su, Huamin Qu, Shixia Liu", "uri": "https://vimeo.com/377189866", "name": "VAST 2019: [TVCG] Visual Genealogy of Deep Neural Networks", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-28T20:57:55+00:00", "description": "Organizers: Brian Fisher (Simon Fraser University), Kristin Cook (Pacific Northwest National Laboratory), David Ebert (Purdue University), Daniel Keim (University of Konstanz)", "uri": "https://vimeo.com/376251557", "name": "VIS 2019: [Panel] Visual Analytics: Past, Present, Future", "year": "2019", "event": "PANEL"}, {"created_time": "2019-11-28T20:34:21+00:00", "description": "Organizers: Christina Stoiber (St. P\u00f6lten University of Applied Sciences, St. P\u00f6lten, Austria), Markus Wagner (St. P\u00f6lten University of Applied Sciences, St. P\u00f6lten, Austria), Davide Ceneda (TU Wien, Vienna, Austria), Theresia Gschwandtner (TU Wien, Vienna, Austria), Margit Pohl (TU Wien, Vienna, Austria), Silvia Miksch (TU Wien, Vienna, Austria), Marc Streit (Johannes Kepler University, Linz, Austria), Dominic Girardi (datavisyn GmbH, Linz, Austria), Wolfgang Aigner (St. P\u00f6lten University of Applied Sciences, St. P\u00f6lten, Austria)", "uri": "https://vimeo.com/376247027", "name": "VIS 2019: [Application Spotlight] Knowledge-assisted Visual Analytics meets Guidance and Onboarding", "year": "2019", "event": ""}, {"created_time": "2019-11-27T00:39:46+00:00", "description": "Organizers: Anders Ynnerman (Department of Science and Technology , Link\u00f6ping University, Norrk\u00f6ping, Sweden), Ingrid Hotz (Department for Media and Information Technology, Link\u00f6ping University, Norrk\u00f6ping, Sweden)", "uri": "https://vimeo.com/375803480", "name": "VIS 2019: [Application Spotlight] Visualization Enabled Scientific Discovery", "year": "2019", "event": ""}, {"created_time": "2019-11-27T00:32:27+00:00", "description": "Organizers: Jamie Waese (Cloud and Cognitive Software, IBM, Toronto, Ontario, Canada), Anne Stevens (Cloud and Cognitive Software, IBM, Toronto, Ontario, Canada), Afrooz Samaei (Cloud and Cognitive Software, IBM, Toronto, Ontario, Canada), Stephen O\u2019Connell (Cloud and Cognitive Software, IBM, Toronto, Ontario, Canada), Frank van Ham (Cloud and Cognitive Software, IBM, Weert, Netherlands)", "uri": "https://vimeo.com/375802456", "name": "VIS 2019: [Application Spotlight] Does AI mean data visualization is dead?", "year": "2019", "event": ""}, {"created_time": "2019-11-27T00:27:47+00:00", "description": "Panelists: Juliana Freire, New York University (Moderator); Jean-Daniel Fekete, Inria; Steve Haroz, Inria; Carlos Scheidegger, University of Arizona", "uri": "https://vimeo.com/375801794", "name": "VIS 2019: [Panel] Exploring Reproducibility in Visualization", "year": "2019", "event": "PANEL"}, {"created_time": "2019-11-27T00:20:12+00:00", "description": "Authors: Ji Hwan Park, Arie Kaufman, Klaus Mueller", "uri": "https://vimeo.com/375800781", "name": "CG&A 2019: Graphoto: Aesthetically Pleasing Charts for Casual Information Visualization", "year": "2019", "event": ""}, {"created_time": "2019-11-27T00:14:17+00:00", "description": "Authors: Adam James Bradley, Mennatallah El-Assady, Katharine Coles, Eric Alexander, Min Chen, Christopher Collins, Stefan J\u00e4nicke, David Joseph Wrisley", "uri": "https://vimeo.com/375799922", "name": "CG&A 2019: Visualization and the Digital Humanities: Moving Toward Stronger Collaborations", "year": "2019", "event": ""}, {"created_time": "2019-11-27T00:09:59+00:00", "description": "Authors: Wolfgang B\u00fcschel, Stefan Vogt, Raimund Dachselt", "uri": "https://vimeo.com/375799338", "name": "CG&A 2019: Augmented Reality Graph Visualizations: Investigation of Visual Styles in 3D Node-Link Diagrams", "year": "2019", "event": ""}, {"created_time": "2019-11-27T00:08:41+00:00", "description": "Authors: Jorge A. Wagner Filho, Carla M. D. S. Freitas, Luciana Nedel", "uri": "https://vimeo.com/375799161", "name": "CG&A 2019: Comfortable Immersive Analytics with the VirtualDesk Metaphor", "year": "2019", "event": ""}, {"created_time": "2019-11-27T00:05:40+00:00", "description": "Authors: G. Elisabeta Marai, Jason Leigh, Andrew Johnson", "uri": "https://vimeo.com/375798732", "name": "CG&A 2019: Immersive Analytics Lessons from the Electronic Visualization Laboratory: a 25 Year Perspective", "year": "2019", "event": ""}, {"created_time": "2019-11-27T00:01:09+00:00", "description": "Authors: Alexander Ivanov, Kurtis Danyluk, Christian Jacob, Wesley Willett", "uri": "https://vimeo.com/375798028", "name": "CG&A 2019: A Walk Among the Data: Exploration and Anthropomorphism in Immersive Unit Visualizations", "year": "2019", "event": ""}, {"created_time": "2019-11-24T16:47:20+00:00", "description": "Speaker: Mollie Pettit, Netflix. (Mollie is a Senior Data Visualization Engineer with Netflix. Previously, Mollie worked as a data visualization contractor, a d3.js instructor with Northwestern University and Metis, and as a data scientist with Datascope Analytics. Mollie\u2019s joy at being involved with data viz community endeavors has led her to found and organize Chicago Data Viz Community Meetup; volunteer as VisFest Organizer, and serve as Knowledge Director of Data Visualization Society. When Mollie is not being a data nerd, she swing dances and bicycles around the city.)", "uri": "https://vimeo.com/375250144", "name": "VIS in Practice 2019: Introducing the Data Visualization Society", "year": "2019", "event": ""}, {"created_time": "2019-11-24T16:29:47+00:00", "description": "Speaker: John Clyne, National Center for Atmospheric Research. (John manages the Visualization and Analysis Systems Technologies (VAST) section at the National Center for Atmospheric Research (NCAR) in Boulder Colorado. VAST is involved in numerous activities related to the visualization and analysis of Earth System Science (ESS) data, including: development of open source community software, research, and production visualization services. John is the chief architect of the widely used VAPOR package. His research interests include volume rendering, flow visualization, and strategies for large, time-varying data visualization.)", "uri": "https://vimeo.com/375248252", "name": "VIS in Practice 2019: Visualization in the Earth System Sciences", "year": "2019", "event": ""}, {"created_time": "2019-11-24T16:13:52+00:00", "description": "Speaker: Callie Neylan, Microsoft. (Callie is a designer, photographer, and writer. She has worked at various startups, software companies, and design firms in Seattle and, while on the East Coast, as a senior interaction designer at NPR. She is now a senior software designer at Microsoft, where she works on the Excel product team, creating data visualization and visual analytics tools.)", "uri": "https://vimeo.com/375246607", "name": "VIS in Practice 2019: Excel as Digital Infrastructure: What Would Jane Jacobs Do?", "year": "2019", "event": ""}, {"created_time": "2019-11-24T16:06:25+00:00", "description": "Speaker: Chad Skelton, Kwantlen Polytechnic University. (Chad is an award-winning data journalist, consultant, and trainer based in Vancouver. Chad worked as a data journalist at The Vancouver Sun until 2015. In 2014, Chad won an international Data Journalism Award for his portfolio of work in the previous year. He has received the Jack Webster Award, British Columbia\u2019s top journalism prize, six times. Chad created The Sun\u2019s public-sector salary database, which has received more than 20 million pageviews, and has built popular interactive tools on everything from commuting patterns to income inequality. He also made a Twitter robot that checks court judgments (@BCCourtBot). Chad has been an instructor at Kwantlen Polytechnic University since 2005, where he teaches the popular Citizen Journalism (JRNL 1220) and Data Visualization (JRNL 3165) courses. Chad also teaches a course in Data Storytelling and Visualization at the University of Florida as part of its online Master\u2019s program in Audience Analytics. Chad has given several talks on data and storytelling, including at the 2015 Tapestry Conference sponsored by Tableau Software and at Simon Fraser University\u2019s Dream Colloquium on Engaging Big Data. Chad has a Bachelor of Applied Arts in Journalism from Ryerson University and a Master of Information Management from Dalhousie University.)", "uri": "https://vimeo.com/375245707", "name": "VIS in Practice 2019: How to Teach Data Viz to Skeptics", "year": "2019", "event": ""}, {"created_time": "2019-11-24T15:46:09+00:00", "description": "Speaker: Frank Hangler, Plot + Scatter. (Frank is a designer, developer, and principal at Plot + Scatter, a data communication company specializing in custom-built, web-based visualizations. He has taught and presented on information visualization to business and academic audiences and is an organizer of the Vancouver Data Visualization Meetup group. Beyond his visualization work, he is interested in the relationship between society, technology, and design in the digital era, and holds an MSc in the Social Science of the Internet from the Oxford Internet Institute.)", "uri": "https://vimeo.com/375243461", "name": "VIS in Practice 2019: Real Change: Lessons from Revisiting an Existing Visualization", "year": "2019", "event": ""}, {"created_time": "2019-11-24T15:43:00+00:00", "description": "Speakers: Meghana Venkataswamy and Johanna Fulda, Clir Renewables Inc. (Meghana is a Data Science Engineer at Clir Renewables Inc. She graduated from the University of British Columbia with a M.Sc in Computer Science. At Clir, she brings together her knowledge of Software Development and Machine Learning to build cloud-based scalable models of wind farm analytics and operations. She is passionate about bringing a compelling digital experience to the user focused on extracting rich insights from complex data.\n\nJohanna is a Data Visualization designer and Software Developer at Clir. She graduated from the University of Munich with a M.Sc. in Computer Science. During those studies she spent a year at the University of British Columbia, working with Tamara Munzner\u2019s InfoVis Group and the UBC School of Journalism. She\u2019s also organizing Vancouver\u2019s Data Visualization Meetup.)", "uri": "https://vimeo.com/375243179", "name": "VIS in Practice 2019: Visualizing Renewable Energies", "year": "2019", "event": ""}, {"created_time": "2019-11-24T15:35:48+00:00", "description": "Panelists: Daniela Oelke (moderator), RJ Andrews, David Feng, Cydney Nielsen, Amber Thomas, etc.", "uri": "https://vimeo.com/375242322", "name": "VIS in Practice 2019: [Panel] Common conceptions we (dis)agree upon", "year": "2019", "event": "PANEL"}, {"created_time": "2019-11-24T15:33:58+00:00", "description": "Speaker: David Feng, Tableau. (After studying computer graphics at Northwestern University, David worked on multivariate and uncertain data visualization at UNC Chapel Hill. He then spent the next 8.5 years in Seattle at the Allen Institute for Brain Science, initially as a data visualization software engineer, and finally as a director overseeing several production data pipelines. This year he joined Tableau to manage several product teams focusing on interactive visual analytics.)", "uri": "https://vimeo.com/375242093", "name": "Vis in Practice 2019: Data Visuaization in Production", "year": "2019", "event": ""}, {"created_time": "2019-11-24T15:29:29+00:00", "description": "Speaker: RJ Andrews. (RJ is the author of the book Info We Trust, a lavish adventure exploring how to inspire the world with data. He blends creative arts and data science to inform. His projects have won recognition and been translated across the world. As an independent creative he does all of his own data processing, analysis, illustration, coding, and motion design using a variety of tools. When not working on his own data stories, RJ helps organizations solve information problems. Learn more at infowetrust.com.)", "uri": "https://vimeo.com/375241645", "name": "VIS in Practice 2019: Making Info We Trust", "year": "2019", "event": ""}, {"created_time": "2019-11-24T15:15:48+00:00", "description": "Speaker: Timothy Wojtaszek, Uber. (Timothy has found himself in visualization in one way or another. Starting with Postscript and fonts at Adobe to his current role as a Senior Engineer II in Uber\u2019s Visualization Team helping to bring autonomous systems a little closer to reality.)", "uri": "https://vimeo.com/375240363", "name": "VIS in Practice 2019: Autonomous Visualization System", "year": "2019", "event": ""}, {"created_time": "2019-11-24T15:12:29+00:00", "description": "Speakers: Eugene Chen and Joey Cherdarchuk, Darkhorse Analytics. (Eugene leads the technical team at Darkhorse Analytics. He enjoys combining his creativity, coding, and design into building elegant, interactive tools and visualizations. Eugene was shortlisted for an Information is Beautiful Award for WorldChatClock.com. He is an Electrical Engineering graduate from the University of Alberta and his background includes roles in development and program management at Microsoft and Blackberry. Beyond technical skills, Eugene is also involved in grassroots organizations such as Edmonton NextGen. He participates in open data initiatives such as Alberta\u2019s Open Data Competition, BetaCityYEG and the federal government\u2019s Open Data/Open Government Consultations.\n\nJoey is a cofounder and the design lead at Darkhorse Analytics. A data visualization expert, his work has been highlighted in the New York Times, the Huffington Post, and the Washington Post. His Breathing City graphic was shortlisted for an Information is Beautiful award. Joey developed his design expertise after a 15-year quantitative analysis and consulting career. It is this analytical foundation that helps him create visuals that are both clear and engaging. He offers training on data visualization and his Data Looks Better Naked series is used in visualization curricula around the world.)", "uri": "https://vimeo.com/375240047", "name": "VIS in Practice 2019: Visualizing Opportunity", "year": "2019", "event": ""}, {"created_time": "2019-11-24T14:53:53+00:00", "description": "Speaker: Amber Thomas, The Pudding. (Amber is a Senior Journalist Engineer at the online editorial, The Pudding. While she has been creating data-driven stories online since 2017, she is, by training, a marine biologist. After a few years of studying animals that lived beneath the waves and communicating science to anyone that would listen, she career-pivoted and has been working with the fine folks at The Pudding ever since.)", "uri": "https://vimeo.com/375238181", "name": "VIS in Practice 2019: Things We\u2019ve Learned From Telling the \u2018Fun\u2019 Data Stories", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:44:43+00:00", "description": "Authors: Aashish Chaudhary, Sankhesh J. Jhaveri, Alvaro Sanchez, Lisa S. Avila, Kenneth M. Martin, Allison Vacanti, Marcus D. Hanwell, Will Schroeder", "uri": "https://vimeo.com/375164617", "name": "CG&A 2019: Cross-Platform Ubiquitous Volume Rendering Using Programmable Shaders in VTK for Scientific and Medical Visualization", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:43:32+00:00", "description": "Authors: Eric Papenhausen, M. Harper Langston, Benoit Meister, Richard A. Lethin, Klaus Mueller", "uri": "https://vimeo.com/375164477", "name": "CG&A 2019: PUMA-V: Optimizing Parallel Code Performance Through Interactive Visualization", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:38:45+00:00", "description": "Authors: Dylan Cashman, Genevi\u00e8ve Patterson, Abigail Mosca, Nathan Watts, Shannon Robinson, Remco Chang", "uri": "https://vimeo.com/375164049", "name": "CG&A 2019: RNNbow: Visualizing Learning Via Backpropagation Gradients in RNNs", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:35:00+00:00", "description": "Authors: Tobias Kauer, Sagar Joglekar, Miriam Redi, Luca Maria Aiello, Daniele Quercia", "uri": "https://vimeo.com/375163629", "name": "CG&A 2019: Mapping and Visualizing Deep-Learning Urban Beautification", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:31:43+00:00", "description": "Authors: Benjamin Karer, Alina Freund, Michael Horst, Inga Scheler, Thomas Kossurok, Franz-Josef Brandt", "uri": "https://vimeo.com/375163262", "name": "CG&A 2019: Designing Effective Visual Interactive Systems despite Sparse Availability of Domain Information", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:25:23+00:00", "description": "Authors: Wei Zeng and Yu Ye", "uri": "https://vimeo.com/375162613", "name": "CG&A 2019: VitalVizor: A Visual Analytics System for Studying Urban Vitality", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:05:26+00:00", "description": "Authors: Duong Nguyen, Lei Zhang, Robert S. Laramee, David Thompson, Rodolfo Ostilla Monico, Guoning Chen", "uri": "https://vimeo.com/375160691", "name": "VIS 2019: [Short Papers] Unsteady Flow Visualization via Physics based Pathline Exploration", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:04:36+00:00", "description": "Authors: Teodoro Collin, Charisee Chiw, L. Ridgway Scott, John Reppy, Gordon L Kindlmann", "uri": "https://vimeo.com/375160621", "name": "VIS 2019: [Short Papers] Point Movement in a DSL for Higher-Order FEM Visualization", "year": "2019", "event": ""}, {"created_time": "2019-11-23T22:02:40+00:00", "description": "Authors: Nina McCurdy and Miriah Meyer", "uri": "https://vimeo.com/375160419", "name": "VIS 2019: [Short Papers] GalStamps: Analyzing Real and Simulated Galaxy Observations", "year": "2019", "event": ""}, {"created_time": "2019-11-23T21:58:41+00:00", "description": "Authors: Khairi Reda and Michael E. Papka", "uri": "https://vimeo.com/375160053", "name": "VIS 2019: [Short Papers] Evaluating Gradient Perception in Color-coded Scalar Fields", "year": "2019", "event": ""}, {"created_time": "2019-11-23T21:56:36+00:00", "description": "Authors: Qiong Zeng, Yinqiao Wang, Ivan Viola, Wenting Zhang, Jian Zhang, Changhe Tu, Yunhai Wang", "uri": "https://vimeo.com/375159868", "name": "VIS 2019: [Short Papers] Data-Driven Colormap Optimization for 2D Scalar Field Visualization", "year": "2019", "event": ""}, {"created_time": "2019-11-23T21:55:47+00:00", "description": "Authors: Stefan Zellmann, Deborah Meurer, Ulrich Lang", "uri": "https://vimeo.com/375159801", "name": "VIS 2019: [Short Papers] Hybrid Grids for Sparse Volume Rendering", "year": "2019", "event": ""}, {"created_time": "2019-11-23T21:53:33+00:00", "description": "Authors: Nate Morrical, Will Usher, Ingo Wald, Valerio Pascucci", "uri": "https://vimeo.com/375159570", "name": "VIS 2019: [Short Papers] Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes using Hardware Accelerated Ray T", "year": "2019", "event": ""}, {"created_time": "2019-11-22T23:34:51+00:00", "description": "Authors: Irene Baeza Rojo, Markus Gross, Tobias G\u00fcnther", "uri": "https://vimeo.com/375029788", "name": "SciVis 2019: [TVCG] Fourier Opacity Optimization for Scalable Exploration", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-22T23:30:53+00:00", "description": "Authors: Yuxin Ma, Anthony K. H. Tung, Wei Wang, Xiang Gao, Zhigeng Pan, Wei Chen", "uri": "https://vimeo.com/375029262", "name": "VAST 2019: [TVCG] ScatterNet: A Deep Subjective Similarity Model for Visual Analysis of Scatterplots", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-22T23:29:19+00:00", "description": "Authors: C\u00edcero L. Pahins, Nivan Ferreira, Jo\u00e3o L. Comba", "uri": "https://vimeo.com/375029056", "name": "SciVis 2019: [TVCG] Real-Time Exploration of Large Spatiotemporal Datasets based on Order Statistics", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-22T23:28:15+00:00", "description": "Authors: Ko-Chih Wang, Tzu-Hsuan Wei, Shareef Naeem, Han-Wei Shen", "uri": "https://vimeo.com/375028920", "name": "SciVis 2019: [TVCG] Ray-based Exploration of Large Time-varying Volume Data Using Proxy Per-ray Distributions", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-22T23:26:37+00:00", "description": "Authors: Aoyu Wu and Huamin Qu", "uri": "https://vimeo.com/375028726", "name": "VAST 2019: [TVCG] Multimodal Analysis of Video Collections: Visual Exploration of Presentation Techniques in TED Talks", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-22T23:19:30+00:00", "description": "Authors: Zhutian Chen, Yijia Su, Yifang Wang, Qianwen Wang, Huamin Qu, Yingcai Wu", "uri": "https://vimeo.com/375027790", "name": "InfoVis 2019: [TVCG] MARVisT: Authoring Glyph-based Visualization in Mobile Augmented Reality", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-22T19:12:23+00:00", "description": "Authors: Jun Han, Jun Tao, Chaoli Wang", "uri": "https://vimeo.com/374982727", "name": "SciVis 2019: [TVCG] FlowNet: A Deep Learning Framework for Clustering and Selection of Streamlines and Stream Surfaces", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-22T19:07:38+00:00", "description": "Authors: Shreeraj Jadhav, Saad Nadeem, Arie Kaufman", "uri": "https://vimeo.com/374981631", "name": "SciVis 2019: [TVCG] FeatureLego: Volume Exploration Using Exhaustive Clustering of Super-Voxels", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-22T19:06:25+00:00", "description": "Authors: Andrea Schnorr, Dirk N. Helmrich, Dominik Denker, Torsten W. Kuhlen, Bernd Hentschel", "uri": "https://vimeo.com/374981357", "name": "SciVis 2019: [TVCG] Feature Tracking by Two-Step Optimization", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-22T19:05:15+00:00", "description": "Authors: Hanqi Guo, Wenbin He, Sangmin Seo, Han-Wei Shen, Emil Mihai Constantinescu, Chunhui Liu, Tom Peterka", "uri": "https://vimeo.com/374981112", "name": "SciVis 2019: [TVCG] Extreme-Scale Stochastic Particle Tracing for Uncertain Unsteady Flow Visualization and Analysis", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-22T18:56:59+00:00", "description": "Authors: Zhaosong Huang, Yafeng Lu, Elizabeth A. Mack, Wei Chen, Ross Maciejewski", "uri": "https://vimeo.com/374979423", "name": "VAST 2019: [TVCG] Exploring the Sensitivity of Choropleths under Attribute Uncertainty", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-22T18:55:33+00:00", "description": "Authors: Yucheng Huang, Lei Shi, Yue Su, Yifan Hu, Hanghang Tong, Chaoli Wang, Tong Yang, Deyun Wang, Shuo Liang", "uri": "https://vimeo.com/374979123", "name": "InfoVis 2019: [TVCG] Eiffel: Evolutionary Flow Map for Influence Graph Visualization", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-22T18:54:05+00:00", "description": "Authors: Jie Li, Siming Chen, Kang Zhang, Gennady Andrienko, Natalia Andrienko", "uri": "https://vimeo.com/374978709", "name": "VAST 2019: [TVCG] COPE: Interactive Exploration of Co-occurrence Patterns in Spatial Time Series", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-22T18:53:01+00:00", "description": "Authors: Zipeng Liu, Shing Hei Zhan, Tamara Munzner", "uri": "https://vimeo.com/374978467", "name": "InfoVis 2019: [TVCG] Aggregated Dendrograms for Visual Comparison Between Many Phylogenetic Trees", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-22T18:47:40+00:00", "description": "Authors: Paulo Ivson, Andr\u00e9 Moreira, Francisco Queiroz, Wallas Santos, Waldemar Celes", "uri": "https://vimeo.com/374977296", "name": "InfoVis 2019: [TVCG] A Systematic Review of Visualization in Building Information Modeling", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-21T16:56:10+00:00", "description": "Authors: Tanja Munz, Michael Burch, Toon van Benthem, Yoeri Poels, Fabian Beck, Daniel Weiskopf", "uri": "https://vimeo.com/374702141", "name": "VIS 2019: [Short Papers] Overlap-Free Drawing of Generalized Pythagoras Trees for Hierarchy Visualization", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:55:06+00:00", "description": "Authors: Maoyuan Sun, David Koop, Jian Zhao, Chris North, Naren Ramakrishnan", "uri": "https://vimeo.com/374701864", "name": "VIS 2019: [Short Papers] Interactive Bicluster Aggregation in Bipartite Graphs", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:53:42+00:00", "description": "Authors: Yong Wang, Daniel Archambault, Hammad Haleem, Torsten Moeller, Yanhong Wu, Huamin Qu", "uri": "https://vimeo.com/374701528", "name": "VIS 2019: [Short Papers] Nonuniform Timeslicing of Dynamic Graphs Based on Visual Complexity", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:52:30+00:00", "description": "Authors: Nicholas Ruta, Naoko Sawada, Katy McKeough, Michael Behrisch, Johanna Beyer", "uri": "https://vimeo.com/374701226", "name": "VIS 2019: [Short Papers] SAX Navigator: Time Series Exploration Through Hierarchical Clustering", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:51:47+00:00", "description": "Authors: Marco Angelini, Graziano Blasilli, Simone Lenti, Alessia Palleschi, Giuseppe Santucci", "uri": "https://vimeo.com/374701026", "name": "VIS 2019: [Short Papers] Towards Enhancing RadViz Analysis and Interpretation", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:51:24+00:00", "description": "Author: Daniel Weidele", "uri": "https://vimeo.com/374700912", "name": "VIS 2019: [Short Papers] Conditional Parallel Coordinates", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:49:19+00:00", "description": "Authors: Radi Muhammad Reza and Benjamin Watson", "uri": "https://vimeo.com/374700400", "name": "VIS 2019: [Short Papers] Hi-D Maps: An Interactive Visualization Technique for Multi-dimensional Categorical Data", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:45:48+00:00", "description": "Authors: Beno\u00eet Colange, Laurent Vuillon, Sylvain Lespinats, Denys Dutykh", "uri": "https://vimeo.com/374699571", "name": "VIS 2019: [Short Papers] Interpreting Distortions in Dimensionality Reduction by Superimposing Neighbourhood Graphs", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:23:44+00:00", "description": "Authors: Yixuan Zhang, Sara Di Bartolomeo, Fangfang Sheng, Holly Jimison, Cody Dunne", "uri": "https://vimeo.com/374693882", "name": "VIS 2019: [Short Papers] Evaluating Alignment Approaches in Superimposed Time-series and Temporal Event-sequence Visualizations", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:22:56+00:00", "description": "Authors: Jessica Magallanes, Lindsey van Gemeren, Steven Wood, Maria-Cruz Villa-Uriol", "uri": "https://vimeo.com/374693697", "name": "VIS 2019: [Short Papers] Analyzing Time Attributes in Event Sequences", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:22:10+00:00", "description": "Authors: Xiangyun Lei, Fred Hohman, Duen Horng Chau, Andrew J Medford", "uri": "https://vimeo.com/374693496", "name": "VIS 2019: [Short Papers] ElectroLens: Understanding atomistic simulations through spatially-resolved visualization of high-dimen", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:14:57+00:00", "description": "Authors: David Abramov, Jasmine Tan Otto, Mahika Dubey, Cassia Artanegara, Pierre Boutillier, Walter Fontana, Angus G. Forbes", "uri": "https://vimeo.com/374691687", "name": "VIS 2019: [Short Papers] RuleVis: Constructing Patterns and Rules for Rule-based Models", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:14:06+00:00", "description": "Authors: Ran Xu, Manu Mathew Thomas, Alex Leow, Olusola A. Ajilore, Angus G. Forbes", "uri": "https://vimeo.com/374691477", "name": "VIS 2019: [Short Papers] TempoCave: Visualizing Dynamic Connectome Datasets to Support Cognitive Behavioral Therapy", "year": "2019", "event": ""}, {"created_time": "2019-11-21T16:09:21+00:00", "description": "Authors: Jonas Karlsson, Marwan Abdellah, Sebastien Speierer, Alessandro Enrico Foni, Samuel Lapere, Felix Schurmann", "uri": "https://vimeo.com/374690348", "name": "VIS 2019: [Short Papers] High Fidelity Visualization of Large Scale Digitally Reconstructed Brain Circuitry with Signed Distance", "year": "2019", "event": ""}, {"created_time": "2019-11-21T15:58:35+00:00", "description": "Authors: JunYoung Choi, Sang-Eun Lee, Eunji Cho, Yutaro Kashiwagi, Shigeo Okabe, Sunghoe Chang, Won-Ki Jeong", "uri": "https://vimeo.com/374687797", "name": "VIS 2019: [Short Papers] Interactive Dendritic Spine Analysis Based on 3D Morphological Features", "year": "2019", "event": ""}, {"created_time": "2019-11-19T15:54:02+00:00", "description": "Authors: Jieqiong Zhao, Morteza Karimzadeh, Ali Masjedi, Taojun Wang, Xiwen Zhang, Melba Crawford, David Ebert", "uri": "https://vimeo.com/374187616", "name": "VIS 2019: [Short Papers] FeatureExplorer: Interactive Feature Selection and Exploration of Regression Models for Hyperspectral I", "year": "2019", "event": ""}, {"created_time": "2019-11-19T15:53:26+00:00", "description": "Authors: Lindsey Sawatzky, Steven Bergner, Fred Popowich", "uri": "https://vimeo.com/374187437", "name": "VIS 2019: [Short Papers] Visualizing RNN States with Predictive Semantic Encodings", "year": "2019", "event": ""}, {"created_time": "2019-11-19T15:51:58+00:00", "description": "Authors: Fred Hohman, Arjun Srinivasan, Steven Drucker", "uri": "https://vimeo.com/374187072", "name": "VIS 2019: [Short Papers] TeleGam: Combining Visualization and Verbalization for Interpretable Machine Learning", "year": "2019", "event": ""}, {"created_time": "2019-11-19T15:51:23+00:00", "description": "Authors: Cheonbok Park, Inyoup Na, Yongjang Jo, Sungbok Shin, Yoo Jaehyo, Bum Chul Kwon, Jian Zhao, Hyungjong Noh, Yeonsoo Lee, Jaegul Choo", "uri": "https://vimeo.com/374186897", "name": "VIS 2019: [Short Papers] SANVis: Visual Analytics for Understanding Self Attention Networks", "year": "2019", "event": ""}, {"created_time": "2019-11-19T15:50:05+00:00", "description": "Authors: Micha\u00ebl Aupetit, Michael Sedlmair, Mostafa M. Abbas, Abdelkader Baggag, Halima Bensmail", "uri": "https://vimeo.com/374186581", "name": "VIS 2019: [Short Papers] Toward Perception-based Evaluation of Clustering Techniques for Visual Analytics", "year": "2019", "event": ""}, {"created_time": "2019-11-19T15:49:21+00:00", "description": "Authors: Jaemin Jo and Jinwook Seo", "uri": "https://vimeo.com/374186426", "name": "VIS 2019: [Short Papers] Disentangled Representation of Data Distributions in Scatterplots", "year": "2019", "event": ""}, {"created_time": "2019-11-19T15:48:38+00:00", "description": "Authors: William P. Porter, Yunhao Xing, Blaise R von Ohlen, Jun Han, Chaoli Wang", "uri": "https://vimeo.com/374186242", "name": "VIS 2019: [Short Papers] A Deep Learning Approach to Selecting Representative Time Steps for Time-Varying Multivariate Data", "year": "2019", "event": ""}, {"created_time": "2019-11-19T15:47:59+00:00", "description": "Authors: Xin Fu, Yun Wang, Haoyu Dong, Weiwei Cui, Haidong Zhang", "uri": "https://vimeo.com/374186092", "name": "VIS 2019: [Short Papers] Visualization Assessment: A Machine Learning Approach", "year": "2019", "event": ""}, {"created_time": "2019-11-19T00:25:57+00:00", "description": "Authors: Juraj P\u00e1lenik, Jan By\u0161ka, Stefan Bruckner, Helwig Hauser", "uri": "https://vimeo.com/374045821", "name": "SciVis 2019: Scale-Space Splatting: Reforming Spacetime for the Cross-Scale Exploration of Integral Measures in Molecular Dynami", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-19T00:05:10+00:00", "description": "Authors: Fritz Lekschas, Michael Behrisch, Benjamin Bach, Peter Kerpedjiev, Nils Gehlenborg, Hanspeter Pfister", "uri": "https://vimeo.com/374042877", "name": "InfoVis 2019: Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-19T00:04:33+00:00", "description": "Authors: Yao Ming, Panpan Xu, Furui Cheng, Huamin Qu, Liu Ren", "uri": "https://vimeo.com/374042799", "name": "VAST 2019: ProtoSteer: Steering Deep Sequence Model with Prototypes", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-19T00:02:20+00:00", "description": "Authors: Robert Kr\u00fcger, Johanna Beyer, Won-Dong Jang, Nam Wook Kim, Artem Sokolov, Peter Sorger, Hanspeter Pfister", "uri": "https://vimeo.com/374042499", "name": "VAST 2019: Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Da", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-19T00:01:33+00:00", "description": "Authors: Chen Chen, Changbo Wang, Xue Bai, Peiying Zhang, Chenhui Li", "uri": "https://vimeo.com/374042397", "name": "InfoVis 2019: GenerativeMap: Visualization and Exploration of Dynamic Density Maps via Generative Learning Model", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-19T00:00:07+00:00", "description": "Authors: Jun Han and Chaoli Wang", "uri": "https://vimeo.com/374042188", "name": "SciVis 2019: TSR-TVD: Temporal Super-Resolution for Time-Varying Data Analysis and Visualization", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-18T23:54:06+00:00", "description": "Authors: Zhutian Chen, Wei Zeng, ZhiGuang Yang, Lingyun Yu, Chi-Wing Fu, Huamin Qu", "uri": "https://vimeo.com/374041372", "name": "SciVis 2019: LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-18T23:53:14+00:00", "description": "Authors: Michael Behrisch, Tobias Schreck, Hanspeter Pfister", "uri": "https://vimeo.com/374041222", "name": "VAST 2019: GUIRO: User-Guided Matrix Reordering", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-17T23:36:54+00:00", "description": "Authors: Hayder Mahdi Al-maneea, Jonathan C Roberts", "uri": "https://vimeo.com/373794692", "name": "VIS 2019: [Short Papers] Towards Quantifying multiple view layouts in visualisation as seen from research publications", "year": "2019", "event": ""}, {"created_time": "2019-11-17T23:35:04+00:00", "description": "Authors: Hwiyeon Kim, Juyoung Oh, Yunha Han, Sungahn Ko, Matthew Brehmer, Bum Chul Kwon", "uri": "https://vimeo.com/373794459", "name": "VIS 2019: [Short Papers] Thumbnails for Data Stories: Survey of Current Practice", "year": "2019", "event": ""}, {"created_time": "2019-11-17T23:34:38+00:00", "description": "Authors: Emily Wall, John Stasko, Alex Endert", "uri": "https://vimeo.com/373794397", "name": "VIS 2019: [Short Papers] Towards a Design Space for Mitigating Cognitive Bias in Visual Analytics", "year": "2019", "event": ""}, {"created_time": "2019-11-17T23:33:23+00:00", "description": "Author: Stephen Redmond", "uri": "https://vimeo.com/373794253", "name": "VIS 2019: [Short Papers] Visual cues in estimation of part-to-whole comparison", "year": "2019", "event": ""}, {"created_time": "2019-11-17T23:30:10+00:00", "description": "Author: Robert Kosara", "uri": "https://vimeo.com/373793866", "name": "VIS 2019: [Short Papers] Evidence for Area as the Primary Visual Cue in Pie Charts", "year": "2019", "event": ""}, {"created_time": "2019-11-17T23:23:38+00:00", "description": "Authors: Linda Woodburn, Yalong Yang, Kim Marriott", "uri": "https://vimeo.com/373793154", "name": "VIS 2019: [Short Papers] Interactive Visualisation of Hierarchical Quantitative Data: an Evaluation", "year": "2019", "event": ""}, {"created_time": "2019-11-17T23:21:12+00:00", "description": "Authors: Matthias Miller, Xuan Zhang, Johannes Fuchs, Michael Blumenschein", "uri": "https://vimeo.com/373792795", "name": "VIS 2019: [Short Papers] Evaluating Ordering Strategies of Star Glyph Axes", "year": "2019", "event": ""}, {"created_time": "2019-11-17T23:17:51+00:00", "description": "Authors: David Pomerenke, Frederik L. Dennig, Daniel Keim, Johannes Fuchs, Michael Blumenschein", "uri": "https://vimeo.com/373792354", "name": "VIS 2019: [Short Papers] Slope-Dependent Rendering of Parallel Coordinates to Reduce Density Distortion and Ghost Clusters", "year": "2019", "event": ""}, {"created_time": "2019-11-17T23:16:13+00:00", "description": "Authors: Emily Wall, Arup Arcalgud, Kuhu Gupta, Andrew Jo", "uri": "https://vimeo.com/373792145", "name": "VIS 2019: [Short Papers] A Markov Model of Users\u2019 Interactive Behavior in Scatterplots", "year": "2019", "event": ""}, {"created_time": "2019-11-14T01:07:19+00:00", "description": "Authors: Daniel J\u00f6nsson, Peter Steneteg, Erik Sund\u00e9n, Rickard Englund, Sathish Kottravel, Martin Falk, Ingrid Hotz, Anders Ynnerman, Timo Ropinski", "uri": "https://vimeo.com/373020911", "name": "SciVis 2019: [TVCG] Inviwo: A Visualization System with Usage Abstraction Levels", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T01:06:22+00:00", "description": "Authors: Baldwin Nsonga, Martin Niemann, Jochen Fr\u00f6hlich, Joachim Staib, Stefan Gumhold, Gerik Scheuermann", "uri": "https://vimeo.com/373020771", "name": "SciVis 2019: [TVCG] Detection and Visualization of Splat and Antisplat Events in Turbulent Flows", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T01:03:42+00:00", "description": "Authors: Mohammad Raji, Alok Hota, Tanner Hobson, Jian Huang", "uri": "https://vimeo.com/373020392", "name": "SciVis 2019: [TVCG] Scientific Visualization as a Microservice", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T01:00:18+00:00", "description": "Authors: Alok Hota & Jian Huang", "uri": "https://vimeo.com/373019920", "name": "SciVis 2019: [TVCG] Embedding Meta-Information into Visualizations", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T00:59:06+00:00", "description": "Authors: Colin Ware, Terece L. Turton, Roxana Bujack, Francesca Samsel, Piyush Shrivastava, David H. Rogers", "uri": "https://vimeo.com/373019757", "name": "SciVis 2019: [TVCG] Measuring and Modeling the Feature Detection Threshold Functions of Colormaps", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T00:58:15+00:00", "description": "Authors: Valentin Bruder, Christoph M\u00fcller, Steffen Frey, Thomas Ertl", "uri": "https://vimeo.com/373019651", "name": "SciVis 2019: [TVCG] On Evaluating Runtime Performance of Interactive Visualizations", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T00:57:11+00:00", "description": "Authors: Wenbin He, Hanqi Guo, Han-Wei Shen, Tom Peterka", "uri": "https://vimeo.com/373019496", "name": "SciVis 2019: [TVCG] eFESTA: Ensemble Feature Exploration with Surface Density Estimates", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T00:53:46+00:00", "description": "Authors: Raghavendra Sridharamurthy, Talha Bin Masood, Adhitya Kamakshidasan, Vijay Natarajan", "uri": "https://vimeo.com/373019044", "name": "SciVis 2019: [TVCG] Edit Distance between Merge Trees", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T00:43:35+00:00", "description": "Authors: Tobias G\u00fcnther & Holger Theisel", "uri": "https://vimeo.com/373017590", "name": "SciVis 2019: [TVCG] Hyper-Objective Vortices", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T00:40:04+00:00", "description": "Authors: Junpeng Wang, Subhashis Hazarika, Cheng Li, Han-Wei Shen", "uri": "https://vimeo.com/373017114", "name": "SciVis 2019: [TVCG] Visualization and Visual Analysis of Ensemble Data: A Survey", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-14T00:34:18+00:00", "description": "Authors: Cindy Xiong, Lisanne van Weelden, Steven Franconeri", "uri": "https://vimeo.com/373016230", "name": "InfoVis 2019: [TVCG] The Curse of Knowledge in Visual Data Communication", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-14T00:26:56+00:00", "description": "Authors: J X Zheng, D F M Goodman, S Pawar", "uri": "https://vimeo.com/373015168", "name": "InfoVis 2019: [TVCG] Graph Drawing by Stochastic Gradient Descent", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-13T23:46:30+00:00", "description": "Authors: Xu Zhu, Miguel A. Nacenta, \u00d6zg\u00fcr Akgun, Peter Nightingale", "uri": "https://vimeo.com/373009233", "name": "InfoVis 2019: [TVCG] How People Visually Represent Discrete Constraint Problem", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-13T23:40:49+00:00", "description": "Authors: Evanthia Dimara, Steven Franconeri, Catherine Plaisant, Anastasia Bezerianos, Pierre Dragicevic", "uri": "https://vimeo.com/373008296", "name": "InfoVis 2019: [TVCG] A Task-based Taxonomy of Cognitive Biases for Information Visualization", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-13T23:35:35+00:00", "description": "Authors: Katherine E. Isaacs & Todd Gamblin", "uri": "https://vimeo.com/373007507", "name": "InfoVis 2019: [TVCG] Preserving Command Line Workflow for a Package Management using ASCII DAG Visualization", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-13T23:30:06+00:00", "description": "Authors: Siming Chen, Natalia Andrienko, Gennady Andrienko, Linara Adilova, Jeremie Barlet, Joerg Kindermann, Phong H. Nguyen, Olivier Thonnard, Cagatay Turkay", "uri": "https://vimeo.com/373006662", "name": "VAST 2019: [TVCG] LDA Ensembles for Interactive Exploration and Categorization of Behaviors", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-13T23:27:48+00:00", "description": "Authors: Chunggi Lee, Yeonjun Kim, Seungmin Jin, Dongmin Kim, Ross Maciejewski, David Ebert, Sungahn Ko", "uri": "https://vimeo.com/373006261", "name": "VAST 2019: [TVCG] A Visual Analytics System for Exploring, Monitoring, and Forecasting Road Traffic Congestion", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-13T23:21:41+00:00", "description": "Authors: Jie Li, Siming Chen, Wei Chen, Gennady Andrienko, Natalia Andrienko", "uri": "https://vimeo.com/373005219", "name": "VAST 2019: [TVCG] Semantics-Space-Time Cube: A Conceptual Framework for Systematic Analysis of Texts in Space and Time", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-13T23:18:35+00:00", "description": null, "uri": "https://vimeo.com/373004721", "name": "VAST 2019: [TVCG] An Analysis of Automated Visual Analysis Classification", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-13T23:15:16+00:00", "description": "Authors: Salman Mahmood & Klaus Mueller", "uri": "https://vimeo.com/373004086", "name": "VAST 2019: [TVCG] Taxonomizer: A Visual Analytics Framework for Constructing Fully Labeled Hierarchies from Multivariate Data", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-13T23:07:12+00:00", "description": "Authors: Quan Li, Ziming Wu, Lingling Yi, Kristanto Sean N, Huamin Qu, Xiaojuan Ma", "uri": "https://vimeo.com/373002523", "name": "VAST 2019: [TVCG] WeSeer: Visual Analysis for Better Information Cascade Prediction of WeChat Articles", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-13T16:02:01+00:00", "description": "Poster \n\nAuthors: Rafael Andr\u00e9s Blanco , Zujany Salazar , Tobias Isenberg\n\nAbstract: We visualize species habitat distribution information based on geo-located images posted on social media. For the example of carnivorous plants, we use published image data to produce interactive maps of the spatial distribution of different species/genuses, histograms of the elevations at which they grow, and plots of the temporal distribution of the photographs. We further discuss the mismatch between our distribution maps and traditionally established maps as well as further possibilities for research with our data.", "uri": "https://vimeo.com/372908424", "name": "[VIS19 Preview] Exploring Carnivorous Plant Habitats based on Images from Social Media (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-11-12T20:52:15+00:00", "description": "Authors: Shuai Chen, Sihang Li, Siming Chen, Xiaoru Yuan", "uri": "https://vimeo.com/372727514", "name": "VAST 2019: R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-12T20:47:41+00:00", "description": "Authors: Minjeong Shin, Alexander Soen, Benjamin T. Readshaw, Stephen Michael Blackburn, Mitchell Whitelaw, Lexing Xie", "uri": "https://vimeo.com/372726460", "name": "VAST 2019: Influence Flowers of Academic Entities", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-12T20:45:37+00:00", "description": "Authors: Jieqiong Zhao, Morteza Karimzadeh, Luke Snyder, Cittayong Surakitbanharn, Zhenyu Cheryl Qian, David Ebert", "uri": "https://vimeo.com/372725937", "name": "VAST 2019: MetricsVis: A Visual Analytics Framework for Evaluating Individual, Team, and Organization Performance", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-12T20:44:07+00:00", "description": "Authors: Zeyu Li, Changhong Zhang, Shichao Jia, Jiawan Zhang", "uri": "https://vimeo.com/372725556", "name": "VAST 2019: Galex: Exploring the Evolution and Intersection of Disciplines", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-12T20:42:28+00:00", "description": "Authors: Gromit Yeuk-Yin Chan, Luis Gustavo Nonato, Alice Chu, Preeti Raghavan, Viswanath Aluru, Claudio T. Silva", "uri": "https://vimeo.com/372725212", "name": "VAST 2019: Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-12T20:40:34+00:00", "description": "Authors: Pepe Eulzer, Sandy Engelhardt, Nils Lichtenberg, Raffaele de Simone, Kai Lawonn", "uri": "https://vimeo.com/372724832", "name": "SciVis 2019: Temporal Views of Flattened Mitral Valve Geometries", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-12T20:34:38+00:00", "description": "Authors: Yifan Wang, Zichun Zhong, Jing Hua", "uri": "https://vimeo.com/372723520", "name": "SciVis 2019: DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D-4D Lung Models from Single-View Projections by Deep", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-12T20:33:39+00:00", "description": "Authors: Andrew Wentzel, Peter Haula, Timothy Basil Luciani, Baher Elgohari, Hesham Elhalawani, Guadalupe Canahuate, David M. Vock, Clifton David Fuller, G. Elisabeta Marai", "uri": "https://vimeo.com/372723303", "name": "SciVis 2019: Cohort-based T-SSIM Visual Computing for Radiation Therapy Prediction and Exploration", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-12T20:24:44+00:00", "description": "Authors: Aditeya Pandey, Harsh Shukla, Geoffrey S. Young, Lei Qin, Amir A. Zamani, Liangge Hsu, Raymond Huang, Cody Dunne, Michelle A. Borkin", "uri": "https://vimeo.com/372721381", "name": "InfoVis 2019: CerebroVis: Designing an Abstract yet Spatially Contextualized Cerebral Arteries Network Visualization", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T20:18:27+00:00", "description": "Authors: Katarina Furmanova, Adam Jur\u010d\u00edk, Barbora Kozlikova, Helwig Hauser, Jan By\u0161ka", "uri": "https://vimeo.com/372719933", "name": "SciVis 2019: Multiscale Visual Drilldown for the Analysis of Large Ensembles of Multi-Body Protein Complexes", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-12T20:12:44+00:00", "description": "Authors: Lin Yan, Yusu Wang, Elizabeth Munch, Ellen Gasparovic, Bei Wang", "uri": "https://vimeo.com/372718601", "name": "SciVis 2019: A Structural Average of Labeled Merge Trees for Uncertainty Visualization", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-12T20:08:45+00:00", "description": "Authors: Jochen G\u00f6rtler, Thilo Spinner, Dirk Streeb, Daniel Weiskopf, Oliver Deussen", "uri": "https://vimeo.com/372717650", "name": "InfoVis 2019: Uncertainty-Aware Principal Component Analysis", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T03:12:08+00:00", "description": "Authors: Min Lu, Shuaiqi Wang, Joel Lanir, Noa Fish, Yang Yue, Daniel CohenOr, Hui Huang", "uri": "https://vimeo.com/372532721", "name": "InfoVis 2019: Winglets: Visualizing Association with Uncertainty in Multi-class Scatterplots", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T03:10:28+00:00", "description": "Authors: Yunhai Wang, Zeyu Wang, Tingting Liu, Michael Correll, Zhanglin Cheng, Oliver Deussen, Michael Sedlmair", "uri": "https://vimeo.com/372532516", "name": "InfoVis 2019: Improving the Robustness of Scagnostics", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T03:09:26+00:00", "description": "Authors: Rafael Veras & Christopher Collins", "uri": "https://vimeo.com/372532401", "name": "InfoVis 2019: Discriminability Tests for Visualization Effectiveness and Scalability", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T03:08:16+00:00", "description": "Authors: Ruizhen Hu, Tingkai Sha, Oliver van Kaick, Oliver Deussen, Hui Huang", "uri": "https://vimeo.com/372532278", "name": "InfoVis 2019: Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T03:07:09+00:00", "description": "Authors: Xin Chen, Tong Ge, Jian Zhang, Baoquan Chen, Chi-Wing Fu, Oliver Deussen, Yunhai Wang", "uri": "https://vimeo.com/372532134", "name": "InfoVis 2019: A Recursive Subdivision Technique for Sampling Multi-class Scatterplots", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T03:00:34+00:00", "description": "Authors: Alex Bigelow, Carolina Nobre, Miriah Meyer, Alexander Lex", "uri": "https://vimeo.com/372531346", "name": "VAST 2019: Origraph: Interactive Network Wrangling", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-12T02:56:58+00:00", "description": "Authors: Seth A Johnson, Francesca Samsel, Greg Abram, Daniel Olson, Andrew Solis, Bridger Herman, Philip Wolfram, Christophe Langlet, Daniel F. Keefe", "uri": "https://vimeo.com/372530934", "name": "SciVis 2019: Artifact-Based Rendering: Harnessing Natural and Traditional Visual Media for More Expressive and Engaging 3D Visua", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-12T02:55:53+00:00", "description": "Authors: Bahador Saket, Samuel Huron, Charles Perin, Alex Endert", "uri": "https://vimeo.com/372530797", "name": "InfoVis 2019: Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T02:48:22+00:00", "description": "Authors: Joyce Ma, Kwan-Liu Ma, Jennifer Frazier", "uri": "https://vimeo.com/372529828", "name": "InfoVis 2019: Decoding Complex Visualizations in Science Museums: An Empirical Study", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T02:42:12+00:00", "description": "Authors: Arvind Satyanarayan, Bongshin Lee, Donghao Ren, Jeffrey Heer, John Stasko, John R Thompson, Matthew Brehmer, Zhicheng Liu", "uri": "https://vimeo.com/372529109", "name": "InfoVis 2019: Critical Reflections of Visualization Authoring Systems", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T02:34:37+00:00", "description": "Authors: Fearn Bishop, Johannes Zagermann, Ulrike Pfeil, Gemma Sanderson, Harald Reiterer, Uta Hinrichs", "uri": "https://vimeo.com/372528128", "name": "InfoVis 2019: Construct-A-Vis: Free-Form Visualization Creation for Children", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T02:33:18+00:00", "description": "Authors: Mosab Khayat, Morteza Karimzadeh, David Ebert, Arif Ghafoor", "uri": "https://vimeo.com/372527965", "name": "VAST 2019: The Validity and Generalizability of Summative Evaluation Methods in Visual Analytics", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-12T02:32:17+00:00", "description": "Authors: Mingming Fan, Ke Wu, Jian Zhao, Yue Li, Winter Wei, Khai Truong", "uri": "https://vimeo.com/372527825", "name": "InfoVis 2019: VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-12T02:30:37+00:00", "description": "Authors: Lace M. K. Padilla, Spencer Castro, P. Samuel Quinan, Ian T. Ruginski, Sarah Creem-Regehr", "uri": "https://vimeo.com/372527629", "name": "InfoVis 2019: Toward Objective Evaluation of Working Memory in Visualizations: A Case Study Using Pupillometry and a Dual-Task P", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-11T17:23:50+00:00", "description": "Authors: Hannah Kim, Dongjin Choi, Barry Drake, Alex Endert, Haesun Park", "uri": "https://vimeo.com/372423638", "name": "VAST 2019: TopicSifter: Interactive Search Space Reduction Through Targeted Topic Modeling", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-11T17:22:50+00:00", "description": "Authors: Melanie Tory, Vidya Setlur", "uri": "https://vimeo.com/372423395", "name": "VAST 2019: Do What I Mean, Not What I Say! Design Considerations for Supporting Intent and Context in Analytical Conversation", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-11T17:17:19+00:00", "description": "Authors: Doris Jung-Lin Lee, John Lee, Tarique Siddiqui, Jaewoo Kim, Aditya Parameswaran, Karrie Karahalios", "uri": "https://vimeo.com/372422066", "name": "VAST 2019: You can\u2019t always sketch what you want: Understanding Sensemaking in Visual Query Systems", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-11T17:16:08+00:00", "description": "Authors: Zhaosong Huang, Ye Zhao, Wei Chen, Shengjie Gao, Kejie Yu, Weixia Xu, Mingjie Tang, Minfeng Zhu, Mingliang Xu", "uri": "https://vimeo.com/372421808", "name": "VAST 2019: A Natural-language-based Visual Query Approach of Uncertain Human Trajectories", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-11T17:13:46+00:00", "description": "Authors: Leilani Battle, R. Jordan Crouser, Audace Nakeshimana, Ananda Montoly, Remco Chang, Michael Stonebraker", "uri": "https://vimeo.com/372421201", "name": "InfoVis 2019: The Role of Latency in Predicting Visual Search Behavior", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-11T17:05:16+00:00", "description": "Authors: Enamul Hoque & Maneesh Agrawala", "uri": "https://vimeo.com/372419088", "name": "InfoVis 2019: Searching the Visual Style and Structure of D3 Visualizations", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-08T18:47:09+00:00", "description": "Authors: Ji Qi, Vincent Bloemen, Shihan Wang, Jarke van Wijk, Huub van de Wetering", "uri": "https://vimeo.com/371940745", "name": "VAST 2019: STBins: Visual Tracking and Comparison of Multiple Data Sequences using Temporal Binning", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-08T18:45:59+00:00", "description": "Authors: Aritra Dasgupta, Hong Wang, Nancy D O\u2019Brien, Susannah Marie Burrows", "uri": "https://vimeo.com/371940545", "name": "InfoVis 2019: Separating the wheat from the chaff: Comparative visual cues for reliable diagnostics of competing models", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-08T18:41:29+00:00", "description": "Authors: Manuela Waldner, Alexandra Diehl, Denis Gracanin, Rainer Splechtna, Claudio Delrieux, Kresimir Matkovic", "uri": "https://vimeo.com/371939694", "name": "InfoVis 2019: Comparison of Radial and Linear charts for Visualizing Daily Patterns", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-08T18:39:42+00:00", "description": "Authors: Guozheng Li, Yu Zhang, Yu Dong, Jie Liang, Jinson Zhang, Jinsong Wang, Michael McGuffin, Xiaoru Yuan", "uri": "https://vimeo.com/371939324", "name": "InfoVis 2019: BarcodeTree: Scalable Comparison of Multiple Hierarchies", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-08T18:38:47+00:00", "description": "Authors: Nicole Jardine, Brian David Ondov, Niklas Elmqvist, Steven Franconeri", "uri": "https://vimeo.com/371939151", "name": "InfoVis 2019: The Perceptual Proxies of Visual Comparison", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-08T18:36:17+00:00", "description": "Authors: Haipeng Zeng, Xingbo Wang, Aoyu Wu, Yong Wang, Quan Li, Alex Endert, Huamin Qu", "uri": "https://vimeo.com/371938644", "name": "VAST 2019: EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-08T18:30:37+00:00", "description": "Authors: Yan Lyu, Xu Liu, Hanyi Chen, Arpan Mangal, Kai Liu, Chao Chen, Brian Lim", "uri": "https://vimeo.com/371937440", "name": "VAST 2019: OD Morphing: balancing simplicity with faithfulness for OD bundling", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-08T18:28:56+00:00", "description": "Authors: Zikun Deng, Di Weng, Jiahui Chen, Ren Liu, Zhibin Wang, Jie Bao, Yu Zheng, Yingcai Wu", "uri": "https://vimeo.com/371936870", "name": "VAST 2019: AirVis: Visual Analytics of Air Pollution Propagation", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-08T18:27:59+00:00", "description": "Authors: Can Liu, Cong Wu, Hanning Shao, Xiaoru Yuan", "uri": "https://vimeo.com/371936681", "name": "InfoVis 2019: SmartCube: An Adaptive Data Management Architecture for the Real-Time Visualization of Spatiotemporal Datasets", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-07T03:08:51+00:00", "description": "Authors: Nicola Pezzotti, Julian Thijssen, Alexander Mordvintsev, Thomas H\u00f6llt, Baldur van Lew, Boudewijn P.F. Lelieveldt, Elmar Eisemann, Anna Vilanova", "uri": "https://vimeo.com/371539338", "name": "VAST 2019: GPGPU Linear Complexity tSNE Optimization", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-07T03:05:08+00:00", "description": "Authors: Honghui Mei, Yating Wei, Shuyue Zhou, Bingru Lin, Yuanzhe Hu, Ying Zhao, Jiazhi Xia, Wei Chen", "uri": "https://vimeo.com/371538892", "name": "InfoVis 2019: RSATree: Distribution-Aware Data Representation of Large-Scale Tabular Datasets for Flexible Visual Query", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-07T03:01:47+00:00", "description": "Authors: Kelvin Li and Kwan-Liu Ma", "uri": "https://vimeo.com/371538412", "name": "InfoVis 2019: P5: Portable Progressive Parallel Processing Pipeline for Interactive Data Analysis and Visualization", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-07T02:58:40+00:00", "description": "Authors: Ying Yang, Michael Wybrow, Yuan-Fang Li, Tobias Czauderna, Yongqun He", "uri": "https://vimeo.com/371538031", "name": "InfoVis 2019: OntoPlot: A Novel Visualisation for Non-hierarchical Associations in Large Ontologies", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-07T02:53:53+00:00", "description": "Authors: Frederik L. Dennig, Thomas E. Polk, Zudi Lin, Tobias Schreck, Hanspeter Pfister, Michael Behrisch", "uri": "https://vimeo.com/371537420", "name": "VAST 2019: FDive: Learning Relevance Models using Pattern-based Similarity Measures", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-07T02:50:29+00:00", "description": "Authors: Yating Wei, Honghui Mei, Ying Zhao, Shuyue Zhou, Bingru Lin, Haojin Jiang, Wei Chen", "uri": "https://vimeo.com/371536997", "name": "VAST 2019: Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-07T02:47:27+00:00", "description": "Authors: Christine Nothelfer and Steven Franconeri", "uri": "https://vimeo.com/371536639", "name": "InfoVis 2019: Measures of the benefit of direct encoding of data deltas for data pair relation perception", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-07T02:44:45+00:00", "description": "Authors: Cindy Xiong, Cristina R Ceja, Casimir Ludwig, Steven Franconeri", "uri": "https://vimeo.com/371536296", "name": "InfoVis 2019: Biased Average Position Estimates in Line and Bar Graphs: Underestimation, Overestimation, and Perceptual Pull", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-06T18:19:01+00:00", "description": "Authors: Pavol Klacansky, Attila Gyulassy, Peer-Timo Bremer, Valerio Pascucci", "uri": "https://vimeo.com/371444168", "name": "SciVis 2019: Toward Localized Topological Data Structures: Querying the Forest for the Tree", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-06T18:12:45+00:00", "description": "Authors: Ashok Jallepalli, Joshua A. Levine, Robert M. (Mike) Kirby", "uri": "https://vimeo.com/371442697", "name": "SciVis 2019: The Effect of Data Transformations on Scalar Field Topological Analysis of High-Order FEM Solutions", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-06T18:03:46+00:00", "description": "Authors: Steve Petruzza, Attila Gyulassy, Samuel Leventhal, Jackson Baglino, Michael Czabaj, Ashely Spears, Valerio Pascucci", "uri": "https://vimeo.com/371440631", "name": "SciVis 2019: High-throughput feature extraction for measuring attributes of deforming open cell foams", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-06T18:00:11+00:00", "description": "Authors: Ragini Rathore, Zachary Leggon, Laurent Lessard, Karen Schloss", "uri": "https://vimeo.com/371439841", "name": "InfoVis 2019: Estimating color-concept associations from image statistics", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-06T17:58:47+00:00", "description": "Authors: Stephen Smart, Keke Wu, Danielle Albers Szafir", "uri": "https://vimeo.com/371439526", "name": "InfoVis 2019: Color Crafting: Automating the Construction of Designer Quality Color Ramps", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-06T00:11:35+00:00", "description": "Authors: Xuanwu Yue, Jiaxin Bai, Qinhan Liu, Yiyang Tang, Abishek Puri, Ke Li, Huamin Qu", "uri": "https://vimeo.com/371271099", "name": "VAST 2019: sPortfolio: Stratified Visual Analysis of Stock Portfolios", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-06T00:06:17+00:00", "description": "Authors: Ying Zhao, Xiaobo Luo, Xiaru Lin, Hairong Wang, Xiaoyan Kui, Fangfang Zhou, Jinsong Wang, Yi Chen, Wei Chen", "uri": "https://vimeo.com/371270285", "name": "VAST 2019: Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-06T00:05:21+00:00", "description": "Authors: Dong Sun, Renfei Huang, Yong Wang, Yuanzhe Chen, Jia Zeng, Mingxuan Yuan, Ting-Chuen Pong, Huamin Qu", "uri": "https://vimeo.com/371270160", "name": "VAST 2019: PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-06T00:03:55+00:00", "description": "Authors: Andreas Walch, Michael Schw\u00e4rzler, Christian Luksch, Elmar Eisemann, Theresia Gschwandtner", "uri": "https://vimeo.com/371269980", "name": "VAST 2019: LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-06T00:02:24+00:00", "description": "Authors: Luke Snyder, Yi-Shan Lin, Morteza Karimzadeh, Dan Goldwasser, David Ebert", "uri": "https://vimeo.com/371269786", "name": "VAST 2019: Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-05T23:55:34+00:00", "description": "Authors: David Gotz, Jonathan Zhang, Wenyuan Wang, Joshua Shrestha, David Borland", "uri": "https://vimeo.com/371268853", "name": "VAST 2019: Visual Analysis of High-Dimensional Event Sequence Data via Dynamic Hierarchical Aggregation", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-05T23:50:30+00:00", "description": "Authors: David Borland, Wenyuan Wang, Jonathan Zhang, Joshua Shrestha, David Gotz", "uri": "https://vimeo.com/371268138", "name": "VAST 2019: Selection Bias Tracking and Detailed Subset Comparison for High-Dimensional Data", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-05T23:45:11+00:00", "description": "Authors: Takanori Fujiwara, Jia-Kai Chou, Shilpika Shilpika, Panpan Xu, Liu Ren, Kwan-Liu Ma", "uri": "https://vimeo.com/371267332", "name": "InfoVis 2019: An Incremental Dimensionality Reduction Method for Visualizing Streaming Multidimensional Data", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-05T23:42:38+00:00", "description": "Authors: Baldwin Nsonga, Gerik Scheuermann, Stefan Gumhold, Jordi Ventosa-Molina, Denis Koschichow, Jochen Fr\u00f6hlich", "uri": "https://vimeo.com/371266864", "name": "SciVis 2019: Analysis of the Near-Wall Flow in a Turbine Cascade by Splat Visualization", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-05T23:40:52+00:00", "description": "Authors: Irene Baeza Rojo, Markus Gross, Tobias G\u00fcnther", "uri": "https://vimeo.com/371266581", "name": "SciVis 2019: Accelerated Monte Carlo Rendering of Finite-Time Lyapunov Exponents", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-04T14:41:03+00:00", "description": "Authors: Fabian Sperrle, Rita Sevastjanova, Rebecca Kehlbeck, Mennatallah El-Assady", "uri": "https://vimeo.com/370888115", "name": "VAST 2019: VIANA: Visual Interactive Annotation of Argumentation", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-04T14:40:11+00:00", "description": "Authors: Mennatallah El-Assady, Rebecca Kehlbeck, Christopher Collins, Daniel Keim, Oliver Deussen", "uri": "https://vimeo.com/370887904", "name": "VAST 2019: Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-04T14:36:25+00:00", "description": "Authors: Yunhai Wang, Xiaowei Chu, Kaiyi Zhang, Chen Bao, Xiaotong Li, Jian Zhang, Chi-Wing Fu, Christophe Hurter, Oliver Deussen, Bongshin Lee", "uri": "https://vimeo.com/370886854", "name": "InfoVis 2019: ShapeWordle: Tailoring Wordles using Shape-aware Archimedean Spirals", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-04T14:23:47+00:00", "description": "Authors: Xi Ye, Shouxing Xiang, Jiazhi Xia, Jing Wu, Yang Chen, Shixia Lu", "uri": "https://vimeo.com/370883679", "name": "VAST 2019: Interactive Correction of Mislabeled Training Data", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-04T14:22:54+00:00", "description": "Authors: Anjul Tyagi, Klaus Mueller, Zhen Cao, Tyler Estro, Erez Zadok", "uri": "https://vimeo.com/370883468", "name": "VAST 2019: ICE: An Interactive Configuration Explorer for High Dimensional Parameter Spaces", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-04T14:21:27+00:00", "description": "Authors: Sebastian Gehrmann, Hendrik Strobelt, Robert Kr\u00fcger, Kathryn Hite, Hanspeter Pfister, Alexander M. Rush", "uri": "https://vimeo.com/370883129", "name": "VAST 2019: Visual Interaction with Deep Learning Models through Collaborative Semantic Inference", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-04T14:19:47+00:00", "description": "Authors: Mosab Khayat, Morteza Karimzadeh, Jieqiong Zhao, David Ebert", "uri": "https://vimeo.com/370882767", "name": "VAST 2019: VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-04T14:15:05+00:00", "description": "Authors: Dylan Cashman, Adam Perer, Remco Chang, Hendrik Strobelt", "uri": "https://vimeo.com/370881785", "name": "VAST 2019: Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-04T14:13:32+00:00", "description": "Authors: Cindy Xiong, Joel Shapiro, Jessica Hullman, Steven Franconeri", "uri": "https://vimeo.com/370881473", "name": "InfoVis 2019: Illusion of Causality in Visualized Data", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-03T14:51:19+00:00", "description": "Authors: Tobias Rapp, Christoph Peters, Carsten Dachsbacher", "uri": "https://vimeo.com/370669755", "name": "SciVis 2019: Void-and-Cluster Sampling of Large Scattered Data and Trajectories", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-03T14:48:54+00:00", "description": "Authors: Haris Mumtaz, Shahid Latif, Fabian Beck, Daniel Weiskopf", "uri": "https://vimeo.com/370669433", "name": "VAST 2019: Exploranative Code Quality Documents", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-03T14:44:40+00:00", "description": "Authors: Kathryn P Williams, Alex Bigelow, Katherine Isaacs", "uri": "https://vimeo.com/370668837", "name": "InfoVis 2019: Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and Concern", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-03T14:43:15+00:00", "description": "Authors: Ke Xu, Yun Wang, Leni Yang, Yifang Wang, Bo Qiao, Si Qin, Yong Xu, Haidong Zhang, Huamin Qu", "uri": "https://vimeo.com/370668674", "name": "VAST 2019: CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing SystemsD", "year": "2019", "event": "VAST"}, {"created_time": "2019-11-01T14:54:56+00:00", "description": null, "uri": "https://vimeo.com/370325606", "name": "SciVis 2019: Deadeye Visualization Revisited Investigation of Preattentiveness and Applicability in Virtual Environments", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-11-01T14:52:21+00:00", "description": "Authors: Andrea Batch, Andrew Cunningham, Maxime Cordeil, Niklas Elmqvist, Tim Dwyer, Bruce H Thomas, Kim Marriott", "uri": "https://vimeo.com/370324954", "name": "InfoVis 2019: There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-11-01T14:47:03+00:00", "description": "Authors: Matthias Kraus, Niklas Kai Weiler, Daniela Oelke, Johannes Kehrer, Daniel Keim, Johannes Fuchs", "uri": "https://vimeo.com/370323673", "name": "InfoVis 2019: The Impact of Immersion on Cluster Identification Tasks", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T15:37:17+00:00", "description": "Authors: Jorge A. Wagner, Wolfgang Stuerzlinger, Luciana Nedel", "uri": "https://vimeo.com/370107834", "name": "InfoVis 2019: Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T01:52:29+00:00", "description": "Authors: Jiachen Wang, Kejian Zhao, Dazhen Deng, Anqi Cao, Xiao Xie, Zheng Zhou, Hui Zhang, Yingcai Wu", "uri": "https://vimeo.com/369981638", "name": "VAST 2019: Tac-Simur: Visual Analytics for Tactic-based Match Simulation of Table Tennis", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-31T01:48:42+00:00", "description": "Authors: Thomas E. Polk, Dominik J\u00e4ckle, Johannes H\u00e4u\u00dfler, Jing Yang", "uri": "https://vimeo.com/369981083", "name": "VAST 2019: CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-31T01:43:53+00:00", "description": "Authors: Amira Chalbi, Jacob Ritchie, Deokgun Park, Jungu Choi, Nicolas Roussel, Niklas Elmqvist, Fanny Chevalier", "uri": "https://vimeo.com/369980464", "name": "InfoVis 2019: Common Fate for Animated Transitions in Visualization", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T01:42:01+00:00", "description": "Authors: Vanessa Pe\u00f1a-Araya, Emmanuel Pietriga, Anastasia Bezerianos", "uri": "https://vimeo.com/369980230", "name": "InfoVis 2019: A Comparison of Visualizations for Identifying Correlation Over Time and Space", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T01:35:45+00:00", "description": "Authors: Matthew Brehmer, Bongshin Lee, Petra Isenberg, Eun Kyoung Choe", "uri": "https://vimeo.com/369979414", "name": "InfoVis 2019: A Comparative Evaluation of Animation and Small Multiples for Trend Visualization on Mobile Phones", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T01:23:54+00:00", "description": "Authors: Ashley Suh, Mustafa Hajij, Bei Wang, Carlos Scheidegger, Paul Rosen", "uri": "https://vimeo.com/369977717", "name": "InfoVis 2019: Persistent Homology Guided Force-Directed Graph Layouts", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T01:16:24+00:00", "description": "Authors: Yunhai Wang, Mingliang Xue, Yanyan Wang, Xinyuan Yan, Baoquan Chen, Chi-Wing Fu, Christophe Hurter", "uri": "https://vimeo.com/369976850", "name": "InfoVis 2019: Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T01:10:13+00:00", "description": "Authors: Yong Wang, Zhihua Jin, Qianwen Wang, Weiwei Cui, Tengfei Ma, Huamin Qu", "uri": "https://vimeo.com/369976078", "name": "InfoVis 2019: DeepDrawing: A Deep Learning Approach to Graph Drawing", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T01:01:02+00:00", "description": "Authors: Oh-Hyun Kwon and Kwan-Liu Ma", "uri": "https://vimeo.com/369974954", "name": "InfoVis 2019: A Deep Generative Model for Graph Layout", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-31T00:58:26+00:00", "description": "Authors: Shusen Liu, Di Wang, Dan Maljovec, Rushil Anirudh, Jayaraman J. Thiagarajan, Sam Ade Jacobs, Brian C. Van Essen, David Hysom, Jae-Seung Yeom, Jim Gaffney, J. Luc Peterson, Peter B. Robinson, Harsh Bhatia, Valerio Pascucci, Brian K. Spears, Peer-Timo Bremer", "uri": "https://vimeo.com/369974559", "name": "VAST 2019: Scalable Topological Data Analysis and Visualization for Interpreting Data-Driven Models in Scientific Applications", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-31T00:57:22+00:00", "description": "Authors: Irene Baeza Rojo and Tobias G\u00fcnther", "uri": "https://vimeo.com/369974413", "name": "SciVis 2019: Vector Field Topology of Time-Dependent Flows in a Steady Reference Frame", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-10-31T00:56:37+00:00", "description": "Authors: Fariba Khan, Lawrence Roy, Eugene Zhang, Botong Qu, Shih-Hsuan Hung, Harry Yeh, Robert S. Laramee, Yue Zhang", "uri": "https://vimeo.com/369974295", "name": "SciVis 2019: Multi-Scale Topological Analysis of Asymmetric Tensor Fields on Surfaces", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-10-31T00:50:47+00:00", "description": "Authors: Robin Bader, Michael Sprenger, Nikolina Ban, Stefan R\u00fcdis\u00fchli, Christoph Sch\u00e4r, Tobias G\u00fcnther", "uri": "https://vimeo.com/369973468", "name": "SciVis 2019: Extraction and Visual Analysis of Potential Vorticity Banners around the Alps", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-10-31T00:48:16+00:00", "description": "Authors: Jonas Lukasczyk, Christoph Garth, Tim Biedert, Ross Maciejewski, Gunther H. Weber, Heike Leitte", "uri": "https://vimeo.com/369973119", "name": "SciVis 2019: Dynamic Nested Tracking Graphs", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-10-30T00:17:14+00:00", "description": "Author: Jessica Hullman", "uri": "https://vimeo.com/369712212", "name": "InfoVis 2019: Why Authors Don\u2019t Visualize Uncertainty", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-30T00:15:46+00:00", "description": "Authors: Evanthia Dimara, Charles Perin", "uri": "https://vimeo.com/369712001", "name": "InfoVis 2019: What is Interaction for Data Visualization?", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-30T00:13:07+00:00", "description": "Authors: Miriah Meyer, Jason Dykes", "uri": "https://vimeo.com/369711672", "name": "InfoVis 2019: Criteria for Rigor in Visualization Design Study", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-30T00:10:02+00:00", "description": "Authors: Kyle Wm Hall, Adam James Bradley, Uta Hinrichs, Samuel Huron, Jo Wood, Christopher Collins, Sheelagh Carpendale", "uri": "https://vimeo.com/369711296", "name": "InfoVis 2019: Design by Immersion: A Transdisciplinary Approach to Problem-Driven Visualizations", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-29T23:54:18+00:00", "description": "Author: Dietmar Offenhuber", "uri": "https://vimeo.com/369708962", "name": "InfoVis 2019: Data by proxy: material traces as autographic visualizations", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-28T00:34:48+00:00", "description": "Speaker: Johanna Drucker, Breslauer Professor of Bibliographical Studies in the Department of Information Studies at UCLA\n\nThe charts, graphs, network diagrams, and other graphics generally grouped under the rubric \u201cinformation visualizations\u201d were almost all developed to serve the empirical, statistical, or social sciences. They use explicit quantities, standard metrics, and unambiguous visual elements to create clear, concise, legible messages. The terms on which visualizations are assessed tends to value these properties\u2014and to be premised on the idea that the best graphics are those that \u201cshow the data\u201d\u2014as if data had inherent form. The difficulty with this entire set of assumptions is that they do not establish foundations for visualizing complex historical events, narrative discourse, or human experience as it is expressed in documents and artifacts where \u201cdata\u201d are not self-evident or standard. The humanities, in adopting visualizations from empirically based fields, has failed to develop methods that engage with the interpretative aspects of its own discourse. Using three case studies\u2014one from news narratives, one from mixed historical chronologies, one from imagined crisis scenarios\u2014this paper argues for the development of visualizations capable of showing variable timescales, comparative chronology, and partial and unfolding temporalities. The basic components of these systems are non-linear, affective, and unable to be standardized in ways that trouble empirical approaches but serve interpretative understandings of experience and its representation in cultural expressions. The questions of how such systems might be designed and where they might have useful applications will be raised to generate discussion.", "uri": "https://vimeo.com/369216256", "name": "VIS 2019: [Capstone] Visualizing Temporality and Chronologies for the Humanities", "year": "2019", "event": "CAPSTONE"}, {"created_time": "2019-10-28T00:19:24+00:00", "description": "Authors: Yuxin Ma, Tiankai Xie, Jundong Li, Ross Maciejewski", "uri": "https://vimeo.com/369214358", "name": "VAST 2019: Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-25T05:08:07+00:00", "description": "(Best Paper Award) \nAuthors: Wenbin He, Junpeng Wang, Hanqi Guo, Ko-Chih Wang, Han-Wei Shen, Mukund Raj, Youssef S. G. Nashed, Tom Peterka", "uri": "https://vimeo.com/368709571", "name": "SciVis 2019: InSituNet: Deep Image Synthesis for Parameter Space Exploration of Ensemble Simulations", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-10-25T05:00:46+00:00", "description": "Speaker: Katy B\u00f6rner, Victor H. Yngve Distinguished Professor of Engineering and Information Science in the Departments of Intelligent Systems Engineering and Information Science, School of Informatics, Computing, and Engineering; core faculty of the Cognitive Science Program; and founding director of the Cyberinfrastructure for Network Science Center (http://cns.iu.edu)\u2014all at Indiana University in Bloomington, Indiana\n\nIn the information age, the ability to read and make data visualizations is as important as the ability to read and write. This talk introduces a theoretical data visualization framework (DVL) meant to empower anyone to systematically render data into insights using temporal, geospatial, topical, and network analyses and visualizations. Exemplarily, the DVL is applied to (1) Map science and technology, see interactive data visualizations from the Places & Spaces: Mapping Science exhibit (http://scimaps.org) and recent PNAS special issue on Modelling and Visualizing Science and Technology Developments (https://www.pnas.org/modeling). (2) Design reference systems and user interfaces within the Human BioMolecular Atlas Program (HuBMAP) (https://commonfund.nih.gov/hubmap) that support the exploration and communication of single-cell data\u2014from the subcellular to the whole body level. (3) Teach Visual Analytics (https://visanalytics.cns.iu.edu) to students around the globe.\n\nReferences\n\nB\u00f6rner, Katy, Andreas Bueckle, and Michael Ginda. 2019. Data visualization literacy: Definitions, conceptual frameworks, exercises, and assessments. PNAS, 116 (6) 1857-1864.\nB\u00f6rner, Katy. 2015. Atlas of Knowledge: Anyone Can Map. Cambridge, MA: The MIT Press.\nB\u00f6rner, Katy. 2010. Atlas of Science: Visualizing What We Know. Cambridge, MA: The MIT Press.", "uri": "https://vimeo.com/368708656", "name": "VIS 2019: [Keynote] Data Visualization Literacy", "year": "2019", "event": "KEYNOTE"}, {"created_time": "2019-10-25T04:23:29+00:00", "description": "Authors: Fred Hohman, Haekyu Park, Caleb Robinson, Duen Horng Chau", "uri": "https://vimeo.com/368704428", "name": "VAST 2019: Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-25T04:18:55+00:00", "description": "(Best Paper Award)\nAuthors: Bryce Morrow, Trevor Manz, Arlene E. Chung, Nils Gehlenborg, David Gotz", "uri": "https://vimeo.com/368703866", "name": "VIS 2019: [Short Paper] Periphery Plots for Contextualizing Heterogeneous Time-Based Charts", "year": "2019", "event": ""}, {"created_time": "2019-10-25T04:12:35+00:00", "description": "(Best Paper Award)\nAuthors: Jagoda Walny, Christian Frisson, Mieka West, Doris Kosminsky, S\u00f8ren Knudsen, Sheelagh Carpendale, Wesley Willett", "uri": "https://vimeo.com/368703151", "name": "InfoVis 2019: Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff", "year": "2019", "event": "INFOVIS"}, {"created_time": "2019-10-25T04:08:05+00:00", "description": "(Best Paper Award)\nAuthors: Bowen Yu and Claudio T. Silva", "uri": "https://vimeo.com/368702648", "name": "VAST 2019: FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-25T04:03:39+00:00", "description": "Authors: \u00c0ngel Alexander Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng, Jamie Morgenstern, Duen Horng Chau", "uri": "https://vimeo.com/368702211", "name": "VAST 2019: FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-25T03:53:31+00:00", "description": "Authors: Yongsu Ahn, Yu-Ru Lin", "uri": "https://vimeo.com/368701050", "name": "VAST 2019: FairSight: Visual Analytics for Fairness in Decision Making", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-25T03:36:05+00:00", "description": "Authors: Thilo Spinner, Udo Schlegel, Hanna Schaefer, Mennatallah El-Assady", "uri": "https://vimeo.com/368699132", "name": "VAST 2019: explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-24T19:27:16+00:00", "description": "Authors: Sarkis Halladjian, Haichao Miao, David Kou\u0159il, Eduard Gr\u00f6ller, Ivan Viola, Tobias Isenberg", "uri": "https://vimeo.com/368618319", "name": "SciVis 2019: ScaleTrotter: Illustrative Visual Travels Across Negative Scales", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-10-24T17:36:38+00:00", "description": "Authors: Alexander Bock, Emil Axelsson, Jonathas Costa, Gene Payne, Micah Acinapura, Vivian Trakinski, Carter B Emmart PhD, Claudio T. Silva, Charles Hansen, Anders Ynnerman", "uri": "https://vimeo.com/368590395", "name": "SciVis 2019: OpenSpace: A System for Astrographics", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-10-24T17:19:49+00:00", "description": "Authors: Tobias Klein, Ivan Viola, Eduard Gr\u00f6ller, Peter Mindek", "uri": "https://vimeo.com/368585818", "name": "SciVis 2019: Multi-Scale Procedural Animations of Microtubule Dynamics Based on Measured Data", "year": "2019", "event": "SCIVIS"}, {"created_time": "2019-10-24T04:17:18+00:00", "description": "Authors: James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viegas, Jimbo Wilson", "uri": "https://vimeo.com/368443363", "name": "VAST 2019: The What-If Tool: Interactive Probing of Machine Learning Models", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-24T04:02:24+00:00", "description": "(Best Paper Honorable Mention)\nAuthors: Subhashis Hazarika, Haoyu Li, Ko-Chih Wang, Han-Wei Shen, Ching-Shan Chou", "uri": "https://vimeo.com/368441728", "name": "VAST 2019: NNVA: Neural Network Assisted Visual Analysis of Yeast Cell Polarization Simulation", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-24T03:58:25+00:00", "description": "(Best Paper Honorable Mention)\nAuthors: Takanori Fujiwara, Oh-Hyun Kwon, Kwan-Liu Ma", "uri": "https://vimeo.com/368441312", "name": "VAST 2019: Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-24T03:50:16+00:00", "description": "Authors: Jiali Liu, Nadia Boukhelifa, James Eagan", "uri": "https://vimeo.com/368440457", "name": "VAST 2019: Understanding the Role of Alternatives in Data Analysis Practices", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-24T03:46:49+00:00", "description": "Authors: Phong H. Nguyen, Rafael Henkin, Siming Chen, Natalia Andrienko, Gennady", "uri": "https://vimeo.com/368440126", "name": "VAST 2019: VASABI: Doing User Behaviour Analytics through Interactive Visual Hierarchical User Profiles", "year": "2019", "event": "VAST"}, {"created_time": "2019-10-23T03:35:07+00:00", "description": "Authors: XIDAO WEN, Yu-Ru Lin, Yongsu Ahn, Konstantinos Pelechrinis, Xi Liu, Nan Cao\n\nAbstract: Tensor Factorization has been widely used in many fields to discover latent patterns from multidimensional data. Interpreting or scrutinizing the tensor factorization results are, however, by no means easy. We introduce FacIt, a generic visual analytic system that directly factorizes tensor-formatted data into a visual representation of patterns to facilitate result interpretation, scrutinization, information query, as well as model selection. Our design consists of (i) a suite of model scrutinizing and inspection tools that allows efficient tensor model selection (commonly known as rank selection problem) and (ii) an interactive visualization design that empowers users with both characteristics- and content-driven pattern discovery. We demonstrate the effectiveness of our system through usage scenarios with policy adoption analysis.", "uri": "https://vimeo.com/368188079", "name": "[VIS19 Preview] FacIt: Factorizing Tensors into Interpretable and Scrutinizable Patterns (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-23T03:32:43+00:00", "description": "Authors: David Abramov, Jasmine Tan Otto, Mahika Dubey, Cassia Artanegara, Pierre Boutillier, Walter Fontana, Angus G. Forbes\n\nAbstract: We introduce RuleVis, a web-based application for defining and editing \u201ccorrect-by-construction\u201d executable rules that model biochemical functionality, which can be used to simulate the behavior of protein-protein interaction networks and other complex systems. Rule-based models involve emergent effects based on the interactions between rules, which can vary considerably with regard to the scale of a model, requiring the user to inspect and edit individual rules. RuleVis bridges the graph rewriting and systems biology research communities by providing an external visual representation of salient patterns that experts can use to determine the appropriate level of detail for a particular modeling context. We describe the visualization and interaction features available in RuleVis and provide a detailed example demonstrating how RuleVis can be used to reason about intracellular interactions.", "uri": "https://vimeo.com/368187833", "name": "[VIS19 Preview] RuleVis: Constructing Patterns and Rules for Rule-based Models (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-23T03:32:27+00:00", "description": "Authors: Catherine Solis, Fahimeh Rajabiyazdi, Fanny Chevalier\n\nAbstract: The experience of attending live orchestra performances is rich in cultural heritage and can be emotionally moving; however, for those unfamiliar with classical music, it can be intimidating. In this work, we explore the use of visual listening guides to supplement live performances with information that supports the casual listener\u2019s increased engagement. We employ human-centred design practices to evaluate a currently implemented guide with users, from which we extracted design requirements. We then identify dimensions of a music piece that may be visualized and created sample guide designs. Finally, we presented these designs to experts of visualization and music theory. Feedback from the two evaluations informs design implications to consider when creating visual guides of classical music for casual listeners.", "uri": "https://vimeo.com/368187805", "name": "[VIS19 Preview] Designing Visual Guides for Casual Listeners of Live Orchestral Music (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-23T03:24:51+00:00", "description": "Authors: Marco Angelini, Graziano Blasilli, Simone Lenti, Alessia Palleschi, Giuseppe Santucci\n\nAbstract: RadViz plots are commonly used to represent multidimensional data because they use the familiar notion of 2D points for encoding data elements, displaying the original data dimensions that act as springs for setting the x and y coordinates. However, this intuitive approach implies several drawbacks and produces misleading visualizations that can confuse the user, even while analyzing a single data point. The paper attacks this problem following the well known idea of changing the order of the dimensions and introducing ancillary visualizations to mitigate some of RadViz drawbacks. In particular, the paper defines the notion of point optimal disposition of the dimensions for a single data point, generalizes this concept to a set of data points, and proposes effective heuristics for dealing with the intractable problem of exploring all the (n-1)!/2 dispositions of the dimensions along the RadViz circumference. Additional views, visual quality metrics, and a circular grid superimposed on the RadViz complement the attribute reordering strategy and provide a better understanding of the actual plot of the data elements.", "uri": "https://vimeo.com/368187060", "name": "[VIS19 Preview] Towards Enhancing RadViz Analysis and Interpretation (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-23T03:22:53+00:00", "description": "Authors: Stefan Zellmann, Deborah Meurer, Ulrich Lang\n\nAbstract: Shallow k-d trees are an efficient empty space skipping data structure for sparse volume rendering and can be constructed in real-time for moderately sized data sets. Larger volume data sets however require deeper k-d trees that sufficiently cull empty space but take longer to construct. In contrast to k-d trees, uniform grids have inferior culling properties but can be constructed in real-time. We propose a hybrid data structure that employs hierarchical subdivision at the root level and a uniform grid at the leaf level to balance construction and rendering times for sparse volume rendering. We provide a thorough evaluation of this spatial index and compare it to state of the art space skipping data structures.", "uri": "https://vimeo.com/368186869", "name": "[VIS19 Preview] Hybrid Grids for Sparse Volume Rendering (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-21T05:02:32+00:00", "description": "John Wenskovitch, Jian Zhao, Scott Carter, Matthew Cooper, Chris North\n\nAbstract: Computational notebooks have become a major medium for data exploration and insight communication in data science. Although expressive, dynamic, and flexible, in practice they are loose collections of scripts, charts, and tables that rarely tell a story or clearly represent the analysis process. This leads to a number of usability issues, particularly in the comprehension and exploration of notebooks. In this work, we design, implement, and evaluate Albireo, a visualization approach to summarize the structure of notebooks, with the goal of supporting more effective exploration and communication by displaying the dependencies and relationships between the cells of a notebook using a dynamic graph structure. We evaluate the system via a case study and expert interviews, with our results indicating that such a visualization is useful for an analyst\u2019s self-reflection during exploratory programming, and also effective for communication of narratives and collaboration between analysts.", "uri": "https://vimeo.com/367670818", "name": "VDS 2019: Albireo: An Interactive Tool for Visually Summarizing Computational Notebook Structure", "year": "2019", "event": "VDS"}, {"created_time": "2019-10-21T04:34:10+00:00", "description": null, "uri": "https://vimeo.com/367668018", "name": "VDS 2019: Poster Fast Forward", "year": "2019", "event": "VDS"}, {"created_time": "2019-10-21T04:32:31+00:00", "description": "Been Kim, Google\n\nAbstract: In this talk, I hope to reflect on some of the progress made in the field of interpretable machine learning. We will reflect on where we are going as a field, and what are the things we need to be aware and be careful as we make progress. With that perspective, I will then discuss some of my recent work 1) sanity checking popular methods and 2) developing more lay person-friendly interpretability method.", "uri": "https://vimeo.com/367667891", "name": "VDS 2019: [Keynote] Interpretability - now what?", "year": "2019", "event": "KEYNOTE"}, {"created_time": "2019-10-21T02:38:49+00:00", "description": "Authors: Cheonbok Park, Inyoup Na, Yongjang Jo, Sungbok Shin, Jaehyo Yoo, Bum Chul Kwon, Jian Zhao, Hyungjong Noh, Yeonsoo Lee, Jaegul Choo\n\nAbstract: Attention networks, a deep neural network architecture inspired by humans' attention mechanism, have seen significant success in image captioning, machine translation, and many other applications. Recently, they have been further evolved into an advanced approach called multi-head self-attention networks, which can encode a set of input vectors, e.g., word vectors in a sentence, into another set of vectors. Such encoding aims at simultaneously capturing diverse syntactic and semantic features within a set, each of which corresponds to a particular attention head, forming altogether multi-head attention. Meanwhile, the increased model complexity prevents users from easily understanding and manipulating the inner workings of models. To tackle the challenges, we present a visual analytics system called SANVis,  which helps users understand the behaviors and the characteristics of multi-head self-attention networks. Using a state-of-the-art self-attention model called Transformer, we demonstrate usage scenarios of \\toolname in machine translation tasks. Our system is available at http://short.sanvis.org.", "uri": "https://vimeo.com/367655755", "name": "[VIS19 Preview] SANVis: Visual Analytics for Understanding Self Attention Networks (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:37:12+00:00", "description": "Authors: Siva Sandeep Subramanian, Pushparaj Prakash Parab, Zerong Liu, Aidong Lu\n\nAbstract:", "uri": "https://vimeo.com/366928350", "name": "[VIS19 Preview] Explainable Visualization of Collaborative Vandal Behaviors in Wikipedia (vizsec short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:36:52+00:00", "description": "Authors: Margaret Varga, Carsten Winkelholz, Susan Tr\u00e4ber-Burdin\n\nAbstract:", "uri": "https://vimeo.com/366928308", "name": "[VIS19 Preview] An Exploration of Cyber Symbology (vizsec short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:36:35+00:00", "description": "Authors: Aritra Dasgupta, Robert Kosara, Min Chen\n\nAbstract:", "uri": "https://vimeo.com/366928284", "name": "[VIS19 Preview] Guess Me If You Can: A Visual Uncertainty Model for Transparent Evaluation of Disclosure Risks in Privacy-...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:36:14+00:00", "description": "Authors: Brett Fouss, Dennis Ross, Allan Wollaber, Steven R Gomez\n\nAbstract:", "uri": "https://vimeo.com/366928237", "name": "[VIS19 Preview] PunyVis: A Visual Analytics Approach for Identifying Homograph Phishing Attacks (vizsec paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:35:50+00:00", "description": "Authors: Alex Ulmer, David Sessler, J\u00f6rn Kohlhammer\n\nAbstract:", "uri": "https://vimeo.com/366928206", "name": "[VIS19 Preview] NetCapVis: Web-based Progressive Visual Analytics for Network Packet Captures (vizsec paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:35:21+00:00", "description": "Authors: Jorge Luis Guerra, Eduardo Veas, Carlos Adrian Catania\n\nAbstract:", "uri": "https://vimeo.com/366928146", "name": "[VIS19 Preview] A Study on Labeling Network Hostile Behavior with Intelligent Interactive Tools (vizsec paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:34:53+00:00", "description": "Authors: Marco Angelini, Graziano Blasilli, Luca Borzacchiello, Emilio Coppa, Daniele Cono D'Elia, Camil Demetrescu, Simone Lenti, Simone Nicchi, Giuseppe Santucci\n\nAbstract:", "uri": "https://vimeo.com/366928111", "name": "[VIS19 Preview] SymNav: Visually Assisting Symbolic Execution (vizsec paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:34:37+00:00", "description": "Authors: Stephen O\u2019Shaughnessy\n\nAbstract:", "uri": "https://vimeo.com/366928082", "name": "[VIS19 Preview] Image-based Malware Classification: A Space Filling Curve Approach (vizsec paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:34:20+00:00", "description": "Authors: Brandon Laughlin, Christopher Collins, Karthik Sankaranarayanan, Khalil El-Khatib\n\nAbstract:", "uri": "https://vimeo.com/366928046", "name": "[VIS19 Preview] A Visual Analytics Framework for Adversarial Text Generation (vizsec paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-17T05:34:01+00:00", "description": "Best Paper\n\nAuthors: Anna-Pia Lohfink, Simon D Duque Anton, Hans Dieter Schotten, Heike Leitte, Christoph Garth\n\nAbstract:", "uri": "https://vimeo.com/366928023", "name": "[VIS19 Preview] Security in Process: Visually Supported Triage Analysis in Industrial Process Data (vizsec paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-10T04:27:22+00:00", "description": "Authors: Robert Gove\n\nAbstract:", "uri": "https://vimeo.com/365443034", "name": "[VIS19 Preview] VizSec (vizsec-cover)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-09T22:27:48+00:00", "description": "Authors: Jian Chen, Guohao Zhang, Wesley Chiou, David H. Laidlaw, and Alexander P. Auchus\n\nWe report empirical study results on the color encoding of ensemble scalar and orientation to visualize diffusion magnetic resonance imaging (DMRI) tubes. The experiment tested six scalar colormaps for average fractional anisotropy (FA) tasks (grayscale, blackbody, diverging, isoluminant-rainbow, extended-blackbody, and coolwarm) and four three-dimensional (3D) spherical colormaps for tract tracing tasks (uniform gray, absolute, eigenmaps, and Boy's surface embedding). We found that extended-blackbody, coolwarm, and blackbody remain the best three approaches for identifying ensemble average in 3D. Isoluminant-rainbow colormap led to the same ensemble mean accuracy as other colormaps. However, more than 50% of the answers consistently had higher estimates of the ensemble average, independent of the mean values. The number of hues, not luminance, influences ensemble estimates of mean values. For ensemble orientation-tracing tasks, we found that both Boy's surface embedding (greatest spatial resolution and contrast) and absolute colormaps (lowest spatial resolution and contrast) led to more accurate answers than the eigenmaps scheme (medium resolution and contrast), acting as the uncanny-valley phenomenon of visualization design in terms of accuracy. Absolute colormap broadly used in brain science is a good default spherical colormap. We could conclude from our study that human visual processing of a chunk of colors differs from that of single colors.", "uri": "https://vimeo.com/365399042", "name": "[VIS19 Preview] Measuring the Effects of Scalar and Spherical Colormaps on Ensembles of DMRI Tubes (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-09T14:46:40+00:00", "description": "Authors: Daniel J\u00f6nsson, Peter Steneteg, Erik Sund\u00e9n, Rickard Englund, Sathish Kottravel, Martin Falk, Ingrid Hotz, Anders Ynnerman, Timo Ropinski\n\nAbstract: The complexity of today's visualization applications demands specific visualization systems tailored for the development of these applications. Frequently, such systems utilize levels of abstraction to improve the application development process, for instance by providing a data flow network editor. Unfortunately, these abstractions result in several issues, which need to be circumvented through an abstraction-centered system design. Often, a high level of abstraction hides low level details, which makes it difficult to directly access the underlying computing platform, which would be important to achieve an optimal performance. Therefore, we propose a layer structure developed for modern and sustainable visualization systems allowing developers to interact with all contained abstraction levels. We refer to this interaction capabilities as usage abstraction levels, since we target application developers with various levels of experience. We formulate the requirements for such a system, derive the desired architecture, and present how the concepts have been exemplary realized within the Inviwo visualization system. Furthermore, we address several specific challenges that arise during the realization of such a layered architecture, such as communication between different computing platforms, performance centered encapsulation, as well as layer-independent development by supporting cross layer documentation and debugging capabilities.", "uri": "https://vimeo.com/365297341", "name": "[VIS19 Preview] Inviwo \u2014 A Visualization System with Usage Abstraction Levels (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-09T14:21:17+00:00", "description": "Authors: Xu Zhu, Miguel A. Nacenta, \u00d6zg\u00fcr Akgun, and Peter Nightingale\n\nAbstract: Problems such as timetabling or personnel allocation can be modeled and solved using discrete constraint programming languages. However, while existing constraint solving software solves such problems quickly in many cases, these systems involve specialized languages that require significant time and effort to learn and apply. These languages are typically text-based and often difficult to interpret and understand quickly, especially for people without engineering or mathematics backgrounds. Visualization could provide an alternative way to model and understand such problems. Although many visual programming languages exist for procedural languages, visual encoding of problem specifications has not received much attention. Future problem visualization languages could represent problem elements and their constraints unambiguously, but without unnecessary cognitive burdens for those needing to translate their problem\u2019s mental representation into diagrams. As a first step towards such languages, we executed a study that catalogs how people represent constraint problems graphically. We studied three groups with different expertise: non-computer scientists, computer scientists and constraint programmers and analyzed their marks on paper (e.g., arrows), gestures (e.g., pointing) and the mappings to problem concepts (e.g., containers, sets). We provide foundations to guide future tool designs allowing people to effectively grasp, model and solve problems through visual representations.", "uri": "https://vimeo.com/365291194", "name": "[VIS19 Preview] How People Visually Represent Discrete Constraint Problem (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:44:44+00:00", "description": "Authors: Quan Li, Ziming Wu, Lingling Yi, Kristanto Sean N, Huamin Qu, and Xiaojuan Ma\n\n\nAbstract: Social media, such as Facebook and WeChat, empowers millions of users to create, consume, and disseminate online information on an unprecedented scale. The abundant information on social media intensifies the competition of WeChat Public Official Articles (i.e., posts) for gaining user attention due to the zero-sum nature of attention. Therefore, only a small portion of information tends to become extremely popular while the rest remains unnoticed or quickly disappears. Such a typical ``long-tail'' phenomenon is very common in social media. Thus, recent years have witnessed a growing interest in predicting the future trend in the popularity of social media posts and understanding the factors that influence the popularity of the posts. Nevertheless, existing predictive models either rely on cumbersome feature engineering or sophisticated parameter tuning, which are difficult to understand and improve. In this paper, we study and enhance a point process-based model by incorporating visual reasoning to support communication between the users and the predictive model for a better prediction result. The proposed system supports users to uncover the working mechanism behind the model and improve the prediction accuracy accordingly based on the insights gained. We use realistic WeChat articles to demonstrate the effectiveness of the system and verify the improved model on a large scale of WeChat articles. We also elicit and summarize the feedback from WeChat domain experts.", "uri": "https://vimeo.com/364571456", "name": "[VIS19 Preview] WeSeer: Visual Analysis for Better Information Cascade Prediction of WeChat Articles (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:22:08+00:00", "description": "Authors: Brad Eric Hollister, Chris Butson, Gordon Duffley, Chris R. Johnson, Paul Rosen\n\nAbstract: At present, approximately ten million people worldwide are afflicted by Parkinson's Disease (PD). One of the most promising therapies for PD is Deep Brain Stimulation (DBS). DBS works via stimulation of targeted central brain regions (nuclei), whose dysfunction is implicated in PD. A key problem with DBS is determining optimal parameters for clinical outcome. While multiple parameters may influence outcomes in DBS, we explore spatial correlation of volume of tissue activated (VTA) to Unified Parkinson's Disease Rating Scale (UPDRS) scores. Using the Neurostimulation Uncertainty Viewer (nuView), we investigate a number of cooperative visualizations for DBS inspection. Surface-to-surface Euclidean distance between VTA and selected brain nuclei are used in a linked 3D and parallel coordinates view of patient outcome. We then present a semivariogram-based approach to measure spatial correlation of patient outcomes with VTA. As a third component, nuView provides a unique visualization of an ensemble of electrode placements to reduce clutter and emphasize electrodes with spatially similar VTA. These methods corroborate a spatial aspect to DBS efficacy.", "uri": "https://vimeo.com/364569960", "name": "[VIS19 Preview] Visual Inspection of DBS Efficacy (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:19:32+00:00", "description": "Authors: Junpeng Wang, Subhashis Hazarika, Cheng Li, Han-Wei Shen\n\nAbstract: Over the last decade, ensemble visualization has witnessed a significant development due to the wide availability of ensemble data, and the increasing visualization needs from a variety of disciplines. From the data analysis point of view, it can be observed that many ensemble visualization works focus on the same facet of ensemble data, use similar data aggregation or uncertainty modeling methods. However, the lack of reflections on those essential commonalities and a systematic overview of those works prevents visualization researchers from effectively identifying new or unsolved problems and planning for further developments. In this paper, we take a holistic perspective and provide a survey of ensemble visualization. Specifically, we study ensemble visualization works in the recent decade, and categorize them from two perspectives: (1) their proposed visualization techniques; and (2) their involved analytic tasks. For the first perspective, we focus on elaborating how conventional visualization techniques (e.g., surface, volume visualization techniques) have been adapted to ensemble data; for the second perspective, we emphasize how analytic tasks (e.g., comparison, clustering) have been performed differently for ensemble data. From the study of ensemble visualization literature, we have also identified several research trends, as well as some future research opportunities.", "uri": "https://vimeo.com/364569796", "name": "[VIS19 Preview] Visualization and Visual Analysis of Ensemble Data: A Survey (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:19:00+00:00", "description": "Authors: Paulo Ivson ; Andr\u00e9 Moreira ; Francisco Queiroz ; Wallas Santos ; Waldemar Celes\n\nAbstract: Building Information Modeling (BIM) employs data-rich 3D CAD models for large-scale facility design, construction, and operation. These complex datasets contain a large amount and variety of information, ranging from design specifications to real-time sensor data. They are used by architects and engineers for various analysis and simulations throughout a facility's life cycle. Many techniques from different visualization fields could be used to analyze these data. However, the BIM domain still remains largely unexplored by the visualization community. The goal of this article is to encourage visualization researchers to increase their involvement with BIM. To this end, we present the results of a systematic review of visualization in current BIM practice. We use a novel taxonomy to identify main application areas and analyze commonly employed techniques. From this domain characterization, we highlight future research opportunities brought forth by the unique features of BIM. For instance, exploring the synergies between scientific and information visualization to integrate spatial and non-spatial data. We hope this article raises awareness to interesting new challenges the BIM domain brings to the visualization community.", "uri": "https://vimeo.com/364569766", "name": "[VIS19 Preview] A Systematic Review of Visualization in Building Information Modeling (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:18:25+00:00", "description": "Authors: J X Zheng, D F M Goodman, S Pawar\n\nAbstract: A popular method of force-directed graph drawing is multidimensional scaling using graph-theoretic distances as input. We present an algorithm to minimize its energy function, known as stress, by using stochastic gradient descent (SGD) to move a single pair of vertices at a time. Our results show that SGD can reach lower stress levels faster and more consistently than majorization, without needing help from a good initialization. We then show how the unique properties of SGD make it easier to produce constrained layouts than previous approaches. We also show how SGD can be directly applied within the sparse stress approximation of Ortmann et al. [1], making the algorithm scalable up to large graphs.", "uri": "https://vimeo.com/364569741", "name": "[VIS19 Preview] Graph Drawing by Stochastic Gradient Descent (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:12:58+00:00", "description": "Authors: Rafael Ballester-Ripoll, Peter Lindstrom, and Renato Pajarola\n\nAbstract: Memory and network bandwidth are decisive bottlenecks when handling high-resolution multidimensional data sets in visualization applications, and they increasingly demand suitable data compression strategies. We introduce a novel lossy compression algorithm for $N$-dimensional data over regular grids. It leverages the higher-order singular value decomposition (HOSVD), a generalization of the SVD to 3 and more dimensions, together with adaptive quantization, run-length and arithmetic coding to store the HOSVD transform coefficients' relative positions as sorted by their absolute magnitude. Our scheme degrades the data particularly smoothly and outperforms other state-of-the-art volume compressors at low-to-medium bit rates, as required in data archiving and management for visualization purposes. Further advantages of the proposed algorithm include extremely fine bit rate selection granularity, bounded resulting $l^2$ error, and the ability to manipulate data at very small cost in the compression domain, for example to reconstruct subsampled or filtered-resampled versions of all (or selected parts) of the data set.", "uri": "https://vimeo.com/364569449", "name": "[VIS19 Preview] TTHRESH: Tensor Compression for Multidimensional Visual Data (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:12:28+00:00", "description": "Authors: Baldwin Nsonga, Martin Niemann, Jochen Fr\u00f6hlich, Joachim Staib, Stefan Gumhold, and Gerik Scheuermann \n\nAbstract:  Splat and antisplat events are a widely found phenomenon in three-dimensional turbulent flow fields. Splats are observed when fluid locally impinges on an impermeable surface transferring energy from the normal component to the tangential velocity components, while antisplats relate to the inverted situation. These events affect a variety of flow properties, such as the transfer of kinetic energy between velocity components and the transfer of heat, so that their investigation can provide new insight into these issues. Here, we propose the first Lagrangian method for the detection of splats and antisplats as features of an unsteady flow field. Our method utilizes the concept of strain tensors on flow-embedded flat surfaces to extract disjoint regions in which splat and antisplat events of arbitrary scale occur. We validate the method with artificial flow fields of increasing complexity. Subsequently, the method is used to analyze application data stemming from a direct numerical simulation of the turbulent flow over a backward facing step. Our results show that splat and antisplat events can be identified efficiently and reliably even in such a complex situation, demonstrating that the new method constitutes a well-suited tool for the analysis of turbulent flows.", "uri": "https://vimeo.com/364569430", "name": "[VIS19 Preview] Detection and Visualization of Splat and Antisplat Events in Turbulent Flows (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:11:29+00:00", "description": "Authors: Mohammad Raji, Alok Hota, Tanner Hobson, Jian Huang\n\nAbstract: In this paper, we propose using a decoupled architecture to create a microservice that can deliver scientific visualization remotely with efficiency, scalability, and superior availability, affordability and accessibility. Through our effort, we have created an open source platform, Tapestry, which can be deployed on Amazon AWS as a production use microservice. The applications we use to demonstrate the efficacy of the Tapestry microservice in this work are: (1) embedding interactive visualizations into lightweight web pages, (2) creating scientific visualization movies that are fully controllable by the viewers, (3) serving as a rendering engine for high-end displays such as power-walls, and (4) embedding data-intensive visualizations into augmented reality devices efficiently. In addition, we show results of an extensive performance study, and suggest how applications can make optimal use of microservices such as Tapestry.", "uri": "https://vimeo.com/364569370", "name": "[VIS19 Preview] Scientific Visualization as a Microservice (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:10:47+00:00", "description": "Authors: Alok Hota, Jian Huang\n\nAbstract: In this work, we study how to co-locate meta information with visualizations by directly embedding information into visualizations. This allows for visualizations to carry provenance and authorship information themselves for reproducibility. We call these self-describing visualizations -- reproducible, authenticatable, and documentable. Self-describing visualizations can be used to extend existing visualization provenance systems. Herein, we start with a survey of existing digital image watermarking literature. We search for and classify watermarking algorithms that can support scientific visualizations. Using our payload-resilience testing framework, we evaluate and recommend algorithms supporting various use cases in the payload-resiliency space, and present guidelines for optimizing visualizations to improve payload capacities and embedding robustness. We demonstrate the efficacy of self-describing visualizations with two sample application implementations: (1) adding an embedding filter as a part the standard rendering pipeline, (2) creating a web reader to automatically and reliably extract provenance information from scientific publications for review and dissemination.", "uri": "https://vimeo.com/364569334", "name": "[VIS19 Preview] Embedding Meta-Information into Visualizations (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:10:17+00:00", "description": "Authors: Ko-Chih Wang, Tzu-Hsuan Wei, Shareef Naeem, and Han-Wei Shen\n\nAbstract: The analysis and visualization of data created from simulations on modern supercomputers is a daunting challenge because the incredible compute power of modern supercomputers allow scientists to generate datasets with very high spatial and temporal resolutions. The limited bandwidth and capacity of networking and storage devices connecting supercomputers to analysis machines become the major bottleneck for data analysis such that simply moving the whole dataset from the supercomputer to a data analysis machine is infeasible. A common approach to visualize high temporal resolution simulation datasets under constrained I/O is to reduce the sampling rate in the temporal domain while preserving the original spatial resolution at the time steps. Data interpolation between the sampled time steps alone may not be a viable option since it may suffer from large errors, especially when using a lower sampling rate. We present a novel ray-based representation storing ray based histograms and depth information that recovers the evolution of volume data between sampled time steps. Our view-dependent proxy allows for a good trade-off between compactly representing the time-varying data and leveraging temporal coherence within the data by utilizing interpolation between time steps, ray histograms, depth information, and codebooks. Our approach is able to provide fast rendering in the context of transfer function exploration to support visualization of feature evolution in time-varying data.", "uri": "https://vimeo.com/364569304", "name": "[VIS19 Preview] Ray-based Exploration of Large Time-varying Volume Data Using Proxy Per-ray Distributions (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:09:14+00:00", "description": "Authors: Jonathan Sarton, Nicolas Courilleau, Yannick Remion,Laurent Lucas\n\nAbstract: In a wide range of scientific fields, 3D datasets production capabilities have widely evolved in recent years, especially with the rapid increase in their sizes. As a result, many large-scale applications, including visualization or processing, have become challenging to address. A solution to this issue lies in providing out-of-core algorithms specifically designed to handle datasets significantly larger than memory. In this article, we present a new approach that extends the broad interactive addressing principles already established in the field of out-of-core volume rendering on GPUs to allow on-demand processing during the visualization stage. We propose a pipeline designed to manage data as regular 3D grids regardless of the underlying application. It relies on a caching approach with a virtual memory addressing system coupled to an efficient parallel management on GPU to provide efficient access to data in interactive time. It allows any visualization or processing application to leverage the flexibility of its structure by managing multi-modality datasets. Furthermore, we show that our system delivers good performance on a single standard PC with low memory budget on the GPU.", "uri": "https://vimeo.com/364569226", "name": "[VIS19 Preview] Interactive Visualization and On-Demand Processing of Large Volume Data: A Fully GPU-Based Out-Of-Core...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:08:28+00:00", "description": "Authors: C\u00edcero L. Pahins, Nivan Ferreira, and Jo\u00e3o L. Comba\n\nAbstract: In recent years sophisticated data structures based on datacubes have been proposed to perform interactive visual exploration of large datasets. While powerful, these approaches overlook the important fact that aggregations used to produce datacubes do not represent the actual distribution of the data being analyzed. As a result, these methods might produce biased results as well as hide important features in the data. In this paper, we introduce the Quantile Datacube Structure (QDS) that bridges this gap by supporting interactive visual exploration based on order statistics. To achieve this, QDS makes use of an efficient non-parametric distribution approximation scheme called p-digest and employs a novel datacube indexing scheme that reduces the memory usage of previous datacube methods. This enables interactive slicing and dicing while accurately approximating the distribution of quantitative variables of interest. We present two case studies that illustrate the ability of QDS to not only build order statistics based visualizations interactively but also to perform event detection on very large datasets. Finally, we present extensive experimental results that validate the effectiveness of QDS regarding memory usage and accuracy in the approximation of order statistics for real-world datasets.", "uri": "https://vimeo.com/364569181", "name": "[VIS19 Preview] Real-Time Exploration of Large Spatiotemporal Datasets based on Order Statistics (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:08:08+00:00", "description": "Authors: Irene Baeza Rojo, Markus Gross and Tobias G\u00fcnther\n\nAbstract: Over the past decades, scientific visualization became a fundamental aspect of modern scientific data analysis. Across all data-intensive research fields, ranging from structural biology to cosmology, data sizes increase rapidly. Dealing with the growing large-scale data is one of the top research challenges of this century. For the visual exploratory data analysis, interactivity, a view-dependent visibility optimization and frame coherence are indispensable. In this work, we extend the recent decoupled opacity optimization framework to enable a navigation without occlusion of important features through large geometric data. By expressing the accumulation of importance and optical depth in Fourier basis, the computation, evaluation and rendering of optimized transparent geometry become not only order-independent, but also operate within a fixed memory bound. We study the quality of our Fourier approximation in terms of accuracy, memory requirements and efficiency for both the opacity computation, as well as the order-independent compositing. We apply the method to different point, line and surface data sets originating from various research fields, including meteorology, health science, astrophysics and organic chemistry.", "uri": "https://vimeo.com/364569154", "name": "[VIS19 Preview] Fourier Opacity Optimization for Scalable Exploration (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:07:45+00:00", "description": "Authors: Colin Ware, Terece L. Turton, Roxana Bujack, Francesca Samsel, Piyush Shrivastava, and David H. Rogers\n\nAbstract: Pseudocoloring is one of the most common techniques used in scientific visualization. To apply pseudocoloring to a scalar field, the field value at each point is represented using one of a sequence of colors (called a colormap). One of the principles applied in generating colormaps is uniformity and previously the main method for determining uniformity has been the application of uniform color spaces. In this paper we present a new method for evaluating the feature detection threshold function across a colormap. The method is used in crowdsourced studies for the direct evaluation of nine colormaps for three feature sizes. The results are used to test the hypothesis that a uniform color space (CIELAB) will accurately model colormapped feature detection thresholds compared to a model where the chromaticity components have reduced weights. The hypothesis that feature detection can be predicted solely on the basis of luminance is also tested. The results reject both hypotheses and we demonstrate how reduced weights on the green-red and blue-yellow terms of the CIELAB color space creates a more accurate model when the task is the detection of smaller features in colormapped data. Both the method itself and modified CIELAB can be used in colormap design and evaluation.", "uri": "https://vimeo.com/364569128", "name": "[VIS19 Preview] Measuring and Modeling the Feature Detection Threshold Functions of Colormaps (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:07:27+00:00", "description": "Authors: Valentin Bruder, Christoph M\u00fcller, Steffen Frey, Thomas Ertl\n\nAbstract: As our field matures, evaluation of visualization techniques has extended from reporting runtime performance to studying user behavior. Consequently, many methodologies and best practices for user studies have evolved. While maintaining interactivity continues to be crucial for the exploration of large data sets, no similar methodological foundation for evaluating runtime performance has been developed. Our analysis of 50 recent visualization papers on new or improved techniques for rendering volumes or particles indicates that only a very limited set of parameters like different data sets, camera paths, viewport sizes, and GPUs are investigated, which make comparison with other techniques or generalization to other parameter ranges at least questionable. To derive a deeper understanding of qualitative runtime behavior and quantitative parameter dependencies, we developed a framework for the most exhaustive performance evaluation of volume and particle visualization techniques that we are aware of, including millions of measurements on ten different GPUs. This paper reports on our insights from statistical analysis of this data, discussing independent and linear parameter behavior and non-obvious effects. We give recommendations for best practices when evaluating runtime performance of scientific visualization applications, which can serve as a starting point for more elaborate models of performance quantification.", "uri": "https://vimeo.com/364569107", "name": "[VIS19 Preview] On Evaluating Runtime Performance of Interactive Visualizations (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:07:05+00:00", "description": "Authors: Andrea Schnorr, Dirk N. Helmrich, Dominik Denker, Torsten W. Kuhlen, Bernd Hentschel\n\nAbstract: Tracking the temporal evolution of features in time-varying data is a key method in visualization. For typical feature definitions, such as vortices, objects are sparsely distributed over the data domain. In this paper, we present a novel approach for tracking both sparse and space-filling features. While the former comprise only a small fraction of the domain, the latter form a set of objects whose union covers the domain entirely while the individual objects are mutually disjunct. Our approach determines the assignment of features between two successive time-steps by solving two graph optimization problems. It first resolves one-to-one assignments of features by computing a maximum-weight, maximum-cardinality matching on a weighted bi-partite graph. Second, our algorithm detects events by creating a graph of potentially conflicting event explanations and finding a weighted, independent set in it. We demonstrate our method's effectiveness on synthetic and simulation data sets, the former of which enables quantitative evaluation because of the availability of ground-truth information. Here, our method performs on par or better than a well-established reference algorithm. In addition, manual visual inspection by our collaborators confirm the results' plausibility for simulation data.", "uri": "https://vimeo.com/364569076", "name": "[VIS19 Preview] Feature Tracking by Two-Step Optimization (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:06:45+00:00", "description": "Authors: Jun Han, Jun Tao, Chaoli Wang\n\nAbstract: For effective flow visualization, identifying representative flow lines or surfaces is an important problem which has been studied. However, no work can solve the problem for both lines and surfaces. In this paper, we present FlowNet, a single deep learning framework for clustering and selection of streamlines and stream surfaces. Given a collection of streamlines or stream surfaces generated from a flow field data set, our approach converts them into binary volumes and then employs an autoencoder to learn their respective latent feature descriptors. These descriptors are used to reconstruct binary volumes for error estimation and network training. Once converged, the feature descriptors can well represent flow lines or surfaces in the latent space. We perform dimensionality reduction of these feature descriptors and cluster the projection results accordingly. This leads to a visual interface for exploring the collection of flow lines or surfaces via clustering, filtering, and selection of representatives. Intuitive user interactions are provided for visual reasoning of the collection with ease. We validate and explain our deep learning framework from multiple perspectives, demonstrate the effectiveness of FlowNet using several flow field data sets of different characteristics, and compare our approach against state-of-the-art streamline and stream surface selection algorithms.", "uri": "https://vimeo.com/364569054", "name": "[VIS19 Preview] FlowNet: A Deep Learning Framework for Clustering and Selection of Streamlines and Stream Surfaces (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:06:21+00:00", "description": "Authors: Wenbin He, Hanqi Guo, Han-Wei Shen, and Tom Peterka\n\nAbstract: We propose surface density estimate (SDE) to model the spatial distribution of surface features\u2014isosurfaces, ridge surfaces, and streamsurfaces\u2014in 3D ensemble simulation data. The inputs of SDE computation are surfaces represented as polygon meshes, and no field datasets are required. The SDE is defined as the kernel density estimate of the infinite set of points on the input surfaces and is approximated by accumulating the surface densities of triangular patches. We also propose an algorithm to guide the selection of a proper kernel bandwidth for SDE computation. An ensemble Feature Exploration method based on Surface densiTy EstimAtes (eFESTA) is then proposed to extract and visualize the major trends of ensemble surface features. For an ensemble of surface features, each surface is first transformed into a density field based on its contribution to the SDE, and the resulting density fields are organized into a hierarchical representation based on the pairwise distances between them. The hierarchical representation is then used to guide visual exploration of the density fields as well as the underlying surface features. We demonstrate the application of our method using isosurface in ensemble scalar fields, Lagrangian coherent structures in uncertain unsteady flows, and streamsurfaces in ensemble fluid flows.", "uri": "https://vimeo.com/364569031", "name": "[VIS19 Preview] eFESTA: Ensemble Feature Exploration with Surface Density Estimates (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:06:21+00:00", "description": "Authors: Radi Muhammad Reza, Benjamin  Watson\n\nAbstract: In this paper, we present \\textit{Hi-D maps}, a novel method for the visualization of multi-dimensional categorical data. Our work addresses the scarcity of techniques for visualizing a large number of data-dimensions in an effective and space-efficient manner. We have mapped the full data-space onto a 2D regular polygonal region. The polygon is cut hierarchically with lines parallel to a user-controlled, ordered sequence of sides, each representing a dimension. We have used multiple visual cues such as orientation, thickness, color, countable glyphs, and text to depict cross-dimensional information. We have added interactivity and hierarchical browsing to facilitate flexible exploration of the display: small areas can be scrutinized for details. Thus, our method is also easily extendable to visualize hierarchical information. Our glyph animations add an engaging aesthetic during interaction. Like many visualizations, Hi-D maps become less effective when a large number of dimensions stresses perceptual limits, but Hi-D maps may add clarity before those limits are reached.", "uri": "https://vimeo.com/364569030", "name": "[VIS19 Preview] Hi-D Maps: An Interactive Visualization Technique for Multi-dimensional Categorical Data (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:05:39+00:00", "description": "Authors: Jochen Jankowai, Ingrid Hotz\n\nAbstract: Iso-surfaces or level-sets provide an effective and frequently used means for feature visualization. However, they are restricted to simple features for uni-variate data. The approach does not scale when moving to multi-variate data or when considering more complex feature definitions. In this paper, we introduce the concept of traits and feature level-sets, which can be understood as a generalization of level-sets as it includes iso-surfaces, and fiber surfaces as special cases. The concept is applicable to a large class of traits defined as subsets in attribute space, which can be arbitrary combinations of points, lines, surfaces and volumes. It is implemented into a system that provides an interface to define traits in an interactive way and multiple rendering options. We demonstrate the effectiveness of the approach using multi-variate data sets of different nature, including vector and tensor data, from different application domains.", "uri": "https://vimeo.com/364568992", "name": "[VIS19 Preview] Generalizing Iso-surfaces to Multi-variate Data (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:05:07+00:00", "description": "Authors: Raghavendra Sridharamurthy, Talha Bin Masood, Adhitya Kamakshidasan, and Vijay Natarajan\n\nAbstract: Topological structures such as the merge tree provide an abstract and succinct representation of scalar fields. They facilitate effective visualization and interactive exploration of feature-rich data. A merge tree captures the topology of sub-level and super-level sets in a scalar field. Estimating the similarity between merge trees is an important problem with applications to feature-directed visualization of time-varying data. We present an approach based on tree edit distance to compare merge trees. The comparison measure satisfies metric properties, it can be computed efficiently, and the cost model for the edit operations is both intuitive and captures well-known properties of merge trees. Experimental results on time-varying scalar fields, 3D cryo electron microscopy data, shape data, and various synthetic datasets show the utility of the edit distance towards a feature-driven analysis of scalar fields.", "uri": "https://vimeo.com/364568965", "name": "[VIS19 Preview] Edit Distance between Merge Trees (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:04:39+00:00", "description": "Authors: Tobias G\u00fcnther, Holger Theisel\n\nAbstract: Almost all properties of vector fields, including magnitude, direction, lambda2 and vorticity change under arbitrary movements of the observer. This is undesirable since measurements of physical properties should ideally not depend on the way the (virtual) measurement device moves. There are some properties that are invariant under certain types of reference frame transformations: Galilean invariance (invariance under equal-speed translation) and objectivity (invariance under any smooth rotation and translation of the reference frame). In this paper, we introduce even harder conditions than objectivity: we demand invariance under any smooth similarity transformation (rotation, translation and uniform scale) as well as invariance under any smooth affine transformation of the reference frame. We show that these new hyper-objective measures allow the extraction of vortices that change their volume or deform. Further, we present a generic approach that transforms almost any vortex measure into a hyper-objective one. We apply our methods to vortex extraction in 2D and 3D vector fields, and analyze the numerical robustness, extraction time and the minimization residuals for the Galilean invariant, objective, and the two new hyper-objective approaches.", "uri": "https://vimeo.com/364568933", "name": "[VIS19 Preview] Hyper-Objective Vortices (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:03:36+00:00", "description": null, "uri": "https://vimeo.com/364568879", "name": "[VIS19 Preview] Visualization and the Digital Humanities: Moving Toward Stronger Collaborations (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:03:17+00:00", "description": "Authors: Shreeraj Jadhav, Saad Nadeem, Arie Kaufman\n\nAbstract: We present a volume exploration framework, FeatureLego, that uses a novel voxel clustering approach for efficient selection of semantic features. We partition the input volume into a set of compact super-voxels that represent the finest selection granularity. We then perform an exhaustive clustering of these super-voxels using a graph-based clustering method. Unlike the prevalent brute-force parameter sampling approaches, we propose an efficient algorithm to perform this exhaustive clustering. By computing an exhaustive set of clusters, we aim to capture as many boundaries as possible and ensure that the user has sufficient options for efficiently selecting semantically relevant features. Furthermore, we merge all the computed clusters into a single tree of meta-clusters that can be used for hierarchical exploration. We implement an intuitive user-interface to interactively explore volumes using our clustering approach. Finally, we show the effectiveness of our framework on multiple real-world datasets of different modalities.", "uri": "https://vimeo.com/364568860", "name": "[VIS19 Preview] FeatureLego: Volume Exploration Using Exhaustive Clustering of Super-Voxels (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:02:44+00:00", "description": "Authors: Hanqi Guo, Wenbin He, Sangmin Seo, Han-Wei Shen, Emil Mihai Constantinescu, Chunhui Liu, and Tom Peterka\n\nAbstract: We present an efficient and scalable solution to estimate uncertain transport behaviors---stochastic flow maps (SFMs)---for visualizing and analyzing uncertain unsteady flows. Computing flow maps from uncertain flow fields is extremely expensive because it requires many Monte Carlo runs to trace densely seeded particles in the flow. We reduce the computational cost by decoupling the time dependencies in SFMs so that we can process shorter sub time intervals independently and then compose them together for longer time periods. Adaptive refinement is also used to reduce the number of runs for each location. We parallelize over tasks---packets of particles in our design---to achieve high efficiency in MPI/thread hybrid programming. Such a task model also enables CPU/GPU coprocessing. We show the scalability on two supercomputers, Mira (up to 256K Blue Gene/Q cores) and Titan (up to 128K Opteron cores and 8K GPUs), that can trace billions of particles in seconds.", "uri": "https://vimeo.com/364568821", "name": "[VIS19 Preview] Extreme-Scale Stochastic Particle Tracing for Uncertain Unsteady Flow Visualization and Analysis (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:01:07+00:00", "description": "Authors: Yucheng Huang, Lei Shi, Yue Su, Yifan Hu, Hanghang Tong, Chaoli Wang, Tong Yang, Deyun Wang, Shuo Liang\n\nAbstract: The visualization of evolutionary influence graphs is important for performing many real-life tasks such as citation analysis and social influence analysis. The main challenges include how to summarize large-scale, complex, and time-evolving influence graphs, and how to design effective visual metaphors and dynamic representation methods to illustrate influence patterns over time. In this work, we present Eiffel, an integrated visual analytics system that applies triple summarizations on evolutionary influence graphs in the nodal, relational, and temporal dimensions. In numerical experiments, Eiffel summarization results outperformed those of traditional clustering algorithms with respect to the influence-flow-based objective. Moreover, a flow map representation is proposed and adapted to the case of influence graph summarization, which supports two modes of evolutionary visualization (i.e., flip-book and movie) to expedite the analysis of influence graph dynamics. We conducted two controlled user experiments to evaluate our technique on influence graph summarization and visualization respectively. We also showcased the system in the evolutionary influence analysis of two typical scenarios, the citation influence of scientific papers and the social influence of emerging online events. The evaluation results demonstrate the value of Eiffel in the visual analysis of evolutionary influence graphs.", "uri": "https://vimeo.com/364568698", "name": "[VIS19 Preview] Eiffel: Evolutionary Flow Map for Influence Graph Visualization (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T05:00:18+00:00", "description": "Authors: Cindy Xiong, Lisanne van Weelden, and Steven Franconeri\n\nAbstract: A viewer can extract many potential patterns from any set of visualized data values. But that means that two people can see different patterns in the same visualization, potentially leading to miscommunication. Here, we show that when people are primed to see one pattern in the data as visually salient, they believe that nai\u0308ve viewers will experience the same visual salience. Participants were told one of multiple backstories about political events that affected public polling data, before viewing a graph that depicted those data. One pattern in the data was particularly visually salient to them given the backstory that they heard. They then predicted what nai\u0308ve viewers would most visually salient on the visualization. They were strongly influenced by their own knowledge, despite explicit instructions to ignore it, predicting that others would find the same patterns to be most visually salient. This result reflects a psychological phenomenon known as the curse of knowledge, where an expert struggles to re-create the state of mind of a novice. The present findings show that the curse of knowledge also plagues the visual perception of data, explaining why people can fail to connect with audiences when they communicate patterns in data.", "uri": "https://vimeo.com/364568644", "name": "[VIS19 Preview] The Curse of Knowledge in Visual Data Communication (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:59:10+00:00", "description": "Authors: Marti A. Hearst, Emily Pedersen, Lekha Patil, Elsie Lee, Paul Laskowski, and Steven Franconeri\n\nAbstract: Word clouds continue to be a popular tool for summarizing textual information, despite their well-documented deficiencies for analytic tasks. Much of their popularity rests on their playful visual appeal. In this paper, we present the results of a series of controlled experiments that show that layouts in which words are arranged into semantically and visually distinct zones are more effective for understanding the underlying topics than standard word cloud layouts. White space separators and/or spatially grouped color coding led to significantly stronger understanding of the underlying topics compared to a standard Wordle layout, while simultaneously scoring higher on measures of aesthetic appeal. This work is an advance on prior research on semantic layouts for word clouds because that prior work has either not ensured that the different semantic groupings are visually or semantically distinct, or has not performed usability studies. An additional contribution of this work is the development of a dataset for a semantic category identification task that can be used for replication of these results or future evaluations of word cloud designs.", "uri": "https://vimeo.com/364568578", "name": "[VIS19 Preview] An Evaluation of Semantically Grouped Word Cloud Designs (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:57:25+00:00", "description": "Authors: Zhutian Chen, Yijia Su, Yifang Wang, Qianwen Wang, Huamin Qu, Yingcai Wu\n\nAbstract: Recent advances in mobile augmented reality (AR) techniques have shed new light on personal visualization for their advantages of fitting visualization within personal routines, situating visualization in a real-world context, and arousing users\u2019 interests. However, enabling non-experts to create data visualization in mobile AR environments is challenging given the lack of tools that allow in-situ design while supporting the binding of data to AR content. Most existing AR authoring tools require working on personal computers or manually creating each virtual object and modifying its visual attributes. We systematically study this issue by identifying the specificity of AR glyph-based visualization authoring tool and distill four design considerations. Following these design considerations, we design and implement MARVisT, a mobile authoring tool that leverages information from reality to assist non-experts in addressing relationships between data and virtual glyphs, real objects virtual glyphs, and real objects and data. With MARVisT, users without visualization expertise can bind data to real-world objects to create expressive AR glyph-based visualizations rapidly and effortlessly, reshaping the representation of the real world with data. We use several examples to demonstrate the expressiveness of MARVisT. A user study with non-experts is also conducted to evaluate the authoring experience of MARVisT.", "uri": "https://vimeo.com/364568467", "name": "[VIS19 Preview] MARVisT: Authoring Glyph-based Visualization in Mobile Augmented Reality (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:56:54+00:00", "description": "Authors: Aritra Dasgupta, Jorge Poco, Bernice Rogowitz , Kyungsik Han, Enrico Bertini, Claudio T. Silva\n\nAbstract: Geographical maps encoded with rainbow color scales are widely used by climate scientists. Despite a plethora of evidence from the visualization and vision sciences literature about the shortcomings of the rainbow color scale, they continue to be preferred over perceptually optimal alternatives. To study and analyze this mismatch between theory and practice, we present a web-based user study that compares the effect of color scales on performance accuracy for climate-modeling tasks. In this study, we used pairs of continuous geographical maps generated using climatological metrics for quantifying pairwise magnitude difference and spatial similarity. For each pair of maps, 39 scientist-observers judged: i) the magnitude of their difference, ii) their degree of spatial similarity, and iii) the region of greatest dissimilarity between them. Besides the rainbow color scale, two other continuous color scales were chosen such that all three of them covaried two dimensions (luminance monotonicity and hue banding), hypothesized to have an impact on task performance. We also analyzed subjective performance measures, such as user confidence, perceived accuracy, preference, and familiarity in using the different color scales. We found that monotonic luminance scales produced significantly more accurate judgments of magnitude difference but were not superior in spatial comparison tasks, and that hue banding had differential effects based on the task and conditions. Scientists expressed the highest preference and perceived confidence and accuracy with the rainbow, despite its poor performance on the magnitude comparison tasks. We also report on interesting interactions among stimulus conditions, tasks, and color scales, that lead to open research questions.", "uri": "https://vimeo.com/364568438", "name": "[VIS19 Preview] The Effect of Color Scales on Climate Scientists\u2019 Objective and Subjective Performance in Spatial Data...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:56:35+00:00", "description": "Authors: Dimara E., Franconeri S., Plaisant C., Bezerianos A., and Dragicevic P.\n\nAbstract: Information visualization designers strive to design data displays that allow for efficient exploration, analysis, and communication of patterns in data, leading to informed decisions. Unfortunately, human judgment and decision making are imperfect and often plagued by cognitive biases. There is limited empirical research documenting how these biases affect visual data analysis\nactivities. Existing taxonomies are organized by cognitive theories that are hard to associate with visualization tasks. Based on a survey of the literature we propose a task-based taxonomy of 154 cognitive biases organized in 7 main categories. We hope the taxonomy will help visualization researchers relate their design to the corresponding possible biases, and lead to new research that detects and addresses biased judgment and decision making in data visualization.", "uri": "https://vimeo.com/364568413", "name": "[VIS19 Preview] A Task-based Taxonomy of Cognitive Biases for Information Visualization (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:56:10+00:00", "description": "Authors: Thom Castermans, Kevin Verbeek , Bettina Speckmann, Michel A. Westenberg, Rob Koopman, Shenghui Wang, Hein van den Berg, and Arianna Betti\n\nAbstract: We propose a novel type of low distortion radial embedding which focuses on one specific entity and its closest neighbors. Our embedding preserves near-exact distances to the focus entity and aims to minimize distortion between the other entities. We present an interactive exploration tool SolarView which places the focus entity at the center of a \u201csolar system\u201d and embeds its neighbors guided by concentric circles. SolarView provides an implementation of our novel embedding and several state-of-the-art dimensionality reduction and embedding techniques, which we adapted to our setting in various ways. We experimentally evaluated our embedding and compared it to these state-of-the-art techniques. The results show that our embedding competes with these techniques and achieves low distortion in practice. Our method performs particularly well when the visualization, and hence the embedding, adheres to the solar system design principle of our application. Nonetheless\u2014as with all dimensionality reduction techniques\u2014the distortion may be high. We leverage interaction techniques to give clear visual cues that allow users to accurately judge distortion. We illustrate the use of SolarView by exploring the high-dimensional metric space of bibliographic entity similarities.", "uri": "https://vimeo.com/364568393", "name": "[VIS19 Preview] SolarView: Low Distortion Radial Embedding with a Focus (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:55:27+00:00", "description": "Authors: Siming Chen, Natalia Andrienko, Gennady Andrienko, Linara Adilova, Jeremie Barlet, Joerg Kindermann, Phong H. Nguyen, Olivier Thonnard, Cagatay Turkay\n\nAbstract: We define behavior as a set of actions performed by some actor during a period of time. We consider the problem of analyzing a large collection of behaviors by multiple actors, more specifically, identifying typical behaviors and spotting anomalous behaviors. We propose an approach leveraging topic modeling techniques -- LDA (Latent Dirichlet Allocation) Ensembles -- to represent categories of typical behaviors by topics that are obtained through topic modeling a behavior collection. When such methods are applied to text in natural languages, the quality of the extracted topics are usually judged based on the semantic relatedness of the terms pertinent to the topics. This criterion, however, is not necessarily applicable to topics extracted from non-textual data, such as action sets, since relationships between actions may not be obvious. We have developed a suite of visual and interactive techniques supporting the construction of an appropriate combination of topics based on other criteria, such as distinctiveness and coverage of the behavior set. Two case studies on analyzing operation behaviors in the security management system and visiting behaviors in an amusement park, and the expert evaluation of the first case study demonstrate the effectiveness of our approach.", "uri": "https://vimeo.com/364568366", "name": "[VIS19 Preview] LDA Ensembles for Interactive Exploration and Categorization of Behaviors (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:55:01+00:00", "description": "Authors: Qianwen Wang, Jun Yuan, Shuxin Chen, Hang Su, Huamin Qu, Shixia Liu\n\nAbstract: A comprehensive and comprehensible summary of existing deep neural networks (DNNs) helps practitioners understand the behavior and evolution of DNNs, offers insights for architecture optimization, and sheds light on the working mechanisms of DNNs. However, this summary is hard to obtain because of the complexity and diversity of DNN architectures. To address this issue, we develop DNN Genealogy, an interactive visualization tool, to offer a visual summary of representative DNNs and their evolutionary relationships. DNN Genealogy enables users to learn DNNs from multiple aspects, including architecture, performance, and evolutionary relationships. Central to this tool is a systematic analysis and visualization of 66 representative DNNs based on our analysis of 140 papers. A directed acyclic graph is used to illustrate the evolutionary relationships among these DNNs and highlight the representative DNNs. A focus+context visualization is developed to orient users during their exploration. A set of network glyphs is used in the graph to facilitate the understanding and comparing of DNNs in the context of the evolution. Case studies demonstrate that DNN Genealogy provides helpful guidance in understanding, applying, and optimizing DNNs. DNN Genealogy is extensible and will continue to be updated to reflect future advances in DNNs.", "uri": "https://vimeo.com/364568338", "name": "[VIS19 Preview] Visual Genealogy of Deep Neural Networks (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:54:29+00:00", "description": "Authors: Chunggi Lee, Yeonjun Kim, Seungmin Jin, Dongmin Kim, Ross Maciejewski, David Ebert, and Sungahn Ko\n\nAbstract: We present an interactive visual analytics system that enables traffic congestion exploration, surveillance, and forecasting based on vehicle detector data. Through domain expert collaboration, we have extracted task requirements, incorporated the Long Short-Term Memory (LSTM) model for congestion forecasting, and designed a weighting method for detecting the causes of congestion and congestion propagation directions. Our visual analytics system is designed to enable users to explore congestion causes, directions, and severity. Congestion conditions of a city are visualized using a Volume-Speed Rivers (VSRivers) visualization that simultaneously presents traffic volumes and speeds. To evaluate our system, we report performance comparison results, wherein our model is more accurate than other forecasting algorithms. We demonstrate the usefulness of our system in the traffic management and congestion broadcasting domains through three case studies and domain expert feedback.", "uri": "https://vimeo.com/364568300", "name": "[VIS19 Preview] A Visual Analytics System for Exploring, Monitoring, and Forecasting Road Traffic Congestion (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:53:58+00:00", "description": "Authors: Siming Chen, Jie Li, Gennady Andrienko, Natalia Andrienko, Yun Wang, Phong H. Nguyen, and Cagatay Turkay\n\nAbstract: Visual analytics usually deals with complex data and uses sophisticated algorithmic, visual, and interactive techniques supporting the analysis. Findings and results of the analysis often need to be communicated to an audience that lacks visual analytics expertise. This requires analysis outcomes to be presented in simpler ways than that are typically used in visual analytics systems. However, not only analytical visualizations may be too complex for target audiences but also the information that needs to be presented. Analysis results may consist of multiple components, which may involve multiple heterogeneous facets. Hence, there exists a gap on the path from obtaining analysis findings to communicating them, within which two main challenges lie: information complexity and display complexity. We address this problem by proposing a general framework \\modi{where data analysis and result presentation are linked by story synthesis, in which the analyst creates and organises story contents. Unlike previous research, where analytic findings are represented by stored display states, we treat findings as data constructs. We focus on selecting, assembling and organizing findings for further presentation rather than on tracking analysis history and enabling dual (i.e., explorative and communicative) use of data displays. In story synthesis, findings are selected, assembled, and arranged in meaningful layouts that take into account the structure of information and inherent properties of its components. We propose a workflow for applying the proposed conceptual framework in designing visual analytics systems and demonstrate the generality of the approach by applying it to two diverse domains, social media and movement analysis.", "uri": "https://vimeo.com/364568266", "name": "[VIS19 Preview] Supporting Story Synthesis: Bridging the Gap between Visual Analytics and Storytelling (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:53:39+00:00", "description": "Authors: Aoyu Wu and Huamin Qu\n\nAbstract: While much research in the educational field has revealed many presentation techniques, they often overlap and are even occasionally contradictory. Exploring presentation techniques used in TED Talks could provide evidence for a practical guideline. This study aims to explore the verbal and non-verbal presentation techniques from a collection of TED Talks. However, such analysis is challenging due to the difficulties of analyzing multimodal video collections consisted of frame images, text, and metadata. This paper proposes a visual analytic system to analyze multimodal content in video collections. The system features three views at different levels: the Projection view with novel glyphs to facilitate cluster analysis regarding presentation styles; the Comparison View to present temporal distribution and concurrences of presentation techniques and support intra-cluster analysis; and the Video View to enable contextualized exploration of a video. We conduct a case study with language education experts and university students to provide anecdotal evidence about the effectiveness of our approach, and report new findings about presentation techniques in TED Talks. Quantitative feedback from a user study confirms the usefulness of our visual system for multimodal analysis of video collections.", "uri": "https://vimeo.com/364568236", "name": "[VIS19 Preview] Multimodal Analysis of Video Collections: Visual Exploration of Presentation Techniques in TED Talks (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:53:07+00:00", "description": "Authors: Zhaosong Huang, Yafeng Lu, Elizabeth A. Mack, Wei Chen, Ross Maciejewski\n\nAbstract: The choropleth map is an essential tool for spatial data analysis. However, the underlying attribute values of a spatial unit greatly influence the statistical analyses and map classification procedures when generating a choropleth map. If the attribute values incorporate a range of uncertainty, a critical task is determining how much the uncertainty impacts both the map visualization and the statistical analysis. In this paper, we present a visual analytics system that enhances our understanding of the impact of attribute uncertainty on data visualization and statistical analyses of these data. Our system consists of a parallel coordinates-based uncertainty specification view, an impact river and impact matrix visualization for region-based and simulation-based analysis, and a dual-choropleth map and t-SNE plot for visualizing the changes in classification and spatial autocorrelation over the range of uncertainty in the attribute values. We demonstrate our system through three use cases illustrating the impact of attribute uncertainty in geographic analysis.", "uri": "https://vimeo.com/364568169", "name": "[VIS19 Preview] Exploring the Sensitivity of Choropleths under Attribute Uncertainty (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:52:15+00:00", "description": "Authors: Jie Li, Siming Chen, Wei Chen, Gennady Andrienko, Natalia Andrienko\n\nAbstract: We propose an approach to analyzing data in which texts are associated with spatial and temporal references with the aim to understand how the text semantics vary over space and time. To represent the semantics, we apply probabilistic topic modeling. After extracting a set of topics and representing the texts by vectors of topic weights, we aggregate the data into a data cube with the dimensions corresponding to the set of topics, the set of spatial locations (e.g., regions), and the time divided into suitable intervals according to the scale of the planned analysis. Each cube cell corresponds to a combination (topic, location, time interval) and contains aggregate measures characterizing the subset of the texts concerning this topic and having the spatial and temporal references within these location and interval. Based on this structure, we systematically describe the space of analysis tasks on exploring the interrelationships among the three heterogeneous information facets, semantics, space, and time. We introduce the operations of projecting and slicing the cube, which are used to decompose complex tasks into simpler subtasks. We then present a design of a visual analytics system intended to support these subtasks. To reduce the complexity of the user interface, we apply the principles of structural, visual, and operational uniformity while respecting the specific properties of each facet. The aggregated data are represented in three parallel views corresponding to the three facets and providing different complementary perspectives on the data. The views have similar look-and-feel to the extent allowed by the facet specifics. Uniform interactive operations applicable to any view support establishing links between the facets. The uniformity principle is also applied in supporting the projecting and slicing operations on the data cube. We evaluate the feasibility and utility of the approach by applying it in two analysis scenarios using geolocated social media data for studying people\u2019s reactions to social and natural events of different spatial and temporal scales.", "uri": "https://vimeo.com/364568102", "name": "[VIS19 Preview] Semantics-Space-Time Cube: A Conceptual Framework for Systematic Analysis of Texts in Space and Time (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:51:55+00:00", "description": "Authors: Yuxin Ma ; Anthony K. H. Tung ; Wei Wang ; Xiang Gao ;Zhigeng Pan ; Wei Chen\n\nAbstract: Similarity measuring methods are widely adopted in a broad range of visualization applications. In this work, we address the challenge of representing human perception in the visual analysis of scatterplots by introducing a novel deep-learning-based approach, ScatterNet, captures perception-driven similarities of such plots. The approach exploits deep neural networks to extract semantic features of scatterplot images for similarity calculation. We create a large labeled dataset consisting of similar and dissimilar images of scatterplots to train the deep neural network. We conduct a set of evaluations including performance experiments and a user study to demonstrate the effectiveness and efficiency of our approach. The evaluations confirm that the learned features capture the human perception of scatterplot similarity effectively. We describe two scenarios to show how ScatterNet can be applied in visual analysis applications.", "uri": "https://vimeo.com/364568078", "name": "[VIS19 Preview] ScatterNet: A Deep Subjective Similarity Model for Visual Analysis of Scatterplots (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:51:32+00:00", "description": "Authors: Connor C. Gramazio, Jeff Huang, David H. Laidlaw\n\nAbstract: We show how mouse interaction log classification can help visualization toolsmiths understand how their tools are used \u201cin the wild\u201d through an evaluation of MAGI \u2013 a cancer genomics visualization tool. Our primary contribution is an evaluation of twelve visual analysis task classifiers, which compares predictions to task inferences made by pairs of genomics and visualization experts. Our evaluation uses common classifiers that are accessible to most visualization evaluators: k-nearest neighbors, linear support vector machines, and random forests. By comparing classifier predictions to visual analysis task inferences made by experts, we show that simple automated task classification can have up to 73% accuracy and can separate meaningful logs from \u201cjunk\u201d logs with up to 91% accuracy. Our second contribution is an exploration of common MAGI interaction trends using classification predictions, which expands current knowledge about ecological cancer genomics visualization tasks. Our third contribution is a discussion of how automated task classification can inform iterative tool design. These contributions suggest that mouse interaction log analysis is a viable method for (1) evaluating task requirements of client-side-focused tools, (2) allowing researchers to study experts on larger scales than is typically possible with in-lab observation, and (3) highlighting potential tool evaluation bias.", "uri": "https://vimeo.com/364568057", "name": "[VIS19 Preview] An Analysis of Automated Visual Analysis Classification: Interactive Visualization Task Inference of Cancer...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:51:13+00:00", "description": "Authors: Hua Guo, David H. Laidlaw\n\nAbstract: This work analyzes sensemaking frameworks and experiments with an iteratively designed visual analysis tool to identify design implications for facilitating research idea generation using visualizations. Our tool, ThoughtFlow, structures and visualizes literature collections using topic models to bridge the information gap between core activities during research ideation. To help users stay focused on a topic while discovering relevant documents, we designed and analyzed usage patterns for two types of embedded visualization that help determine document relevance while minimizing distraction. We analyzed how research ideation outcomes and processes differ when using ThoughtFlow and conventional search engines by augmenting insight-based evaluation with concept-map analysis. Our results suggest that operations afforded by topic models match well with later ideation stages when coherent topics have emerged, but not with early stages when users are still relying heavily on individual keywords to gather background knowledge. We also present qualitative evidence that citation sparklines encourage more exploration of recommended references, and that a preference for paper thumbnails may depend on the consistency between the evidence and the current mental frame.", "uri": "https://vimeo.com/364568027", "name": "[VIS19 Preview] Topic-based Exploration and Embedded Visualizations for Research Idea Generation (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:50:56+00:00", "description": "Authors: Salman Mahmood, Klaus Mueller\n\nAbstract: Organizing multivariate data spaces by their dimensions or attributes can be a rather difficult task. Most of the work in this area focuses on the statistical aspects such as correlation clustering, dimension reduction, and the like. These methods typically produce hierarchies in which the leaf nodes are labeled by the attribute names while the inner nodes are often represented by just a statistical measure and criterion, such as a threshold. This makes them difficult to understand for mainstream users. Taxonomies in science, biology, engineering, etc. on the other hand, are easy to comprehend since they provide meaningful labels at the inner nodes as well. Labeling inner nodes of taxonomies automatically requires the identification of hypernyms. Our proposed framework, called Taxonomizer, takes a visual analytics approach to meet this challenge. It appeals to the wisdom of humans to liaise with state of the art data analytics, neural word embeddings, and lexical databases. It consists of a set of visual tools that starts out with an automatically computed hierarchy where the leaf nodes are the original data attributes, and it then allows users to sculpt high-quality taxonomies for any multivariate dataset.", "uri": "https://vimeo.com/364568009", "name": "[VIS19 Preview] Taxonomizer: A Visual Analytics Framework for Constructing Fully Labeled Hierarchies from Multivariate Data...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:50:35+00:00", "description": "Authors: Maoyuan Sun, Jian Zhao, Hao Wu, Kurt Luther, Chris\nNorth, Naren Ramakrishnan\n\n\nAbstract: Exploring coordinated relationships (e.g., shared\nrelationships between two sets of entities) is an important analytics\ntask in a variety of real-world applications, such as discovering\nsimilarly behaved genes in bioinformatics, detecting malware collusions\nin cyber security, and identifying products bundles in marketing\nanalysis. Coordinated relationships can be formalized as biclusters. In\norder to support visual exploration of biclusters, bipartite graphs\nbased visualizations have been proposed, and edge bundling is used to\nshow biclusters. However, it suffers from edge crossings due to possible\noverlaps of biclusters, and lacks in-depth understanding of its impact\non user exploring biclusters in bipartite graphs. To address these, we\npropose a novel bicluster-based seriation technique that can reduce edge\ncrossings in bipartite graphs drawing and conducted a user experiment to\nstudy the effect of edge bundling and this proposed technique on\nvisualizing biclusters in bipartite graphs. We found that they both had\nimpact on reducing entity visits for users exploring biclusters, and\nedge bundles helped them find more justified answers. Moreover, we\nidentified four key trade-offs that inform the design of future\nbicluster visualizations. The study results suggest that edge bundling\nis critical for exploring biclusters in bipartite graphs, which helps to\nreduce low-level perceptual problems and support high-level inferences.", "uri": "/channels/721847https://vimeo.com/364567994", "name": "[VIS19 Preview] The Effect of Edge Bundling and Seriation on Sensemaking of Biclusters in Bipartite Graphs (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:50:17+00:00", "description": "Authors: Jie Li, Siming Chen, Kang Zhang, Gennady Andrienko, and Natalia Andrienko\n\n\nAbstract: Spatial time series is a common type of data dealt with in many domains, such as economic statistics and environmental science. There have been many studies focusing on finding and analyzing various kinds of events in time series; the term \u2018event\u2019 refers to significant changes or occurrences of particular patterns formed by consecutive attribute values. We focus on a further step in event analysis: discover temporal relationship patterns between event locations, i.e., repeated cases when there is a specific temporal relationship (same time, before, or after) between events occurring at two locations. This can provide important clues for understanding the formation and spreading mechanisms of events and interdependencies among spatial locations. We propose a visual exploration framework COPE (Co-Occurrence Pattern Exploration), which allows users to extract events of interest from data and detect various co-occurrence patterns among them. Case studies and expert reviews were conducted to verify the effectiveness and scalability of COPE using two real-world datasets.", "uri": "https://vimeo.com/364567978", "name": "[VIS19 Preview] COPE: Interactive Exploration of Co-occurrence Patterns in Spatial Time Series (TVCG)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:39:30+00:00", "description": "Authors: JunYoung Choi, Sang-Eun Lee, Eunji Cho, Yutaro Kashiwagi, Shigeo Okabe, Sunghoe Chang, Won-Ki Jeong\n\nAbstract: Dendritic spines are submicron scale protrusions on neuronal dendrites that form the postsynaptic sites of excitatory neuronal inputs. The morphological changes of dendritic spines reflect alterations in physiological conditions and are further indicators of various neuropsychiatric conditions. However, due to the highly dynamic and heterogeneous nature of spines, accurate measurement and object analysis of spine morphology is a major challenge in neuroscience research. Here, we propose an interactive 3D dendritic spine analysis system that displays 3D rendering of spines and plots the high-dimensional features extracted from the 3D mesh of spines in three graph types (parallel coordinate plot, radar plot, and 2D scatter plot with t-Distributed Stochastic Neighbor Embedding). With this system, analysts can effectively explore and analyze the dendritic spine in a 3D manner with high-dimensional features. For the system, we have constructed a set of morphological high-dimensional features from the 3D mesh of dendritic spines.", "uri": "https://vimeo.com/364567245", "name": "[VIS19 Preview] Interactive Dendritic Spine Analysis Based on 3D Morphological Features (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-06T04:38:57+00:00", "description": "Authors: Nicholas Ruta, Naoko Sawada,  Katy McKeough,  Michael Behrisch,  Johanna Beyer\n\nAbstract: Comparing many long time series is challenging to do by hand. Clustering time series enables data analysts to discover relevance between and anomalies among multiple time series. However, even after reasonable clustering, analysts have to scrutinize correlations between clusters or similarities within a cluster. We developed SAX Navigator, an interactive visualization tool, that allows users to hierarchically explore global patterns as well as individual observations across large collections of time series data. Our visualization provides a unique way to navigate time series that involves a \u201cvocabulary of patterns\u201d developed by using a dimensionality reduction technique, Symbolic Aggregate approXimation (SAX). With SAX, the time series data clusters efficiently and is quicker to query at scale. We demonstrate the ability of SAX Navigator to analyze patterns in large time series data based on three case studies for an astronomy data set. We verify the usability of our system through a think-aloud study with an astronomy domain scientist.", "uri": "https://vimeo.com/364567202", "name": "[VIS19 Preview] SAX Navigator: Time Series Exploration Through Hierarchical Clustering (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-03T01:21:27+00:00", "description": "Authors: Emily Wall, Arup Arcalgud, KUHU GUPTA, Andrew Jo\n\nAbstract: Recently, Wall et al. proposed a set of computational metrics for quantifying cognitive bias based on user interaction sequences. The metrics rely on a Markov model to predict a user's next interaction based on the current interaction. The metrics characterize how a user's actual interactive behavior deviates from a theoretical baseline, where \"unbiased behavior\" was previously defined to be equal probabilities of all interactions. In this paper, we analyze the assumptions made of these metrics. We conduct a study in which participants, subject to anchoring bias, interact with a scatterplot to complete a categorization task. Our results indicate that, rather than equal probabilities of all interactions, unbiased behavior across both bias conditions can be better approximated by proximity of data points.", "uri": "https://vimeo.com/363954377", "name": "[VIS19 Preview] A Markov Model of Users' Interactive Behavior in Scatterplots (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-03T01:13:26+00:00", "description": "Authors: Supporters chairs\n\nAbstract:", "uri": "https://vimeo.com/363953375", "name": "[VIS19 Preview] VIS Supporters (cover-video-supporters)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-03T01:05:11+00:00", "description": "Authors: Jessica Magallanes, Lindsey van Gemeren, Steven Wood, Maria-Cruz Villa-Uriol\n\nAbstract: Event data is present in a variety of domains such as electronic health records, daily living activities and web clickstream records. Current visualization methods to explore event data focus on discovering sequential patterns but present limitations when studying time attributes in event sequences. Time attributes are especially important when studying waiting times or lengths of visit in patient flow analysis. We propose a visual analytics methodology that allows the identification of trends and outliers in respect of duration and time of occurrence in event sequences. The proposed method presents event data using a single Sequential and Time Patterns overview. User-driven alignment by multiple events, sorting by sequence similarity and a novel visual encoding of events allows the comparison of time trends across and within sequences. The proposed visualization allows the derivation of findings  that otherwise could not be obtained using traditional visualizations. The proposed  methodology has been applied to a real-world dataset provided by Sheffield Teaching Hospitals NHS Foundation Trust, for which four classes of conclusions were derived.", "uri": "https://vimeo.com/363952342", "name": "[VIS19 Preview] Analyzing Time Attributes in Temporal Event Sequences (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-03T01:01:44+00:00", "description": "Authors: David Pomerenke, Frederik L. Dennig, Daniel Keim, Johannes Fuchs, Michael Blumenschein\n\nAbstract: Parallel coordinates are a popular technique to visualize multi-dimensional data. However, they face a significant problem influencing the perception and interpretation of patterns. The distance between two parallel lines differs based on their slope. Vertical lines are rendered longer and closer to each other than horizontal lines. This problem is inherent in the technique and has two main consequences: (1) clusters which have a steep slope between two axes are visually more prominent than horizontal clusters. (2) Noise and clutter can be perceived as clusters, as a few parallel vertical lines visually emerge as a ghost cluster. Our paper makes two contributions:  First, we formalize the problem and show its impact. Second, we present a novel technique to reduce the effects by rendering the polylines of the parallel coordinates based on their slope: horizontal lines are rendered with the default width, lines with a steep slope with a thinner line. Our technique avoids density distortions of clusters, can be computed in linear time, and can be added on top of most parallel coordinate variations. To demonstrate the usefulness, we show examples and compare them to the classical rendering.", "uri": "https://vimeo.com/363951887", "name": "[VIS19 Preview] Slope-Dependent Rendering of Parallel Coordinates to Reduce Density Distortion and Ghost Clusters (short...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-03T00:59:29+00:00", "description": "Authors: Lindsey Sawatzky, Steven Bergner, Fred Popowich\n\nAbstract: Recurrent Neural Networks are an effective and prevalent tool used to model sequential data such as natural language text.\nHowever, their deep nature and massive number of parameters pose a challenge for those intending to study precisely how they work.\nWe present a visual technique that gives a high level intuition behind the semantics of the hidden states within Recurrent Neural Networks.\nThis semantic encoding allows for hidden states to be compared throughout the model independent of their internal details.\nThe proposed technique is displayed in a proof of concept visualization tool which is demonstrated to visualize the natural language processing task of language modelling.", "uri": "https://vimeo.com/363951597", "name": "[VIS19 Preview] Visualizing RNN States with Predictive Semantic Encodings (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:59:07+00:00", "description": "Authors: Ran Xu, Manu Mathew Thomas, Alex Leow, Olusola A. Ajilore, Angus G. Forbes\n\nAbstract: We introduce TempoCave, a novel visualization application for analyzing dynamic brain networks, or connectomes. TempoCave provides a range of functionality to explore metrics related to the activity patterns and modular affiliations of different regions in the brain. These patterns are calculated by processing raw data retrieved functional magnetic resonance imaging (fMRI) scans, which creates a network of weighted edges between each brain region, where the weight indicates how likely these regions are to activate synchronously. In particular, we support the analysis needs of clinical psychologists, who examine these modular affiliations and weighted edges and their temporal dynamics, utilizing them to understand relationships between neurological disorders and brain activity, which could have significant impact on the way in which patients are diagnosed and treated. We summarize the core functionality of TempoCave, which supports a range of comparative tasks, and runs both in a desktop mode and in an immersive mode. Furthermore, we present a real world use case that analyzes pre- and post-treatment connectome datasets from 27 subjects in a clinical study investigating the use of cognitive behavior therapy to treat major depression disorder, indicating that TempoCave can provide new insight into the dynamic behavior of the human brain.", "uri": "https://vimeo.com/363456447", "name": "[VIS19 Preview] TempoCave: Visualizing Dynamic Connectome Datasets to Support Cognitive Behavioral Therapy (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:58:51+00:00", "description": "Authors: Jieqiong Zhao, Morteza Karimzadeh, Ali Masjedi, Taojun Wang, Xiwen Zhang, Melba Crawford, David Ebert\n\nAbstract: Feature selection is used in machine learning to improve predictions, decrease computation time, reduce noise, and tune models based on limited sample data. In this article, we present FeatureExplorer, a visual analytics system that supports the dynamic evaluation of regression models and importance of feature subsets through the interactive selection of features in high-dimensional feature spaces typical of hyperspectral images. The interactive system allows users to iteratively refine and diagnose the model by selecting features based on their domain knowledge, interchangeable (correlated) features, feature importance, and the resulting model performance.", "uri": "https://vimeo.com/363456411", "name": "[VIS19 Preview] FeatureExplorer: Interactive Feature Selection and Exploration of Regression Models for Hyperspectral Images...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:56:51+00:00", "description": "Authors: Marti Hearst,Melanie Tory, Vidya Setlur\n\nAbstract: Natural language interfaces for data visualizations tools are growing in importance, but little research has been done on  how a system should respond to questions that contain vague modifiers like ``high'' and ``expensive.''  This paper makes a first step toward design guidelines  for this problem, based on existing research from cognitive linguistics and the results of a new empirical study with 274 crowdsourcing participants.  A comparison of four bar chart-based views finds that highlighting the top items according to distribution-sensitive values is preferred in most cases and is a good starting point as a design guideline.", "uri": "https://vimeo.com/363456115", "name": "[VIS19 Preview] Toward Interface Defaults for Vague Modifiers in Natural Language Interfaces for Visual Analysis (short...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:38:18+00:00", "description": "Authors: Xiangyun Lei, Fred Hohman, Duen Horng Chau, Andrew J Medford\n\nAbstract: In recent years, machine learning (ML) has gained significant popularity in the field of chemical informatics and electronic structure theory. These techniques often require researchers to engineer abstract \"features\" that encode chemical concepts into a mathematical form compatible with the input to machine-learning models. However, there is no existing tool to connect these abstract features back to the actual chemical system, making it difficult to diagnose failures and to build intuition about the meaning of the features. We present ElectroLens, a new visualization tool for high-dimensional spatially-resolved features to tackle this problem. The tool visualizes high-dimensional data sets for atomistic and electron environment features by a series of linked 3D views and 2D plots. The tool is able to connect different derived features and their corresponding regions in 3D via interactive selection. It is built to be scalable, and integrate with existing infrastructure.", "uri": "https://vimeo.com/363453632", "name": "[VIS19 Preview] ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:37:51+00:00", "description": "Authors: Hayder Mahdi Al-maneea, Jonathan C Roberts\n\nAbstract: We present initial results of a quantitative analysis of how developers layout the visualisations in their multiple view systems. Many developers create multiple view systems and the technique is commonly used by the visualisation community. Each visualisation shows data in a different way, and often user interaction is coordinated between the views. But it is not always clear to know how many views a developer should use, or what would be the best layout. We extract images of visualisation tools, across TVCG journal, conference, posters and workshop papers 2012-2018 to analyse the quantity and layout of the views in these visualisation systems. Focusing on view juxtaposition, we code the layout of 491 images and analyse view topology in juxtaposed views. Our analysis acts as a starting point to help designers create better visualisations, acts as a taxonomy of visualisation layouts, and provides a quantitative analysis of how many views developers have used in their visualisation systems.", "uri": "https://vimeo.com/363453583", "name": "[VIS19 Preview] Towards quantifying multiple view layouts in visualisation as seen from research publications (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:37:26+00:00", "description": "best paper\n\nAuthors: Bryce Morrow, Trevor Manz, Arlene E. Chung, Nils Gehlenborg, David Gotz\n\nAbstract: Patterns in temporal data can often be found across different scales, such as days, weeks, and months, making effective visualization of time-based data challenging. Here we propose a new approach for providing focus and context in time-based charts to enable interpretation of patterns across time scales. Our approach employs a focus zone with a time and a second axis, that can either represent quantities or categories, as well as a set of adjacent periphery plots that can aggregate data along the time, value, or both dimensions. We present a framework for periphery plots and describe two use cases that demonstrate the utility of our approach.", "uri": "https://vimeo.com/363453522", "name": "[VIS19 Preview] Periphery Plots for Contextualizing Heterogeneous Time-Based Charts (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:33:37+00:00", "description": "Authors: Amon Ge, Hyeju Jang, Giuseppe Carenini, Kendall Ho, Young Ji Lee\n\nAbstract: Evaluating topic modeling results requires communication between domain  and NLP experts. OCTVis is a visual interface to compare the quality of two topic models when mapped against a domain ontology. Its design is based on detailed data and task models, and was tested in a case study in the healthcare domain.", "uri": "https://vimeo.com/363452963", "name": "[VIS19 Preview] OCTVis: Ontology-based Comparison of Topic Models (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:33:16+00:00", "description": "Authors: Jonas Karlsson, Marwan Abdellah,Sebastien Speierer,Alessandro Enrico Foni,Samuel Lapere, Felix Schurmann\n\nAbstract: We explore a first proof-of-concept application for visualizing large scale digitally reconstructed brain circuitry using signed distance functions. The significance of our method is demonstrated in comparison with using implicit geometry that is limited to provide the natural look of neurons or explicit geometry that requires huge amounts of memory and has limited scalability with larger circuits.", "uri": "https://vimeo.com/363452920", "name": "[VIS19 Preview] High Fidelity Visualization of Large Scale Digitally Reconstructed Brain Circuitry with Signed Distance...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:28:51+00:00", "description": "Authors: Tanja Munz, Michael Burch, Toon van Benthem, Yoeri Poels, Fabian Beck,  Daniel Weiskopf\n\nAbstract: Generalized Pythagoras trees were developed for visualizing hierarchical data, producing organic, fractal-like representations. However, the drawback of the original layout algorithm is visual overlap of tree branches. To avoid such overlap, we introduce an adapted drawing algorithm using ellipses instead of circles to recursively place tree nodes representing the subhierarchies. Our technique is demonstrated by resolving overlap in diverse real-world and generated datasets, while comparing the results to the original approach.", "uri": "https://vimeo.com/363452307", "name": "[VIS19 Preview] Overlap-free Drawing of Generalized Pythagoras Trees for Hierarchy Visualization (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:28:39+00:00", "description": "Authors: Huan He, Bo Dong, Qinghua Zheng, Dehai Di, Yating Lin\n\nAbstract: Self-paced online learning not only provides the opportunities of learning anytime but also chanllenges students' time management, especially in the context of learning multiple courses at same time. The inappropriate scheduling of multiple courses may affect student engagement and learning performance, thus how to arrange the study time of multiple courses is a concern of both instructors and students. Existing studies related to student engagement and time management in online learning mainly focus on providing self-regulated learning strategies and evaluating learning performance. However, these methods have limited abilities to gain intuitive understanding of the time management of multi-course learning. To address this issue, we present LearnerVis to help users analyze how students schedule their multi-course learning. LearnerVis visualize the temporal features of learning process, and it enables users to customize student groups to compare the differences in student engagement and time management. A case study is conducted to demonstrate the usefulness of the system with real-word dataset.", "uri": "https://vimeo.com/363452286", "name": "[VIS19 Preview] Visual Analysis of the Time Management of Learning Multiple Courses in Online Learning Environment (short...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:28:15+00:00", "description": "honorable mention\n\nAuthors: Pavel Govyadinov, Tasha Womack, Jason Eriksen, David Mayerich, Guoning Chen\n\nAbstract: Microvessels are frequent targets for research into tissue development and disease progression. These complex and subtle differences between networks are currently difficult to visualize, making sample comparisons subjective and difficult to quantify. These challenges are due to the structure of microvascular networks, which are sparse but space-filling. This results in a complex and interconnected mesh that is difficult to represent and impractical to interpret using conventional visualization techniques. We develop a \\textbf{bi-modal} visualization framework, leveraging graph-based and geometry-based techniques to achieve interactive visualization of microvascular networks. This framework allows researchers to objectively interpret the complex and subtle variations that arise when comparing microvascular networks.", "uri": "https://vimeo.com/363452229", "name": "[VIS19 Preview] Graph-assisted Visualization of Microvascular Networks (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:27:28+00:00", "description": "Authors: Matthias Miller, Xuan Zhang,Johannes Fuchs, Micahel Blumenschein\n\nAbstract: Star glyphs are a well-researched visualization technique to represent multi-dimensional data. They are often used in small multiple settings for a visual comparison of many data points. However, their overall visual appearance is strongly influenced by the ordering of dimensions. To this end, two orthogonal categories of layout strategies are proposed in the literature: order dimensions by similarity to get homogeneously shaped glyphs vs. order by dissimilarity to emphasize spikes and salient shapes. While there is evidence that salient shapes support clustering tasks, evaluation, and direct comparison of data-driven ordering strategies has not received much research attention. We contribute an empirical user study to evaluate the efficiency, effectiveness, and user confidence in visual clustering tasks using star glyphs. In comparison to similarity-based ordering, our results indicate that dissimilarity-based star glyph layouts support users better in clustering tasks, especially when clutter is present.", "uri": "https://vimeo.com/363452113", "name": "[VIS19 Preview] Evaluating Ordering Strategies of Star Glyph Axes (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:27:15+00:00", "description": "Authors: Nate Morrical,Will Usher, Ingo Wald, Valerio Pascucci\n\nAbstract: Sample based ray marching is an effective method for direct volume rendering of unstructured meshes. However, sampling such meshes remains expensive, and strategies to reduce the number of samples taken have received relatively little attention. In this paper, we introduce a method for rendering unstructured meshes using a combination of a coarse spatial acceleration structure and hardware-accelerated ray tracing. Our approach enables efficient empty space skipping and adaptive sampling of unstructured meshes, and outperforms a reference ray marcher by up to 7 times.", "uri": "https://vimeo.com/363452086", "name": "[VIS19 Preview] Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes using Hardware Accelerated Ray...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:27:02+00:00", "description": "Authors: Micha\u00ebl Aupetit, Michael Sedlmair, Mostafa M. Abbas, Abdelkader Baggag, Hamad Bin Khalifa, Halima Bensmail\n\nAbstract: Automatic clustering techniques play a central role in Visual Analytics\nby helping analysts to discover interesting patterns in high-dimensional\ndata. Evaluating these clustering techniques, however,\nis difficult due to the lack of universal ground truth. Instead,\nclustering approaches are usually evaluated based on a subjective\nvisual judgment of low-dimensional scatterplots of different datasets.\nAs clustering is an inherent human-in-the-loop task, we propose a\nmore systematic way of evaluating clustering algorithms based on\nquantification of human perception of clusters in 2D scatterplots.\nThe core question we are asking is in how far existing clustering\ntechniques align with clusters perceived by humans. To do so, we\nbuild on a dataset from a previous study [1], in which 34 human\nsubjects labeled 1000 synthetic scatterplots in terms of whether they\ncould see one or more than one cluster. Here, we use this dataset to\nbenchmark state-of-the-art clustering techniques in terms of how far\nthey agree with these human judgments. More specifically, we assess\n1437 variants of K-means, Gaussian Mixture Models, CLIQUE,\nDBSCAN, and Agglomerative Clustering techniques on these benchmarks\ndata. We get unexpected results. For instance, CLIQUE and\nDBSCAN are at best in slight agreement on this basic cluster counting\ntask, while model-agnostic Agglomerative clustering can be\nup to a substantial agreement with human subjects depending on the\nvariants. We discuss how to extend this perception-based clustering\nbenchmark approach, and how it could lead to the design of\nperception-based clustering techniques that would better support\nmore trustworthy and explainable models of cluster patterns.", "uri": "https://vimeo.com/363452065", "name": "[VIS19 Preview] Toward Perception-based Evaluation of Clustering Techniques for Visual Analytics (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:26:51+00:00", "description": "Authors: Yixuan Zhang,Sara Di Bartolomeo,Fangfang Sheng,Holly Jimison,Cody Dunne\n\nAbstract: Composite temporal event sequence visualizations have included sentinel event alignment techniques to cope with data volume and variety. Prior work has demonstrated the utility of using single-event alignment for understanding the precursor, co-occurring, and aftereffect events surrounding a sentinel event. However, the usefulness of single-event alignment has not been sufficiently evaluated in composite visualizations. Furthermore, recently proposed dual-event alignment techniques have not been empirically evaluated. In this work, we designed tasks around temporal event sequence and timing analysis and conducted a controlled experiment on Amazon Mechanical Turk to examine four sentinel event alignment approaches: no sentinel event alignment (NoAlign), single-event alignment (SingleAlign), dual-event alignment with left justification (DualLeft), and dual-event alignment with stretch justification (DualStretch). Differences between approaches were most pronounced with more rows of data. For understanding intermediate events between two sentinel events, dual-event alignment was the clear winner for correctness---71% vs. 18% for NoAlign and SingleAlign. For understanding the duration between two sentinel events, NoAlign was the clear winner: correctness---88% vs. 36% for DualStretch---completion time---55 seconds vs. 101 seconds for DualLeft---and error---1.5% vs. 8.4% for DualStretch. For understanding precursor and aftereffect events, there was no significant difference among approaches. A free copy of this paper, the evaluation stimuli and data, and source code are available at https://osf.io/78fs5", "uri": "https://vimeo.com/363452044", "name": "[VIS19 Preview] Evaluating Alignment Approaches in Superimposed Time-series and Temporal Event-sequence Visualizations...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:26:37+00:00", "description": "Authors: Jian Zhao, Maoyuan Sun, Francine Chen, Patrick Chiu\n\nAbstract: The analysis of bipartite networks is critical in a variety of application domains, such as exploring entity co-occurrences in intelligence analysis and investigating gene expression in bio-informatics. One important task is missing link prediction, which infers the existence of unseen links based on currently observed ones. In this paper, we propose MissBiN that involves analysts in the loop for making sense of link prediction results. MissBiN combines a novel method for link prediction and an interactive visualization for examining and understanding the algorithm outputs. Further, we conducted quantitative experiments to assess the performance of the proposed link prediction algorithm, and a case study to evaluate the overall effectiveness of MissBiN.", "uri": "https://vimeo.com/363452011", "name": "[VIS19 Preview] MissBiN: Visual Analysis of Missing Links in Bipartite Networks (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:26:22+00:00", "description": "Authors: Hwiyeon Kim, Juyoung Oh, Yunha Han,  Sungahn Ko, Matthew Brehmer, Bum Chul Kwon\n\nAbstract: When people browse online news, small thumbnail images accompanying links to articles attract their attention and help them to decide which articles to read. As an increasing proportion of online news can be construed as data journalism, we have witnessed a corresponding increase in the incorporation of visualization in article thumbnails. However, there is little research to support alternative design choices for visualization thumbnails, which include resizing, cropping, simplifying, and embellishing charts appearing within the body of the associated article. We therefore sought to better understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. This paper presents our findings from a survey of visualization thumbnails collected online and from conversations with data journalists and news graphics designers. Our study reveals that there exists an uncharted design space, one that is in need of further empirical study. Our work can thus be seen as a first step toward providing structured guidance on how to design thumbnails for data stories.", "uri": "https://vimeo.com/363451965", "name": "[VIS19 Preview] Thumbnails for Data Stories: A Survey of Current Practices (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:26:07+00:00", "description": "Authors: Emily Wall, John Stasko, Alex Endert\n\nAbstract: The use of cognitive heuristics often leads to fast and effective decisions. However, they can also systematically and predictably lead to errors known as cognitive biases. Strategies for minimizing or mitigating these biases, however, remain largely non-technological (e.g., training courses). The growing use of visual analytic (VA) tools for analysis and decision making enables a new class of bias mitigation strategies. In this work, we explore the ways in which the design of visualizations (vis) may be used to mitigate cognitive biases. We derive a design space comprised of 8 dimensions that can be manipulated to impact a user's cognitive and analytic processes and describe them through an example hiring scenario. This design space can be used to guide and inform future vis systems that may integrate cognitive processes more closely.", "uri": "https://vimeo.com/363451921", "name": "[VIS19 Preview] Towards a Design Space for Mitigating Cognitive Bias in Visual Analytics (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-10-01T01:25:53+00:00", "description": "Authors: Beno\u00eet Colange, Laurent Vuillon, Sylvain Lespinats, Denys Dutykh\n\nAbstract: To perform visual data exploration, many dimensionality reduction\nmethods have been developed. These tools allow data analysts to represent\nmultidimensional data in a 2D or 3D space, while preserving\nas much relevant information as possible. Yet, they cannot preserve\nall structures simultaneously and they induce some unavoidable distortions.\nHence, many criteria have been introduced to evaluate a\nmap\u2019s overall quality, mostly based on the preservation of neighbourhoods.\nSuch global indicators are currently used to compare several\nmaps, which helps to choose the most appropriate mapping method\nand its hyperparameters. However, those aggregated indicators tend\nto hide the local repartition of distortions. Thereby, they need to be\nsupplemented by local evaluation to ensure correct interpretation of\nmaps.\nIn this paper, we describe a new method, called MING, for \u201cMap\nInterpretation using Neighbourhood Graphs\u201d. It offers a graphical\ninterpretation of pairs of map quality indicators, as well as local\nevaluation of the distortions. This is done by displaying on the map\nthe nearest neighbours graphs computed in the data space and in the\nembedding. Shared and unshared edges exhibit reliable and unreliable\nneighbourhood information conveyed by the mapping. By this\nmean, analysts may determine whether proximity (or remoteness) of\npoints on the map faithfully represents similarity (or dissimilarity)\nof original data, within the meaning of a chosen map quality criteria.\nWe apply this approach to two pairs of widespread indicators: precision/\nrecall and trustworthiness/continuity, chosen for their wide use\nin the community, which will allow an easy handling by users.", "uri": "https://vimeo.com/363451896", "name": "[VIS19 Preview] Interpreting Distortions in Dimensionality Reduction by Superimposing Neighbourhood Graphs (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:29:14+00:00", "description": "Authors: Anamaria Crisan, Tamara Munzner\n\nAbstract: Domain experts are inundated with new and heterogeneous types of data and require better and more specific types of data visualization systems to help them. In this paper, we consider the data landscape that domain experts seek to understand, namely the set of datasets that are either currently available or could be obtained. Experts need to understand this landscape to triage which data analysis projects might be viable, out of the many possible research questions that they could pursue. We identify data reconnaissance and task wrangling as processes that experts undertake to discover and identify sources of data that could be valuable for some specific analysis goal. These processes have thus far not been formally named or defined by the research community. We provide formal definitions of data reconnaissance and task wrangling and describe how they relate to the data landscape that domain experts must uncover. We propose a conceptual framework with a four-phase cycle of acquire, view, assess, and pursue that occurs within three distinct chronological stages, which we call fog and friction, informed data ideation, and demarcation of final data. Collectively, these four phases embedded within three temporal stages delineate an expert's progressively evolving understanding of the data landscape. We describe and provide concrete examples of these processes within the visualization community through an initial systematic analysis of previous design studies, identifying situations where there is evidence that they were at play.  We also comment on the response of domain experts to this framework, and suggest design implications stemming from these processes to motivate future research directions. As technological changes will only keep adding unknown terrain to the data landscape, data reconnaissance and task wrangling are important processes that need to be more widely understood and supported by the data visualization tools. By articulating a concrete understanding of this challenge and its implications, our work impacts the design and evaluation of data visualization systems.", "uri": "https://vimeo.com/363042588", "name": "[VIS19 Preview] Uncovering Data Landscapes through Data Reconnaissance and Task Wrangling (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:28:52+00:00", "description": "Authors: Duong Nguyen, Lei Zhang, Robert S. Laramee, David Thompson,  Rodolfo Ostilla Monico,Guoning Chen\n\nAbstract: This work proposes to analyze the time-dependent characteristics of the physical attributes measured along pathlines derived from unsteady flows, which can be represented as a series of time activity curves (TAC). A new TAC-based unsteady flow visualization and analysis framework is proposed. The center of this framework is a new event-based distance metric (EDM) that compares the similarity of two TACs, from which a new spatio-temporal, hierarchical clustering of pathlines based on their physical attributes and an attribute-based pathline exploration are proposed. These techniques are integrated into a visual analytics system, which has been applied to a number of unsteady flow in 2D and 3D to demonstrate its utility.", "uri": "https://vimeo.com/363042555", "name": "[VIS19 Preview] Unsteady Flow Visualization via Physics based Pathline Exploration (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:28:29+00:00", "description": "Authors: Mariana Shimabukuro, Jessica Zipf, Mennatallah El-Assady, Christopher Collins\n\nAbstract: This paper presents a visualization technique for cross-linguistic error analysis in large learner corpora. H-Matrix combines a matrix, which is commonly used by linguists to investigate cross-linguistic patterns, with a tree diagram to aggregate and interactively re-weight the importance of matrix rows to create custom investigative views. Our technique can help experts to perform data operations, such as, feature aggregation, filtering, ordering and language comparison interactively without having to reprocess the data. H-Matrix dynamically links the high-level multi-language overview to the extracted textual examples, and a reading view where linguists can see the detected features in context, confirm and generate hypotheses.", "uri": "https://vimeo.com/363042512", "name": "[VIS19 Preview] H-Matrix: Hierarchical Matrix for Visual Analysis of Cross-Linguistic Features in Large Learner Corpora...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:27:42+00:00", "description": "Authors: Mallika Agarwal, Arjun Srinivasan, John Stasko\n\nAbstract: An increasing number of data visualization tools are being designed for touch-based devices ranging from smartwatches to large wall-sized displays. While most of these tools have focused on exploring novel techniques to manually specify visualizations, recent touch-based visualization systems have begun to explore interface and interaction techniques for attribute-based visualization recommendations as a way to aid users (particularly novices) during data exploration. Advancing this line of work, we present a visualization system, VisWall, that enables visual data exploration in both single user and co-located collaborative settings on large touch displays. Coupling the concepts of direct combination and derivable visualizations, VisWall enables rapid construction of multivariate visualizations using attributes of previously created visualizations. By blending visualization recommendations and naturalistic interactions, VisWall seeks to help users visually explore their data by allowing them to focus more on aspects of the data (particularly, data attributes) rather than specifying and reconfiguring visualizations. We discuss the design, interaction techniques, and operations employed by VisWall along with a scenario of how these can be used to facilitate various tasks during visual data exploration.", "uri": "https://vimeo.com/363042417", "name": "[VIS19 Preview] VisWall: Visual Data Exploration using Direct Combination on Large Touch Displays (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:26:52+00:00", "description": "Authors: Marti Hearst, Melanie Tory\n\nAbstract: Conversational interfaces, such as chatbots, are increasing in prevalence, and have been shown to be preferred by and help users to complete tasks more efficiently than standard web interfaces in some cases.  However, little is understood about if and how information should be visualized during the course of an interactive conversation.  \nThis paper describes  studies in which participants report their preferences for viewing visualizations in  chat-style interfaces when answering questions about comparisons and trends.  We find a significant split in preferences among participants; approximately 40% prefer not to see charts and graphs in the context of  a conversational interface.  For those who do prefer to see charts, most preferred to see additional supporting context beyond the direct answer to the question.  These  results  have important ramifications for the design of conversational interfaces to data.", "uri": "https://vimeo.com/363042343", "name": "[VIS19 Preview] Would You Like A Chart With That? Incorporating Visualizations into Conversational Interfaces (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:26:27+00:00", "description": "Authors: Stephen Redmond\n\nAbstract: Pie charts were first published in 1801 by William Playfair and have caused some controversy since. Despite the suggestions of many experts against their use, several empirical studies have shown that pie charts are at least as good as alternatives. From Brinton to Few on one side and Eells to Kosara on the other, there appears to have been a hundred-year war waged on the humble pie. In this paper a set of experiments are reported that compare the performance of pie charts and horizontal bar charts with various visual cues. Amazon\u2019s Mechanical Turk service was employed to perform the tasks of estimating segments in various part-to-whole charts. The results lead to recommendations for data visualization professionals in developing dashboards.", "uri": "https://vimeo.com/363042298", "name": "[VIS19 Preview] Visual cues in estimation of part-to-whole comparison (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:26:02+00:00", "description": "Authors: Khairi Reda, Michael E. Papka\n\nAbstract: Color mapping is a commonly used technique for visualizing scalar fields. While there exists advice for choosing effective colormaps, it is unclear if current guidelines apply equally across task types. We study the perception of gradients and evaluate the effectiveness of three colormaps at depicting gradient magnitudes. In a crowdsourced experiment, we determine the just-noticeable differences (JNDs) at which participants can reliably compare and judge variations in gradient between two scalar fields. We find that participants exhibited lower JNDs with a diverging (cool-warm) or a spectral (rainbow) scheme, as compared with a monotonic-luminance colormap (viridis). The results support a hypothesis that apparent discontinuities in the color ramp may help viewers discern subtle structural differences in gradient. We discuss these findings and highlight future research directions for colormap evaluation.", "uri": "https://vimeo.com/363042264", "name": "[VIS19 Preview] Evaluating Gradient Perception in Color-Coded Scalar Fields (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:25:28+00:00", "description": "Authors: Jaemin Jo, Jinwook Seo\n\nAbstract: We present a data-driven approach to obtain a disentangled and interpretable representation that can characterize bivariate data distributions of scatterplots. We first collect tabular datasets from the Web and build a training corpus consisting of over one million scatterplot images. Then, we train a state-of-the-art disentangling model, \u03b2-variational autoencoder, to derive a disentangled representation of the scatterplot images. The main output of this work is a list of 32 representative features that can capture the underlying structures of bivariate data distributions. Through latent traversals, we seek for high-level semantics of the features and compare them to previous human-derived concepts such as scagnostics measures. Finally, using the 32 features as an input, we build a simple neural network to predict the perceptual distances between scatterplots that were previously scored by human annotators. We found Pearson\u2019s correlation coefficient between the predicted and perceptual distances was above 0.75, which indicates the effectiveness of our representation in the quantitative characterization of scatterplots.", "uri": "https://vimeo.com/363042207", "name": "[VIS19 Preview] Disentangled Representation of Data Distributions in Scatterplots (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:24:52+00:00", "description": "Authors: Fred Hohman, Arjun Srinivasan,  Steven Drucker\n\nAbstract: While machine learning (ML) continues to find success in solving previously-thought hard problems, interpreting and exploring ML models remains challenging. Recent work has shown that visualizations are a powerful tool to aid debugging, analyzing, and interpreting ML models. However, depending on the complexity of the model (e.g., number of features), interpreting these visualizations can be difficult and may require additional expertise. Alternatively, textual descriptions, or verbalizations, can be a simple, yet effective way to communicate or summarize key aspects about a model, such as the overall trend in a model's predictions or comparisons between pairs of data instances. With the potential benefits of visualizations and verbalizations in mind, we explore how the two can be combined to aid ML interpretability. Specifically, we present a prototype system, TeleGam, that demonstrates how visualizations and verbalizations can collectively support interactive exploration of ML models, for example, generalized additive models (GAMs). We describe TeleGam's interface and underlying heuristics to generate the verbalizations. We conclude by discussing how TeleGam can serve as a platform to conduct future studies for understanding user expectations and designing novel interfaces for interpretable ML.", "uri": "https://vimeo.com/363042141", "name": "[VIS19 Preview] TeleGam: Combining Visualization and Verbalization for Interpretable Machine Learning (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:24:23+00:00", "description": "Authors: Qiong Zeng, Yinqiao Wang, Jian Zhang, Wenting Zhang, Changhe Tu,  Ivan Viola,  Yunhai Wang\n\nAbstract: Colormapping is an effective and popular visual representation to analyze data patterns for 2D scalar fields. Scientists usually adopt a default colormap and adjust it to fit data in a trial-and-error process. Even though a few colormap design rules and measures are proposed, there is no automatic algorithm to directly optimize a default colormap for better revealing spatial patterns hidden in unevenly distributed data, especially the boundary characteristics. To fill this gap, we conduct a pilot study with six domain experts and summarize three  requirements for automated  colormap adjustment. \nWe formulate the colormap adjustment as a nonlinear constrained optimization problem, and develop an efficient GPU-based implementation accompanying with a few interactions. We demonstrate the usefulness of our method with two  case studies", "uri": "https://vimeo.com/363042086", "name": "[VIS19 Preview] Data-Driven Colormap Optimization for 2D Scalar Field Visualization (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:23:52+00:00", "description": "Authors: Chiara Hergl,  Thomas Nagel, Olaf Kolditz,  Gerik Scheuermann\n\nAbstract: Many materials like wood, biological tissue, composites or rock have anisotropic mechanical properties. They become increasingly important in modern material, earth, and life sciences. \nThe stress-strain response of such materials can be characterized (to first-order) by the three-dimensional fourth-order stiffness tensor. \nThere are different anisotropy classes, i.e. material symmetries, that differ in the number and orientation of symmetry planes characteristic of the material. \nA three-dimensional fourth-order stiffness tensor of a hyperelastic material has up to 21 independent coefficients representing both moduli and orientation information which challenges any visualization method.\nTherefore, we use a fourth-order tensor decomposition to compute the anisotropy classes and the position of the corresponding symmetry planes.\nTo facilitate judgment of the significance of the amount of anisotropy, we construct an isotropic material.\nBased on these computations, we design a glyph that represents the stiffness tensor.\nWe demonstrate our method in a finite deformation setting of an initially isotropic hyperelastic material of Ogden class which is often modeling biological tissue.\nUpon deformation, the stiffness tensor can evolve along with its symmetry creating an inhomogeneous, unsteady fourth-order tensor field in three dimensions.", "uri": "https://vimeo.com/363042029", "name": "[VIS19 Preview] Visualization of Symmetries in Fourth-Order Stiffness Tensors (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:23:14+00:00", "description": "Authors: Robert Kosara\n\nAbstract: The long-standing assumption of angle as the primary visual cue used to read pie charts has recently been called into question. We conducted a controlled, preregistered study using parallel-projected 3D pie charts. Angle, area, and arc length differ dramatically when projected and change over a large range of values. Modeling these changes and comparing them to study participants' estimates allows us to rank the different visual cues by model fit. Area emerges as the most likely cue used to read pie charts.", "uri": "https://vimeo.com/363041944", "name": "[VIS19 Preview] Evidence for Area as the Primary Visual Cue in Pie Charts (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:22:00+00:00", "description": "Authors: Michail Schwab,  James Tompkin, Jeff Huang, Michelle A. Borkin\n\nAbstract: The creation of data visualizations has become easier as the skill-barrier to our tools has decreased. However, adding interactivity, such as gestures for pan and zoom, still requires significant coding expertise. We introduce an open-source library - EasyPZ.js - for the creation of multi-scale (pan and zoom) visualizations across desktop and mobile devices. EasyPZ is fully customizable and extendable with flexible options for interaction design. For example, it is easy to choose gestures which are compatible with selection interactions such as clicking. EasyPZ can be enabled on any SVG-based visualization on the web with one line of code, or by simply clicking a bookmark without requiring commitment to code changes. With this library, we provide ways for the visualization community to more easily author interactive multi-scale visualizations.", "uri": "https://vimeo.com/363041807", "name": "[VIS19 Preview] EasyPZ.js: Interaction Binding For Pan and Zoom Visualizations (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:21:22+00:00", "description": "Authors: Martin Reckziegel, Stefan J\u00e4nicke\n\nAbstract: Visually conveying time-dependent changes in tag maps is insufficiently addressed by current approaches.\nTypically, for each time range a tag map is determined, and the change between tag maps of subsequent time\nranges is progressively visualized. Our method compares tag maps locally in order to enable a continuous \ndisplay of geographical topic changes among subsequent time ranges. We further provide an alternate \ntag map variant focusing on frequency changes instead of relative frequency values to visualize the\ngeospatial-temporal rise and fall of topics.", "uri": "https://vimeo.com/363041739", "name": "[VIS19 Preview] Time Varying Predominance Tag Maps (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:20:41+00:00", "description": "Authors: Ulrik G\u00fcnther, Tobias Pietzsch, Aryaman Gupta, Kyle I. S. Harrington, Stefan Gumhold,  Pavel Tomancak, Ivo F. Sbalzarini\n\nAbstract: Life science today involves computational analysis of a large amount and variety of data, such as volumetric data acquired by state-of-the-art microscopes, or mesh data from analysis of such data or simulations. Visualization is often the first step in making sense of data, and a crucial part of building and debugging analysis pipelines. It is therefore important that visualizations can be quickly prototyped, as well as developed or embedded into full applications. In order to better judge spatiotemporal relationships, immersive hardware, such as Virtual or Augmented Reality (VR/AR) headsets and associated controllers are becoming invaluable tools. In this work we introduce scenery, a flexible VR/AR visualization framework for the Java VM that can handle mesh and large volumetric data, containing multiple views, timepoints, and color channels. scenery is free and open-source software, works on all major platforms, and uses the Vulkan or OpenGL rendering APIs. We introduce scenery's main features and example applications, such as its use in VR for microscopy, in the biomedical image analysis software Fiji, or for visualizing agent-based simulations.", "uri": "https://vimeo.com/363041672", "name": "[VIS19 Preview] scenery: Flexible Virtual Reality Visualization on the Java VM (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:19:30+00:00", "description": "Authors: Xin Fu,  Yun Wang, Haoyu Dong,  Weiwei Cui, Haidong Zhang\n\nAbstract: Researchers assess visualizations from multiple aspects, such as aesthetics, memorability, engagement, and efficiency. However, these assessments are mostly carried out through user studies. There is a lack of automatic visualization assessment approaches, which hinders further applications like visualization recommendation, indexing, and generation. In this paper, we propose automating the visualization assessment process with modern machine learning approaches. We utilize a semi-supervised learning method, which first employs Variational Autoencoder (VAE) to learn effective features from visualizations, subsequently training machine learning models for different assessment tasks. Then, we can automatically assess new visualization images by predicting their scores or rankings with the trained model. To evaluate our method, we run two different assessment tasks, namely, aesthetics and memorability, on different visualization datasets. Experiments show that our method can learn effective visual features and achieves good performance on these assessment tasks.", "uri": "https://vimeo.com/363041569", "name": "[VIS19 Preview] Visualization Assessment: A Machine Learning Approach (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:18:46+00:00", "description": "Authors: Alan Lundgard, Crystal Lee, Arvind Satyanarayan\n\nAbstract: Accessibility\u2014the process of designing for people with disabilities (PWD)\u2014is an important but under-explored challenge in the visualization research community. Without careful attention, and if PWD are not included as equal participants throughout the process, there is a danger of perpetuating a vision-first approach to accessible design that marginalizes the lived experience of disability (e.g., by creating overly simplistic \"sensory translations\" that map visual to non-visual modalities in a one-to-one fashion). In this paper, we present a set of sociotechnical considerations for research in accessible visualization design, drawing on literature in disability studies, tactile information systems, and participatory methods. We identify that using state-of-the-art technologies may introduce more barriers to access than they remove, and that expectations of research novelty may not produce outcomes well-aligned with the needs of disability communities. Instead, to promote a more inclusive design process, we emphasize the importance of clearly communicating goals, following existing accessibility guidelines, and treating PWD as equal participants who are compensated for their specialized skills. To illustrate how these considerations can be applied in practice, we discuss a case study of an inclusive design workshop held in collaboration with the Perkins School for the Blind.", "uri": "https://vimeo.com/363041501", "name": "[VIS19 Preview] Sociotechnical Considerations for Accessible Visualization Design (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:13:14+00:00", "description": "Authors: Linda Woodburn, Yalong Yang, Kim Marriott\n\nAbstract: We have compared three common visualisations for hierarchical quantitative data, treemaps, icicle plots and sunburst charts as well as a semicircular variant of sunburst charts we call the sundown chart. In a pilot study, we found that the sunburst chart was least preferred. In a controlled study with 12 participants, we compared treemaps, icicle plots and sundown charts. Treemap was the least preferred and had a slower performance on a basic navigation task and slower performance and accuracy in hierarchy understanding tasks. The icicle plot and sundown chart had similar performance with slight user preference for the icicle plot.", "uri": "https://vimeo.com/363040988", "name": "[VIS19 Preview] Interactive Visualisation of Hierarchical Quantitative Data: an Evaluation (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:12:44+00:00", "description": "Authors: Daniel Weidele\n\nAbstract: Parallel Coordinates are a popular data visualization technique for multivariate data. Dating back to as early as 1880 PC are nearly as old as John Snow's famous cholera outbreak map of 1855, which is  frequently regarded as a historic landmark for modern data visualization. Numerous extensions have been proposed to address integrity, scalability and readability. We make a new case to employ PC on conditional data, where additional dimensions are only unfolded if certain criteria are met in an observation. Compared to standard PC which operate on a flat set of dimensions the ontology of our input to Conditional Parallel Coordinates is of hierarchical nature. We therefore briefly review related work around hierarchical PC using aggregation or nesting techniques. Our contribution is a visualization to seamlessly adapt PC for conditional data under preservation of intuitive interaction patterns to select or highlight polylines. We conclude with intuitions on how to operate CPC on two data sets: an AutoML hyperparameter search log, and session results from a conversational agent.", "uri": "https://vimeo.com/363040925", "name": "[VIS19 Preview] Conditional Parallel Coordinates (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:12:08+00:00", "description": "Authors: William P. Porter, Yunhao Xing, Blaise R von Ohlen,  Jun Han, Chaoli Wang\n\nAbstract: We present a deep learning approach that selects representative time steps from a given time-varying multivariate data set. Our solution leverages an autoencoder that implicitly learns feature descriptors of each individual volume in a latent space. These feature descriptors are used to reconstruct respective volumes for error estimation during network training. We then perform dimensionality reduction of these feature descriptors and select representative time steps in the projected space. Unlike previous approaches, our solution can handle time-varying multivariate data sets where the multivariate features can be learned using a multichannel input to the autoencoder. We demonstrate the effectiveness of our approach using several time-varying multivariate data sets and compare our selection results with those generated using an information-theoretic approach.", "uri": "https://vimeo.com/363040865", "name": "[VIS19 Preview] A Deep Learning Approach to Selecting Representative Time Steps for Time-Varying Multivariate Data (short...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:11:29+00:00", "description": "Authors: Maoyuan Sun, David Koop, Jian Zhao,  Chris North, Naren Ramakrishnan\n\nAbstract: Exploring coordinated relationships is important for sensemaking of data in various fields, such as intelligence analysis. To support such investigations, visual analysis tools use biclustering to mine relationships in bipartite graphs and visualize the resulting biclusters with standard graph visualization techniques. Due to overlaps among biclusters, such visualizations can be cluttered (e.g., with many edge crossings), when there are a large number of biclusters. Prior work attempted to resolve this problem by automatically ordering nodes in a bipartite graph. However, visual clutter is still a serious problem, since the number of displayed biclusters remains unchanged. We propose bicluster aggregation as an alternative approach, and have developed two methods of interactively merging biclusters. These interactive bicluster aggregations help organize similar biclusters and reduce the number of displayed biclusters. Initial expert feedback indicates potential usefulness of these techniques in practice.", "uri": "https://vimeo.com/363040807", "name": "[VIS19 Preview] Interactive Bicluster Aggregation in Bipartite Graphs (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:10:33+00:00", "description": "Authors: Teodoro Collin,  Charisee Chiw,  L. Ridgway Scott, John Reppy, Gordon L Kindlmann\n\nAbstract: Scientific visualization tools tend to be flexible in some ways (e.g., for exploring isovalues) while restricted in other ways, such as working only on regular grids, or only on unstructured meshes (as used in the finite element method, FEM). Our work seeks to expose the common structure of visualization methods, apart from the specifics of how the fields being visualized are formed. Recognizing that previous approaches to FEM visualization depend on efficiently updating computed positions within a mesh, we took an existing visualization domain-specific language, and added a mesh position type and associated arithmetic operators. These are orthogonal to the visualization method itself, so existing programs for visualizing regular grid data work, with minimal changes, on higher-order FEM data. We reproduce the efficiency gains of an earlier guided search method of mesh position update for computing streamlines, and we demonstrate a novel ability to uniformly sample ridge surfaces of higher-order FEM solutions defined on curved meshes.", "uri": "https://vimeo.com/363040730", "name": "[VIS19 Preview] Point Movement in a DSL for Higher-Order FEM Visualization (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:09:53+00:00", "description": "Authors: Yong Wang, Daniel Archambault,  Hammad Haleem,  Torsten Moeller, Yanhong Wu,  Huamin Qu\n\nAbstract: Uniform timeslicing of dynamic graphs has been used due to its convenience and uniformity across the time dimension. However, uniform timeslicing does not take the data set into account, which can generate cluttered timeslices with edge bursts and empty timeslices with few interactions. The graph mining filed has explored nonuniform timeslicing methods specifically designed to preserve graph features for mining tasks. In this paper, we propose a nonuniform timeslicing approach for dynamic graph visualization. Our goal is to create timeslices of equal visual complexity. To this end, we adapt histogram equalization to create timeslices with a similar number of events, balancing the visual complexity across timeslices and conveying more important details of timeslices with bursting edges. A case study has been conducted, in comparison with uniform timeslicing, to demonstrate the effectiveness of our approach.", "uri": "https://vimeo.com/363040656", "name": "[VIS19 Preview] Nonuniform Timeslicing of Dynamic Graphs Based on Visual Complexity (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:09:31+00:00", "description": "Authors: Nina McCurdy, Miriah Meyer \n\nAbstract: One way astronomers and astrophysicists study galaxy formation and evolution is by analyzing and comparing real galaxy observations, captured by telescopes, and simulated galaxy observations, generated from theoretical models. They approach this through a combination of statistical and visual analysis, conducted either independently or sequentially. During the first year of an ongoing design study with astronomers and astrophysicists, we explored approaches to integrating statistical and visual analysis to enhance understanding of these data. Contributions from this stage of the study include a data and task abstraction for statistically and visually analyzing real and simulated galaxy observations, as well as an initial design, implemented in a prototype called GalStamps, and evaluated through two case studies with domain experts.", "uri": "https://vimeo.com/363040619", "name": "[VIS19 Preview] GalStamps: Analyzing Real and Simulated Galaxy Observations (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-28T22:08:59+00:00", "description": "Authors: Alessio Arleo,  Christos Tsigkanos, Chao Jia, Roger A. Leite, Ilir Murturi, Manfred Klaffenboeck, Schahram Dustdar,   Michael Wimmer,  Silvia Miksch, Johannes Sorger\n\nAbstract: Investment planning requires knowledge of the financial landscape on a large scale, both in terms of geo-spatial and industry sector distribution. There is plenty of data available, but it is scattered across heterogeneous sources (newspapers, open data, etc.), which makes it difficult for financial analysts to understand the big picture. In this paper, we present Sabrina, a financial data analysis and visualization approach that incorporates a pipeline for the generation of firm-to-firm financial transaction networks. The pipeline is capable of fusing the ground truth on individual firms in a region with (incremental) domain knowledge on general macroscopic aspects of the economy. Sabrina unites these heterogeneous data sources within a uniform visual interface that enables the visual analysis process. In a user study with three domain experts, we illustrate the usefulness of Sabrina, which eases their analysis process.", "uri": "https://vimeo.com/363040561", "name": "[VIS19 Preview] Sabrina: Modeling and Visualization of Economy Data with Incremental Domain Knowledge (short paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-25T15:17:12+00:00", "description": "Authors: Crisan, Anamaria\n\nAbstract:", "uri": "https://vimeo.com/362329763", "name": "[VIS19 Preview] Biovis Challenges (meetups)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-25T15:16:39+00:00", "description": "Authors: Ropinski, Timo\n\nAbstract:", "uri": "https://vimeo.com/362329638", "name": "[VIS19 Preview] Inviwo Meetup (meetups)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-25T15:16:21+00:00", "description": "Authors: Hullman, Jessica\n\nAbstract:", "uri": "https://vimeo.com/362329572", "name": "[VIS19 Preview] VIS 2019 Job Fair Meetup (meetups)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-25T15:16:05+00:00", "description": "Authors: Moreland, Kenneth\n\nAbstract:", "uri": "https://vimeo.com/362329510", "name": "[VIS19 Preview] Meetups: VisLies! (meetups)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-25T15:15:48+00:00", "description": "Authors: Diehl, Alexandra\n\nAbstract:", "uri": "https://vimeo.com/362329439", "name": "[VIS19 Preview] Viewpoints on Diversity through the VIS history (meetups)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-25T15:15:30+00:00", "description": "Authors: Archambault, Daniel\n\nAbstract:", "uri": "https://vimeo.com/362329372", "name": "[VIS19 Preview] Good Food Meets Good Science (meetups)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-24T16:21:14+00:00", "description": "Authors: Vis20 Chairs\n\nAbstract:", "uri": "https://vimeo.com/362089166", "name": "[VIS19 Preview] Vis2020 Kickoff Video (vis20kickoff)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-24T16:13:52+00:00", "description": "Authors: Meetups Chairs\n\nAbstract:", "uri": "https://vimeo.com/362087360", "name": "[VIS19 Preview] Meetups Cover Video (meetups)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-24T15:55:24+00:00", "description": "Authors: Silvio Rizzi\n\nAbstract:", "uri": "https://vimeo.com/362082879", "name": "[VIS19 Preview] Scivis Contest (scivis-contest)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-24T06:01:06+00:00", "description": "Authors: Miriah Meyer, Jason Dykes\n\nAbstract: We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are informed, reflexive, abundant, plausible, resonant, and transparent. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline.", "uri": "https://vimeo.com/361976920", "name": "[VIS19 Preview] Criteria for Rigor in Visualization Design Study (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-19T21:38:06+00:00", "description": "Poster \n\nAuthors: Marjan Eggermont , S\u00f8ren Knudsen , Richard Pusch , Sheelagh Carpendale\n\nAbstract: We present Biomole, an interactive visualization designed to explore a data set created to support bio-inspiration design published by the Ask Nature organization. The Ask Nature data set contains a hierarchy of functions which we encode within different visual elements of Biomole; most importantly interactive data painting of functions allows people to search for functional co-occurrence. The movement of the visual elements is a metaphorical nod to the bio-inspired nature of the data and the uncertainty of the design solution space.", "uri": "https://vimeo.com/361166406", "name": "[VIS19 Preview] Biomole: Visualizing functional co-occurrence (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:37:52+00:00", "description": "Poster \n\nAuthors: Ala Abuthawabeh , Micha\u00ebl Aupetit\n\nAbstract: User interaction to create clusters and annotate data is crucial in the Visual Interactive Labeling process. In this work we get the feedback from a clinician researcher expert on sleep and activity data analysis, using a basic interactive labeling interface. We study the design of a new interface to improve its usability based on a Voronoi treemap visual metaphor and the Centroidal Voronoi Diagram technique. We propose new interactions to create, move, split and merge groups of items in this framework.", "uri": "https://vimeo.com/361166366", "name": "[VIS19 Preview] Toward Interactive Labeling with Voronoi Treemaps (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:37:40+00:00", "description": "Poster \n\nAuthors: Abeer Alsaiari , Dr Andrew E Johnson\n\nAbstract: There is a steadily growing interest in leveraging ecosystems of digital devices that go beyond a single desktop for visual data analysis and exploration. However, multi-device ecologies pose several challenges as information and tasks are scattered among separate devices and displays. To understand the challenges associated with multi-device environments for information visualization, we performed an exploratory study designed to examine how users employ different tools to perform different kinds of activities in approaching a visual analysis task. Previous work examined three factors (users, tools, and tasks) independently. We study the synthesis of these factors. To do this, we adopted a hybrid analysis approach that focuses on three different aspects: users, tools, and tasks. We believe this analysis will help us identify associated challenges and better inform design goals in developing multi-device tools for visual data analysis.", "uri": "https://vimeo.com/361166333", "name": "[VIS19 Preview] Towards Understanding Collaborative Visual Data Analysis in Multi- Device Environments (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:36:42+00:00", "description": "Poster \n\nAuthors: Andie Green-Harrison , S\u00f8ren Knudsen , Sheelagh Carpendale\n\nAbstract: We present a visualization that makes use of a novel highlighting and animation technique. The primary motivation behind the design of this visualization was to enable people to look at data about healthcare services, and specifically to increase their knowledge about the impact of the amount of healthcare workers per capita. We use animation in the process of showing changes within an individual country over the course of several years, as well as to facilitate comparisons between multiple countries at once, by letting people keep track of selected countries. We discuss the motivation behind our use of animation, our choice to include a search bar, and potential areas for improvement and revision.", "uri": "https://vimeo.com/361166141", "name": "[VIS19 Preview] Visualizing Density of Healthcare Workers Across Time and Countries (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:36:29+00:00", "description": "Poster \n\nAuthors: Jorin Weatherston , Charles Perin , Margaret-Anne Storey\n\nAbstract: We propose a visualization design space for representing unquantifiable uncertainty in percent composition drug checking test results using pie and cake charts. The design space generates alternatives for use in a visual drug report to facilitate decision-making concern-ing drug use. Currently, this communication does not capture the uncertainty in drug checking tests, leading to poor and potentially harmful decisions. The design alternatives aim to empower people who use drugs and facilitate harm reduction efforts. Our visualizations may apply to other drug checking services and to scenarios where end users need to understand unquantifiable uncertainty.", "uri": "https://vimeo.com/361166097", "name": "[VIS19 Preview] Visualizing Unquantifiable Uncertainty in Drug Checking Test Results (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:36:13+00:00", "description": "Poster \n\nAuthors: Gaoqi He , Xiaohui Bian , Alexander Garbarino , Hadi Kharrazi , Jesus J. Caban , Ping Zhang , Jian Chen\n\nAbstract: We present Necklace, a multivariate comparative data exploration interface for medical claims: billing information submitted by hospitals for payment by commercial and government health plans. Necklace uses a series of interconnected rings to represent event co- occurrences: nodes on rings represent events, and novel glyph small multiples to reveal relationships suitable for multilayer network visualizations. Our tool has two novel methods: a guided interaction for visual exploration of complex relationships and a spatial-saving glyph small multiples (GSM) to compare features on multilayer networks.", "uri": "https://vimeo.com/361166042", "name": "[VIS19 Preview] Necklace: A Guided Interaction and Comparison Metaphorical Interface for Medical Claim Analysis (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:36:01+00:00", "description": "Poster infovis honorable mention\n\nAuthors: Caitlyn M. McColeman , Enrico Bertini , Steven Franconeri\n\nAbstract: The goal of the present work is to establish a set of low level perceptual processes that are used when observers complete different tasks in response to various types of simple data visualizations. This work summarizes candidates for perceptual (and higher-level cognitive) processes across five different data visualization types (heatmaps, bar graphs, etc.) and across ten different tasks (informed by Amar et al. (2005)}; e.g., \"Order the data from most to least\"). We find evidence that (1) selective attention is a common process across most types of graphs and most types of tasks, (2) different types of graphs elicit different processes even when the observers are completing the same task, and (3) different types of tasks elicit different processes, even from the same graph.", "uri": "https://vimeo.com/361165993", "name": "[VIS19 Preview] A qualitative assessment of basic perceptual tasks employed while reading common data visualizations...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:35:49+00:00", "description": "Poster \n\nAuthors: Zhengliang Liu , Melanie Bancilhon , Alvitta Ottley\n\nAbstract: Estimating proportions is a common visualization task. In this paper, we present the findings of a large-scale experiment that compares the effectiveness of helping users in estimating proportions across six common visualization designs. We analyzed 406 participants' decisions over real monetary gains, resulting in 10,150 observations. Participants were most accurate with an icon array design and least accurate with an area proportioned circle design. They also consistently overestimated low probabilities and underestimated high probabilities across all charts. Using our findings, we rank the representations based on users' ability to accurately estimate the underlying proportions, and provide evidence-based guidelines for visualization designers.", "uri": "https://vimeo.com/361165938", "name": "[VIS19 Preview] Icons are Best: Ranking Visualizations for Proportion Estimation (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:35:36+00:00", "description": "Poster VAST Best Poster\n\nAuthors: Meng Xia , Mr Huan Wei , Min XU , Leo Yu Ho Lo , Yong Wang , Rong Zhang , Huamin Qu\n\nAbstract: With the increasing popularity of online education, many E-learning platforms have been launched to facilitate the education of K-12 (from kindergarten to 12th grade) students, where their learning logs such as grades, login time and mouse movement are often recorded. However, it still remains unclear how to make use of these detailed learning behavior data to improve the design of learning resources and gain deep insights into students' thinking and learning styles. In this work, we propose a visual analytics system to analyze student learning behaviors on a K-12 mathematics E-learning platform. It supports both correlation analytics between different attributes and detailed visualization of user mouse movements. Our case studies on a real dataset show that our system can better guide the design of learning materials (e.g., math questions) as well as facilitate quick interpretation of students' problem-solving and learning styles.", "uri": "https://vimeo.com/361165889", "name": "[VIS19 Preview] Visual Analytics of Student Learning Behaviors on K-12 Mathematics E-learning Platforms (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:35:22+00:00", "description": "Poster \n\nAuthors: Fabricio Layedra , Tiffany Wun , Gonzalo Mendez , Bongshin Lee , Sheelagh Carpendale\n\nAbstract: TabbyGraph supports the interactive construction of tabular-based visualizations (e.g., bar charts) from relational data visualization layouts (e.g., node-link diagrams), with the aim of combining graph and chart analysis. TabbyGraph enables agency in manipulating and defining visual elements, while automatically generating those visual elements once defined (e.g., positioning nodes along data attributes). We demonstrate the potential of using TabbyGraph to create scatterplots and bar charts from node-link diagram layouts.", "uri": "https://vimeo.com/361165851", "name": "[VIS19 Preview] TabbyGraph: Interactive Graph Transformation (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:35:11+00:00", "description": "Poster \n\nAuthors: Jordon Johnson , Emily Hindalong , Giuseppe Carenini\n\nAbstract: We have designed and implemented a visualization tool to facilitate group decision making for cases where the most appropriate preference model is to score alternatives directly. Our tool allows remote and asynchronous preference elicitation as well as individual or group preference analysis via a set of interactive visualizations. A preliminary evaluation of our tool by means of two user studies suggests that it can be effective in identifying areas of (dis)agreement and enabling more participatory discussions.", "uri": "https://vimeo.com/361165816", "name": "[VIS19 Preview] Supporting Group Decision Making by Interactive Elicitation and Visualization of Alternative Scores (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:35:00+00:00", "description": "Poster \n\nAuthors: Miss Ivana Kocanova , Muhammad Adnan , Dr Georgios Aivaliotis , Roy Ruddle\n\nAbstract: Convenience stores stock a small range of everyday products. When a customer makes a transaction, each product they buy can be treated as an 'event' and the overall contents of the customer's shopping basket is a set of events. Unfortunately, existing event set mining and visualization techniques are poorly suited to the typical complexity of the data (e.g., 140,000 product combinations in 360,000 transactions). To address that we have developed a visual analytic workflow, which starts by using high utility itemset mining to reduce the complexity of the data by a factor of 1000, and then allows users to investigate the composition of transactions and create transaction sketches. Examples are a product-perspective sketch, which reveals sets of products that are often bought together, and a time-perspective sketch that shows how those sets change from breakfast to lunch, dinner and then night.", "uri": "https://vimeo.com/361165770", "name": "[VIS19 Preview] A Visual Analytics Workflow for Investigating Customers' Transactions in Convenience Stores (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:34:49+00:00", "description": "Poster infovis honorable mention\n\nAuthors: Dylan Wootton , Ethan Ransom , Alexander Lex\n\nAbstract: The Arctic seascape plays an important role in North American trade and defense; however, over recent years, the Arctic has undergone radical changes, altering current shipping routes and opening up new previously blocked paths. This work presents Arctic Explorer, a tool to visualize sea-ice concentration along paths over time. Arctic Explorer could be used to circumnavigate areas expected to be blocked with ice and discover new routes that may have opened up.", "uri": "https://vimeo.com/361165729", "name": "[VIS19 Preview] Arctic Explorer: Visualization of Sea-Ice Concentration along Arctic Shipping Routes (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:34:36+00:00", "description": "Poster \n\nAuthors: Adhitya Kamakshidasan , Vijay Natarajan\n\nAbstract: Topology based analysis and feature tracking is a well studied area. In this work, we focus exclusively on a dataset called the von Karman street, and apply topology-based methods to understand its vortices. For this analysis, we adapt the recently proposed edit distance between merge trees. We discern several interesting results. One, we observe spatial periodicity between the vortices, alternating every half-cycle. Two, we observe a distinct difference in spatial probability of vortex regions during a half-cycle. Further, we compare the accuracy of our spatial probability with an off-the-shelf machine learning approach.", "uri": "https://vimeo.com/361165691", "name": "[VIS19 Preview] Topological Analysis of the 2D von K\u00e1rm\u00e1n street (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:34:19+00:00", "description": "Poster \n\nAuthors: Adhitya Kamakshidasan , Vijay Natarajan\n\nAbstract: Topological abstractions such as merge trees play an important role in scalar field analysis. However, such abstractions are difficult to interpret and time-consuming. To bridge this gap, we propose \\textit{mergescapes}, a force directed landscape for understanding merge trees. We illustrate the usefulness of this design using an application to the study of protein structures.", "uri": "https://vimeo.com/361165644", "name": "[VIS19 Preview] Understanding Merge Trees with Force-directed Landscapes (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:34:08+00:00", "description": "Poster \n\nAuthors: Shayan Monadjemi , Roman Garnett , Alvitta Ottley\n\nAbstract: Modeling and predicting interactions is an essential challenge for the visual analytics community. In this work, we demonstrate how we can apply machine learning to model user interactions through time and data space. We present a Bayesian inference method that creates a probability distribution over the user\u2019s areas of interest in continuous dimensions of data, and we show that we can use this model to predict interactions before they occur. Furthermore, we determine that our algorithm can surpass the reported accuracy of previously published prediction techniques.", "uri": "https://vimeo.com/361165601", "name": "[VIS19 Preview] User Interaction Modeling through Time and Data Space (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:33:57+00:00", "description": "Poster \n\nAuthors: Ilkin Safarli , Alexander Lex\n\nAbstract: Considering node and edge attribute is crucial for many network exploration and analysis tasks. However, effective visualization of both structure and attributes is a challenging problem, especially for dense graphs. In this poster, we introduce TaMax, a technique designed to visualize dense multivariate graphs with a diverse set of node and edge attributes based on adjacency matrices. In TaMax, node attributes are visualized in a table that is juxtaposed with the matrix, while edge attributes visualized in the cells. We investigate different ways to visualize multiple edge attributes: dividing each cells into sub-cells showing different edge attributes or overlaying a secondary attribute with opacity over a cell. Furthermore, TaMax addresses the scalability problem by allowing flexible grouping based on node attributes and querying based on edge attributes.", "uri": "https://vimeo.com/361165565", "name": "[VIS19 Preview] TaMax: Visualizing Dense Multivariate Graphs with Adjacency Matrices (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:33:46+00:00", "description": "Poster \n\nAuthors: Kiran Gadhave , Hendrik Strobelt , Nils Gehlenborg , Alexander Lex\n\nAbstract: UpSet is a technique for visualization and analysis of sets and their intersections which was introduced at InfoVis 2014. The technique visualizes the elements and their set memberships in a matrix layout along with an aligned bar chart to display intersection sizes. As the approach provides a more accurate representation of the size of set intersections and scales better with respect to the number of sets than other approaches, UpSet plots are frequently used in papers in the biomedical domain. However, we believe that this popularity is mostly due to an R version that generates UpSet figures, and not because of our interactive JavaScript implementation that was published with the original paper. Why is the R version more popular? We believe it has multiple reasons, but one of them is that the original tool is an academic prototype, with the usual shortcomings for practical use. This poster presents a new version of UpSet---a tool that was developed to address requests for additional functionality over the original implementation based on user feedback. We describe in detail which changes we undertook to improve the paper prototype into a tool. The new version adds features to improve adoption, improve sharing insights, and allow customization. For example, it allows embedding of the tool in webpages and supports tracking of provenance to provide a undo and redo functionality. The new implementation also can be used as a JavaScript/TypeScript library, which makes it easy to integrate UpSet in larger systems.", "uri": "https://vimeo.com/361165523", "name": "[VIS19 Preview] UpSet 2: From Prototype to Tool (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:33:35+00:00", "description": "Poster \n\nAuthors: Mr Joseph William Mearman , Peter W. S. Butcher , Mr James R Jackson , Professor Jonathan C Roberts\n\nAbstract: As students progress through their educational journey, both teachers and students often need to discuss a student's progress. But, due to confidentiality and other limitations, it can be difficult to discuss individual students, or understand the whole cohort. We present tangible visualisations of academic attainment data. Each student is represented by one object, that has been printed, cut and crafted from light card. These physical representations become digital surrogates, where academics can lay out the objects to tell one story, or move them to discuss another. In this paper we present our tangible visualisations, explain our creation process and discuss several prototypes.", "uri": "https://vimeo.com/361165484", "name": "[VIS19 Preview] Tangible crafted multiple-view visualisation of academic attainment data (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:33:11+00:00", "description": "Poster \n\nAuthors: Caitlyn M. McColeman , Mi Feng , Lane Harrison , Steven Franconeri\n\nAbstract: Perceptual precision is a critical consideration when choosing visual encodings for data. When a viewer reads a chart, how well can their visual system extract the depicted numbers? This precision is often empirically measured with a verbally-reported ratio judgment task. However, verbal tasks not only require perceptual ratio judgments, but also a translation of that ratio into a verbal or typed response, potentially adding noise to the underlying signal these studies are seeking to measure. Here we report the first precision measurements of visual value extraction that remove these other sources of noise and bias for position and length judgments in bar graphs. The results reveal a surprising repulsion effect for stacked bar graphs, such that reported values near the 50 percent mark are repulsed farther from that value.", "uri": "https://vimeo.com/361165407", "name": "[VIS19 Preview] 50/50 perception often isn\u2019t 20/20: The role of context and value on perceptual biases in ratio perception...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:31:52+00:00", "description": "Poster \n\nAuthors: Jillian Aurisano , Abhinav Kumar , Abeer Alsaiari , Barbara Di Eugenio , Dr Andrew E Johnson\n\nAbstract: Interaction with many views on large displays is a growing area of interest in the visualization research community. We present an exploratory, observational pre-design evaluation of interaction in a large display environment in the context of a real data analysis session. Participants explored a local crime dataset, expressing interaction intentions to a remote mediator through speech and mid-air gestures. The mediator responded by presenting and juxtaposing multiple visualizations on the large display. We observed cases where participants interacted through reference to existing views in ways the produced many views of the data. In some cases, participants indicated a single target, but expressed multiple operations to perform on that target in parallel, producing many views at once. We term this an interaction that is one-to-many. In other cases, participants indicated multiple targets and specified one or many operations to perform on these targets collectively. We term these interactions many-to-many. We present these cases to aid in the design of interactions for visualization systems that support the display and juxtaposition of many views, so that these designs might accommodate the desired behaviors of users.", "uri": "https://vimeo.com/361165154", "name": "[VIS19 Preview] Evaluation of scalable interactions over multiple views in large display environments (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:22:58+00:00", "description": "Poster \n\nAuthors: Hiroaki Natsukawa , Shungo Ishino , Yosuke Onoue , Koji Kyoda , Dr Shuichi Onami , Koji Koyamada\n\nAbstract: Genes are responsible for protein synthesis and ultimately shape us. Both genes and phenotypic characters have complex interactions. Therefore, connecting biologically different hierarchies, such as genomics and phenomics is an important approach to understand the biological mechanism. Here, we propose an explorative visualization system that enables biologists to explore interactively both the phenotypic character network and the genotypic network of the nematode Caenorhabditis elegans. These two networks are visualized in multi-view and linked based on the relationship between gene and phenotypic character investigated by RNAi technology. This tool supports biologists to narrow down the complex networks with the filtering function, the user interaction and domain knowledge, leads to discovering new findings and hypotheses in the life sciences field.", "uri": "https://vimeo.com/361163501", "name": "[VIS19 Preview] Explorative Visualization for Phenotypic-Genotypic Network (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:22:14+00:00", "description": "Poster \n\nAuthors: Steve D\u00fcbel , Christine Ripken , Heidrun Schumann\n\nAbstract: Designing visualizations of spatial data and 3D terrain is challenging. Usually the designer has to focus on selected relevant aspects of the data, emphasize them, and attenuate less relevant information. Yet, it is not clear upfront if design decisions with respect to emphasis lead to the desired result. Conducting user studies is often infeasible due to their high costs. We propose to utilize saliency maps early in the design phase to help designers assess their visualizations. Saliency maps provide information about the uniqueness of visual features in an images and hence can predict which part of a visualization is likely to attract the user's attention. Our approach is to compute saliency maps in real-time and show them alongside the actual visualization. This provides designers with immediate feedback on the effectiveness of their design decisions.", "uri": "https://vimeo.com/361163341", "name": "[VIS19 Preview] Using Saliency to Support the Design of Visualizations of Spatial Data in 3D Terrain (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:21:15+00:00", "description": "Poster \n\nAuthors: Nicolas Grossmann , Oscar Casares-Magaz , Ludvig P. Muren , Vitali Moiseenko , John Einck , Eduard Gr\u00f6ller , Dr Renata Georgia Raidou\n\nAbstract: The Pelvis Runner is a visual analysis tool for the exploration of the variability of segmented pelvic organs in multiple patients, across the course of radiation therapy treatment. Radiation treatment is performed through the course of weeks, during which the anatomy of the patient changes. This variability may be responsible for side-effects, due to potential over-irradiation of healthy tissues. Exploring and analyzing organ variability in patient cohorts can help clinical researchers to design more robust treatment strategies. Our work addresses, first, the global exploration and analysis of pelvic organ shape variability in an abstracted tabular view for the entire cohort. Second, local exploration and analysis of the variability is provided on demand in anatomical 2D/3D views for cohort partitions. The Pelvis Runner has been evaluated by two clinical researchers, and is a promising basis for the exploration of pelvic organ variability.", "uri": "https://vimeo.com/361163162", "name": "[VIS19 Preview] Pelvis Runner: A Visual Analytics Tool for Pelvic Organ Variability Exploration in Prostate Cancer Cohorts...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:20:52+00:00", "description": "Poster \n\nAuthors: Taerin Yoon , Hyunwoo Han , Hyoji Ha , Juwon Hong , Kyungwon Lee\n\nAbstract: We present a system that can determine the motivation and topic of citations when a user is searching for appropriate reference literature. To build the system, we extracted citation sentences from papers from the IEEE Information Visualization Conference and analyzed the motives for citing specific papers. We also generalized the topics by extracting keywords from the sentences. With this motive and topic data, a paper-search interface service was designed. Finally, we checked the motive and topic information for a cited sentence by using our system on a user scenario.", "uri": "https://vimeo.com/361163080", "name": "[VIS19 Preview] A Paper-Search System to Identify Citation Networks and Motivations (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:19:50+00:00", "description": "Poster VAST Honorable Mention\n\nAuthors: Siming Chen , Xing Gao , Jie Li , Gennady Andrienko , Natalia Andrienko\n\nAbstract: Significant events, such as large sport tournaments or presidential elections, develop over a long time in the physical world. In parallel, people's reactions to these events expressed in social media evolve in the cyber world. We are interested in exploring dynamic relationships between these two processes. We propose a \\textit{DualFlow} visualization, which uses a storyline metaphor for presenting the evolution of a real-world event arousing discussions in social media together with the evolution of these discussions. We illustrate our approach by example of social media discussions of the football World Cup 2018. DualFlow visualizes the course of the whole championship including games of all 32 teams and shows the evolution of the related reactions of social media users, i.e., how the interests of the users were distributed among the teams and how this distribution changed over time. Analysts can brush a period and select teams to explore the changing focus patterns of social media users as the tournament goes on. Major keywords of the discussion are shown on demand for explaining the content of the discussion.", "uri": "https://vimeo.com/361162892", "name": "[VIS19 Preview] Visual Analytics for Integrated Evolution of Physical and Cyber-Events: A Case of the World Cup in Social...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:19:28+00:00", "description": "Poster \n\nAuthors: Jasmine Yen-Ching Shih , Dr. Eric Shaffer , Ranjitha Kumar , Sujay Khandekar\n\nAbstract: The rise of mobile digital technology has had immense impact on how people communicate. One of the most significant changes is the use of emoji, in which visual symbols are used to concisely express information such as emotions or ideas. In an effort to better understand how people use emoji, we have built and applied a suite of visual analytics tools to a dataset recording activity on the Emoji-first mobile app Opico.", "uri": "https://vimeo.com/361162822", "name": "[VIS19 Preview] Using Visual Analytics to Understand Emoji-first Communication (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:19:06+00:00", "description": "Poster \n\nAuthors: Martin Bellgardt , Alexander Enneking , Torsten Wolfgang Kuhlen\n\nAbstract: Artificial neural networks (ANNs) often achieve a better performance when used to model decision processes than the previous state of the art. Unfortunately, our understanding of how this performance is achieved is still lacking. Looking at visualizations of ANNs might help to get a more intuitive understanding of their inner workings. One way to visualize neural networks is as a node-link diagram. This is usually done with small, exemplary ANNs in an educational context. However, we suspect that a node-link visualization of larger networks might also be suitable in a professional setting. In this work, we present a system that can be used by machine learning experts to visualize ANNs using immersive virtual reality. We also report on first findings after applying our system to a real world use case.", "uri": "https://vimeo.com/361162743", "name": "[VIS19 Preview] An Immersive Node-Link Visualization for Artificial Neural Networks (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:18:01+00:00", "description": "Poster infovis honorable mention\n\nAuthors: Chufan Lai , Zhixian Lin , Can Liu , Yun Han , Ruike Jiang , Xiaoru Yuan\n\nAbstract: In this paper, we propose a technique for automatically annotating visualizations based on the user's textual descriptions. In our approach, the annotating task is fulfilled by performing a series of automatic visual searches. First, the description of the visualization is parsed into search requests for certain visual entities. At the same time, all visual entities exhibited in the visualization, along with their visual properties are extracted using Object Detection techniques based on Mask-RCNN models. Knowing what are there and what to look for, we then fulfill the generated search requests, so as to anchor each descriptive sentence to the described focal areas. In the next step, the corresponding annotations can be crafted efficiently. We have built a prototype tool that allows the user to upload a visualization image with its descriptions, and generates customized annotations.", "uri": "https://vimeo.com/361162531", "name": "[VIS19 Preview] Automatic Annotation of Visualizations (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:17:41+00:00", "description": "Poster \n\nAuthors: Hyeonsik Gong, Hyunwoo Han, Kyungwon Lee\n\nAbstract: The focus of this visualization is mainly to develop an interactive and exploratory system to assist users with easily identifying the distinctive characteristics of a photo of a city uploaded to Instagram. In addition, clustering algorithms are applied in this project to group photographs based on the labels extracted from the Google Vision API. The main hexagon layout allows users to easily identify the similarities and differences among clusters. Using radar charts, users can compare the characteristics of cities or clusters and also determine an optimal algorithm for clustering photograph data. Furthermore, users can view the visuals of photos that correspond to the selected data, with clusters, hashtags, labels, and colors, which can then be used for better clustering photograph algorithms.", "uri": "https://vimeo.com/361162477", "name": "[VIS19 Preview] An Interactive Exploratory Visualization System for Comparing Clusters of Instagram Photos (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:17:19+00:00", "description": "Poster \n\nAuthors: Uzma Haque Syeda , Prasanth Murali , Michelle A. Borkin\n\nAbstract: Service-Learning is a pedagogical model that integrates classroom learning objectives with community needs and service goals. In this work, we present a case study implementation of Service-Learning within a graduate visualization curriculum, utilizing the design study \"lite\" methodology to meet the project's goals within three months. In the case study, we worked with the Chester Square Neighborhood Association in Boston, MA, USA to help them improve their resident urban park by leveraging existing data, identifying current issues debilitating the park, and providing visualizations for insight. This case study demonstrates the \"lite\" methodology as a potential solution for a short-term design study approach as well as a viable approach in conjunction with Service-Learning, to facilitate visualization for social good.", "uri": "https://vimeo.com/361162402", "name": "[VIS19 Preview] Chester Square Park: A Case Study of Visualization for Social Good using Design Study \"Lite\" Methodology...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:16:49+00:00", "description": "Poster \n\nAuthors: Md Naimul Hoque , Darius Coelho , Klaus Mueller\n\nAbstract: Kaggle, founded in 2010, has become the leading platform for data scientists to collaboratively explore and build data-based models, participate in competitions, and communicate with each other mostly through interactive notebooks and forum discussions. The growing community has a large userbase with 2.9M current users across 194 countries. This large community regularly produces a large number of solutions (Kernels) for problems and datasets posted on the website. These solutions tend to be of high quality as they are often commissioned by companies like Google, LinkedIn through competitions where users can win prizes for building the best data-based models. Visualization, an integral part of data science, is employed in a large portion of these kernels to either explore data or present results. In this project, we examine the content of these kernels to understand the visualization practices among Kaggle data scientists. Our work reveals insights about the libraries used, the most popular visual representations and the types of color palettes used by these data scientists.", "uri": "https://vimeo.com/361162303", "name": "[VIS19 Preview] Examining the Visualization Practices of Data Scientists on Kaggle (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:16:23+00:00", "description": "Poster \n\nAuthors: Rupayan Neogy , Emily Hu , Arvind Satyanarayan\n\nAbstract: Visualization researchers have begun exploring real-time visualization synchronization, but the focus has been on the technology supporting such synchronizations, and very little research looks at the front-end. In this paper, we present Visualive, an exploration of the representation of real-time users in the space of a visualization. Visualive represents online users by embedding dynamic color-coded widgets within a visualization, allowing viewers to easily identify remote actions being performed without leaving the context of a visualization. Visualive is an initial dive into merging collaboration and synchronization with data visualization without compromising on the latter experience.", "uri": "https://vimeo.com/361162229", "name": "[VIS19 Preview] Visualive: Representing Synchronized Visualization Interactions (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:16:00+00:00", "description": "Poster scivis best poster\n\nAuthors: Jung Who Nam , Charles Hobie Perry , Barry Ty Wilson , Daniel F. Keefe\n\nAbstract: We present a first demonstration of multiple complementary linked view, 2D + VR visualization on a mobile, low-cost VR device along with an application to visualizing multivariate forestry data.", "uri": "https://vimeo.com/361162150", "name": "[VIS19 Preview] Linked View Visualization Using Clipboard-Style Mobile VR: Application to Communicating Forestry Data...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:15:28+00:00", "description": "Poster \n\nAuthors: Sydney Brougham , Jonathan P. Leidig , Prof. Greg Wolffe\n\nAbstract: Visual analytics for real-time, geospatial data requires the flexibility to explore areas of interest and the optimization of information foraging tasks. The objective of this project was to develop a generalized visualization technique suitable for this specific type of data and its related tasks. The Plotly Dash framework was used to support a set of customized services related to real-time data. The interface generates overviews and navigation for quantitative geospatial observations and points of interest. Foraging challenges and data overload were mitigated via design considerations for human psychology and other core principles of visual analytics. These techniques allow users to mine current statuses and discern trends, particularly with regard to geo-temporal patterns and outlier detection.", "uri": "https://vimeo.com/361162035", "name": "[VIS19 Preview] Interactive Visualization for Real-Time Geospatial Data with Plotly Dash (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:15:04+00:00", "description": "Poster \n\nAuthors: Haihan Lin , Carolina Nobre , Amanda Bakian , Alexander Lex\n\nAbstract: The analysis of multiple time series and associated quantitative or categorical attributes is an important task. Air pollution data, for example, is captured along many dimensions. This data is valuable for studying the association between air quality and diseases risks. In these cases, researchers need to view a large amount of data for multiple cases simultaneously, limiting the space that is available for each time series. In this abstract, we introduce clipped graphs, a hybrid clipped area chart that uses redundant color coding for visualizing time series data with skewed distributions and relatively rare peaks. We designed clipped graphs for use in compact tabular layouts. We use binned color scales for the full data range but clip outliers above a pre-defined threshold. The clipped peaks can be revealed through interaction. We integrate clipped graphs into an existing multivariate data visualization system, which visualizes clinical genealogies and detailed data about individuals in a tabular layout.", "uri": "https://vimeo.com/361161980", "name": "[VIS19 Preview] Clipped Graphs: A Compact Time-Series Encoding (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:14:45+00:00", "description": "Poster \n\nAuthors: Fran\u00e7ois L\u00e9vesque , Marielle Saint-Germain , Thomas Hurtut\n\nAbstract: This design study tackles the visualization of a multivariate bipartite graph of artists (songwriters, composer, singer) and songs. Furthermore, this dataset has an additional tree structure for the song nodes, inherited from the musical adaptation relations. In tight collaboration with a national library and archives agency, we propose an artist-centered interactive representation, focusing on several identified user tasks. Data and task abstraction, design considerations, and a first insight-based evaluation are presented.", "uri": "https://vimeo.com/361161927", "name": "[VIS19 Preview] Muzlink: Three Level Connected Timelines for Artist-Centered Musical Adaptations Exploration (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:14:25+00:00", "description": "Poster \n\nAuthors: Weina Jin , Sheelagh Carpendale , Ghassan Hamarneh , Diane Gromala\n\nAbstract: Researchers in the re-emerging field of explainable/interpretable artificial intelligence (XAI) have not paid enough attention to the end users of AI, who may be lay persons or domain experts such as doctors, drivers, and judges. We took an end-user-centric lens and conducted a literature review of 59 technique papers on XAI algorithms and/or visualizations. We grouped the existing explanatory forms in the literature into the end-user-friendly XAI taxonomy. It consists of three forms that explain AI's decisions: feature attribute, instance, and decision rules/trees. We also analyzed the visual representations for each explanatory form, and summarized them as the XAI visual vocabularies. Our work is a synergy of XAI algorithm, visualization, and user-centred design. It provides a practical toolkit for AI developers to define the explanation problem from a user-centred perspective, and expand the visualization space of explanations to develop more end-user-friendly XAI systems.", "uri": "https://vimeo.com/361161864", "name": "[VIS19 Preview] Bridging AI Developers and End Users: an End-User-Centred Explainable AI Taxonomy and Visual Vocabularies...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:14:05+00:00", "description": "Poster \n\nAuthors: David Saffo , Michail Schwab , Michelle A. Borkin , Cody Dunne\n\nAbstract: In many domains, it is important to understand both the topology and geography of a network. For researchers, for example, it is important to understand the topics in a field, where this research is taking place, and which researchers collaborate. Geography plays a key role in collaboration as it is more feasible with physical proximity. Researchers are also likely to continue existing collaborations. The interplay between topology and geography is of particular interest. Within-institution collaborations are different in nature from far-distance collaborations, as long-distance collaborations often occur less out of convenience but for more specific complementary expertise. These long-distance collaborations can connect otherwise separate social groups. However, existing approaches either focus on the geospatial location of researchers, or on the social aspect of collaborations alone. We present GeoSocialVis, an interactive visualization tool for scientometrics analyses with a focus on displaying the geosocial co-authorship network. GeoSocialVis uses a novel force layout that strikes a user-defined balance between showing network topology and the geographic locations of the authors. Users can explore relevant publications, researchers, groups, and institutions by adjusting the balance between topology and geography, searching by keywords, and with details-on-demand. We demonstrate the utility of GeoSocialVis with a case study with IEEE Information Visualization 2008\u20132018 publication data. Source code and an interactive demo are available online at dsaffo.github.io/GeoSocialVis", "uri": "https://vimeo.com/361161780", "name": "[VIS19 Preview] GeoSocialVis: Visualizing Geosocial Academic Co-Authorship Networksby Balancing Topology- and Geography-...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:13:26+00:00", "description": "Poster \n\nAuthors: Nils Rodrigues , Daniel Weiskopf\n\nAbstract: Dot plots are a kind of frequency plot that shows a glyph for each individual data point. We propose several definitions for the frequencies in the underlying data as well as its visual representations. Based on these definitions, we further propose metrics to quantitatively assess the faithfulness and consistency of the visualization outcome with respect to the underlying data. This is an essential step toward creating high-quality dot plots as it can serve as a reference for the implementation and evaluation of layout algorithms.", "uri": "https://vimeo.com/361161681", "name": "[VIS19 Preview] Toward Quality Metrics for Dot Plots (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:13:01+00:00", "description": "Poster \n\nAuthors: Dr. Justin A Kauffman , Steve Satterfield , Wesley Griffin , Michael Donahue\n\nAbstract: Reversal of magnetization in micromagnetic devices is a complicated and fully three-dimensional problem, which makes it challenging to understand. In order to gain some insight into the dynamics associated with the reversal process we decided to explore the data visually in an immersive and interactive way via a Cave Automatic Virtual Environment (CAVE) at the National Institute of Standards and Technology (NIST), the NIST CAVE. Before displaying the data in the interactive environment we processed it with ParaView and utilize its built-in, preexisting features to demonstrate the need for further visual exploration of this data. The combination of ParaView and a CAVE provides domain scientists with an initial view of their data in this interactive setting, which can lead to further physical understanding and insight on how to increase the impact of the visual analysis tool.", "uri": "https://vimeo.com/361161600", "name": "[VIS19 Preview] Visual Analysis of Micromagnetic Magnetization Reversal (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:12:24+00:00", "description": "Poster \n\nAuthors: Dr. Don Engel , Mark Murnane , Nicole Trenholm , Ben Daniels , Jeffrey Halverson\n\nAbstract: Spherical displays such as Science on a Sphere and Magic Planet enable viewers to view data which is intrinsically spherical in nature without distortion. Much of this data is geographic, such as visualizations of observed or modelled data about a planet's surface, surface temperatures, weather patterns, and tectonic activity. These displays have also enabled a new form of spherical storytelling using two-dimensional content, including content captured from 360\u00ba cameras and displayed inside-out on the surface of a sphere. While spherical data and commercial off-the-shelf cameras have become increasingly available, and while the volume of freely available content intended for use on spherical displays continues to grow (e.g. https://sos.noaa.gov/Datasets/ and 360\u00ba videos on YouTube), there remains a very limited set of tools for viewing this data on a spherical surface. Modern Science on a Sphere displays are primarily found only in museums. They are complex and expensive, depending on projection mapping from four 4K projectors and on paid software licenses from NOAA. Magic Planet displays are primarily found in classrooms and are intended to be used with the proprietary software made available by their manufacturer, Global Imagination. Here, we describe our ongoing work to create open-source alternatives for spherical and VR hardware, the first of which, released in conjunction with this publication as version 1.0, is a remote-controlled, platform-independent software alternative for displaying on Magic Planet hardware. Future releases will support stereoscopic monitors and head-mounted displays such as the HTC Vive Pro.", "uri": "https://vimeo.com/361161486", "name": "[VIS19 Preview] Open-Source Presentation of Spherical Content on Repurposed Hardware (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:11:27+00:00", "description": "Poster \n\nAuthors: Suhyun Lim , Chanhee Park , Hyunwoo Han , Jaejong Ho , Junyup Hong , Soojung Lee , Kyungwon Lee\n\nAbstract: Meetings are essential in team-based workplaces. Office workers want to check the context of a meeting and the main topics discussed for reminders and summaries of meetings. However, there is a lack of studies that can show the topics and contexts of meeting at once. Therefore, this study proposes a visualization interface that shows the topics and context of meetings together. A speech section graph shows the context of a meeting. Further, a topic map provides the main points of the meeting. The visualization and interface design were modified several times to enhance usability through interviews with target users. We expect this system to improve work efficiency.", "uri": "https://vimeo.com/361161319", "name": "[VIS19 Preview] A Narrative Topic Map Visualization to Summarize and Recall a Meeting (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:11:03+00:00", "description": "Poster \n\nAuthors: Sean McCulloch , Bradley Hoefel , Joe Gildner , Shameem Ahmed , Moushumi Sharmin\n\nAbstract: We investigate challenges and opportunities in visualizing time-series data to better understand the unique challenges experienced by neurodiverse college students in everyday life. We present visualizations based on 2,372,838 units of sensor data and 1,146.63 hours of sleep data collected from a mixed-method study with 20 college students. Our visualizations highlight unique patterns of stress, navigation, and sleep quality experienced by neurodiverse college students. The proposed visualizations are a first step in investigating how to visualize such data to better represent the lived experiences of neurodiverse students which can facilitate technology design and policy-making.", "uri": "https://vimeo.com/361161246", "name": "[VIS19 Preview] Towards Understanding the Life of Neurodiverse College Students Through Visualization (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:10:36+00:00", "description": "Poster \n\nAuthors: Muzhe Zhou , Haizhou Shi , Siliang Tang , Ligang Ling , Professor Yueting Zhuang\n\nAbstract: In this work, to help non-professional users intuitively interact with the database system and better exploit the correlations of the data, we design a system named ChartNet, which takes in the natural-language query and outputs the visualization of the result based on the database. Our system runs in two phases: first it converts the natural-language query to the SQL statement, and then it uses SQL query result to predict the possible visualization scheme. To test the efficiency of our system, we first examine the accuracy of the intermediate SQL query results on Spider. Finally, we present the generated charts to showcase the effectiveness of the visualization.", "uri": "https://vimeo.com/361161140", "name": "[VIS19 Preview] ChartNet: End-to-end Visual Answering System for Natural Language Questions (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:10:17+00:00", "description": "Poster \n\nAuthors: Guozheng Li , Min Tian , Qinmei Xu , Wentao Zhang , Michael McGuffin , Xiaoru Yuan\n\nAbstract: GoTree is a declarative grammar allowing clients to instantiate tree visualizations by specifying three aspects: visual elements, layout, and coordinate system. Within the set of all possible tree visualization techniques, we identify a subset of techniques that are both \"unit-decomposable\" and \"axis-decomposable\" (terms which we define). For techniques within this subset, GoTree gives the user flexible, fine-grained control over the parameters of the techniques, supporting both explicit (with visible links) and implicit (where links are implied by positioning or adjacency) techniques. Outside this subset, GoTree gives the user access to additional built-in techniques that are pre-defined, albeit without the same fine-grained control.", "uri": "https://vimeo.com/361161085", "name": "[VIS19 Preview] GoTree: A Grammar of Tree Visualizations (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:09:41+00:00", "description": "Poster \n\nAuthors: Mariana Shimabukuro , Christopher Collins\n\nAbstract: In this project, we present a visualization for investigating the relationship between commonly used words in Portuguese and their translations in English. This cross-linguistic analysis can help us to understand English word choices made by Portuguese native speakers and the influence of language transfer effects. In this paper, we discuss how word frequency is commonly used as a resource for both textual and cross-linguistic analysis. Moreover, we briefly explain the data processing pipeline building on machine translation and word frequencies from large corpora. This research reveals interesting open questions related to linguistic visualizations and future directions for investigating language transfer effects.", "uri": "https://vimeo.com/361160969", "name": "[VIS19 Preview] Cross-Linguistic Word Frequency Visualization for PT and EN (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:09:13+00:00", "description": "Poster \n\nAuthors: Joong-Youn Lee , Jinah Park\n\nAbstract: Streamline is one of the most frequently utilized visualization methods to analyze the flow structure of a CFD dataset. It is a challenging problem, however, to find a set of streamlines which shows the most prominent flow across the entire set of flow fields while preventing visual clutter. In this paper, we propose a smart seed location method for effective streamline placements. The proposed method evaluates every grid point in the dataset in terms of the importance score which consists of spirality and coverage to determine if a streamline from the point may reveal interesting flow features using a regression model. We adopt five different flow features as a feature vector for machine-learning models and evaluate the proposed method using a real-world CFD dataset.", "uri": "https://vimeo.com/361160866", "name": "[VIS19 Preview] Importance-based Streamline Seeding Method using a Regression Model (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:08:45+00:00", "description": "Poster \n\nAuthors: Alexander Hiemann , Thomas Kautz , Johannes Waschke , Mario Hlawitschka\n\nAbstract: Sensor-based data acquisition is an important aspect of training methodology in many different types of sports. Especially the analysis of data gathered from sensor-based positioning systems or motion sensors is a relevant source of information that is used to achieve performance optimization and to generate competitive advantages. Many recent approaches address the subject of sensor-fusion of athletes position data and high frequent motion data gathered from inertial measurement units (IMU) in order to enhance position accuracy and to increase the temporal resolution of position data. Besides this \u201ctechnological sensor fusion\u201d approach, data from both data sources can be integrated into a combined multi-modal visualization in order to provide deeper insights for training theory and to allow for further in-depth analysis. In this paper, we introduce a visualization-based information fusion approach that combines the insights from both data sources without the need for extensive technological integration. In addition, this visualization technique improves the interpretability of inertial data for athletes and coaches by establishing the relation to the spatial context of the position data. We use this visualization technique for a detailed analysis of motion data of athletes and accordingly to derive a detailed step analysis of skaters in short track and speed skating.", "uri": "https://vimeo.com/361160755", "name": "[VIS19 Preview] Visual Information Fusion of Spatio-Temporal Multi-Modal Data in Sports (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:08:14+00:00", "description": "Poster \n\nAuthors: Takayuki Itoh\n\nAbstract: Quality of training datasets is essential for the quality of machine learning. Machine learning projects often invite multiple workers for these annotation tasks for training dataset creation. It is important to observe what types of contents multiple workers make different annotations, or which workers often make abnormal annotations, to guarantee the quality of training datasets. This poster presents a tool for the visualization of abnormalness of annotations by multiple workers. The tool generates a matrix of abnormalness of annotations for each of the images by each of the workers and displays as a heatmap. This poster introduces an example using a training dataset where estimated ages are annotated to 7,748 pictures of human faces by eight workers.", "uri": "https://vimeo.com/361160644", "name": "[VIS19 Preview] Visualization of individual variation of multiple annotators working on training datasets for machine...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:07:50+00:00", "description": "Poster \n\nAuthors: Haejin Jeong , hyoung-oh Jeong , Assistant Professor Semin Lee , Won-Ki Jeong\n\nAbstract: Single-cell RNA sequencing (scRNA-seq) is increasingly used to study gene expression of cells at the single cell level. Analyzing scRNA-seq data allows analysts to discover and characterize cell types. In most cell type estimation processes, analysts need to identify differentially expressed genes in all cell populations and need to know which genes are associated with which cell types. This process is cumbersome and takes a long time. To solve this problem, we developed a visual analysis tool that quickly finds differentially expressed genes in each cell population and gives references for better cell type estimation. Our tool makes single-cell analysis more convenient. We proved the convenience of our tool through a user study which compares ours with existing tools.", "uri": "https://vimeo.com/361160553", "name": "[VIS19 Preview] scAnalyzer: an Interactive Tool for Convenient Visual Analysis and Exploration of Single-Cell Data (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:07:29+00:00", "description": "Poster \n\nAuthors: Laura South , Michail Schwab , Daniel Krasnonosenkikh , Lu Wang , Nick Beauchamp , Michelle A. Borkin\n\nAbstract: Political discourse surrounds us in our daily lives and the decisions reached through debates have lasting effects on our lives. No visualization framework currently exists to enable exploration and easy comprehension of political debate transcripts for a general audience. Current visualization frameworks are geared towards academic or research-driven audiences and are not suitable for more informal use cases. We conducted a design study to examine the tasks associated with a person who is casually interested in politics and wants to gain a general understanding of what happened in a debate in an efficient and comprehensible manner. We use this task analysis and abstraction to inform the construction of DebateVis, a novel framework for debate visualization that provides summaries of speaker behavior, including attack and agreement statements, and topic distributions throughout a given debate.", "uri": "https://vimeo.com/361160485", "name": "[VIS19 Preview] DebateVis: A Framework for Visualizing Political Debates (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:07:01+00:00", "description": "Poster infovis best poster\n\nAuthors: Zezhong Wang , Miss Lovisa Frida Johanna Sundin , Dr Dave Murray-Rust , Benjamin Bach\n\nAbstract: A cheat sheet is a set of concise graphical and textual explanations, inspired by infographics, data comics, and cheat sheets in other domains. Cheat sheets aim to support learning, teaching, and the regular use of both common and novel visualization techniques in a variety of contexts. To design cheat sheets for visualization techniques, we describe four components of a cheat sheet: anatomy, visual patterns, pitfalls and introduction and show examples for three visualization techniques. Our cheat sheets address the increasing need for accessible material that supports understanding data visualization techniques, their use, their fallacies and so forth by a larger audience.", "uri": "https://vimeo.com/361160398", "name": "[VIS19 Preview] Exploring Cheat Sheets for Data Visualization Techniques (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:06:08+00:00", "description": "Poster \n\nAuthors: Doris Jung-Lin Lee , Jaewoo Kim , Renxuan Wang , Aditya Parameswaran\n\nAbstract: Scatterplots are one of the simplest and most commonly-used visualizations for understanding quantitative, multidimensional data. However, since scatterplots only depict two attributes at a time, analysts often need to manually generate and inspect large numbers of scatterplots to make sense of large datasets with many attributes. We present a visual query system for scatterplots, SCATTERSEARCH, that enables users to visually search and browse through large collections of scatterplots. Users can query for other visualizations based on a region of interest or find other scatterplots that \u201clook similar\u201d to a selected one. We present two demo scenarios, provide a system overview of SCATTERSEARCH, and outline future directions.", "uri": "https://vimeo.com/361160201", "name": "[VIS19 Preview] SCATTERSEARCH: Visual Querying of Scatterplots Visualizations (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:05:44+00:00", "description": "Poster \n\nAuthors: Aditeya Pandey , Peter Bex , Michelle A. Borkin\n\nAbstract: Categorization tasks are common in everyday life, from sorting objects to a doctor diagnosing a patient's disease. In many categorization tasks, classification information is visually represented. Past work in psychology and information visualization has shown that anthropomorphic representations of data can aid in the quick understanding and recall of information. To utilize this phenomenon, categorization tasks can represent multidimensional binary classification information (e.g, symptom present/absent) as anthropomorphic glyphs. Categorization tasks can utilize this phenomenon and visually represent multidimensional binary classification information (e.g., symptom present/absent in a medical diagnosis) with anthropomorphic glyphs. However it remains to be investigated if anthropomorphic visualizations continue to be beneficial when conveying abstract information that is not directly related to parts of the human body. We study the effects of anthropomorphic and abstract glyph designs on the accuracy of abstract probabilistic categorization tasks. In our within-subject evaluation, 480 participants categorized two of four different glyph visualizations each of which encode 3 abstract probabilistic features. We hypothesized that if visual representation affects accuracy then anthropomorphic glyphs would lead to higher categorization accuracy. However, contrary to our hypothesis, subjects were significantly more accurate at categorization with the most abstract glyph design.", "uri": "https://vimeo.com/361160093", "name": "[VIS19 Preview] Effect of Glyph Design on Probabilistic Categorization Accuracy (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:05:19+00:00", "description": "Poster \n\nAuthors: Wesley Griffin , Katjana Krha\u010d , Kamran Sayrafian\n\nAbstract: Wireless Capsule Endoscopy (WCE) is the only painless and effective diagnostic technology for inspecting the entire gastrointestinal (GI) tract. A typical WCE includes a miniature camera that periodically transmits images as it passes through the GI tract. Comprehensive study of wireless communication for ingestible electronics like WCE is a very challenging task. To initiate this study, we previously developed an innovative immersive platform containing an enhanced 3D human body model. Our platform enables flexible placement of a capsule inside the GI tract and multiple receivers around the abdomen area. In this work, we describe a new tool for our platform: an interactive tool for the NIST CAVE to enable researchers to select a set of appropriate capsule locations along the centerline passing through the GI tract. The tool enabled researchers to identify a set of capsule locations that exhibit a more uniform distance distribution from the on-body receivers and thus lead to a more accurate statistical pathloss model for wireless communication.", "uri": "https://vimeo.com/361160006", "name": "[VIS19 Preview] Sample Location Selection for Statistical Pathloss Modeling of Wireless Capsule Endoscopy (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:04:31+00:00", "description": "Poster \n\nAuthors: Kensuke Yasufuku\n\nAbstract: Air traffic simulations are used to reproduce air traffic flow and evaluate changes under different conditions. In the development phase, visual analytics tools are of great use when dealing with a large amount of trajectory data from aircraft. This study proposes an interactive visual analytics tool that can verify the data assimilation step and the effectiveness of optimization. We implemented the 3D-CG projection method and visual operations so that the movements of aircraft can be grasped intuitively. The system also provides interactive GUI operation which enables the user to filter the arriving and landing aircraft of an airport even when airport information is not included in a large amount of the aircraft trajectory data.", "uri": "https://vimeo.com/361159836", "name": "[VIS19 Preview] An Interactive Visual Analytics Tool for Air Traffic Flow (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:03:45+00:00", "description": "Poster \n\nAuthors: Cristina R Ceja , Caitlyn M. McColeman , Cindy Xiong , Steven Franconeri\n\nAbstract: Position is regarded as the most precise form of visual encoding for data. But recent work has shown that position encodings (in the form of bar graphs) can also produce systematic biases, where viewers overestimate values when they reproduce the bars from memory. But this work used unconventional bars that were quite wide. Would these overestimation biases persist across different bar aspect ratios? We explored accuracy and bias in position reproductions across three aspect ratios of bars (Wide, Square, and Tall), while holding their area constant. Overestimation biases were strong for the Wide bars, weak for the Square bars, and absent in the Tall bars, presenting a perceptual psychology mystery to be solved.", "uri": "https://vimeo.com/361159643", "name": "[VIS19 Preview] Position Estimates in Bar Graphs: Error and Bias (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-19T21:03:23+00:00", "description": "Poster \n\nAuthors: Dr Victor Padilla-Sanchez PhD\n\nAbstract: Macromolecular assemblies like complete viruses are impossible to visualize with regular computers due to the large amount of necessary memory. Complete viruses like bacteriophage t4, hiv-1 and others are so big that regular computers can\u2019t visualize them therefore I have been using supercomputers to analyze these viruses as complete organisms for the last six months. This project started from the need to analyze entire organisms like bacteriophage t4 and hiv-1 viruses. UCSF Chimera molecular visualization software is used in this gpu supercomputers of huge ram memory where I have assembled together an entire hiv-1 infection molecular model, t4 bacteriophage on E. coli membrane and several other visualizations, using pdb structures and cryoEM reconstructions from the PDB and EMDB databanks. The project significance has been to visualize and analyze macromolecular assemblies like entire viruses and parts of cells at atomic resolution for the first time. Only with supercomputers I have achieved this goal of modelling, visualizing and analyzing these huge structural assemblies that can reach ~300 million atoms and beyond. I expect to continue using supercomputers and analyzing these macromolecular complexes at atomic resolution which for example may serve to develop viral display vaccines against diseases.", "uri": "https://vimeo.com/361159553", "name": "[VIS19 Preview] Macromolecular Assemblies Visualization with UCSF Chimera (poster)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-17T08:11:00+00:00", "description": "Honorable Mention\n\nAuthors: Takanori Fujiwara, Oh-Hyun Kwon, Kwan-Liu Ma\n\nAbstract: Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides a good first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additional analysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g., density-based clustering methods) to identify clusters, effective methods for understanding a cluster's characteristics are still lacking. A cluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforward task when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlights the essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastive principal component analysis (cPCA). Our method can calculate each feature's relative contribution to the contrast between one cluster and other clusters. With our cPCA-based method, we have created an interactive system including a scalable visualization of clusters' feature contributions. We demonstrate the effectiveness of our method and system with case studies using several publicly available datasets.", "uri": "https://vimeo.com/360484919", "name": "[VIS19 Preview] Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-17T08:08:19+00:00", "description": "Authors: Rafael Veras, Christopher Collins\n\nAbstract: The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure's utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.", "uri": "https://vimeo.com/360484488", "name": "[VIS19 Preview] Discriminability Tests for Visualization Effectiveness and Scalability (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-17T08:07:36+00:00", "description": "Authors: Arvind Satyanarayan, Bongshin Lee, Donghao Ren, Jeffrey Heer, John Stasko, John Thompson, Matthew Brehmer, Zhicheng Liu\n\nAbstract: An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed -- Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.", "uri": "https://vimeo.com/360484371", "name": "[VIS19 Preview] Critical Reflections on Visualization Authoring Systems (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-17T08:06:55+00:00", "description": "Authors: Amira Chalbi, Jacob Ritchie, Deokgun Park, Jungu Choi, Nicolas Roussel, Niklas Elmqvist, Fanny Chevalier\n\nAbstract: The Law of Common Fate from Gestalt psychology states that visual objects moving with the same velocity along parallel trajectories will be perceived by a human observer as grouped. However, the concept of common fate is much broader than mere velocity; in this paper we explore how common fate results from coordinated changes in luminance and size. We present results from a crowdsourced graphical perception study where we asked workers to make perceptual judgements on a series of trials involving four graphical objects under the influence of conflicting static and dynamic visual factors (position, size and luminance) used in conjunction. Our results yield the following rankings for visual grouping: motion &gt; (dynamic luminance, size, luminance); dynamic size &gt; (dynamic luminance, position); and dynamic luminance &gt; size. We also conducted a followup experiment to evaluate the three dynamic visual factors in a more ecologically valid setting, using both a Gapminder-like animated scatterplot and a thematic map of election data.The results indicate that in practice the relative grouping strengths of these factors may depend on various parameters including the visualization characteristics and the underlying data. We discuss design implications for animated transitions in data visualization.", "uri": "https://vimeo.com/360484281", "name": "[VIS19 Preview] Common Fate for Animated Transitions in Visualization (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-17T08:04:49+00:00", "description": "Authors: Kyle Hall, Adam Bradley, Uta Hinrichs, Samuel Huron, Jo Wood, Christopher Collins, Sheelagh Carpendale\n\nAbstract: While previous work exists on how to conduct and disseminate insights from problem-driven visualization projects and design studies, the literature does not address how to accomplish these goals in transdisciplinary teams in ways that advance all disciplines involved. In this paper we introduce and define a new methodological paradigm we call design by immersion, which provides an alternative perspective on problem-driven visualization work. Design by immersion embeds transdisciplinary experiences at the center of the visualization process by having visualization researchers participate in the work of the target domain (or domain experts participate in visualization research). Based on our own combined experiences of working on cross-disciplinary, problem-driven visualization projects, we present six case studies that expose the opportunities that design by immersion enables, including (1) exploring new domain-inspired visualization design spaces, (2) enriching domain understanding through personal experiences, and (3) building strong transdisciplinary relationships. Furthermore, we illustrate how the process of design by immersion opens up a diverse set of design activities that can be combined in different ways depending on the type of collaboration, project, and goals. Finally, we discuss the challenges and potential pitfalls of design by immersion.", "uri": "https://vimeo.com/360483922", "name": "[VIS19 Preview] Design by Immersion: A Transdisciplinary  Approach to Problem-Driven Visualizations (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-17T08:03:44+00:00", "description": "Best Paper\n\nAuthors: Jagoda Walny, Christian Frisson, Mieka West, Doris Kosminsky, S\u00f8ren Knudsen, Sheelagh Carpendale, Wesley Willett\n\nAbstract: Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations.  While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.", "uri": "https://vimeo.com/360483702", "name": "[VIS19 Preview] Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-17T03:46:29+00:00", "description": "Authors: Bahador Saket, Samuel Huron, Charles Perin, Alex Endert\n\nAbstract: We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.", "uri": "https://vimeo.com/360445782", "name": "[VIS19 Preview] Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-17T03:46:16+00:00", "description": "Authors: Evanthia Dimara, Charles Perin\n\nAbstract: Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "uri": "https://vimeo.com/360445743", "name": "[VIS19 Preview] What is Interaction for Data Visualization? (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-17T03:46:01+00:00", "description": "Authors: Fritz Lekschas, Michael Behrisch, Benjamin Bach, Peter Kerpedjiev, Nils Gehlenborg, Hanspeter Pfister\n\nAbstract: We present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario.", "uri": "https://vimeo.com/360445699", "name": "[VIS19 Preview] Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T21:41:11+00:00", "description": "Authors: Matthew Brehmer, Matthew Larsen, Daniela Oelke\n\nAbstract: VisInPractice at IEEE VIS 2019 will feature invited talks by visualization practitioners on Monday, October 21st, 2019 in Vancouver, BC, Canada at the Vancouver Convention Centre in Ballroom B. The VisInPractice program is an opportunity for visualization practitioners and researchers to meet and share experiences, insights, and ideas in applying visualization and visual analytics to real use cases.", "uri": "https://vimeo.com/360156318", "name": "[VIS19 Preview] VisInPractice 2019 (vip)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T21:40:54+00:00", "description": "Authors: Anastasia Bezerianos, Adam Perer, Chaoli Wang, Natalia Andrienko\n\nAbstract: Video FF for the Workshop schedule.", "uri": "https://vimeo.com/360156283", "name": "[VIS19 Preview] Workshop schedule (workshop)", "year": "2019", "event": "WORKSHOP, PREVIEW"}, {"created_time": "2019-09-15T21:40:42+00:00", "description": "(Panel) \n\nOrganizers / Panelists: Tina Winters, Juliana Freire, Jean-Daniel Fekete, Steve Haroz, Carlos Scheidegger\n\nAbstract: The National Academies of Sciences, Engineering, and Medicine has recently released the report Reproducibility and Replicability in Science. This report has prompted discussions within many disciplines about the extent of reproducibility and replicability challenges and strategies for improving. This panel discussion will begin with an overview of the National Academies reports, followed by comments from other panelists on particular reproducibility challenges faced by the visualization community and how those challenges might be addressed. Topics to be addressed include what should be reproduced in the vis world, web-based technology and replicability, what will be needed to reproduce and extend current vis work in the future, open research problems in supporting reproducibility and steps the vis community can take to broaden the adoption of reproducibility best practices.", "uri": "https://vimeo.com/360156262", "name": "[VIS19 Preview] Exploring Reproducibility in Vis: Expanding on the National Academies' Report on Reproducibility and...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T21:40:24+00:00", "description": "(Panel) \n\nOrganizers / Panelists: Brian David Fisher, Petra Isenberg, David H. Laidlaw, Melanie Tory, Daniel Weiskopf, Alfie Abdul-Rahman\n\nAbstract: Empirical studies are an indispensable component in visualization as they allow us to not only understand the interactions between humans and computers, but they also enable us to evaluate and confirm our results and experiments. In this panel, we ask five researchers \u2013 \u201cWhat are the top research questions for empirical studies in visualization?\u201d Our panelists will cover a range of topics including: - A focus on cognitive science as well as synthesizing theoretical perspectives; - Causes of empirical studies focusing on narrow hypotheses vs. broad hypotheses as well as mechanisms for tracking them; - Understanding human-computer interaction in visualization and how we can predict performance; - The need for a great diversity of empirical studies and the gap between research and practice; and - Empirical research beyond user studies, quantitative characterizations to evaluate visualization, as well as the role of eye tracking. Express your comments, questions, feedback, and suggestions at http://thisisalfie.com/top-research-questions", "uri": "https://vimeo.com/360156233", "name": "[VIS19 Preview] Top Research Questions for Empirical Studies in Visualization (panel)", "year": "2019", "event": "PANEL, PREVIEW"}, {"created_time": "2019-09-15T21:40:12+00:00", "description": "(Panel) \n\nOrganizers / Panelists: Brian David Fisher, Kristin Cook, Dr. David Ebert, Daniel Keim\n\nAbstract: Visual analytics began in 2004 with the publication of Illuminating the Path, an R&amp;D Agenda in Visual Analytics. After 15 years we have brought together leaders of the three groups that helped to build visual analytics: Kirstin Cook, co-editor of Illuminating the Path, Daniel Keim, a leader in VIsMaster, and David Ebert, the leader of VACCINE to discuss the history, present state, and future of visual analytics, \"the science of analytical reaoning facilitated by interactive visual interfaces\". In an open discussion with attendees panelists will give their perspectives on the origins and evolving goals of VA, what has worked, what has not, and what we have learned in the VA effort that might help us to build a more diverse and impactful VIS.", "uri": "https://vimeo.com/360156204", "name": "[VIS19 Preview] Visual Analytics Past, Present, and Future (panel)", "year": "2019", "event": "PANEL, PREVIEW"}, {"created_time": "2019-09-15T21:40:00+00:00", "description": "Authors: Phong H. Nguyen, Rafael Henkin, Siming Chen, Natalia Andrienko, Gennady Andrienko, Olivier Thonnard, Cagatay Turkay\n\nAbstract: User behaviour analytics (UBA) systems offer sophisticated models that capture users' behaviour over time with an aim to identify fraudulent activities that do not match with their profile. Making decisions based on such systems; however, requires an in-depth understanding of user behaviour. Motivated by this problem, we present a visual analytics approach to help analysts gain a comprehensive, multi-faceted understanding of user behaviour. As a multidisciplinary team of cyber-security experts and visualisation researchers, we take a user-centred approach to design a visual analytics framework supporting the analysis of collections of users, their organisations and the numerous sessions of activities they conduct on digital applications. The framework is centred around the concept of interactive visual user profiles, where the profiles are built based on features derived from user sessions and visualised with task-informed designs to facilitate interactive exploration and investigation. We externalise a series of analyst goals and tasks, and evaluate our methods through a number of use cases that demonstrate how these tasks are addressed. We observe that with the aid of interactive visual interfaces, analysts are able to conduct exploratory and investigative analysis effectively, and able to unravel the intricacies of user behaviour to make decisions effectively in the evaluation of suspicious users and activities.", "uri": "https://vimeo.com/360156177", "name": "[VIS19 Preview] VASABI: Doing User Behaviour Analytics through Interactive Visual Hierarchical User Profiles (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:39:47+00:00", "description": "Authors: Jieqiong Zhao, Morteza Karimzadeh, Luke Snyder, Cittayong Surakitbanharn, Zhenyu Cheryl Qian, David Ebert\n\nAbstract: Evaluating employee performance in organizations with varying workloads and tasks is a challenging problem. An efficient evaluation system can help an organization to understand shortcomings and improve overall productivity. However, it is important to understand how quantitative measurements of employee achievements relate to supervisor observations (match or mismatch), what the main drivers of good performance are, and how to combine these complex and flexible performance evaluation metrics into an accurate portrayal of organizational performance. To facilitate this process, we summarized common organizational performance analyses into four visual exploration task categories. Additionally, we developed MetricsVis, a unique visual analytics framework composed of multiple coordinated views to support the dynamic evaluation and comparison of individual, team, and organizational performance. MetricsVis provides three primary visual components to expedite performance evaluation: (1) a reorderable performance matrix to demonstrate the details of individual employees; (2) a group performance view that highlights aggregate performance and individual contributions for each group; (3) a projection view to illustrate employees with similar specialties to facilitate shift assignments and training. We demonstrate both the usability of our framework with two case studies from medium-sized law enforcement agencies and highlight its broader applicability to other domains.", "uri": "https://vimeo.com/360156150", "name": "[VIS19 Preview] MetricsVis: A Visual Analytics Framework for Evaluating Individual, Team, and Organization Performance (vast...", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:39:23+00:00", "description": "Authors: Anjul Tyagi, Klaus Mueller, Zhen Cao, Tyler Estro, Erez Zadok\n\nAbstract: There are many applications where users seek to explore the impact of the settings of several categorical variables with respect to one dependent numerical variable. For example, a computer systems analyst might want to study how the type of file system or storage device affects system performance. A usual choice is the method of Parallel Sets designed to visualize multivariate categorical variables. However, we found that the magnitude of the parameter impacts on the numerical variable cannot be easily observed here. We also attempted a dimension reduction approach based on Multiple Correspondence Analysis but found that the SVD-generated 2D layout resulted in a loss of information. We hence propose a novel approach, the Interactive Configuration Explorer (ICE), which directly addresses the need of analysts to learn how the dependent numerical variable is affected by the parameter settings given multiple optimization objectives. No information is lost as ICE shows the complete distribution and statistics of the dependent variable in context with each categorical variable. Analysts can interactively filter the variables to optimize for certain goals such as achieving a system with maximum performance, low variance, etc. Our system was developed in tight collaboration with a group of systems performance researchers and its final effectiveness was evaluated with expert interviews, a comparative user study, and two case studies.", "uri": "https://vimeo.com/360156102", "name": "[VIS19 Preview] ICE: An Interactive Configuration Explorer for High Dimensional Parameter Spaces (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:38:54+00:00", "description": "Authors: Robert Kr\u00fcger, Johanna Beyer, Won-Dong Jang, Nam Wook Kim, Artem Sokolov, Peter Sorger, Hanspeter Pfister\n\nAbstract: Facetto is a scalable visual analytics application to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of tumor and other tissue samples. This data is collected using Cyclic Immuno-Fluorescence (CyCIF), an optical microscopy method that obtains detailed images of the tumor micro-environment for better diagnostics and therapies. The resulting data contains dozens of features for millions of cells, which renders manual analysis impossible. Likewise, purely automated methods fail because the input data varies too much due to experimental process variations and the unknown number of different cell types. To cope with these challenges, Facetto enables a semi-automated analysis of the image and feature space together with tools for analytical provenance. We enable detailed cell type discovery by integrating unsupervised learning (Expectation Maximization Clustering) and supervised classification (Convolutional Neural Networks) into the image and feature exploration process. Experts can cluster the data to discover new cancer and immune types and use the clustering results to train a network that classifies new cells accordingly. Likewise, a classifier\u2019s output can be clustered to discover cell phenotypes. To support the phenotype analysis, we developed a novel hierarchical approach to keep track of the performed analysis steps and the extracted data subsets (e.g., cells of a specific phenotype). During the analysis, users can build up a phenotype tree and visually interact with the resulting hierarchical structures of the high-dimensional feature and image space. We report on use-cases and qualitative expert feedback where domain scientists have used Facetto to explore various large-scale CyCIF datasets. We demonstrate how Facetto helped them in steering the clustering and classification process, analyze and understand the analysis results, and gain new scientific insights into cell phenotypes.", "uri": "https://vimeo.com/360156054", "name": "[VIS19 Preview] Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T21:37:35+00:00", "description": "Authors: Sebastian Gehrmann, Hendrik Strobelt, Robert Kr\u00fcger, Kathryn Hite, Hanspeter Pfister, Alexander M. Rush\n\nAbstract: Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem. To show the feasibility of CSI, we present a co-designed case study of a document summarization system. The study demonstrates how humans can retain agency without losing the benefits of using deep learning systems.", "uri": "https://vimeo.com/360155863", "name": "[VIS19 Preview] Visual Interaction with Deep Learning Models through Collaborative Semantic Inference (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:37:12+00:00", "description": "Authors: Shusen Liu, Di Wang, Dan Maljovec, Rushil Anirudh, Jayaraman J. Thiagarajan, Sam Ade Jacobs, Brian C. Van Essen, David Hysom, Jae-Seung Yeom, Jim Gaffney, J. Luc Peterson, Peter B. Robinson, Harsh Bhatia, Valerio Pascucci, Brian K. Spears, Peer-Timo Bremer\n\nAbstract: With the rapid adoption of machine learning techniques for large-scale applications in science and engineering comes the convergence of two grand challenges in visualization. First, the utilization of black box models (e.g., deep neural networks) calls for advanced techniques in exploring and interpreting model behaviors. Second, the rapid growth in computing has produced enormous data sets, which requires techniques that can handle millions or more samples. Although some solutions to these interpretability challenges have been proposed, they typically do not scale beyond thousands of samples, nor do they provide the high-level intuition scientists are looking for. Here, we present the first scalable solution to explore and analyze high-dimensional functions often encountered in the scientific data analysis pipeline. By combining a new streaming neighborhood graph construction, the corresponding topology computation, and a novel data aggregation scheme, called \\textit{topology aware datacubes}, we enable interactive exploration of both the topological and the geometric aspect of high-dimensional data. Following two use cases from high-energy-density (HED) physics and computational biology, we demonstrate how these capabilities have led to crucial new insights in both applications.", "uri": "https://vimeo.com/360155819", "name": "[VIS19 Preview] Scalable Topological Data Analysis and Visualization for Interpreting Data-Driven Models in Scientific...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T21:36:57+00:00", "description": "Authors: Jiali Liu, Nadia Boukhelifa, James Eagan\n\nAbstract: Data workers come from various domains and engage in data analysis activities as a part of their daily work, but do not formally identify as data scientists. These data workers often need to explore alternatives throughout the sensemaking process, ranging from a diverse set of hypotheses and theories, to a variety of data sources, algorithms, methods, tools, and visual designs. We conducted semi-structured interviews with 12 data workers with different types of expertise in order to better understand and characterize the role of alternatives in their analyses. We conducted four types of analysis to understand (1) why data workers explore alternatives; (2) the different notions of alternatives and how they fit into the sensemaking process; (3) the high-level processes around alternatives; and (4) their strategies to generate, explore, and manage those alternatives. We find that participants' diverse level of domain and computational expertise, experience with different tools, and collaboration within their broader context plays an important role in how they explore these alternatives. These findings call out the need for more attention towards a deeper understanding of alternatives and the need for better tools to facilitate the exploration, interpretation and management of alternatives. Drawing upon these analyses and findings, we present a framework based on participants\u2019 (1) level of focus, (2) abstraction level, (3) analytic processes, and (4) strategies employed. We show how this framework can help understand how data workers consider such alternatives in their analyses and how tool designers might create tools to better support them.", "uri": "https://vimeo.com/360155794", "name": "[VIS19 Preview] Understanding the Role of Alternatives in Data Analysis Practices (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:36:45+00:00", "description": "Honorable Mention\n\nAuthors: Subhashis Hazarika, Haoyu Li, Ko-Chih Wang, Han-Wei Shen, Ching-Shan Chou\n\nAbstract: Complex computational models are often designed to simulate real-world physical phenomena in many scientific disciplines. However, these simulation models tend to be computationally very expensive and involve a large number of simulation input parameters which need to be analyzed and properly calibrated before the models can be applied for real scientific studies. We propose a visual analysis system to facilitate interactive exploratory analysis of high-dimensional input parameter space for a complex yeast cell polarization simulation. The proposed system can assist the computational biologists, who designed the simulation model, to visually calibrate the input parameters by modifying the parameter values and immediately visualizing the predicted simulation outcome without having the need to run the original expensive simulation for every instance. Our proposed visual analysis system is driven by a trained neural network-based surrogate model as the backend analysis framework. Surrogate models are widely used in the field of simulation sciences to efficiently analyze computationally expensive simulation models. In this work, we demonstrate the advantage of using neural networks as surrogate models for visual analysis by incorporating some of the recent advances in the field of uncertainty quantification, interpretability and explainability of neural network-based models. We utilize the trained network to perform interactive parameter sensitivity analysis of the original simulation at multiple levels-of-detail as well as recommend optimal parameter configurations using the activation maximization framework of neural networks. We also facilitate detail analysis of the trained network to extract useful insights about the simulation model, learned by the network, during the training process. We perform two case studies and evaluate our results by comparing with the original simulation model outcomes as well as the findings from previous parameter analysis performed by our experts.", "uri": "https://vimeo.com/360155766", "name": "[VIS19 Preview] NNVA: Neural Network Assisted Visual Analysis of Yeast Cell Polarization Simulation (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:36:31+00:00", "description": "Authors: Nicola Pezzotti, Julian Thijssen, Alexander Mordvintsev, Thomas H\u00f6llt, Baldur van Lew, Boudewijn P.F. Lelieveldt, Elmar Eisemann, Anna Vilanova\n\nAbstract: In recent years the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has become one of the most used and insightful techniques for exploratory data analysis of high-dimensional data. It reveals clusters of high-dimensional data points at different scales while only requiring minimal tuning of its parameters. Despite this, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of t-SNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the t-SNE embedding for large datasets. In this work, we present a novel approach to the minimization of the t-SNE objective function that heavily relies on modern graphics hardware and has linear computational complexity. Our technique decreases the computational cost of running t-SNE on datasets by orders of magnitude and retains or improves on the accuracy of past approximated techniques. We propose to approximate the repulsive forces between data points by splatting kernel textures for each data point. This approximation allows us to reformulate the t-SNE minimization problem as a series of tensor operations that can be efficiently executed on the graphics card. An efficient implementation of our technique is integrated and available for use in the widely used Google TensorFlow.js, and the High-Dimensional Inspector (HDI) C++ library.", "uri": "https://vimeo.com/360155729", "name": "[VIS19 Preview] GPGPU Linear Complexity tSNE Optimization (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:36:11+00:00", "description": "Authors: Michael Behrisch, Tobias Schreck, Hanspeter Pfister\n\nAbstract: Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads, hence, us to the question of trustworthiness and explainability of these fully automated\u2014often heuristic\u2014black-box processes. We present GUIRO, a Visual Analytics system that helps general users, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering and rearranging of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated the usefulness of GUIRO in a guided explorative user study with 12 subjects. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. Moreover, we demonstrate the workflow of our system in a case study. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.", "uri": "https://vimeo.com/360155686", "name": "[VIS19 Preview] GUIRO: User-Guided Matrix Reordering (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:35:56+00:00", "description": "Authors: Ji Qi, Vincent Bloemen, Shihan Wang, Jarke van Wijk, Huub van de Wetering\n\nAbstract: While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period of time, and how does such comparison change over time. This paper presents a visual technique named STBins to answer those questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal bins. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.", "uri": "https://vimeo.com/360155660", "name": "[VIS19 Preview] STBins: Visual Tracking and Comparison of Multiple Data Sequences using Temporal Binning (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:35:44+00:00", "description": "Authors: Gromit Yeuk-Yin Chan, Luis Gustavo Nonato, Alice Chu, Preeti Raghavan, Viswanath Aluru, Claudio Silva\n\nAbstract: The brachial plexus is a complex network of peripheral nerves that enables sensing from and control of the movements of the arms and hand. Injuries to the brachial plexus invariably involve changes to the muscles, and impede movement, particularly in the absence of timely nerve repair. Nowadays, there is still a lack of understanding on the coordination between the muscles to generate simple movements, leading to confusion as to how to best treat patients with this type of peripheral nerve injury. Sophisticated motion analysis assessments can produce a rich dataset of electromyographic signals from multiple muscles recorded with joint movements during real-world tasks. These assessments provide detailed information on muscle coordination during natural movements to develop customized strategies to restore arm and hand functions. However, tools for the analysis and visualization of the data in a succinct and interpretable manner are currently not available. Thus, the usefulness of the available data in answering clinical questions and generating hypotheses for research is limited. To address this challenge, we have developed Motion Browser, an interactive visual analytics system that helps users analyze bundles of time series integrated from muscle signals with movement and visual task information. Motion Browser provides an efficient framework to extract and compare muscle activity patterns across both arms of a single individual or across individuals, and to help users explore and interpret the multimodal data in the context of different tasks. The system was developed as a result of a collaborative endeavor between computer scientists and orthopedic surgery and rehabilitation physicians. We present case studies which show that physicians are able to utilize the information displayed to understand how individuals with brachial plexus injuries use their muscles to initiate appropriate treatment and generate new hypotheses for future research.", "uri": "https://vimeo.com/360155635", "name": "[VIS19 Preview] Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T21:35:31+00:00", "description": "Authors: Dong Sun, Renfei Huang, Yong Wang, Yuanzhe Chen, Jia Zeng, Mingxuan Yuan, Ting-Cheun Pong, Huamin Qu\n\nAbstract: Production planning in the manufacturing industry is crucial for fully utilizing factory resources (e.g., machines, raw materials, product components and workers) and reducing costs. With the advent of industry 4.0, plenty of data recording the status of factory resources have been collected and further involved in production planning, which brings an unprecedented opportunity to understand, evaluate and adjust complex production plans through a data-driven approach. However, developing a systematic analytics approach for production planning is often challenging due to the large volume of production data, the complex dependence between products, and unexpected changes in the market demand, material supplies and the production process. Previous studies only provide summarized results and fail to show details for comparative analysis of production plans. Besides, the rapid adjustment to the plan in the case of an unanticipated incident is also not supported. In this paper, we propose PlanningVis, a visual analytics system to support the exploration and comparison of production plans with three levels of details: a plan overview presenting the overall difference between plans, a product view visualizing various properties of individual products, and a production detail view displaying the product dependency, the daily production output and production capacity usage in related factories. By integrating an automatic planning algorithm with interactive visual explorations, PlanningVis is able to facilitate efficient optimization of daily production planning as well as supporting a quick response to unanticipated incidents in manufacturing. Two case studies with real-world data and carefully designed interviews with domain experts from a world-leading manufacturing company demonstrate the effectiveness and usability of PlanningVis.", "uri": "https://vimeo.com/360155601", "name": "[VIS19 Preview] PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:35:12+00:00", "description": "Authors: Yao Ming, Panpan Xu, Furui Cheng, Huamin Qu, Liu Ren\n\nAbstract: Recently we have witnessed a growing adoption of deep sequence models in many application domains including predictive health care, natural language analysis and machine log analysis due to their superior performance compared to traditional machine learning models. However, the intricate process of training and fine-tuning the models confines their accessibility to expert machine learning practitioners. The black-box nature of many deep sequence models (e.g. LSTMs) also makes it a difficult task to incorporate domain-specific knowledge or constraints known to the domain experts in the model. In ProtoVis (Prototype Visualization), we tackle the challenge of directly involving the domain experts in steering a deep sequence model without relying on machine learning practitioners as intermediaries. Our approach utilizes ProSeNet (Prototype Sequence Network), which combines deep sequence modeling with prototype learning for both predictive accuracy and interpretability. Prototype learning is a form of case-based reasoning which imitates the common human problem-solving process of consulting past experiences to solve new problems. The key component of ProSeNet is a small set of exemplar cases (i.e. prototypes) which are constructed using historical data. In ProtoVis they serve both as an efficient visual summary of the original data and explanations of the model decisions. With ProtoVis the domain experts can inspect, critique and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies on a wide range of application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results of the case studies and the expert interviews demonstrate that ProtoVis can indeed help domain experts obtain interpretable models with more concise prototypes. Quantitative results also show that the fine-tuned model is able to incorporate user specified prototypes with similar predictive accuracy as the original model.", "uri": "https://vimeo.com/360155554", "name": "[VIS19 Preview] Steering Deep Sequence Model with Prototypes (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:34:59+00:00", "description": "Authors: Mosab Khayat, Morteza Karimzadeh, Jieqiong Zhao, David Ebert\n\nAbstract: Social media applications such as Twitter are filled with Spambots. Detecting these malicious accounts are essential yet challenging as they continually evolve and evade traditional detection techniques. In this work, we propose VASSL, a visual analytics tool that aims at assisting the detection and labeling process. Our tool enhances the performance and the scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling techniques which provide new insights that enable discriminating spambots. The tool allows users to select and analyze groups of accounts in an interactive manner, which would allow for detecting spambots that cannot be identified when examined individually.", "uri": "https://vimeo.com/360155523", "name": "[VIS19 Preview] VASSL: A Visual Analytics Toolkit for Social Spambot Labeling (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:34:43+00:00", "description": "Authors: Mosab Khayat, Morteza Karimzadeh, David Ebert, Arif Ghafoor\n\nAbstract: Many evaluation methods have been applied to assess the usefulness of Visual Analytics (VA) solutions. These methods branch from a variety of origins with different assumptions and goals which cause confusion about their proofing capabilities. Moreover, the seldom discussion about the evaluation processes may limit our potentials to develop new evaluation methods specialized for VA. In this paper, we present an analysis of summative evaluation methods that have been used to assess VA solutions. We propose a new metric called summative quality and use it during the analysis to rank evaluation methods according to their quality of proving usefulness. First, we provide a survey of the evaluation methods that appear in VAST literature in the past two years. Then, we show our analysis of these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them.", "uri": "https://vimeo.com/360155500", "name": "[VIS19 Preview] The Validity and Generalizability of Summative Evaluation Methods in Visual Analytics (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:33:38+00:00", "description": "Authors: Shuai Chen, Sihang Li, Siming Chen, Xiaoru Yuan\n\nAbstract: We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and insight analysis of the process of information reposting in social media. As one single original social media post can bring large cascades of repostings (i.e, retweets) on online networks, involving thousands, even millions of people with different opinions and chains of information propagation. In our proposed R-Map, the reposting tree structures can be spatialized with the highlighted key players and embedded semantic information. The whole reposting progress, with reposting and the following information, together with the semantic opinion relationships are represented as rivers, bridges, and routes, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation on the major players' influence on the information diffusion process. We demonstrate our implemented system on real", "uri": "https://vimeo.com/360155374", "name": "[VIS19 Preview] R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:33:20+00:00", "description": "Authors: Yongsu Ahn, Yu-Ru Lin\n\nAbstract: Data-driven decision making about individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts on proposing fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. A viable tool to facilitate fair decision making is in an urgent need. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions -- understanding, measuring, diagnosing and mitigating biases -- that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.", "uri": "https://vimeo.com/360155342", "name": "[VIS19 Preview] FairSight: Visual Analytics for Fairness in Decision Making (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:33:00+00:00", "description": "Authors: Minjeong Shin, Alexander Soen, Benjamin T. Readshaw, Stephen Michael Blackburn, Mitchell Whitelaw, Lexing Xie\n\nAbstract: We present the Influence Flower, a new visual metaphor for the influence profile of academic entities, including people, projects, institutions, conferences and journals. While many tools quantify influence, we aim to expose the flow of influence between entities. The Influence Flower is an ego-centric graph, with a query entity placed in the centre. The petals are styled to reflect the strength of influence to and from other entities of the same or different type. For example, one can break down the incoming and outgoing influences of a research lab by research topics. The Influence Flower uses a recent snapshot of Microsoft Academic Graph, consisting of 200+ million authors, their publications, and 1+ billion citations. An interactive web app, InfluenceMap, is constructed around this central metaphor for searching and curating visualisations. We also propose a visual comparison method that highlights change in influence patterns over time. We demonstrate through several case studies that the Influence Flower supports data-driven inquiries about researchers\u2019 careers over time; about paper(s) and projects, including those with delayed recognition; about the interdisciplinary profile of a research institution; about the shifting topical trends in conferences. We also use this tool on influence data beyond academic citations, by contrasting the academic and Twitter activities of a researcher.", "uri": "https://vimeo.com/360155315", "name": "[VIS19 Preview] Influence Flowers of Academic Entities (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:32:16+00:00", "description": "Authors: \u00c0ngel Alexander Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng, Jamie Morgenstern, Duen Horng Chau\n\nAbstract: The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FairVis' coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public in understanding and creating more equitable algorithmic systems.", "uri": "https://vimeo.com/360155233", "name": "[VIS19 Preview] FairVis: Visual Analytics for Discovering Intersectional Bias in Machine Learning (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:31:46+00:00", "description": "Authors: Dylan Cashman, Adam Perer, Remco Chang, Hendrik Strobelt\n\nAbstract: Deep learning models require the configuration of many layers and parameters in order to get good results. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case and validation study, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space", "uri": "https://vimeo.com/360155168", "name": "[VIS19 Preview] Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:31:31+00:00", "description": "Authors: Melanie Tory, Vidya Setlur\n\nAbstract: Natural language can be a useful modality for creating and interacting with visualizations but users often have unrealistic expectations about the intelligence of natural language systems. The gulf between user expectations and system capabilities may lead to a disappointing user experience. So \u2014 if we want to engineer a natural language system, just how smart does it need to be? This work takes a retrospective look at how we answered this question in the design of Ask Data, a natural language interaction feature for Tableau. Specifically, we examine two aspects of perceived system intelligence: the ability to understand the analytic intent behind an input utterance and the ability to interpret an utterance contextually (i.e. taking into account the current visualization state and recent actions). Our aim was to understand the extent to which a system would need to support these two aspects of intelligence to enable a positive user experience. We first describe a pre-design Wizard of Oz study that offered insight into this question and narrowed the space of designs under consideration for development. We then reflect on the impact of this study on system development, examining how design implications from the study played out in practice. Our work contributes insights for the design of natural language interaction in visual analytics as well as a reflection on the value of pre-design empirical studies in the development of visual analytic systems.", "uri": "https://vimeo.com/360155144", "name": "[VIS19 Preview] Do What I Mean, Not What I Say! Design Considerations for Supporting Intent and Context in Analytical...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T21:31:12+00:00", "description": "Authors: Thomas E. Polk, Dominik J\u00e4ckle, Johannes H\u00e4u\u00dfler, Jing Yang\n\nAbstract: Tennis players and coaches of all proficiency levels seek to understand and improve their play. Summary statistics alone are inadequate to provide the insights players need to improve their games. Spatio-temporal data capturing player and ball movements is likely to provide the actionable insights needed to identify player strengths, weaknesses, and strategies. To fully utilize this spatio-temporal data, we need to integrate it with domain-relevant context meta-data. In this paper, we propose CourtTime, a novel approach to perform data-driven visual analysis of individual tennis matches. Our visual approach introduces a novel visual metaphor, namely 1-D Space-Time Charts that enable the analysis of single points at a glance based on small multiples. We also employ user-driven sorting and clustering techniques and a layout technique that aligns the last few shots in a point to facilitate shot pattern discovery. We discuss the usefulness of CourtTime via an extensive case study and report on feedback from a tennis player and a teaching pro.", "uri": "https://vimeo.com/360155110", "name": "[VIS19 Preview] CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:30:18+00:00", "description": "Authors: Frederik L. Dennig, Thomas E. Polk, Zudi Lin, Tobias Schreck, Hanspeter Pfister, Michael Behrisch\n\nAbstract: The detection of interesting patterns in large high-dimensional datasets is difficult because of their dimensionality and pattern complexity. Therefore, analysts require automated support for the extraction of relevant patterns. In this paper, we present FDive, a visual active learning system that helps to create visually explorable relevance models assisted by learning a pattern-based similarity. We use a small set of user-provided labels to rank similarity measures, consisting of feature descriptor and distance function combinations, by their ability to distinguish relevant from irrelevant data. Based on the best-ranked similarity measure, the system calculates an interactive Self-Organizing Map-based relevance model, which classifies data according to the cluster affiliation. It also automatically prompts further relevance feedback to improve its accuracy. Uncertain areas, especially near the decision boundaries, are highlighted and can be refined by the user. We evaluate our approach by comparison to state-of-the-art feature selection techniques and demonstrate the usefulness of our approach by a case study classifying electron microscopy images of brain cells. The results show that FDive enhances both the quality and understanding of relevance models and can thus lead to new insights for brain research.", "uri": "https://vimeo.com/360155013", "name": "[VIS19 Preview] FDive: Learning Relevance Models using Pattern-based Similarity Measures (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:30:02+00:00", "description": "Authors: David Borland, Wenyuan Wang, Jonathan Zhang, Joshua Shrestha, David Gotz\n\nAbstract: The collection of large, complex datasets has become commonplace across a wide variety of domain. Visual analytics tools are increasingly playing a key role in exploring and answering complex questions about these large datasets. However, many visualizations are not designed to concurrently visualize the large number of dimensions present in complex datasets (e.g. tens of thousands of distinct codes in an electronic health record system). This fact, combined with the ability of many visual analytics systems to enable rapid, ad-hoc specification of groups, or cohorts, of individuals based on a small subset of visualized dimensions, leads to the possibility of introducing selection bias--when a given cohort is created based on a specified set of dimensions, differences across many other unseen dimensions may also be introduced. These unintended side effects may result in the cohort no longer being representative of the larger population intended to be studied, and can negatively affect the validity of any subsequent analysis. We present techniques for selection bias tracking and visualization that can be incorporated into high-dimensional exploratory visual analytics systems. These techniques include: (1) tree-based cohort provenance and visualization, with a user-specified baseline cohort that all other cohorts are compared against, and visual encoding of the ``drift'' for each cohort, indicating where selection bias may have occurred, and (2) two novel visualization approaches to compare in detail the per-dimension differences between the baseline and a user-specified focus cohort, based on existing data hierarchies. We present example use cases in the context of a medical temporal event sequence visual analytics tool, and report findings from domain expert user interviews.", "uri": "https://vimeo.com/360154972", "name": "[VIS19 Preview] Selection Bias Tracking and Detailed Subset Comparison for High-Dimensional Data (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:29:49+00:00", "description": "Authors: Zeyu Li, Changhong Zhang, Shichao Jia, Jiawan Zhang\n\nAbstract: Revealing the evolution of science and intersections among its sub-fields is extremely important for understanding the characteristics of discipline, discovering new topics, and even predicting the future. The current works either focus on building a skeleton of science, lacking interaction, detail exploration and interpretation, or on the lower topic level, missing the high-level macro perspective. To fill this gap, we design and implement Galex, a hierarchical visual analysis system, combining with advanced text mining technologies, could help analysts to comprehend the evolution and intersection of one discipline rapidly. We divide Galex into the following three progressively fine-grained levels: discipline level, area level and institution level. The cooperations between interactions enable analysts to explore an arbitrary piece of history and an arbitrary part of the knowledge space of one discipline. Using a flexible spotlight component, analysts could freely choose and quickly understand an exploration region. A tree metaphor allows perceiving the expansion, decline and intersection of topics intuitively. A synchronous spotlight interaction helps to compare research content among institutions easily. Three case studies and expert reviews demonstrate the effectiveness of our system.", "uri": "https://vimeo.com/360154945", "name": "[VIS19 Preview] Galex: Exploring the Evolution and Intersection of Disciplines (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:29:28+00:00", "description": "Authors: David Gotz, Jonathan Zhang, Wenyuan Wang, Joshua Shrestha, David Borland\n\nAbstract: Temporal event data are collected across a broad range of domains, and a variety of visual analytics techniques have been developed to empower analysts working with this form of data. These techniques generally display aggregate statistics computed over sets of event sequences that share common patterns. Such techniques are often hindered, however, by the high-dimensionality of many real-world event sequence datasets because the large number of distinct event types within such data prevents effective aggregation. A common coping strategy for this challenge is to group event types together as a pre-process, prior to visualization, so that each group can be represented within an analysis as a single event type. This approach can be highly effective because it directly reduces the dimensionality of the event dataset. However, computing these event groupings as a pre-process also places significant constraints on the analysis. This paper presents a new visual analytics approach for dynamic hierarchical dimension aggregation. The approach leverages a predefined hierarchy of dimensions to computationally quantify the informativeness of alternative levels of grouping within the hierarchy at runtime. This information is then interactively visualized, enabling users to dynamically explore the hierarchy to select the most appropriate level of grouping to use at any individual step within an analysis. Key contributions include an efficient and tunable algorithm for interactively determining the most informative set of event groupings for a specific analysis context from within a large-scale hierarchy of event types, and a scented scatter-plus-focus visualization design with an optimization-based layout algorithm that supports interactive hierarchical exploration of alternative event type groupings. While these contributions are generalizable to other types of problems, we apply them to high-dimensional event sequence analysis using large-scale event type hierarchies from the medical domain. We describe their use within a medical cohort analysis tool called Cadence, demonstrate an example in which the proposed technique supports better views of event sequence data, and report findings from domain expert interviews.", "uri": "https://vimeo.com/360154915", "name": "[VIS19 Preview] Visual Analysis of High-Dimensional Event Sequence Data via Dynamic Hierarchical Aggregation (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:29:16+00:00", "description": "Authors: Haipeng Zeng, Xingbo Wang, Aoyu Wu, Yong Wang, Quan Li, Alex Endert, Huamin Qu\n\nAbstract: Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually checking and exploring the presentation videos is often tedious and time-consuming and there still lacks an effective tool to help users conduct an efficient and in-depth multi-level analysis. In this paper, we propose EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotional coherence across text, facial and audio modalities in TED Talk videos. Our visualization system features a channel view and a projection view that together enable users to obtain a quick overview of emotional coherence and its temporal evolution. In addition, a detail view and word view enable detailed emotion exploration and comparison from the frame level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios and interviews with domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotional coherence in presentations.", "uri": "https://vimeo.com/360154890", "name": "[VIS19 Preview] EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:29:00+00:00", "description": "Authors: Luke Snyder, Yi-Shan Lin, Morteza Karimzadeh, Dan Goldwasser, David Ebert\n\nAbstract: Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework in the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.", "uri": "https://vimeo.com/360154855", "name": "[VIS19 Preview] Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:28:40+00:00", "description": "Authors: Yuxin Ma, Tiankai Xie, Jundong Li, Ross Maciejewski\n\nAbstract: Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.", "uri": "https://vimeo.com/360154813", "name": "[VIS19 Preview] Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:28:26+00:00", "description": "Authors: Alex Bigelow, Carolina Nobre, Miriah Meyer, Alexander Lex\n\nAbstract: Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.", "uri": "https://vimeo.com/360154783", "name": "[VIS19 Preview] Origraph: Interactive Network Wrangling (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:28:12+00:00", "description": "Authors: Thilo Spinner, Udo Schlegel, Hanna Schaefer, Mennatallah El-Assady\n\nAbstract: Interactive and explainable machine learning have become invaluable when promoting trust and accountability in automated decision-making. In this paper, we propose a framework for interactive and explainable machine learning. Its core is the XAI pipeline comprising three phases: (1) the understanding of machine learning models, (2) the diagnosis of model limitations using different explainable AI methods and (3) the refinement and optimization of models. The pipeline is embedded in a larger framework of eight global monitoring and steering mechanisms including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for explainable and interactive machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with six participants across different expertise levels to examine how our system influences their workflow and decision-making processes. The results confirm that our tightly integrated system leads to an interactive and more informed machine learning process.", "uri": "https://vimeo.com/360154764", "name": "[VIS19 Preview] explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:27:58+00:00", "description": "Authors: Ke Xu, Yun Wang, Leni Yang, Yifang Wang, Bo Qiao, Si Qin, Yong Xu, Haidong Zhang, Huamin Qu\n\nAbstract: Cloud computing is pervasively used by businesses and individuals for big data analysis and services nowadays. Detecting and analyzing potential anomalous performances in cloud computing systems is essential for avoiding losses to customers, as well as ensuring the efficient operation of the system. To this end, recent research has developed a variety of automated techniques to identify the anomalies in cloud computing performance, and these techniques are usually conducted by tracking the performance metrics of the system (e.g., CPU, memory, and disk I/O), represented by a multivariate time series. However, given the complex characteristics of these performance data, such as the huge data volume and time variation, the effectiveness of these automated methods is affected. Thus substantial human judgment on the automated analysis results is required for anomaly interpretation. In this paper, we present a unified visual analytics system named CloudDet to interactively detect, inspect, and diagnose anomalies in cloud computing. A novel unsupervised anomaly detection algorithm is developed to identify the anomalies based on the specific temporal pattern of the given metrics data (e.g., the periodic pattern), the results of which are visualized in our system to indicate the occurrences of anomalies. Moreover, rich visualization and interaction designs are used to help understand the anomalies with both spatial and temporal context. We demonstrate the effectiveness of CloudDet through a quantitative evaluation, two case studies with real-world data, and interviews with domain experts.", "uri": "https://vimeo.com/360154740", "name": "[VIS19 Preview] CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing SystemsD (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:27:32+00:00", "description": "Authors: James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viegas, Jimbo Wilson\n\nAbstract: A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.", "uri": "https://vimeo.com/360154686", "name": "[VIS19 Preview] The What-If Tool: Interactive Probing of Machine Learning Models (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:27:08+00:00", "description": "Authors: Yating Wei, Honghui Mei, Ying Zhao, Shuyue Zhou, Bingru Lin, Haojin Jiang, Wei Chen\n\nAbstract: Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments using well-prepared synthetic data and recruit participants to complete the experiments based on their subjective experience. With a detailed analysis of the experimental results, we test against three hypotheses and summarize a series of instructive findings to inspire the design decisions of scatterplots in various creation, exploration and sharing scenarios.", "uri": "https://vimeo.com/360154646", "name": "[VIS19 Preview] Evaluating Perceptual Bias During Geometric Scaling of Scatterplots (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:26:52+00:00", "description": "Authors: Zhaosong Huang, Ye Zhao, Wei Chen, Shengjie Gao, Kejie Yu, Weixia Xu, Mingjie Tang, Minfeng Zhu, Mingliang Xu\n\nAbstract: Querying is essential for exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell towers) in which it resides, instead of accurate GPS coordinates. It is hard to determine a unique destination of trajectory. On the other hand, domain experts and general users prefer a natural way, such as using a natural-language-based sentence, to analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective fuzzy query over uncertain mobile trajectory data. Our implemented visual interface facilitates condition specification, situation-aware analysis, and semantic exploration of large trajectory data. Experimental studies on real datasets demonstrate the effectiveness of our approach.", "uri": "https://vimeo.com/360154619", "name": "[VIS19 Preview] A Natural-language-based Visual Query Approach of Uncertain Human Trajectories (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:26:40+00:00", "description": "Authors: Zikun Deng, Di Weng, Jiahui Chen, Ren Liu, Zhibin Wang, Jie Bao, Yu Zheng, Yingcai Wu\n\nAbstract: Air pollution has become a serious public health problem for many cities around the world. To find the cause of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in capturing and interpreting the uncertain propagation patterns of air pollution efficiently based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of patterns presentation; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach with two case studies conducted with a real-world dataset and received positive feedback from domain experts.", "uri": "https://vimeo.com/360154598", "name": "[VIS19 Preview] AirVis: Visual Analytics of Air Pollution Propagation (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:26:25+00:00", "description": "Authors: Haris Mumtaz, Shahid Latif, Fabian Beck, Daniel Weiskopf\n\nAbstract: Information about the quality of source code is necessary to reduce the costs of maintenance and improve reusability in software engineering. Although there exist many code inspection and software quality tools, information is often presented in a way that lacks putting the data into context. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that explain and show source code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create the textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept generalizes to multivariate data and report lessons learned in a broader scope.", "uri": "https://vimeo.com/360154571", "name": "[VIS19 Preview] Exploranative Code Quality Documents (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:26:05+00:00", "description": "Best Paper\n\nAuthors: Bowen Yu, Claudio Silva\n\nAbstract: Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization system that utilizes the state-of-the-art natural language processing technique to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem, and a formal user study.", "uri": "https://vimeo.com/360154533", "name": "[VIS19 Preview] FlowSense: A Natural Language Interface for Visual Data Exploration within a Dataflow System (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:25:51+00:00", "description": "Authors: Doris Jung-Lin Lee, John Lee, Tarique Siddiqui, Jaewoo Kim, Aditya Parameswaran, Karrie Karahalios\n\nAbstract: Visual query systems (VQSs) empower users to interactively search for line charts with desired visual patterns, typically specified using intuitive sketch-based interfaces. Despite decades of past work on VQSs, these efforts have not translated to adoption in practice, possibly because VQSs are largely evaluated in unrealistic lab-based settings. To remedy this gap in adoption, we collaborated with experts from three diverse domains\u2014astronomy, genetics, and material science\u2014via a year-long participatory design process to develop a VQS that supports their workflow and analytical needs, and evaluate how VQSs can be used in practice. Our study results reveal that ad-hoc sketch-only querying is not as commonly used as prior work suggests, since analysts are often unable to precisely express their patterns of interest. In addition, we characterize three essential sensemaking processes supported by our enhanced VQS. We discover that participants employ all three processes, but in different proportions, depending on the analytical needs in each domain. Our findings discovered the need for integrating all three sensemaking processes into the design of future VQSs to make them useful and practical, by addressing a wide range of analytical inquiries.", "uri": "https://vimeo.com/360154505", "name": "[VIS19 Preview] You can\u2019t always sketch what you want: Understanding Sensemaking in Visual Query Systems (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:25:38+00:00", "description": "Authors: Xuanwu Yue, Jiaxin Bai, Qinhan Liu, Yiyang Tang, Abishek Puri, Ke Li, Huamin Qu\n\nAbstract: Quantitative Investment, developed upon the solid foundation of robust financial theories, is at the center stage in investment industry today. The essence of quantitative investment is the multi-factor model, one that explains the relationship between the risk and return of equities. Enormous factor data has been generated by the multi-factor model. However, the group of experienced active portfolio managers is having a hard time navigating through a massive amount of factor data. Portfolio analysis and factor research have been limited owing to the lack of intuitive visual analytics tools. Previous portfolio visualization systems have mainly focused on the relationship between the portfolio return and stock holdings, which is insufficient for actionable insights and understanding market trends. In this paper, we present sPortfolio, which, to the best of our knowledge, is the first visualization attempt to explore the factor investment area. In particular, sPortfolio provides a holistic picture of the factor data, aiming at facilitating the analysis at three different levels: a Risk-Factor level for general market situation analysis, a Multiple-Portfolio level for understanding the portfolio strategies and a Single-Portfolio level for investigating detailed operations. The effectiveness and usability of the system are demonstrated through three case studies. The system has passed the pilot study and is to be deployed in industry.", "uri": "https://vimeo.com/360154484", "name": "[VIS19 Preview] sPortfolio: Stratified Visual Analysis of Stock Portfolios (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:25:19+00:00", "description": "Authors: Fred Hohman, Haekyu Park, Caleb Robinson, Duen Horng Chau\n\nAbstract: Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed based off of millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, the first interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a state-of-the-art image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.", "uri": "https://vimeo.com/360154453", "name": "[VIS19 Preview] Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T21:24:49+00:00", "description": "Authors: Andreas Walch, Michael Schw\u00e4rzler, Christian Luksch, Elmar Eisemann, Theresia Gschwandtner\n\nAbstract: LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that are expected to deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree.", "uri": "https://vimeo.com/360154391", "name": "[VIS19 Preview] LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T21:24:28+00:00", "description": "Authors: Yan Lyu, Xu Liu, Hanyi Chen, Arpan Mangal, Brian Lim, Kai Liu, Chao Chen\n\nAbstract: OD bundling is a promising method to identify key origin-destination (OD) patterns, but the bundling can mislead the interpretation of actual trajectories traveled. We present OD Morphing, an interactive OD bundling technique that improves geographical faithfulness to actual trajectories while preserving visual simplicity for OD pattern. OD Morphing iteratively identifies critical waypoints from the actual trajectory network with a min-cut algorithm and transitions OD bundles to pass through the identified waypoints with a smooth morphing method. Furthermore, we extend OD Morphing to support bundling at interaction speeds to enable users to interactively transition between degrees of faithfulness to aid sensemaking. We introduce metrics for faithfulness and simplicity to evaluate their trade-off achieved by OD morphed bundling. We demonstrate OD Morphing on real-world city-scale taxi trajectory and USA domestic planned flight datasets.", "uri": "https://vimeo.com/360154344", "name": "[VIS19 Preview] OD Morphing: balancing simplicity with faithfulness for OD bundling (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:24:09+00:00", "description": "Authors: Hannah Kim, Dongjin Choi, Barry Drake, Alex Endert, Haesun Park\n\nAbstract: Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or ``targets'' rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.", "uri": "https://vimeo.com/360154312", "name": "[VIS19 Preview] TopicSifter: Interactive Search Space Reduction Through Targeted Topic Modeling (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:23:53+00:00", "description": "Authors: Xi Ye, Shouxing Xiang, Jiazhi Xia, Jing Wu, Yang Chen, Shixia Lu\n\nAbstract: In this paper, we develop a visual analysis method for interactively improving the quality of labeled data, which is essential to the success of supervised and semi-supervised learning. This is achieved through the use of user-selected trusted items. We employ a bi-level optimization model to accurately match the labels of the trusted items and to minimize the training loss. Based on this model, a scalable data correction algorithm is developed to handle tens of thousands of labeled data efficiently. The selection of the trusted items is facilitated by an incremental tSNE with improved computational efficiency and layout stability to ensure a smooth transition between different levels. To prioritize the display of buggy data, we have taken specific consideration of outliers, items whose labels are different from those of its neighboring items, in the sampling process, and used a density map to reflect outlier ratios. We evaluated our method on real-world datasets through quantitative evaluation and case studies, and the results were generally favorable.", "uri": "https://vimeo.com/360154277", "name": "[VIS19 Preview] Interactive Correction of Mislabeled Training Data (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:23:39+00:00", "description": "Authors: Ying Zhao, Xiaobo Luo, Xiaru Lin, Hairong Wang, Xiaoyan Kui, Fangfang Zhou, Jinsong Wang, Yi Chen, Wei Chen\n\nAbstract: Traditional radio monitoring and management largely depends on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and often results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environment, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal sorting method to process radio signal data and constructs a situation assessment model to gain a qualitative and quantitative description of electromagnetic situation. We then design and implement a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand electromagnetic situation by the joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of this approach and forecast future work directions.", "uri": "https://vimeo.com/360154252", "name": "[VIS19 Preview] Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:23:26+00:00", "description": "Authors: Fabian Sperrle, Rita Sevastjanova, Rebecca Kehlbeck, Mennatallah El-Assady\n\nAbstract: Argumentation Mining addresses the challenging tasks of identifying argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of human intuition and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a Visual Analytics approach that augments the manual annotation process by reacting to the users\u2019 interactions and continuously suggesting which text fragments to annotate next. The accuracy of those automatic suggestions is improved over time by incorporating language modeling and learning from user interactions, training a measure of argument similarity. Based on a long-term collaboration with domain experts we identify and model five high-level analysis tasks. We enable close-reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless semantic zooming, visually anchoring all close- and distant-reading layers. We evaluate our system through a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration of text and graph views.", "uri": "https://vimeo.com/360154233", "name": "[VIS19 Preview] VIANA: Visual Interactive Annotation of Argumentation (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:23:09+00:00", "description": "Authors: Mennatallah El-Assady, Rebecca Kehlbeck, Christopher Collins, Daniel Keim, Oliver Deussen\n\nAbstract: We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "uri": "https://vimeo.com/360154204", "name": "[VIS19 Preview] Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T21:22:57+00:00", "description": "Authors: Jiachen Wang, Kejian Zhao, Dazhen Deng, Anqi Cao, Xiao Xie, Zheng Zhou, Hui Zhang, Yingcai Wu\n\nAbstract: Simulative analysis in competitive sports can provide prospective insights, which can help improve the performance of players in future matches. However, adequately simulating the complex competition process and intuitively explaining the simulation result to domain experts are typically challenging. This work presents a design study to address these challenges in table tennis. We propose a novel hybrid second-order Markov Chain model to characterize and simulate the competition process in table tennis. Compared with existing methods, our approach is the first to support the effective simulation of tactics, which represent high-level competition strategies in table tennis. Furthermore, we introduce a visual analytics system called Tac-Simur based on the proposed model for simulative visual analytics. Tac-Simur enables users to easily navigate different players and their tactics based on their respective performances in matches to identify the player and the tactics of interest for further analysis. Then, users can utilize the system to interactively explore diverse simulation tasks and intuitively explain the simulation results. The effectiveness and usefulness of this work are demonstrated by two case studies, in which domain experts utilize Tac-Simur to find interesting and valuable insights. The domain experts also provide positive feedback on the usability of Tac-Simur. Our work can be extended to other similar sports such as tennis and badminton.", "uri": "https://vimeo.com/360154177", "name": "[VIS19 Preview] Tac-Simur: Visual Analytics for Tactic-based Match Simulation of Table Tennis (vast paper)", "year": "2019", "event": "VAST, PREVIEW"}, {"created_time": "2019-09-15T02:24:53+00:00", "description": "VIS Arts Program\n\nAuthors: Rodrigo Rosales, Ana Carolina Robles\n\nAbstract: The purpose of this collaboration is to offer elements for discussion on the role of intersections among art, design, and technology for approaching the socialization environmental issue as an exercise of visualization praxis. Concretely, it is appealed the case of an interactive piece, expressly designed, called trasTocar (to disrupt) exhibited in Zanbatha, the municipal museum in the inner state of M\u00e9xico. This artwork agglomerates multiple perspectives for discussion: the theoretical border between art and design to solve communication problems; to inform people about altered and polluted surroundings landscape because of their individual actions; the addressing to spectators through technology to modify their behavior; and, the possibility to obtain data from this interaction.", "uri": "https://vimeo.com/360050866", "name": "[VIS19 Preview] Embroidering Translations between Digital Art and Design for a Sustainable Environment (visap paper)", "year": "2019", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2019-09-15T02:24:11+00:00", "description": "VIS Arts Program\n\nAuthors: Mahika Dubey, Jasmine Tan Otto, Angus G. Forbes\n\nAbstract: This paper introduces Data Brushes, an interactive web application to explore neural style transfer using models trained on data visualizations. Our application includes two distinct modes that invite casual creators to interact with deep convolutional neural networks to co-create custom artworks. The first mode, \u2018magic markers\u2019, mimics painting with a brush on a canvas, enabling users to paint a style onto selected areas of an image. The second mode, \u2018compositing stamps\u2019, uses a real-time method for applying style filters to selected portions of an image. Specifically, we focus on style transfer networks created from canonical and contemporary works of data visualization and data art. In addition to enabling a novel cre- ative workflow, the process of interactively modifying an image via multiple style transfer networks reveals meaningful features encoded within the networks, and provides insight into the effect particular networks have on different images, or even different regions of an image, such as border artifacts and spatial stability or instability. To evaluate Data Brushes, we gathered expert feedback from participants of a data science symposium and ran an observational study, finding that our application facilitates the creative exploration of neural style transfer for data art and enhances user intuition regarding the expressive range of particular style transfer features.", "uri": "https://vimeo.com/360050826", "name": "[VIS19 Preview] Data Brushes: Interactive Style Transfer for Data Art (visap paper)", "year": "2019", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2019-09-15T02:23:46+00:00", "description": "VIS Arts Program\n\nAuthors: Graham Wakefield, Haru Hyunkyung Ji\n\nAbstract: \u201cInfranet\u201d is a generative artwork interweaving data visualization and sonification, artificial intelligence, and evolutionary algorithms in a population of artificial life agents, thriving upon geospatial data of the infrastructure of a city as its sustenance and canvas. Each exhibit of Infranet utilizes public data available on the host city; including Gwangju, South Korea (2018) and New York, USA (2019). This paper documents the motivations behind the work, its design and subsequent implementation in details. At its heart is the speculative question: can the data of a city be a good habitat for new forms of life? Our design in response utilizes neural networks at individual, as well as population-wide scales, along with horizontal gene transfer and contagion/entrainment as means for the living beings to open-endedly discover the variety in the data habitat.", "uri": "https://vimeo.com/360050791", "name": "[VIS19 Preview] Infranet: A Geospatial Data-Driven Neuro-Evolutionary Artwork (visap paper)", "year": "2019", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2019-09-15T02:23:13+00:00", "description": "VIS Arts Program\n\nAuthors: Karin von Ompteda\n\nAbstract: How can we break down the psychological boundaries which impede people from engaging in the issue of climate change? This paper is focused on the opportunities afforded by the practice of data manifestation; the communication of quantitative information through objects, installations, and sensory experiences. Through an analysis of two data manifestation projects, a series of techniques are presented which address the boundaries that separate people from global climate change in the areas of scale, geography, time, and environmental philosophy. These projects convert data to a human (versus global) scale, transport data from far off lands into our spaces, bring the future consequences of climate change into our present, and disrupt the philosophical separation between the human and non-human world. Data manifestation is further considered in the context of the relevant psychology literature focused on psychological distance.", "uri": "https://vimeo.com/360050756", "name": "[VIS19 Preview] Data Manifestation: Merging the Human World and Global Climate Change (visap paper)", "year": "2019", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2019-09-15T02:22:47+00:00", "description": "VIS Arts Program\n\nAuthors: Dietmar Offenhuber\n\nAbstract: We present two examples of autographic visualizations - displays based on material traces rather than digital data - that aim to make environmental pollution visible and legible. Using particulate matter and ground- level ozone pollution as case studies, this pictorial illustrates the design principles of creating autographic visualizations that make phenomena reveal themselves.", "uri": "https://vimeo.com/360050734", "name": "[VIS19 Preview] Dustmark and Ozone Tattoos: Autographic displays of air pollution (visap pictorial)", "year": "2019", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2019-09-15T02:22:26+00:00", "description": "VIS Arts Program\n\nAuthors: Francesca Samsel, Seth A Johnson, Annie Bares, Daniel F. Keefe\n\nAbstract: As scientific data becomes larger and more complex, an equally rich visual vocabulary is needed to fully articulate its insights. We present a series of images that are made possible by a recent technical development \u201cArtifact-Based Rendering\u201d,  a component of our broader effort to create a methodology for scientific visualization that draws on principles of art and design theory.", "uri": "https://vimeo.com/360050707", "name": "[VIS19 Preview] Scientific Visualization: Enriching Vocabulary via the Human Hand (visap pictorial)", "year": "2019", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2019-09-15T02:22:07+00:00", "description": "VIS Arts Program\n\nAuthors: Andrew Richardson\n\nAbstract: Mapping The Prelude is an investigation which explores the potential for a creative code-driven approach to re-locate Wordsworth's epic autobiographical poem, The Prelude, in the context of the wider geographic landscape which informed and inspired his work. Situated between disciplines of creative data manipulation and digital humanities, the project begins to examine novel ways to re-map the 'landscape' of a literary text in relationship with its wider geo-spatial data: exploring the border area between literary and geographic spaces and creating new encounters of each. This annotated portfolio outlines the work produced during the investigation and considers areas of future research.", "uri": "https://vimeo.com/360050681", "name": "[VIS19 Preview] Mapping The Prelude: A Visualisation of Wordsworth's Poetry (visap pictorial)", "year": "2019", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2019-09-15T02:21:20+00:00", "description": "Authors: Ying Yang, Michael Wybrow, Yuan-Fang Li, Tobias Czauderna, Yongqun He\n\nAbstract: Ontologies are formal representations of concepts and complex relationships among them. They have been widely used to capture comprehensive domain knowledge in areas such as biology and medicine, where large and complex ontologies can contain hundreds of thousands of concepts. Especially due to the large size of ontologies, visualisation is useful for authoring, exploring and understanding their underlying data. Existing ontology visualisation tools generally focus on the hierarchical structure, giving much less emphasis to non-hierarchical associations. In this paper we present OntoPlot, a novel visualisation specifically designed to facilitate the exploration of all concept associations whilst still showing an ontology\u2019s large hierarchical structure. This hybrid visualisation combines icicle plots, visual compression techniques and interactivity, improving space-efficiency and reducing visual structural complexity. We conducted a user study with domain experts to evaluate the usability of OntoPlot, comparing it with the de facto ontology editor Prot\u00e9g\u00e9.The results confirm that OntoPlot attains our design goals for association-related tasks and is strongly favoured by domain experts.", "uri": "https://vimeo.com/360050594", "name": "[VIS19 Preview] OntoPlot: A Novel Visualisation for Non-hierarchical Associations in Large Ontologies (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:21:07+00:00", "description": "Authors: Leilani Battle, R. Jordan Crouser, Audace Nakeshimana, Ananda Montoly, Remco Chang, Michael Stonebraker\n\nAbstract: Latency in a visualization system is widely believed to affect user behavior in measurable ways, such as requiring the user to wait for the visualization system to respond, leading to interruption of the analytic flow. While this effect is frequently observed and widely accepted, precisely how latency affects different analysis scenarios is less well understood. In this paper, we examine the role of latency in the context of visual search, an essential task in data foraging and exploration using visualization. We conduct a series of studies on Amazon Mechanical Turk and find that under certain conditions, latency is a statistically significant predictor of visual search behavior, which is consistent with previous studies. However, our results also suggest that task type, task complexity, and other factors can modulate the effect of latency, in some cases rendering latency statistically insignificant in predicting user behavior. This suggests a more nuanced view of the role of latency than previously reported. Building on these results and the findings of prior studies, we propose design guidelines for measuring and interpreting the effects of latency when evaluating performance on visual search tasks.", "uri": "https://vimeo.com/360050582", "name": "[VIS19 Preview] The Role of Latency in Predicting Visual Search Behavior (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:20:55+00:00", "description": "Authors: Xin Chen, Tong Ge, Jian Zhang, Baoquan Chen, Chi-Wing Fu, Oliver Deussen, Yunhai Wang\n\nAbstract: We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map.  By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy.  A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.", "uri": "https://vimeo.com/360050562", "name": "[VIS19 Preview] A Recursive Subdivision Technique for Sampling Multi-class Scatterplots (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:20:41+00:00", "description": "Authors: Aritra Dasgupta, Hong Wang, Nancy O'Brien, Susannah Burrows\n\nAbstract: Experts in data and physical sciences have to regularly grapple with the problem of competing models. Be it analytical or physics-based models, a cross-cutting challenge for experts is to reliably diagnose which model outcomes appropriately predict or simulate real-world phenomena. Expert judgment involves reconciling information across many, and often, conflicting criteria that describe the quality of model outcomes. In this paper, through a design study with climate scientists, we develop a deeper understanding of the problem and solution space of model diagnostics, resulting in the following contributions: i) a problem and task characterization using which we map experts\u2019 model diagnostics goals to multi-way visual comparison tasks, ii) a design space of comparative visual cues for letting experts quickly understand the degree of disagreement among competing models and gauge the degree of stability of model outputs with respect to alternative criteria, and iii) design and evaluation of MyriadCues, an interactive visualization interface for exploring alternative hypotheses and insights about good and bad models by leveraging comparative visual cues. We present case studies and subjective feedback by experts, which validate how MyriadCues enables more transparent model diagnostic mechanisms, as compared to the state of the art.", "uri": "https://vimeo.com/360050545", "name": "[VIS19 Preview] Separating the wheat from the chaff: Comparative visual cues for transparent diagnostics of competing models...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:20:28+00:00", "description": "Authors: Kelvin Li, Kwan-Liu Ma\n\nAbstract: We present P5, a web-based visualization toolkit that combines declarative visualization grammar and GPU computing for progressive data analysis and visualization. To interactively analyze and explore big data, progressive analytics and visualization methods have recently emerged. Progressive visualizations of incrementally refining results have the advantages of allowing users to steer the analysis process and make early decisions. P5 leverages declarative grammar for specifying visualization designs and exploits GPU computing to accelerate progressive data processing and rendering. The declarative specifications can be modified during progressive processing to create different visualizations for analyzing the intermediate results. To enable user interactions for progressive data analysis, P5 utilizes the GPU to automatically aggregate and index data based on declarative interaction specifications to facilitate effective interactive visualization. We demonstrate the effectiveness and usefulness of P5 through a variety of example applications and several performance benchmark tests.", "uri": "https://vimeo.com/360050527", "name": "[VIS19 Preview] P5: Portable Progressive Parallel Processing Pipeline for Interactive Data Analysis and Visualization...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:20:16+00:00", "description": "Authors: Ragini Rathore, Zachary Leggon, Laurent Lessard, Karen Schloss\n\nAbstract: To interpret the meanings of colors in visualizations of categorical information, people must determine how distinct colors correspond to different concepts. This process is easier when assignments between colors and concepts in visualizations match people's expectations, making color palettes semantically interpretable. Efforts have been underway to optimize color palette design for semantic interpretablity, but this requires having good estimates of human color-concept associations. Obtaining these data from humans is costly, which motivates the need for automated methods. We developed and evaluated a new method for automatically estimating color-concept associations in a way that strongly correlates with human ratings. Building on prior studies using Google Images, our approach operates directly on Google Image search results without the need for humans in the loop. Specifically, we evaluated several methods for extracting raw pixel content of the images in order to best estimate color-concept associations obtained from human ratings. The most effective method extracted colors using a combination of cylindrical sectors and color categories in color space. We demonstrate that our approach can accurately estimate average human color-concept associations for different fruits using only a small set of images. The approach also generalizes moderately well to more complicated recycling-related concepts of objects that can appear in any color.", "uri": "https://vimeo.com/360050505", "name": "[VIS19 Preview] Estimating color-concept associations from image statistics (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:20:03+00:00", "description": "Authors: Guozheng Li, Yu Zhang, Yu Dong, Jie Liang, Jinson Zhang, Jinsong Wang, Michael McGuffin, Xiaoru Yuan\n\nAbstract: We propose BarcodeTree (BCT), a novel visualization technique for comparing topological structures and node attribute values of multiple trees. BCT can provide an overview of one hundred shallow and stable trees simultaneously, without aggregating individual nodes. Each BCT is shown within a single row using a style similar to a barcode, allowing trees to be stacked vertically with matching nodes aligned horizontally to ease comparison and maintain space efficiency. We design several visual cues and interactive techniques to help users understand the topological structure and compare trees. In an experiment comparing two variants of BCT with icicle plots, the results suggest that BCTs make it easier to visually compare trees by reducing the vertical distance between different trees. We also present two case studies involving a dataset of hundreds of trees to demonstrate BCT's utility.", "uri": "https://vimeo.com/360050493", "name": "[VIS19 Preview] BarcodeTree: Scalable Comparison of Multiple Hierarchies (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:19:37+00:00", "description": "Authors: Can Liu, Cong Wu, Hanning Shao, Xiaoru Yuan\n\nAbstract: Interactive visualization and exploration of large spatiotemporal data sets is difficult without carefully-designed data pre-processing and management tools. We propose a novel architecture for spatiotemporal data management. The architecture can dynamically update itself based on user queries. Datasets is stored in a tree-like structure to support memory sharing among cuboids in a logical structure of data cubes. An update mechanism is designed to create or remove cuboids on it, according to the analysis of the user queries, with the consideration of memory size limitation. Data structure is dynamically optimized according to different user queries. During a query process, user queries are recorded to predict the performance increment of the new cuboid. The creation or deletion of a cuboid is determined by performance increment. Experiment results show that our prototype system deliveries good performance towards user queries on different spatiotemporal datasets, which costing small memory size with comparable performance compared with other state-of-the-art algorithms.", "uri": "https://vimeo.com/360050463", "name": "[VIS19 Preview] SmartCube: An Adaptive Data Management Architecture for the Real-Time Visualization of Spatiotemporal...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:19:24+00:00", "description": "Authors: Takanori Fujiwara, Jia-Kai Chou, Shilpika Shilpika, Panpan Xu, Liu Ren, Kwan-Liu Ma\n\nAbstract: Dimensionality reduction (DR) methods are commonly used for analyzing and visualizing multidimensional data. However, when data is a live streaming feed, conventional DR methods cannot be directly used because of their computational complexity and inability to preserve the projected data positions at previous time points. In addition, the problem becomes even more challenging when the dynamic data records have a varying number of dimensions as often found in real-world applications. This paper presents an incremental DR solution. We enhance an existing incremental PCA method in several ways to ensure its usability for visualizing streaming multidimensional data. First, we use geometric transformation and animation methods to help preserve a viewer's mental map when visualizing the incremental results. Second, to handle data dimension variants, we use an optimization method to estimate the projected data positions, and also convey the resulting uncertainty in the visualization. We demonstrate the effectiveness of our design with two case studies using real-world datasets.", "uri": "https://vimeo.com/360050443", "name": "[VIS19 Preview] An Incremental Dimensionality Reduction Method for Visualizing Streaming Multidimensional Data (infovis...", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:19:12+00:00", "description": "Authors: Enamul Hoque, Maneesh Agrawala\n\nAbstract: We present a search engine for D3 visualizations that allows queries based on their visual style and underlying structure. To build the engine we crawl a collection of 7860 D3 visualizations from the Web and deconstruct each one to recover its data, its data-encoding marks and the encodings describing how the data is mapped to visual attributes of the marks. We also extract axes and other non-data-encoding attributes of marks (e.g., typeface, background color). Our search engine indexes this style and structure information as well as metadata about the webpage containing the chart. We show how visualization developers can search the collection to find visualizations that exhibit specific design characteristics and thereby explore the space of possible designs. We also demonstrate how researchers can use the search engine to identify commonly used visual design patterns and we perform such a demographic design analysis across our collection of D3 charts. A user study reveals that visualization developers found our style and structure based search engine to be significantly more useful and satisfying for finding different designs of D3 charts, than a baseline search engine that only allows keyword search over the webpage containing a chart.", "uri": "https://vimeo.com/360050425", "name": "[VIS19 Preview] Searching the Visual Style and Structure of D3 Visualizations (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:18:57+00:00", "description": "Authors: Jessica Hullman\n\nAbstract: Clear presentation of uncertainty is an exception rather than rule in media articles, data-driven reports, and consumer applications, despite proposed techniques for communicating sources of uncertainty in data. This work considers, Why do so many visualization authors choose not to visualize uncertainty? I contribute a detailed characterization of practices, associations, and attitudes related to uncertainty communication among visualization authors, derived from the results of surveying 90 authors who regularly create visualizations for others as part of their work, and interviewing thirteen influential visualization designers. My results highlight challenges that authors face and expose assumptions and inconsistencies in beliefs about the role of uncertainty in visualization. In particular, a clear contradiction arises between authors\u2019 acknowledgment of the value of depicting uncertainty and the norm of omitting direct depiction of uncertainty. To help explain this contradiction, I present a rhetorical model of uncertainty omission in visualization-based communication. I also adapt a formal statistical model of how viewers judge the strength of a signal in a visualization to visualization-based communication, to argue that uncertainty communication necessarily reduces degrees of freedom in viewers\u2019 statistical inferences and to expose logical contradictions in the beliefs implied by the rhetorical model. I conclude with recommendations for how visualization research on uncertainty communication could better serve practitioners\u2019 current needs and values while deepening understanding of assumptions that reinforce omission.", "uri": "https://vimeo.com/360050405", "name": "[VIS19 Preview] Why Authors Don't Visualize Uncertainty (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:18:46+00:00", "description": "Authors: Lace Padilla, Spencer Castro, P. Samuel Quinan, Ian Ruginski, Sarah Creem-Regehr\n\nAbstract: Cognitive science has established widely used and validated procedures for evaluating working memory in numerous applied domains, but surprisingly few studies have employed these methodologies to assess claims about the impacts of visualizations on working memory. The lack of information visualization research that uses validated procedures for measuring working memory may be due, in part, to the absence of cross-domain methodological guidance tailored explicitly to the unique needs of visualization research. This paper presents a set of clear, practical, and empirically validated methods for evaluating working memory during visualization tasks and provides readers with guidance in selecting an appropriate working memory evaluation paradigm. As a case study, we illustrate multiple methods for evaluating working memory in a visual-spatial aggregation task with geospatial data. The results show that the use of dual-task experimental designs (simultaneous performance of several tasks compared to single-task performance) and pupil dilation can reveal working memory demands associated with task difficulty and dual-tasking. In a dual-task experimental design, measures of task completion times and pupillometry revealed the working memory demands associated with both task difficulty and dual-tasking. Pupillometry demonstrated that participants\u2019 pupils were significantly larger when they were completing a more difficult task and when multitasking. We propose that researchers interested in the relative differences in working memory between visualizations should consider a converging methods approach, where physiological measures and behavioral measures of working memory are employed to generate a rich evaluation of visualization effort.", "uri": "https://vimeo.com/360050392", "name": "[VIS19 Preview] Toward Objective Evaluation of Working Memory in Visualizations: A Case Study Using Pupillometry and a Dual-...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:18:26+00:00", "description": "Authors: Katy Williams, Alex Bigelow, Katherine Isaacs\n\nAbstract: Common pitfalls in visualization projects include lack of data availability and the domain users\u2019 needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a \u201cchicken and egg\u201d problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the \u201cmoving target\u201d of both the data and the domain experts\u2019concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs.", "uri": "https://vimeo.com/360050356", "name": "[VIS19 Preview] Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:18:14+00:00", "description": "Authors: Stephen Smart, Keke Wu, Danielle Szafir\n\nAbstract: Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color. We do this using an algorithmic approach that models designer practices by analyzing patterns in the structure of designer-crafted color ramps. We construct these models from a corpus of 222 expert-designed color ramps, and use the results to automatically generate ramps that mimic designer practices. We evaluate our approach through an empirical study comparing the outputs of our approach with designer-crafted color ramps. Our models produce ramps that support accurate and aesthetically pleasing visualizations at least as well as designer ramps and that outperform conventional mathematical approaches.", "uri": "https://vimeo.com/360050339", "name": "[VIS19 Preview] Color Crafting: Automating the Construction of Designer Quality Color Ramps (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:18:00+00:00", "description": "Authors: Matt Whitlock, Keke Wu, Danielle Szafir\n\nAbstract: Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face data- and platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into field practices. We use a design probe coupling mobile, cloud, and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data\u2019s utility for various field operations and new directions for visual analytics tools to transform fieldwork.", "uri": "https://vimeo.com/360050322", "name": "[VIS19 Preview] Designing for Mobile and Immersive Visual Analytics in the Field (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:17:35+00:00", "description": "Authors: Manuela Waldner, Alexandra Diehl, Denis Gracanin, Rainer Splechtna, Claudio Delrieux, Kresimir Matkovic\n\nAbstract: Radial charts are generally considered less effective than linear charts. Perhaps the only exception is in visualizing periodical time-dependent data, which is believed to be naturally supported by the radial layout. It has been demonstrated that the drawbacks of radial charts outweigh the benefits of this natural mapping. Visualization of daily patterns, as a special case, has not been systematically evaluated using radial charts. In contrast to yearly or weekly recurrent trends, the analysis of daily patterns on a radial chart may benefit from our trained skill on reading radial clocks that are ubiquitous in our culture. In a crowd-sourced experiment with 92 non-expert users, we evaluated the accuracy, efficiency, and subjective ratings of radial and linear charts for visualizing daily traffic accident patterns. We systematically compared juxtaposed 12-hours variants and single 24-hours variants for both layouts in four low-level tasks and one high-level interpretation task. Our results show that over all tasks, the most elementary 24-hours linear bar chart is most accurate and efficient and is also preferred by the users. This provides strong evidence for the use of linear layouts \u2013 even for visualizing periodical daily patterns.", "uri": "https://vimeo.com/360050294", "name": "[VIS19 Preview] Comparison of Radial and Linear Charts for Visualizing Daily Patterns (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:17:22+00:00", "description": "Honorable Mention\n\nAuthors: Nicole Jardine, Brian Ondov, Niklas Elmqvist, Steven Franconeri\n\nAbstract: Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., \u201cbiggest delta\u201d, \u201cbiggest correlation\u201d) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the \u201cbiggest mean\u201d and \u201cbiggest range\u201d between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a \u201cMean length\u201d proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a \u201cHull Area\u201d proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.", "uri": "https://vimeo.com/360050277", "name": "[VIS19 Preview] The Perceptual Proxies of Visual Comparison (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:17:10+00:00", "description": "Authors: Weiwei Cui, Xiaoyu Zhang, Yun Wang, He Huang, Bei Chen, Lei Fang, Haidong Zhang, Jian-Guang Lou, Dongmei Zhang\n\nAbstract: Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.", "uri": "https://vimeo.com/360050265", "name": "[VIS19 Preview] Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:16:58+00:00", "description": "Authors: Yunhai Wang, Xiaowei Chu, Kaiyi Zhang, Chen Bao, Xiaotong Li, Jian Zhang, Chi-Wing Fu, Christophe Hurter, Oliver Deussen, Bongshin Lee\n\nAbstract: We present a new technique to enable the creation of shape-bounded Wordles, we call {\\em ShapeWordle\\/}, in which we fit words to form a given shape. To guide word placement within a shape, we extend the traditional Archimedean spirals to be shape-aware by formulating the spirals in a differential form using the distance field of the shape. To handle non-convex shapes, we introduce a multi-centric Wordle layout method that segments the shape into parts for our shape-aware spirals to adaptively fill the space and generate word placements. In addition, we offer a set of editing interactions to facilitate the creation of semantically-meaningful Wordles. Lastly, we present three evaluations: a comprehensive comparison of our results against the state-of-the-art technique (WordArt), case studies with 14 users, and a gallery to showcase the coverage of our technique.", "uri": "https://vimeo.com/360050251", "name": "[VIS19 Preview] ShapeWordle: Tailoring Wordles using Shape-aware Archimedean Spirals (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:16:41+00:00", "description": "Authors: Yunhai Wang, Mingliang Xue, Yanyan Wang, Xinyuan Yan, Baoquan Chen, Chi-Wing Fu, Christophe Hurter\n\nAbstract: Many edge bundling techniques (i.e., data simpli\ufb01cation as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and dif\ufb01cult to set up. As a result, this hinders the user ability to create ef\ufb01cient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: \ufb01rst, the user investigates different edge bundling results and speci\ufb01es areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these speci\ufb01ed areas. Lastly, the user can further \ufb01ne-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations.", "uri": "https://vimeo.com/360050223", "name": "[VIS19 Preview] Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:16:24+00:00", "description": "Authors: Fearn Bishop, Johannes Zagermann, Ulrike Pfeil, Gemma Sanderson, Harald Reiterer, Uta Hinrichs\n\nAbstract: Building data analysis skills is part of modern elementary school curricula. Recent research has explored how to facilitate children's understanding of visual data representations through completion exercises which highlight links between concrete and abstract mappings. This approach scaffolds visualization activities by presenting a target visualization to children. But how can we engage children in more free-form visual data mapping exercises that are driven by their own mapping ideas? How can we scaffold a creative exploration of visualization techniques and mapping possibilities? We present Construct-A-Vis, a tablet-based tool designed to explore the feasibility of free-form and constructive visualization activities with elementary school children. Construct-A-Vis provides adjustable levels of scaffolding visual mapping processes. It can be used by children individually or as part of collaborative activities. Findings from a study with elementary school children using Construct-A-Vis individually and in pairs highlight the potential of this free-form constructive approach, as visible in children's diverse visualization outcomes and their critical engagement with the data and mapping processes. Based on our study findings we contribute insights into the design of free-form visualization tools for children, including the role of tool-based scaffolding mechanisms and shared interactions to guide visualization activities with children.", "uri": "https://vimeo.com/360050203", "name": "[VIS19 Preview] Construct-A-Vis: Free-Form Visualization Creation for Children (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:15:58+00:00", "description": "Authors: Yunhai Wang, Zeyu Wang, Tingting Liu, Michael Correll, Zhanglin Cheng, Oliver Deussen, Michael Sedlmair\n\nAbstract: In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "uri": "https://vimeo.com/360050165", "name": "[VIS19 Preview] Improving the Robustness of Scagnostics (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:15:43+00:00", "description": "Authors: Jochen G\u00f6rtler, Thilo Spinner, Dirk Streeb, Daniel Weiskopf, Oliver Deussen\n\nAbstract: We present a technique to perform dimensionality reduction on data that is subject to uncertainty. Our method is a generalization of traditional principal component analysis (PCA) to multivariate probability distributions. In comparison to non-linear methods, linear dimensionality reduction techniques have the advantage that the characteristics of such probability distributions remain intact after projection. We derive a representation of the PCA sample covariance matrix that respects potential uncertainty in each of the inputs, building the mathematical foundation of our new method: uncertainty-aware PCA. In addition to the accuracy and performance gained by our approach over sampling-based strategies, our formulation allows us to perform sensitivity analysis with regard to the uncertainty in the data. For this, we propose factor traces as a novel visualization that enables to better understand the influence of uncertainty on the chosen principal components. We provide multiple examples of our technique using real-world datasets. As a special case, we show how to propagate multivariate normal distributions through PCA in closed form. Furthermore, we discuss extensions and limitations of our approach.", "uri": "https://vimeo.com/360050141", "name": "[VIS19 Preview] Uncertainty-Aware Principal Component Analysis (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:15:28+00:00", "description": "Authors: Min Lu, Shuaiqi Wang, Joel Lanir, Noa Fish, Yang Yue, Daniel CohenOr, Hui Huang\n\nAbstract: This work proposes Winglets, an enhancement to the classic scatterplot to better perceptually pronounce multiple classes by improving the perception of association and uncertainty of points to their related cluster. Designed as a pair of dual-sided strokes belonging to a data point, Winglets leverage the Gestalt principle of Closure to shape the perception of the form of the clusters, rather than use an explicit divisive encoding. Through a subtle design of two dominant attributes, length and orientation, Winglets enable viewers to perform a mental completion of the clusters. A controlled user study was conducted to examine the efficiency of Winglets in perceiving the cluster association and the uncertainty of certain points. The results show Winglets form a more prominent association of points into clusters and improve the perception of associating uncertainty.", "uri": "https://vimeo.com/360050129", "name": "[VIS19 Preview] Winglets: Visualizing Association with Uncertainty in Multi-class Scatterplots (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:15:16+00:00", "description": "Authors: Zhutian Chen, Yun Wang, Qianwen Wang, Yong Wang, Huamin Qu\n\nAbstract: Designers need to consider not only perceptual effectiveness but also visual styles when creating an infographic.  This process can be difficult and time consuming for professional designers, not to mention non-expert users, leading to the demand of automated infographics design. As a first step, we focus on timeline infographics, which have been widely used for centuries. We contribute an end-to-end approach that automatically extracts an extensible timeline template from a bitmap image. Our approach adopts a deconstruction and reconstruction paradigm.  At the deconstruction stage, we propose a multi-task deep neural network that simultaneously parses two kinds of information from a bitmap timeline: 1) the global information, i.e., the representation, scale, layout, and orientation of the timeline, and 2) the local information, i.e., the location, category, and pixels of each visual element on the timeline.  At the reconstruction stage, we propose a pipeline with three techniques, i.e.,Non-Maximum Merging,RedundancyRecover, andDL GrabCut, to extract an extensible template from the infographic, by utilizing the deconstruction results. To evaluate the effectiveness of our approach, we synthesize a timeline dataset (4296 images) and collect a real-world timeline dataset (393 images)from the Internet. We first report quantitative evaluation results of our approach over the two datasets. Then, we present examples of automatically extracted templates and timelines automatically generated based on these templates to qualitatively demonstrate the performance. The results confirm that our approach can effectively extract extensible templates from real-world timeline infographics.", "uri": "https://vimeo.com/360050115", "name": "[VIS19 Preview] Towards Automated Infographic Design: Deep Learning-based Auto-Extraction of Extensible Timeline (infovis...", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:15:02+00:00", "description": "Authors: Dietmar Offenhuber\n\nAbstract: Information visualization limits itself, per definition, to the domain of symbolic information. This paper discusses arguments why the field should also consider forms of data that are not symbolically encoded, including physical traces and material indicators. Continuing a provocation presented by Pat Hanrahan in his 2004 IEEE Vis capstone address, this paper compares physical traces to visualizations and describes the techniques and visual practices for producing, revealing, and interpreting them. By contrasting information visualization with a speculative counter model of autographic visualization, this paper examines the design principles for material data. Autographic visualization addresses limitations of information visualization, such as the inability to directly reflect the material circumstances of data generation. The comparison between the two models allows probing the epistemic assumptions behind information visualization and uncovers linkages with the rich history of scientific visualization and trace reading. The paper begins by discussing the gap between data visualizations and their corresponding phenomena and proceeds by investigating how material visualizations can bridge this gap. It contextualizes autographic visualization with paradigms such as data physicalization and indexical visualization and grounds it in the broader theoretical literature of semiotics, science and technology studies (STS), and the history of scientific representation. The main section of the paper proposes a foundational design vocabulary for autographic visualization and offers examples of how citizen scientists already use autographic principles in their displays, which seem to violate the canonical principles of information visualization but succeed at fulfilling other rhetorical purposes in evidence construction. The paper concludes with a discussion of the limitations of autographic visualization, a roadmap for the empirical investigation of trace perception, and thoughts about how information visualization and autographic visualization techniques can contribute to each other.", "uri": "https://vimeo.com/360050094", "name": "[VIS19 Preview] Data by proxy \u2013 material traces as autographic visualizations (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:14:50+00:00", "description": "Authors: Vanessa Pe\u00f1a-Araya, Emmanuel Pietriga, Anastasia Bezerianos\n\nAbstract: Observing the relationship between two or more variables over space and time is essential in many domains. For instance, looking, for different countries, at the evolution of both the life expectancy at birth and the fertility rate will give an overview of their demographics. The choice of visual representation for such multivariate data is key to enabling analysts to extract patterns and trends. Prior work has compared geo-temporal visualization techniques for a single thematic variable that evolves over space and time, or for two variables at a specific point in time. But how effective visualization techniques are at communicating correlation between two variables that evolve over space and time remains to be investigated. We report on a study comparing three techniques that are representative of different strategies to visualize geo-temporal multivariate data: either juxtaposing all locations for a given time step, or juxtaposing all time steps for a given location; and encoding thematic attributes either using symbols overlaid on top of map features, or using visual channels of the map features themselves. Participants performed a series of tasks that required them to identify if two variables were correlated over time and if there was a pattern in their evolution. Tasks varied in granularity for both dimensions: time (all time steps, a subrange of steps, one step only) and space (all locations, locations in a subregion, one location only). Our results show that a visualization\u2019s effectiveness depends strongly on the task to be carried out. Based on these findings we present a set of design guidelines about geo-temporal visualization techniques for communicating correlation.", "uri": "https://vimeo.com/360050071", "name": "[VIS19 Preview] A Comparison of Visualizations for Identifying Correlation Over Time and Space (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:14:39+00:00", "description": "Authors: Chen Chen, Changbo Wang, Xue Bai, Peiying Zhang, Chenhui Li\n\nAbstract: The density map is widely used for data sampling, time-varying detection, ensemble representation, etc. The visualization of dynamic evolution is a challenging task when exploring spatiotemporal data. Many approaches have been provided to explore the variation of data patterns over time, which commonly need multiple parameters and preprocessing works. Image generation is a well-known topic in deep learning, and a variety of generating models have been promoted in recent years. In this paper, we introduce a general pipeline called GenerativeMap to extract dynamics of density maps by generating interpolation information. First, a trained generative model comprises an important part of our approach, which can generate nonlinear and natural results by implementing a few parameters. Second, a visual presentation is proposed to show the density change, which is combined with the level of detail and blue noise sampling for a better visual effect. Third, for dynamic visualization of large-scale density maps, we extend this approach to show the evolution in regions of interest, which costs less to overcome the drawback of the learning-based generative model. We demonstrate our method on different types of cases, and we evaluate and compare the approach from multiple aspects. The results help identify the effectiveness of our approach and confirm its applicability in different scenarios.", "uri": "https://vimeo.com/360050049", "name": "[VIS19 Preview] GenerativeMap: Visualization and Exploration of Dynamic Density Maps via Generative Learning Model (infovis...", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:14:26+00:00", "description": "Authors: Andrea Batch, Andrew Cunningham, Maxime Cordeil, Niklas Elmqvist, Tim Dwyer, Bruce Thomas, Kim Marriott\n\nAbstract: Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.", "uri": "https://vimeo.com/360050033", "name": "[VIS19 Preview] There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:14:11+00:00", "description": "Authors: Ashley Suh, Mustafa Hajij, Bei Wang, Carlos Scheidegger, Paul Rosen\n\nAbstract: Graphs are commonly used to encode relationships among entities, yet their abstractness makes them difficult to analyze. Node-link diagrams are popular for drawing graphs, and force-directed layouts provide a flexible method for node arrangements that use local relationships in an attempt to reveal the global shape of the graph. However, clutter and overlap of unrelated structures can lead to confusing graph visualizations. This paper leverages the persistent homology features of an undirected graph as derived information for interactive manipulation of force-directed layouts. We first discuss how to efficiently extract 0-dimensional persistent homology features from both weighted and unweighted undirected graphs. We then introduce the interactive persistence barcode used to manipulate the force-directed graph layout. In particular, the user adds and removes contracting and repulsing forces generated by the persistent homology features, eventually selecting the set of persistent homology features that most improve the layout. Finally, we demonstrate the utility of our approach across a variety of synthetic and real datasets.", "uri": "https://vimeo.com/360050009", "name": "[VIS19 Preview] Persistent Homology Guided Force-Directed Graph Layouts (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:13:49+00:00", "description": "Authors: Christine Nothelfer, Steven Franconeri\n\nAbstract: The power of data visualization is not to convey absolute values of individual data points, but to allow the exploration of relations (increases or decreases in a data value) among them. One approach to highlighting these relations is to explicitly encode the numeric differences (deltas) between data values. Because this approach removes the context of the individual data values, it is important to measure how much of a performance improvement it actually offers, especially across differences in encodings and tasks, to ensure that it is worth adding to a visualization design. Across 3 different tasks, we measured the increase in visual processing efficiency for judging the relations between pairs of data values, from when only the values were shown, to when the deltas between the values were explicitly encoded, across position and length visual feature encodings (and slope encodings in Experiments 1 &amp; 2). In Experiment 1, the participant\u2019s task was to locate a pair of data values with a given relation (e.g., Find the \u2018small bar to the left of a tall bar\u2019 pair) among pairs of the opposite relation, and we measured processing efficiency from the increase in response times as the number of pairs increased. In Experiment 2, the task was to judge which of two relation types was more prevalent in a briefly presented display of 10 data pairs (e.g., Are there more \u2018small bar to the left of a tall bar\u2019 pairs or more \u2018tall bar to the left of a small bar\u2019 pairs?). In the final experiment, the task was to estimate the average delta within a briefly presented display of 6 data pairs (e.g., What is the average bar height difference across all \u2018small bar to the left of a tall bar\u2019 pairs?). Across all three experiments, visual processing of relations between data value pairs was significantly better when directly encoded as deltas rather than implicitly between individual data points, and varied substantially depending on the task (improvement ranged from 25% to 95%). Considering the ubiquity of bar charts and dot plots, relation perception for individual data values is highly inefficient, and confirms the need for alternative designs that provide not only absolute values, but also direct encoding of critical relationships between those values.", "uri": "https://vimeo.com/360049981", "name": "[VIS19 Preview] Measures of the benefit of direct encoding of data deltas for data pair relation perception (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:13:33+00:00", "description": "Authors: Honghui Mei, Wei Chen, Yating Wei, Yuanzhe Hu, Shuyue Zhou, Bingru Lin, Ying Zhao, Jiazhi Xia\n\nAbstract: Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts\u2019 flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity.", "uri": "https://vimeo.com/360049964", "name": "[VIS19 Preview] RSATree: Distribution-Aware Data Representation of Large-Scale Tabular Datasets for Flexible Visual Query...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:13:20+00:00", "description": "Authors: Ruizhen Hu, Tingkai Sha, Oliver van Kaick, Oliver Deussen, Hui Huang\n\nAbstract: We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.", "uri": "https://vimeo.com/360049945", "name": "[VIS19 Preview] Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:13:04+00:00", "description": "Authors: Yong Wang, Zhihua Jin, Qianwen Wang, Weiwei Cui, Tengfei Ma, Huamin Qu\n\nAbstract: Node-link diagrams are widely used to facilitate network explorations. However, when using a graph drawing technique to visualize networks, users often need to tune different algorithm-specific parameters iteratively by comparing the corresponding drawing results in order to achieve a desired visual effect. Such a trial and error process is often tedious and time-consuming, especially for non-expert users. Inspired by the excellent capability of deep learning techniques in data modeling and prediction, we aim at exploring the possibility of applying deep learning techniques to graph drawing. Specifically, we propose a graph-LSTM-based approach to directly map network structures to graph drawings. Given a set of layout examples as the training dataset, we train the proposed graph-LSTM-based model to capture their layout characteristics. Then, the trained model is used to generate graph drawings of a similar style for new networks. We evaluated the proposed approach on two regular drawings (i.e., grid layout and star layout) and two general drawings (i.e., ForceAtlas2 and PivotMDS) in both qualitative and quantitative ways, which provides support for the effectiveness of our approach. A time cost assessment on the drawings of small graphs with 20 to 50 nodes is also conducted. We further report the lessons we learned and discuss its limitations and future work.", "uri": "https://vimeo.com/360049919", "name": "[VIS19 Preview] DeepDrawing: A Deep Learning Approach to Graph Drawing (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:12:52+00:00", "description": "Authors: Mingming Fan, Ke Wu, Jian Zhao, Yue Li, Winter Wei, Khai Truong\n\nAbstract: Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML's input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind).", "uri": "https://vimeo.com/360049905", "name": "[VIS19 Preview] VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:12:39+00:00", "description": "Authors: Aditeya Pandey, Harsh Shukla, Geoffrey S. Young, Lei Qin, Amir A. Zamani, Liangge Hsu, Raymond Huang, Cody Dunne, Michelle Borkin\n\nAbstract: Blood circulation in the human brain is supplied through a network of cerebral arteries. If a clinician suspects a patient has a stroke or other cerebrovascular condition they order imaging tests. Neuroradiologists visually search the resulting scans for abnormalities. Their visual search tasks correspond to the abstract network analysis tasks of browsing and path following. To assist neuroradiologists in identifying cerebral artery abnormalities we designed CerebroVis, a novel abstract---yet spatially contextualized---cerebral artery network visualization. In this design study, we contribute a novel framing and definition of the cerebral artery system in terms of network theory and characterize neuroradiologist domain goals as abstract visualization and network analysis tasks. Through an iterative, user-centered design process we developed an abstract network layout technique which incorporates cerebral artery spatial context. The abstract visualization enables increased domain task performance over 3D geometry representations, while including spatial context helps preserve the user's mental map of the underlying geometry. We provide open source implementations of our network layout technique and prototype cerebral artery visualization tool. We demonstrate the robustness of our technique by successfully laying out 61 open source brain scans. We evaluate the effectiveness of our layout through a mixed methods study with three neuroradiologists. In a formative controlled experiment our study participants used CerebroVis and a conventional 3D visualization to examine real cerebral artery imaging data and to identify a simulated intracranial artery stenosis. Participants were more accurate at identifying stenoses using CerebroVis (absolute risk difference 13%). A free copy of this paper, the evaluation stimuli and data, and source code are available at https://osf.io/e5sxt/.", "uri": "https://vimeo.com/360049888", "name": "[VIS19 Preview] CerebroVis: Designing an Abstract yet Spatially Contextualized Cerebral Arteries Network Visualization...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:12:26+00:00", "description": "Authors: Joyce Ma, Kwan-Liu Ma, Jennifer Frazier\n\nAbstract: This study describes a detailed analysis of museum visitors\u2019 decoding process as they used a visualization designed to support exploration of a large, complex dataset. Quantitative and qualitative analyses revealed that it took, on average, 43 seconds for visitors to decode enough of the visualization to see patterns and relationships in the underlying data represented, and 54 seconds to arrive at their first correct data interpretation. Furthermore, visitors decoded throughout and not only upon initial use of the visualization. The study analyzed think-aloud data to identify issues visitors had mapping the visual representations to their intended referents, examine why they occurred, and consider if and how these decoding issues were resolved. The paper also describes how multiple visual encodings both helped and hindered decoding and concludes with implications on the design and adaptation of visualizations for informal science learning venues.", "uri": "https://vimeo.com/360049861", "name": "[VIS19 Preview] Decoding a Complex Visualization in a Science Museum \u2013 An Empirical Study (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:12:06+00:00", "description": "Authors: Cindy Xiong, Cristina Ceja, Casimir Ludwig, Steven Franconeri\n\nAbstract: In visual depictions of data, position (i.e., the vertical height of a line or a bar) is believed to be the most precise way to encode information compared to other encodings (e.g., hue). Not only are other encodings less precise than position, but they can also be prone to systematic biases (e.g., color category boundaries can distort perceived differences between hues). By comparison, position's high level of precision may seem to protect it from such biases. In contrast, across three empirical studies, we show that while position may be a precise form of data encoding, it can also produce systematic biases in how values are visually encoded, at least for reports of average position across a short delay. In displays with a single line or a single set of bars, reports of average positions were significantly biased, such that line positions were underestimated and bar positions were overestimated. In displays with multiple data series (i.e., multiple lines and/or sets of bars), this systematic bias still persisted. We also observed an effect of \"perceptual pull\", where the average position estimate for each series was 'pulled' toward the other. These findings suggest that, although position may still be the most precise form of visual data encoding, it can also be systematically biased.", "uri": "https://vimeo.com/360049835", "name": "[VIS19 Preview] Biased Average Position Estimates in Line and Bar Graphs: Underestimation, Overestimation, and Perceptual...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-15T02:11:55+00:00", "description": "Authors: Cindy Xiong, Joel Shapiro, Jessica Hullman, Steven Franconeri\n\nAbstract: Students who eat breakfast more frequently tend to have a higher grade point average. From this data, many people might confidently state that a before-school breakfast program would lead to higher grades. This is a reasoning error, because correlation does not necessarily indicate causation -- X and Y can be correlated without one directly causing the other. While this error is pervasive, its prevalence might be amplified or mitigated by the way that the data is presented to a viewer. Across three crowdsourced experiments, we examined whether how simple data relations are presented would mitigate this reasoning error. The first experiment tested examples similar to the breakfast-GPA relation, varying in the plausibility of the causal link. We asked participants to rate their level of agreement that the relation was correlated, which they rated appropriately as high. However, participants also expressed high agreement with a causal interpretation of the data. Levels of support for the causal interpretation were not equally strong across visualization types: causality ratings were highest for text descriptions and bar graphs, but weaker for scatter plots. But is this effect driven by bar graphs aggregating data into two groups or by the visual encoding type? We isolated data aggregation versus visual encoding type and examined their individual effect on perceived causality. Overall, different visualization designs afford different cognitive reasoning affordances across the same data. High levels of data aggregation by graphs tend to be associated with higher perceived causality in data. Participants perceived line and dot visual encodings as more causal than bar encodings. Our results demonstrate how some visualization designs trigger stronger causal links while choosing others can help mitigate unwarranted perceptions of causality.", "uri": "https://vimeo.com/360049821", "name": "[VIS19 Preview] Illusion of Causality in Visualized Data (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:11:42+00:00", "description": "Authors: Yun Wang, Zhida Sun, Haidong Zhang, Weiwei Cui, Ke Xu, Xiaojuan Ma, Dongmei Zhang\n\nAbstract: Fact sheets with vivid graphical design and intriguing statistical insights are prevalent for presenting raw data. They can help audiences understand data-related facts effectively and make a deep impression. However, designing a fact sheet requires both data and design expertise and is a laborious and time-consuming process. One needs to not only understand the data in depth but also produce intricate graphical representations. To assist in the design process, we present DataShot which, to the best of our knowledge, is the first automated system that creates fact sheets automatically from tabular data.  First, we conduct a qualitative analysis of 245 infographic examples to explore general infographic design space at both the sheet and element levels.  We identify common infographic structures, sheet layouts, fact types, and visualization styles during the study. Based on these findings, we propose a fact sheet generation pipeline, consisting of fact extraction, fact composition, and presentation synthesis, for the auto-generation workflow.To validate our system, we present use cases with three real-world datasets.  We conduct an in-lab user study to understand the usage of our system. Our evaluation results show that DataShot can efficiently generate satisfactory fact sheets to support further customization and data presentation", "uri": "https://vimeo.com/360049797", "name": "[VIS19 Preview] DataShot: Automatic Generation of Fact Sheet from Tabular Data (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:11:30+00:00", "description": "Authors: Matthew Brehmer, Bongshin Lee, Petra Isenberg, Eun Kyoung Choe\n\nAbstract: We compare the efficacy of animated and small multiples variants of scatterplots on mobile phones for comparing trends in multivariate datasets. Visualization is increasingly prevalent in mobile applications and mobile-first websites, yet there is little prior visualization research dedicated to small displays. In this paper, we build upon previous experimental research carried out on larger displays that assessed animated and non-animated variants of scatterplots. Incorporating similar experimental stimuli and tasks, we conducted an experiment where 96 crowdworker participants performed nine trend comparison tasks using their mobile phones. We found that those using a small multiples design consistently completed tasks in less time, albeit with slightly less confidence than those using an animated design. The accuracy results were more task-dependent, and we further interpret our results according to the characteristics of the individual tasks, with a specific focus on the trajectories of target and distractor data items in each task. We identify cases that appear to favor either animation or small multiples, providing new questions for further experimental research and implications for visualization design on mobile devices. Lastly, we provide a reflection on our evaluation methodology.", "uri": "https://vimeo.com/360049784", "name": "[VIS19 Preview] A Comparative Evaluation of Animation and Small Multiples for Trend Visualization on Mobile Phones (infovis...", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:11:02+00:00", "description": "Authors: Jorge Wagner, Wolfgang Stuerzlinger, Luciana Nedel\n\nAbstract: A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "uri": "https://vimeo.com/360049740", "name": "[VIS19 Preview] Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration (infovis...", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:10:21+00:00", "description": "Authors: Oh-Hyun Kwon, Kwan-Liu Ma\n\nAbstract: Different layouts can characterize different aspects of the same graph. Finding a \"good\" layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "uri": "https://vimeo.com/360049688", "name": "[VIS19 Preview] A Deep Generative Model for Graph Layout (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-15T02:10:04+00:00", "description": "Authors: Matthias Kraus, Niklas Weiler, Daniela Oelke, Johannes Kehrer, Daniel Keim, Johannes Fuchs\n\nAbstract: Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen,(2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.", "uri": "https://vimeo.com/360049662", "name": "[VIS19 Preview] The Impact of Immersion on Cluster Identification Tasks (infovis paper)", "year": "2019", "event": "INFOVIS, PREVIEW"}, {"created_time": "2019-09-14T18:39:33+00:00", "description": "Honorable Mention\n\nAuthors: Jules Vidal, Joseph Budin, Julien Tierny\n\nAbstract: This paper presents an efficient algorithm for the progressive approximation of Wasserstein barycenters of persistence diagrams, with applications to the visual analysis of ensemble data. Given a set of scalar fields, our approach enables the computation of a persistence diagram which is representative of the set, and which visually conveys the number, data ranges and saliences of the main features of interest found in the set. Such representative diagrams are obtained by computing explicitly the discrete Wasserstein barycenter of the set of persistence diagrams, a notoriously computationally intensive task. In particular, we revisit efficient algorithms for Wasserstein distance approximation [12,51] to extend previous work on barycenter estimation [94]. We present a new fast algorithm, which progressively approximates the barycenter by iteratively increasing the computation accuracy as well as the number of persistent features in the output diagram. Such a progressivity drastically improves convergence in practice and allows to design an interruptible algorithm, capable of respecting computation time constraints. This enables the approximation of Wasserstein barycenters within interactive times. We present an application to ensemble clustering where we revisit the k-means algorithm to exploit our barycenters and compute, within execution time constraints, meaningful clusters of ensemble data along with their barycenter diagram. Extensive experiments on synthetic and real-life data sets report that our algorithm converges to barycenters that are qualitatively meaningful with regard to the applications, and quantitatively comparable to previous techniques, while offering an order of magnitude speedup when run until convergence (without time constraint). Our algorithm can be trivially parallelized to provide additional speedups in practice on standard workstations. We provide a lightweight C++ implementation of our approach that can be used to reproduce our results.", "uri": "https://vimeo.com/360007493", "name": "[VIS19 Preview] Progressive Wasserstein Barycenters of Persistence Diagrams (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:30:54+00:00", "description": "Authors: Aashish Chaudhary, Sankhesh J. Jhaveri, Alvaro Sanchez, Lisa S. Avila, Kenneth M. Martin, Allison Vacanti, Marcus D. Hanwell, Will Schroeder\n\nAbstract: The visualization toolkit (VTK) is a popular cross-platform, open source toolkit for scientific and medical data visualization, processing, and analysis. It supports a wide variety of data formats, algorithms, and rendering techniques for both polygonal and volumetric data. In particular, VTK's volume rendering module has long provided a comprehensive set of features such as plane clipping, color and opacity transfer functions, lighting, and other controls needed for visualization. However, due to VTK's legacy OpenGL backend and its reliance on a deprecated API, the system did not take advantage of the latest improvements in graphics hardware or the flexibility of a programmable pipeline. Additionally, this dependence on an antiquated pipeline posed restrictions when running on emerging computing platforms, thereby limiting its overall applicability. In response to these shortcomings, the VTK community developed a new and improved volume rendering module, which not only provides a modern graphics processing unit-based implementation, but also augments its capabilities with new features such as fast volume clipping, gradient-magnitude-based opacity modulation, render to texture, and hardware-based volume picking.", "uri": "https://vimeo.com/360000042", "name": "[VIS19 Preview] Cross-Platform Ubiquitous Volume Rendering Using Programmable Shaders in VTK for Scientific and Medical...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:30:33+00:00", "description": "Authors: Eric Papenhausen, M. Harper Langston, Benoit Meister, Richard A. Lethin, Klaus Mueller\n\nAbstract: Performance optimization for parallel loop-oriented programs compromises between parallelism and locality. We present a visualization interface that allows programmers to assist the compiler in generating optimal code. It greatly improves the user's understanding of the transformations that took place and aids in making additional transformations in a visually intuitive way.", "uri": "https://vimeo.com/360000013", "name": "[VIS19 Preview] PUMA-V: Optimizing Parallel Code Performance Through Interactive Visualization (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:30:21+00:00", "description": "Authors: Dylan Cashman, Genevi\u00e8ve Patterson, Abigail Mosca, Nathan Watts, Shannon Robinson, Remco Chang\n\nAbstract: We present RNNbow, an interactive tool for visualizing the gradient flow during backpropagation in training of recurrent neural networks. By visualizing the gradient, as opposed to activations, RNNbow offers insight into how the network is learning. We show how it illustrates the vanishing gradient and the training process.", "uri": "https://vimeo.com/359999999", "name": "[VIS19 Preview] RNNbow: Visualizing Learning Via Backpropagation Gradients in RNNs (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:30:09+00:00", "description": "Authors: Tobias Kauer, Sagar Joglekar, Miriam Redi, Luca Maria Aiello, Daniele Quercia\n\nAbstract: Information visualization has great potential to make sense of the increasing amount of data generated by complex machine-learning algorithms. We design a set of visualizations for a new deep-learning algorithm called FaceLift (goodcitylife.org/facelift). This algorithm is able to generate a beautified version of a given urban image (such as from Google Street View), and our visualizations compare pairs of original and beautified images. With those visualizations, we aim at helping practitioners understand what happened during the algorithmic beautification without requiring them to be machine-learning experts. We evaluate the effectiveness of our visualizations to do just that with a survey among practitioners. From the survey results, we derive general design guidelines on how information visualization makes complex machine-learning algorithms more understandable to a general audience.", "uri": "https://vimeo.com/359999969", "name": "[VIS19 Preview] Mapping and Visualizing Deep-Learning Urban Beautification (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:29:39+00:00", "description": "Authors: Benjamin Karer, Alina Freund, Michael Horst, Inga Scheler, Thomas Kossurok, Franz-Josef Brandt\n\nAbstract: Tailoring visualization and interaction design to the demands of an application is challenging if domain information is unavailable or confidential. This article discusses a process-centric design approach, allowing for inferring design goals indirectly from informal interviews with practitioners and stakeholders. The design benefits from their expertise, while secrets remain secret.", "uri": "https://vimeo.com/359999909", "name": "[VIS19 Preview] Designing Effective Visual Interactive Systems despite Sparse Availability of Domain Information (cg&amp;a...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:29:27+00:00", "description": "Authors: Wei Zeng, Yu Ye\n\nAbstract: Creating lively places with high urban vitality is an ultimate goal for urban planning and design. The VitalVizor visual analytics system employs well-established visualization and interaction techniques to facilitate user exploration of spatial physical entities and non-spatial urban design metrics when studying urban vitality.", "uri": "https://vimeo.com/359999892", "name": "[VIS19 Preview] VitalVizor: A Visual Analytics System for Studying Urban Vitality (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:28:51+00:00", "description": "Authors: Hi Jwan Park, Arie Kaufman, Klaus Mueller\n\nAbstract: Graphoto is a framework that automatically generates a photo or adjusts an existing one to match a line graph. Since aesthetics is an important element in visualizing personal data, Graphoto provides users with aesthetically pleasing displays for casual line graph information visualization. More specifically, after creating a line graph of the input data, a photo that resembles the input data on the line graph is selected from a photo archive. If a selected photo does not match the line graph, we deform the photo to match the line graph. Once the photo matches the line graph of the data, the line graph is superimposed on the photo using different colors and styles depending on the user's artistic preferences. Additional embellishments are integrated into Graphoto, such as tick marks and text labels. We further present a user study to show the effectiveness of Graphoto in terms of data interpretation and aesthetics.", "uri": "https://vimeo.com/359999850", "name": "[VIS19 Preview] Graphoto: Aesthetically Pleasing Charts for Casual Information Visualization (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:28:33+00:00", "description": "Authors: Wolfgang B\u00fcschel, Stefan Vogt, Raimund Dachselt\n\nAbstract: Three-dimensional node-link diagrams are an important class of visualization for immersive analysis. Yet, there is little knowledge on how to visualize edges to support efficient analysis. We present an exploration of the design space for edge styles and discuss the results of a user study comparing six different edge variants.", "uri": "https://vimeo.com/359999817", "name": "[VIS19 Preview] Augmented Reality Graph Visualizations - Investigation of Visual Styles in 3D Node-Link Diagrams (cg&amp;a...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:28:11+00:00", "description": "Authors: Jorge A. Wagner Filho, Carla M. D. S. Freitas, Luciana Nedel\n\nAbstract: The VirtualDesk metaphor is an opportunity for more comfortable and efficient immersive data exploration, using tangible interaction with the analyst's physical work desk and embodied manipulation of mid-air data representations. In this paper, we present an extended discussion of its underlying concepts, and review and compare two previous case studies where promising results were obtained in terms of user comfort, engagement, and usability. We also discuss findings of a novel study conducted with geovisualization experts, pointing directions for improvement and future research.", "uri": "https://vimeo.com/359999785", "name": "[VIS19 Preview] Comfortable Immersive Analytics with the VirtualDesk Metaphor (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:27:32+00:00", "description": "Authors: G. Elisabeta Marai, Jason Leigh, Andrew Johnson\n\nAbstract: This paper provides a 25-year-long perspective on immersive analytics through the lens of first-in-kind technological advancements introduced at the Electronic Visualization Laboratory, University of Illinois at Chicago, along with the challenges and lessons learned from multiple immersive analytics projects.", "uri": "https://vimeo.com/359999721", "name": "[VIS19 Preview] Immersive Analytics Lessons from the Electronic Visualization Laboratory: a 25 Year Perspective (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:26:57+00:00", "description": "Authors: Alexander Ivanov, Kurtis Danyluk, Christian Jacob, Wesley Willett\n\nAbstract: We examine the potential for immersive unit visualizations-interactive virtual environments populated with objects representing individual items in a dataset. Our virtual reality prototype highlights how immersive unit visualizations can allow viewers to examine data at multiple scales, support immersive exploration, and create affective personal experiences with data.", "uri": "https://vimeo.com/359999645", "name": "[VIS19 Preview] A Walk Among the Data: Exploration and Anthropomorphism in Immersive Unit Visualizations (cg&amp;a paper)", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:26:40+00:00", "description": "Authors: Steve Petruzza, Attila Gyulassy, Samuel Leventhal, Jackson Baglino, Michael Czabaj, Ashely Spears, Valerio Pascucci\n\nAbstract: Metallic open-cell foams are promising structural materials with applications in multifunctional systems such as biomedical implants, energy absorbers in impact, noise mitigation, and batteries. There is a high demand for means to understand and correlate the design space of material performance metrics to the material structure in terms of attributes such as density, ligament and node properties, void sizes, and alignments. Currently, X-ray Computed Tomography (CT) scans of these materials are segmented either manually or with skeletonization approaches that may not accurately model the variety of shapes present in nodes and ligaments, especially irregularities that arise from manufacturing, image artifacts, or deterioration due to compression. In this paper, we present a new workflow for analysis of open-cell foams that combines a new density measurement to identify nodal structures, and topological approaches to identify ligament structures between them. Additionally, we provide automated measurement of foam properties. We demonstrate stable extraction of features and time-tracking in an image sequence of a foam being compressed. Our approach allows researchers to study larger and more complex foams than could previously be segmented only manually, and enables the high-throughput analysis needed to predict future foam performance.", "uri": "https://vimeo.com/359999616", "name": "[VIS19 Preview] High-throughput feature extraction for measuring attributes of deforming open cell foams (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:26:24+00:00", "description": "Authors: Tobias Klein, Ivan Viola, Eduard Gr\u00f6ller, Peter Mindek\n\nAbstract: Biologists often use computer graphics to visualize structures, which due to physical limitations are not possible to image with a microscope. One example for such structures are microtubules, which are present in every eukaryotic cell. They are part of the cytoskeleton maintaining the shape of the cell and playing a key role in the cell division. In this paper, we propose a scienti\ufb01cally accurate multi-scale procedural model of microtubule dynamics as a novel application scenario for procedural animation, which can generate visualizations of their overall shape, molecular structure, as well as animations of the dynamic behaviour of their growth and disassembly. The model is spanning tens of micrometers down to atomic resolution. All the aspects of the model are driven by scienti\ufb01c data. The advantage over a traditional, manual animation approach is that when the underlying data change, for instance due to new evidence, the model can be recreated immediately. The procedural animation concept is presented in its generic form, with several novel extensions, allowing an easy translation to other domains with emergent multi-scale behavior.", "uri": "https://vimeo.com/359999585", "name": "[VIS19 Preview] Multi-Scale Procedural Animations of Microtubule Dynamics Based on Measured Data (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:25:15+00:00", "description": "Authors: Andrew Wentzel, Peter Haula, Timothy Basil Luciani, Baher Elgohari, Hesham Elhalawani, Guadalupe Canahuate, David M. Vock, Clifton David Fuller, G. Elisabeta Marai\n\nAbstract: We describe a visual computing approach to radiation therapy (RT) planning, based on spatial similarity within a patient cohort. In radiotherapy for head and neck cancer treatment, dosage to organs at risk surrounding a tumor is a large cause of treatment toxicity. Along with the availability of patient repositories, this situation has lead to clinician interest in understanding and predicting RT outcomes based on previously treated similar patients. To enable this type of analysis, we introduce a novel topology-based spatial similarity measure, T-SSIM, and a predictive algorithm based on this similarity measure. We couple the algorithm with a visual steering interface that intertwines visual encodings for the spatial data and statistical results, including a novel parallel-marker encoding that is spatially aware. We report quantitative results on a cohort of 165 patients, as well as a qualitative evaluation with domain experts in radiation oncology, data management, biostatistics, and medical imaging, who are collaborating remotely.", "uri": "https://vimeo.com/359999477", "name": "[VIS19 Preview] Cohort-based T-SSIM Visual Computing for Radiation Therapy Prediction and Exploration (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:24:58+00:00", "description": "Authors: Irene Baeza Rojo, Tobias G\u00fcnther\n\nAbstract: The topological analysis of unsteady vector fields remains to this day one of the largest challenges in flow visualization. We build up on recent work on vortex extraction to define a time-dependent vector field topology for 2D and 3D flows. In our work, we split the vector field into two components: a vector field in which the flow becomes steady, and the remaining ambient flow that describes the motion of topological elements (such as sinks, sources and saddles) and feature curves (vortex corelines and bifurcation lines). To this end, we expand on recent local optimization approaches by modeling spatially-varying deformations through displacement transformations from continuum mechanics. We compare and discuss the relationships with existing local and integration-based topology extraction methods, showing for instance that separatrices seeded from saddles in the optimal frame align with the integration-based streakline vector field topology. In contrast to the streakline-based approach, our method gives a complete picture of the topology for every time slice, including the steps near the temporal domain boundaries. With our work it now becomes possible to extract topological information even when only few time slices are available. We demonstrate the method in several analytical and numerically-simulated flows and discuss practical aspects, limitations and opportunities for future work.", "uri": "https://vimeo.com/359999444", "name": "[VIS19 Preview] Vector Field Topology of Time-Dependent Flows in a Steady Reference Frame (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:24:31+00:00", "description": "Authors: Sarkis Halladjian, Haichao Miao, David Kou\u0159il, Eduard Gr\u00f6ller, Ivan Viola, Tobias Isenberg\n\nAbstract: We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels---the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out---instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.", "uri": "https://vimeo.com/359999391", "name": "[VIS19 Preview] ScaleTrotter: Illustrative Visual Travels Across Negative Scales (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:24:00+00:00", "description": "Authors: Andrey Krekhov, Sebastian Cmentowski, Andre Waschk, Jens Krueger\n\nAbstract: Visualizations rely on highlighting to attract and guide our attention. To make an object of interest stand out independently from a number of distractors, the underlying visual cue, e.g., color, has to be preattentive. In our prior work, we introduced Deadeye as an instantly recognizable highlighting technique that works by rendering the target object for one eye only. In contrast to prior approaches, Deadeye excels by not modifying any visual properties of the target. However, in the case of 2D visualizations, the method requires an additional setup to allow dichoptic presentation, which is a considerable drawback. As a follow-up to requests from the community, this paper explores Deadeye as a highlighting technique for 3D visualizations, because such stereoscopic scenarios support dichoptic presentation out of the box. Deadeye suppresses binocular disparities for the target object, so we cannot assume the applicability of our technique as a given fact. With this motivation, the paper presents quantitative evaluations of Deadeye in VR, including configurations with multiple heterogeneous distractors as an important robustness challenge. After confirming the preserved preattentiveness (all average accuracies above 90 %) under such real-world conditions, we explore VR volume rendering as an example application scenario for Deadeye. We depict a possible workflow for integrating our technique, conduct an exploratory survey to demonstrate benefits and limitations, and finally provide related design implications.", "uri": "https://vimeo.com/359999349", "name": "[VIS19 Preview] Deadeye Visualization Revisited: Investigation of Preattentiveness and Applicability in Virtual Environments...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:23:46+00:00", "description": "Authors: Yifan Wang, Zichun Zhong, Jing Hua\n\nAbstract: This paper introduces a deep neural network based method, i.e., DeepOrganNet, to generate and visualize fully high-fidelity 3D / 4D organ geometric models from single-view medical images with complicated background in real time. Traditional 3D / 4D medical image reconstruction requires near hundreds of projections, which cost insufferable computational time and deliver undesirable high imaging / radiation dose to human subjects. Moreover, it always needs further notorious processes to segment or extract the accurate 3D organ models subsequently. The computational time and imaging dose can be reduced by decreasing the number of projections, but the reconstructed image quality is degraded accordingly. To our knowledge, there is no method directly and explicitly reconstructing multiple 3D organ meshes from a single 2D medical grayscale image on the fly. Given single-view 2D medical images, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end DeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung models with a variety of geometric shapes by learning the smooth deformation fields from multiple templates based on a trivariate tensor-product deformation technique, leveraging an informative latent descriptor extracted from input 2D images. The proposed method can guarantee to generate high-quality and high-fidelity manifold meshes for 3D / 4D lung models; while, all current deep learning based approaches on the shape reconstruction from a single image cannot. The major contributions of this work are to accurately reconstruct the 3D organ shapes from 2D single-view projection, significantly improve the procedure time to allow on-the-fly visualization, and dramatically reduce the imaging dose for human subjects. Experimental results are evaluated and compared with the traditional reconstruction method and the state-of-the-art in deep learning, by using extensive 3D and 4D examples, including both synthetic phantom and real patient datasets. The efficiency of the proposed method shows that it only needs several milliseconds to generate organ meshes with 10K vertices, which has great potential to be used in real-time image guided radiation therapy (IGRT).", "uri": "https://vimeo.com/359999320", "name": "[VIS19 Preview] DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D Lung Models from Single-View...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:23:30+00:00", "description": "Authors: Jonas Lukasczyk, Christoph Garth, Tim Biedert, Ross Maciejewski, Gunther H Weber, Heike Leitte\n\nAbstract: This work describes an approach for the interactive visual analysis of large-scale simulations, where numerous superlevel set components and their evolution are of primary interest. The approach first derives, at simulation runtime, a specialized Cinema database that consists of images of component groups, and topological abstractions. This database is processed by a novel graph operation-based nested tracking graph algorithm (GO-NTG) that dynamically computes NTGs for component groups based on size, overlap, persistence, and level thresholds. The resulting NTGs are in turn used in a feature-centered visual analytics framework to query specific database elements and update feature parameters, facilitating flexible post hoc analysis.", "uri": "https://vimeo.com/359999289", "name": "[VIS19 Preview] Dynamic Nested Tracking Graphs (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:23:18+00:00", "description": "Authors: Baldwin Nsonga, Gerik Scheuermann, Stefan Gumhold, Jordi Ventosa-Molina, Denis Koschichow, Jochen Fr\u00f6hlich\n\nAbstract: Turbines are essential components of jet planes and power plants. Therefore, their efficiency and service life are of central engineering interest. In the case of jet planes or thermal power plants, the heating of the turbines due to the hot gas flow is critical. Besides effective cooling, it is a major goal of engineers to minimize heat transfer between gas flow and turbine by design. Since it is known that splat events have a substantial impact on the heat transfer between flow and immersed surfaces, we adapt a splat detection and visualization method to a turbine cascade simulation in this case study. Because splat events are small phenomena, we use a direct numerical simulation resolving the turbulence in the flow as the base of our analysis. The outcome shows promising insights into splat formation and its relation to vortex structures. This may lead to better turbine design in the future.", "uri": "https://vimeo.com/359999261", "name": "[VIS19 Preview] Analysis of the Near-Wall Flow in a Turbine Cascade by Splat Visualization (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:23:03+00:00", "description": "Authors: Katarina Furmanova, Adam Jur\u010d\u00edk, Barbora Kozlikova, Helwig Hauser, Jan By\u0161ka\n\nAbstract: When studying multi-body protein complexes, biochemists use computational tools that can suggest hundreds or thousands of their possible spatial configurations. However, it is not feasible to experimentally verify more than only a very small subset of them. In this paper, we propose a novel multiscale visual drilldown approach that was designed in tight collaboration with proteomic experts, enabling a systematic exploration of the configuration space. Our approach takes advantage of the hierarchical structure of the data \u2013 from the whole ensemble of protein complex configurations to the individual configurations, their contact interfaces, and the interacting amino acids. Our new solution is based on interactively linked 2D and 3D views for individual hierarchy levels. At each level, we offer a set of selection and filtering operations that enable the user to narrow down the number of configurations that need to be manually scrutinized. Furthermore, we offer a dedicated filter interface, which provides the users with an overview of the applied filtering operations and enables them to examine their impact on the explored ensemble. This way, we maintain the history of the exploration process and thus enable the user to return to an earlier point of the exploration. We demonstrate the effectiveness of our approach on two case studies conducted by collaborating proteomic experts.", "uri": "https://vimeo.com/359999233", "name": "[VIS19 Preview] Multiscale Visual Drilldown for the Analysis of Large Ensembles of Multi-Body Protein Complexes (scivis...", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:22:48+00:00", "description": "Authors: Ashok Jallepalli, Joshua A Levine, Mike Kirby\n\nAbstract: High-order finite element methods (HO-FEM) are gaining popularity in the simulation community due to their success in solving complex flow dynamics. There is an increasing need to analyze the data produced as output by these simulations. \n Simultaneously, topological analysis tools are emerging as powerful methods for investigating simulation data. However, most of the current approaches to topological analysis have had limited application to HO-FEM simulation data for two reasons.\n First, the current topological tools are designed for linear data (polynomial degree one), but the polynomial degree of the data output by these simulations is typically higher (routinely up to polynomial degree six). Second, the simulation data and derived quantities of the simulation data have discontinuities at element boundaries, and these discontinuities do not match the input requirements for the topological tools. \n One solution to both issues is to transform the high-order data to achieve low-order, continuous inputs for topological analysis. Nevertheless, there has been little work evaluating the possible transformation choices and their downstream effect on the topological analysis. We perform an empirical study to evaluate two commonly used data transformation methodologies along with the recently introduced L-SIAC filter for processing high-order simulation data. Our results show diverse behaviors are possible. We offer some guidance about how best to consider a pipeline of topological analysis of HO-FEM simulations with the currently available implementations of topological analysis.", "uri": "https://vimeo.com/359999206", "name": "[VIS19 Preview] The Effect of Data Transformations on Scalar Field Topological Analysis of High-Order FEM Solutions (scivis...", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:22:34+00:00", "description": "Authors: Pepe Eulzer, Sandy Engelhardt, Nils Lichtenberg, Raffaele de Simone, Kai Lawonn\n\nAbstract: The mitral valve, one of the four valves in the human heart, controls the bloodflow between the left atrium and ventricle and may suffer from various pathologies. Malfunctioning valves can be treated by reconstructive surgeries, which have to be carefully planned and evaluated. While current research focuses on the modeling and segmentation of the valve, we base our work on existing segmentations of patient-specific mitral valves, that are also time-resolved (3D+t) over the cardiac cycle. The interpretation of the data can be ambiguous, due to the complex surface of the valve and multiple time steps. We therefore propose a software prototype to analyze such 3D+t data, by extracting pathophysiological parameters and presenting them via dimensionally reduced visualizations. For this, we rely on an existing algorithm to unroll the convoluted valve surface towards a flattened 2D representation. In this paper, we show that the 3D+t data can be transferred to 3D or 2D representations in a way that allows the domain expert to faithfully grasp important aspects of the cardiac cycle. In this course, we not only consider common pathophysiological parameters, but also introduce new observations that are derived from landmarks within the segmentation model.Our analysis techniques were developed in collaboration with domain experts and a survey showed that the insights have the potential to support mitral valve diagnosis and the comparison of the pre- and post-operative condition of a patient.", "uri": "https://vimeo.com/359999188", "name": "[VIS19 Preview] Temporal Views of Flattened Mitral Valve Geometries (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:22:23+00:00", "description": "Authors: Tobias Rapp, Christoph Peters, Carsten Dachsbacher\n\nAbstract: We propose a data reduction technique for scattered data based on statistical sampling. Our void-and-cluster sampling technique finds a representative subset that is optimally distributed in the spatial domain with respect to the blue noise property. In addition, it can adapt to a given density function, which we use to sample regions of high complexity in the multivariate value domain more densely. Moreover, our sampling technique implicitly defines an ordering on the samples that enables progressive data loading and a continuous level-of-detail representation. We extend our technique to sample time-dependent trajectories, for example pathlines in a time interval, using an efficient and iterative approach. Furthermore, we introduce a local and continuous error measure to quantify how well a set of samples represents the original dataset. We apply this error measure during sampling to guide the number of samples that are taken. Finally, we use this error measure and other quantities to evaluate the quality, performance, and scalability of our algorithm.", "uri": "https://vimeo.com/359999170", "name": "[VIS19 Preview] Void-and-Cluster Sampling of Large Scattered Data and Trajectories (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:22:10+00:00", "description": "Authors: Fariba Khan, Lawrence Roy, Eugene Zhang, Botong Qu, Shih-Hsuan Hung, Harry Yeh, Robert S. Laramee, Yue Zhang\n\nAbstract: Asymmetric tensor fields have found applications in many science and engineering domains, such as fluid dynamics. Recent advances in the visualization and analysis of 2D asymmetric tensor fields focus on pointwise analysis of the tensor field and effective visualization metaphors such as colors, glyphs, and hyperstreamlines. \n \n In this paper, we provide a novel multi-scale topological analysis framework for asymmetric tensor fields on surfaces. Our multi-scale framework is based on the notions of eigenvalue and eigenvector graphs. At the core of our framework are the identification of atomic operations that modify the graphs and the scale definition that guides the order in which the graphs are simplified to enable clarity and focus for the visualization of topological analysis on data of different sizes. We also provide efficient algorithms to realize these operations. Furthermore, we provide physical interpretation of these graphs. \n \n To demonstrate the utility of our system, we apply our multi-scale analysis to data in computational fluid dynamics.", "uri": "https://vimeo.com/359999144", "name": "[VIS19 Preview] Multi-Scale Topological Analysis of Asymmetric Tensor Fields on Surfaces (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:21:49+00:00", "description": "Authors: Irene Baeza Rojo, Markus Gross, Tobias G\u00fcnther\n\nAbstract: Time-dependent fluid flows often contain numerous hyperbolic Lagrangian coherent structures, which act as transport barriers that guide the advection. The finite-time Lyapunov exponent is a commonly-used approximation to locate these repelling or attracting structures. Especially on large numerical simulations, the FTLE ridges can become arbitrarily sharp and very complex. Thus, the discrete sampling onto a grid for a subsequent direct volume rendering is likely to miss sharp ridges in the visualization. For this reason, an unbiased Monte Carlo-based rendering approach was recently proposed that treats the FTLE field as participating media with single-scattering. This method constructs a ground truth rendering without discretization, but it is prohibitively slow with render times in the order of days or weeks for a single image. In this paper, we accelerate the rendering process significantly, which allows us to compute video sequence of high-resolution FTLE animations in a much more reasonable time frame. For this, we follow two orthogonal approaches to improve on the rendering process: the volumetric light path integration in gradient domain and an acceleration of the transmittance estimation. We analyze the convergence and performance of the proposed method and demonstrate the approach by rendering complex FTLE fields in several 3D vector fields.", "uri": "https://vimeo.com/359999098", "name": "[VIS19 Preview] Accelerated Monte Carlo Rendering of Finite-Time Lyapunov Exponents (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:21:28+00:00", "description": "Authors: Zhutian Chen, Wei Zeng, ZhiGuang Yang, Lingyun Yu, Chi-Wing Fu, Huamin Qu\n\nAbstract: Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an attention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conducted a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections.", "uri": "https://vimeo.com/359999060", "name": "[VIS19 Preview] LassoNet: Deep Lasso-Selection of 3D Point Clouds (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:20:55+00:00", "description": "Best paper\n\nAuthors: Wenbin He, Junpeng Wang, Hanqi Guo, Ko-Chih Wang, Han-Wei Shen, Mukund Raj, Youssef S. G. Nashed, Tom Peterka\n\nAbstract: We propose InSituNet, a deep learning based surrogate model to support parameter space exploration for ensemble simulations that are visualized in situ. In situ visualization, generating visualizations at simulation time, is becoming prevalent in handling large-scale simulations because of the I/O and storage constraints. However, in situ visualization approaches limit the flexibility of post-hoc exploration because the raw simulation data are no longer available. Although multiple image-based approaches have been proposed to mitigate this limitation, those approaches lack the ability to explore the simulation parameters. Our approach allows flexible exploration of parameter space for large-scale ensemble simulations by taking advantage of the recent advances in deep learning. Specifically, we design InSituNet as a convolutional regression model to learn the mapping from the simulation and visualization parameters to the visualization results. With the trained model, users can generate new images for different simulation parameters under various visualization settings, which enables in-depth analysis of the underlying ensemble simulations. We demonstrate the effectiveness of InSituNet in combustion, cosmology, and ocean simulations through quantitative and qualitative evaluations.", "uri": "https://vimeo.com/359998980", "name": "[VIS19 Preview] InSituNet: Deep Image Synthesis for Parameter Space Exploration of Ensemble Simulations (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:20:42+00:00", "description": "Authors: Robin Bader, Michael Sprenger, Nikolina Ban, Stefan R\u00fcdis\u00fchli, Christoph Sch\u00e4r, Tobias G\u00fcnther\n\nAbstract: Potential vorticity is among the most important scalar quantities in atmospheric dynamics. For instance, potential vorticity plays a key role in particularly strong wind peaks in extratropical cyclones and it is able to explain the occurrence of frontal rain bands. Potential vorticity combines the key quantities of atmospheric dynamics, namely rotation and stratification. Under suitable wind conditions elongated banners of potential vorticity appear in the lee of mountains. Their role in atmospheric dynamics has recently raised considerable interest in the meteorological community for instance due to their influence in aviation wind hazards and maritime transport. In order to support meteorologists and climatologists in the analysis of these structures, we developed an extraction algorithm and a visual exploration framework consisting of multiple linked views. For the extraction we apply a predictor-corrector algorithm that follows streamlines and realigns them with extremal lines of potential vorticity. Using the agglomerative hierarchical clustering algorithm, we group banners from different sources based on their proximity. To visually analyze the time-dependent banner geometry, we provide interactive overviews and enable the query for detail on demand, including the analysis of different time steps, potentially correlated scalar quantities, and the wind vector field. In particular, we study the relationship between relative humidity and the banners for their potential in indicating the development of precipitation. Working with our method, the collaborating meteorologists gained a deeper understanding of the three-dimensional processes, which may spur follow-up research in the future.", "uri": "https://vimeo.com/359998953", "name": "[VIS19 Preview] Extraction and Visual Analysis of Potential Vorticity Banners around the Alps (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:20:21+00:00", "description": "Authors: Lin Yan, Yusu Wang, Elizabeth Munch, Ellen Gasparovic, Bei Wang\n\nAbstract: Physical phenomena in science and engineering are frequently modeled using scalar fields.\n In scalar field topology, graph-based topological descriptors such as merge trees, contour trees, and Reeb graphs are commonly used to characterize topological changes in the (sub)level sets of scalar fields.\n One of the biggest challenges and opportunities to advance topology-based visualization is to understand and incorporate uncertainty into such topological descriptors to effectively reason about their underlying data.\n In this paper, we study a structural average of a set of labeled merge trees and use it to encode uncertainty in data.\n Specifically, we compute a 1-center tree that minimizes its maximum distance to any other tree in the set under a well-defined metric called the interleaving distance.\n We provide heuristic strategies that compute structural averages of merge trees whose labels do not fully agree.\n We further provide an interactive visualization system that resembles a numerical calculator that takes as input a set of merge trees and outputs a tree as their structural average.\n We also highlight structural similarities between the input and the average and incorporate uncertainty information for visual exploration.\n We develop a novel measure of uncertainty, referred to as consistency, via a metric-space view of the input trees.\n Finally, we demonstrate an application of our framework through merge trees that arise from ensembles of scalar fields.\n Our work is the first to employ interleaving distances and consistency to study a global, mathematically rigorous, structural average of merge trees in the context of uncertainty visualization.", "uri": "https://vimeo.com/359998912", "name": "[VIS19 Preview] A Structural Average of Labeled Merge Trees for Uncertainty Visualization (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:20:07+00:00", "description": "Authors: Seth A Johnson, Francesca Samsel, Greg Abram, Daniel Olson, Andrew Solis, Bridger Herman, Philip Wolfram, Christophe Langlet, Daniel F. Keefe\n\nAbstract: We introduce Artifact-Based Rendering (ABR), a framework of tools, algorithms, and processes that makes it possible to produce real, data-driven 3D scientific visualizations with a visual language derived entirely from colors, lines, textures, and forms created using traditional physical media or found in nature. A theory and process for ABR is presented to address three current needs: (i) designing better visualizations by making it possible for non-programmers to rapidly design and critique many alternative data-to-visual mappings; (ii) expanding the visual vocabulary used in scientific visualizations to depict increasingly complex multivariate data; (iii) bringing a more engaging, natural, and human-relatable handcrafted aesthetic to data visualization. New tools and algorithms to support ABR include front-end applets for constructing artifact-based colormaps, optimizing 3D scanned meshes for use in data visualization, and synthesizing textures from artifacts. These are complemented by an interactive rendering engine with custom algorithms and interfaces that demonstrate multiple new visual styles for depicting point, line, surface, and volume data. A within-the-research-team design study provides early evidence of the shift in visualization design processes that ABR is believed to enable when compared to traditional scientific visualization systems. Qualitative user feedback on applications to climate science and brain imaging support the utility of ABR for scientific discovery and public communication.", "uri": "https://vimeo.com/359998878", "name": "[VIS19 Preview] Artifact-Based Rendering: Harnessing Natural and Traditional Visual Media for More Expressive and Engaging...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:19:35+00:00", "description": "Authors: Alexander Bock, Emil Axelsson, Jonathas Costa, Gene Payne, Micah Acinapura, Vivian Trakinski, Carter B Emmart PhD, Claudio Silva, Charles Hansen, Anders Ynnerman\n\nAbstract: Human knowledge about the cosmos is rapidly increasing as instruments and simulations are generating new data supporting the formation of theory and understanding of the vastness and complexity of the universe. OpenSpace is a software system that takes on the mission of providing an integrated view of all these sources of data and supports interactive exploration of the known universe from the millimeter scale showing instruments on spacecrafts to billions of light years when visualizing the early universe. The ambition is to support research in astronomy and space exploration, science communication at museums and in planetariums as well as bringing exploratory astrographics to the class room. There is a multitude of challenges that need to be met in reaching this goal such as the data variety, multiple spatio-temporal scales, collaboration capabilities, etc. Furthermore, the system has to be flexible and modular to enable rapid prototyping and inclusion of new research results or space mission data and thereby shorten the time from discovery to dissemination. To support the different use cases the system has to be hardware agnostic and support a range of platforms and interaction paradigms. In this paper we describe how OpenSpace meets these challenges in an open source effort that is paving the path for the next generation of interactive astrographics.", "uri": "https://vimeo.com/359998824", "name": "[VIS19 Preview] OpenSpace: A System for Astrographics (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:19:01+00:00", "description": "Authors: Juraj P\u00e1lenik, Jan By\u0161ka, Stefan Bruckner, Helwig Hauser\n\nAbstract: Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space/time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein\u2013ligand simulation.", "uri": "https://vimeo.com/359998771", "name": "[VIS19 Preview] Scale-Space Splatting: Reforming Spacetime for the Cross-Scale Exploration of Integral Measures in Molecular...", "year": "2019", "event": "PREVIEW"}, {"created_time": "2019-09-14T17:18:42+00:00", "description": "Honorable Mention\n\nAuthors: Pavol Klacansky, Attila Gyulassy, Peer-Timo Bremer, Valerio Pascucci\n\nAbstract: Topological approaches to data analysis can answer complex questions about the number, connectivity, and scale of intrinsic features in scalar data. \n However, the global nature of many topological structures makes their computation challenging at scale, and thus often limits the size of data that can be processed.\n One key quality to achieving scalability and performance on modern architectures is data locality, i.e., a process operates on data that resides in a nearby memory system, avoiding frequent jumps in data access patterns. \n From this perspective, topological computations are particularly challenging because the implied data structures represent features that can span the entire data set, often requiring a global traversal phase that limits their scalability.\n Traditionally, expensive preprocessing is considered an acceptable trade-off as it accelerates all subsequent queries.\n Most published use cases, however, explore only a fraction of all possible queries, most often those returning small, local features. \n In these cases, much of the global information is not utilized, yet computing it dominates the overall response time. We address this challenge for merge trees, one of the most commonly used topological structures.\n In particular, we propose an alternative representation, the merge forest, a collection of local trees corresponding to regions in a domain decomposition.\n Local trees are connected by a bridge set that allows us to recover any necessary global information at query time. The resulting system couples (i) a preprocessing that scales linearly in practice with (ii) fast runtime queries that provide the same functionality as traditional queries of a global merge tree.\n We test the scalability of our approach on a shared-memory parallel computer and demonstrate how data structure locality enables the analysis of large data with an order of magnitude performance improvement over the status quo. Furthermore, a merge forest reduces the memory overhead compared to a global merge tree and enables the processing of data sets that are an order of magnitude larger than possible with previous algorithms.", "uri": "https://vimeo.com/359998742", "name": "[VIS19 Preview] Toward Localized Topological Data Structures: Querying the Forest for the Tree (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T17:18:12+00:00", "description": "Authors: Jun Han, Chaoli Wang\n\nAbstract: We present TSR-TVD, a novel deep learning framework that generates temporal super-resolution (TSR) of time-varying data (TVD) using adversarial learning. TSR-TVD is the first work that applies the recurrent generative network (RGN), a combination of the recurrent neural network (RNN) and generative adversarial network (GAN), to generate temporal high-resolution volume sequences from low-resolution ones. The design of TSR-TVD includes a generator and a discriminator. The generator takes a pair of volumes as input and outputs the synthesized intermediate volume sequence through forward and backward predictions. The discriminator takes the synthesized intermediate volumes as input and produces a score indicating the realness of the volumes. Our method handles multivariate data as well where the trained network from one variable is applied to generate TSR for another variable. To demonstrate the effectiveness of TSR-TVD, we show quantitative and qualitative results with several time-varying multivariate data sets and compare our method against standard linear interpolation and solutions solely based on RNN or CNN.", "uri": "https://vimeo.com/359998660", "name": "[VIS19 Preview] TSR-TVD: Temporal Super-Resolution for Time-Varying Data Analysis and Visualization (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T01:13:03+00:00", "description": "Authors: Tobias Rapp\n\nAbstract: We propose a data reduction technique for scattered data based on statistical sampling. Our void-and-cluster sampling technique finds a representative subset that is optimally distributed in the spatial domain with respect to the blue noise property. In addition, it can adapt to a given density function, which we use to sample regions of high complexity in the multivariate value domain more densely. Moreover, our sampling technique implicitly defines an ordering on the samples that enables progressive data loading and a continuous level-of-detail representation. We extend our technique to sample time-dependent trajectories, for example pathlines in a time interval, using an efficient and iterative approach. Furthermore, we introduce a local and continuous error measure to quantify how well a set of samples represents the original dataset. We apply this error measure during sampling to guide the number of samples that are taken. Finally, we use this error measure and other quantities to evaluate the quality, performance, and scalability of our algorithm.", "uri": "https://vimeo.com/359910998", "name": "[VIS19 Preview] Void-and-Cluster Sampling of Large Scattered Data and Trajectories (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2019-09-14T01:12:49+00:00", "description": "Honorable Mention\n\nAuthors: Jules Vidal\n\nAbstract: This paper presents an efficient algorithm for the progressive approximation of Wasserstein barycenters of persistence diagrams, with applications to the visual analysis of ensemble data. Given a set of scalar fields, our approach enables the computation of a persistence diagram which is representative of the set, and which visually conveys the number, data ranges and saliences of the main features of interest found in the set. Such representative diagrams are obtained by computing explicitly the discrete Wasserstein barycenter of the set of persistence diagrams, a notoriously computationally intensive task. In particular, we revisit efficient algorithms for Wasserstein distance approximation [12,51] to extend previous work on barycenter estimation [94]. We present a new fast algorithm, which progressively approximates the barycenter by iteratively increasing the computation accuracy as well as the number of persistent features in the output diagram. Such a progressivity drastically improves convergence in practice and allows to design an interruptible algorithm, capable of respecting computation time constraints. This enables the approximation of Wasserstein barycenters within interactive times. We present an application to ensemble clustering where we revisit the k-means algorithm to exploit our barycenters and compute, within execution time constraints, meaningful clusters of ensemble data along with their barycenter diagram. Extensive experiments on synthetic and real-life data sets report that our algorithm converges to barycenters that are qualitatively meaningful with regard to the applications, and quantitatively comparable to previous techniques, while offering an order of magnitude speedup when run until convergence (without time constraint). Our algorithm can be trivially parallelized to provide additional speedups in practice on standard workstations. We provide a lightweight C++ implementation of our approach that can be used to reproduce our results.", "uri": "https://vimeo.com/359910973", "name": "[VIS19 Preview] Progressive Wasserstein Barycenters of Persistence Diagrams (scivis paper)", "year": "2019", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-11-13T09:10:58+00:00", "description": "Authors: Nina McCurdy, Julie Gerdes, Miriah Meyer", "uri": "https://vimeo.com/300462407", "name": "InfoVis 2018: A Framework for Externalizing Implicit Error Using Visualization", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T09:09:24+00:00", "description": "Authors: Hayeong Song, Danielle Albers Szafir", "uri": "https://vimeo.com/300462209", "name": "InfoVis 2018: Where\u2019s My Data? Evaluating Visualizations with Missing Data", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T09:04:16+00:00", "description": "Authors: Jessica Hullman, Xiaoli Qiao, Michael Correll, Alex Kale, Matthew Kay", "uri": "https://vimeo.com/300461524", "name": "InfoVis 2018: In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T09:02:16+00:00", "description": "Authors: Alex Kale, Francis Nguyen, Matthew Kay, Jessica Hullman", "uri": "https://vimeo.com/300461252", "name": "InfoVis 2018: Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T08:59:18+00:00", "description": "Authors: Le Liu, Lace M. K. Padilla, Sarah Creem-Regehr, Donald House", "uri": "https://vimeo.com/300460857", "name": "InfoVis 2018: Visualizing Uncertain Tropical Cyclone Predictions using Representative Samples from Ensembles of Forecast Tracks", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T08:56:03+00:00", "description": "Authors: Eugene Wu, Remco Chang, Abigail Mosca, Gabriel Ryan", "uri": "https://vimeo.com/300460421", "name": "InfoVis 2018: At a Glance: Approximate Entropy as a Measure of Line Chart Visualization Complexity", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T08:51:55+00:00", "description": "Authors: Brian David Ondov, Nicole Jardine, Niklas Elmqvist, Steven Franconeri", "uri": "https://vimeo.com/300459877", "name": "InfoVis 2018: Face to Face: Evaluating Visual Comparison", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T08:50:06+00:00", "description": "Authors: Evanthia Dimara, Gilles Bailly, Anastasia Bezerianos, Steven Franconeri", "uri": "https://vimeo.com/300459670", "name": "InfoVis 2018: Mitigating the Attraction Effect with Visualizations", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T08:47:34+00:00", "description": "Authors: Yunhai Wang, Zeyu Wang, Chi-Wing Fu, Hansj\u00f6rg Schmauder, Oliver Deussen, Daniel Weiskopf", "uri": "https://vimeo.com/300459364", "name": "InfoVis 2018: Image-based Aspect Ratio Selection", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T08:42:09+00:00", "description": "Authors: Michael Correll, Mingwei Li, Gordon L Kindlmann, Carlos Scheidegger", "uri": "https://vimeo.com/300458719", "name": "InfoVis 2018: Looks Good To Me: Visualizations As Sanity Checks", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T08:39:14+00:00", "description": "Authors: Yunhai Wang, Xin Chen, Tong Ge, Chen Bao, Michael Sedlmair, Chi-Wing Fu, Oliver Deussen, Baoquan Chen", "uri": "https://vimeo.com/300458395", "name": "InfoVis 2018: Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-13T08:33:38+00:00", "description": "Authors: Karen B. Schloss, Connor C. Gramazio, Allison T. Silverman, Madeline L. Parker, Audrey S. Wang", "uri": "https://vimeo.com/300457748", "name": "InfoVis 2018: Mapping Color to Meaning in Colormap Data Visualizations", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T13:05:15+00:00", "description": "Authors: Ali Sarvghad, Bahador Saket, Alex Endert, Nadir Weibel", "uri": "https://vimeo.com/299870620", "name": "InfoVis 2018: Embedded Merge & Split: Visual Adjustment of Data Grouping", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T13:01:04+00:00", "description": "Authors: Donghao Ren, Bongshin Lee, Matthew Brehmer", "uri": "https://vimeo.com/299869945", "name": "InfoVis 2018: Charticulator: Interactive Construction of Bespoke Chart Layouts", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:58:24+00:00", "description": "Authors: Qianwen Wang, Zhen Li, Siwei Fu, Weiwei Cui, Huamin Qu", "uri": "https://vimeo.com/299869533", "name": "InfoVis 2018: Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:55:38+00:00", "description": "Authors: Tan Tang, Sadia Rubab, Jiewen Lai, Weiwei Cui, Lingyun Yu, Yingcai Wu", "uri": "https://vimeo.com/299869127", "name": "InfoVis 2018: iStoryline: Effective Convergence to Hand-drawn Storylines", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:53:32+00:00", "description": "Authors: Jo Wood, Alexander Kachkaev, Jason Dykes", "uri": "https://vimeo.com/299868841", "name": "InfoVis 2018: Design Exposition with Literate Visualization", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:46:54+00:00", "description": "Authors: Ethan Kerzner, Sarah Goodwin, Jason Dykes, Sara V Jones, Miriah Meyer", "uri": "https://vimeo.com/299867802", "name": "InfoVis 2018: A Framework for Creative Visualization-Opportunities Workshops", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:38:26+00:00", "description": "Authors: Mathieu Le Goc, Charles Perin, Sean Follmer, Jean-Daniel Fekete, Pierre Dragicevic", "uri": "https://vimeo.com/299866532", "name": "InfoVis 2018: Dynamic Composite Data Physicalization Using Wheeled Micro-Robots", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:34:17+00:00", "description": "Authors: Biswaksen Patnaik, Andrea Batch, Niklas Elmqvist", "uri": "https://vimeo.com/299865847", "name": "InfoVis 2018: Information Olfactation: Harnessing Scent to Convey Data", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:30:02+00:00", "description": "Authors: Ronell Sicat, Jiabao Li, JunYoung Choi, Maxime Cordeil, Won-Ki Jeong, Benjamin Bach, Hanspeter Pfister", "uri": "https://vimeo.com/299865218", "name": "InfoVis 2018: DXR: A Toolkit for Building Immersive Data Visualizations", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:18:10+00:00", "description": "Authors: Christophe Hurter, Nathalie Henry Riche, Steven Drucker, Maxime Cordeil, Richard Alligier, Romain Vuillemot", "uri": "https://vimeo.com/299863517", "name": "InfoVis 2018: FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:15:07+00:00", "description": "Authors: Yalong Yang, Tim Dwyer, Bernhard Jenny, Kim Marriott, Maxime Cordeil, Haohui Chen", "uri": "https://vimeo.com/299862988", "name": "InfoVis 2018: Origin-Destination Flow Maps in Immersive Environments", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:10:51+00:00", "description": "Authors: Alper Sarikaya, Michael Correll, Lyn Bartram, Melanie Tory, Danyel A Fisher", "uri": "https://vimeo.com/299862354", "name": "InfoVis 2018: What Do We Talk About When We Talk About Dashboards?", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:08:19+00:00", "description": "Authors: Arjun Srinivasan, Steven Drucker, Alex Endert, John Stasko", "uri": "https://vimeo.com/299862032", "name": "InfoVis 2018: Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:04:14+00:00", "description": "Authors: Sriram Karthik Badam, Zhicheng Liu, Niklas Elmqvist", "uri": "https://vimeo.com/299861380", "name": "InfoVis 2018: Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:01:49+00:00", "description": "Authors: Shusen Liu, Zhimin Li, Tao Li, Vivek Srikumar, Valerio Pascucci, Peer-Timo Bremer", "uri": "https://vimeo.com/299861004", "name": "InfoVis 2018: NLIZE: A Perturbation-Driven Visual Interrogation Tool for Analyzing and Interpreting Natural Language Inference M", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T12:00:21+00:00", "description": "Authors: Daniel Haehn, James Tompkin, Hanspeter Pfister", "uri": "https://vimeo.com/299860837", "name": "InfoVis 2018: Evaluating \u2018Graphical Perception\u2019 with CNNs", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:58:36+00:00", "description": "Authors: Tanja Blascheck, Lonni Besan\u00e7on, Anastasia Bezerianos, Bongshin Lee, Petra Isenberg", "uri": "https://vimeo.com/299860576", "name": "InfoVis 2018: Glanceable Visualization: Studies of Data Comparison Performance on Smartwatches", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:55:13+00:00", "description": "Authors: Matthew Brehmer, Bongshin Lee, Petra Isenberg, Eun Kyoung Choe", "uri": "https://vimeo.com/299859980", "name": "InfoVis 2018: Visualizing Ranges over Time on Mobile Phones: A Task-Based Crowdsourced Evaluation", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:50:30+00:00", "description": "Authors: Ricardo Langner, Ulrike Kister, Raimund Dachselt", "uri": "https://vimeo.com/299859256", "name": "InfoVis 2018: Multiple Coordinated Views at Large Displays for Multiple Users: Empirical Findings on User Behavior, Movements, a", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:48:04+00:00", "description": "Authors: Hariharan Subramonyam and Eytan Adar", "uri": "https://vimeo.com/299858934", "name": "InfoVis 2018: SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:43:42+00:00", "description": "Authors: Sriram Karthik Badam, Andreas Mathisen, Roman R\u00e4dle, Clemens Nylandsted Klokmose, Niklas Elmqvist", "uri": "https://vimeo.com/299858359", "name": "InfoVis 2018: Vistrates: A Component Model for Ubiquitous Analytics", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:39:27+00:00", "description": "Authors: Timothy Major and Rahul C. Basole", "uri": "https://vimeo.com/299857687", "name": "InfoVis 2018: Graphicle: Exploring Units, Networks, and Context in a Blended Visualization Approach", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:36:04+00:00", "description": "Authors: Yunhai Wang, Yanyan Wang, Yinqi Sun, Haifeng Zhang, Chi-Wing Fu, Michael Sedlmair, Baoquan Chen, Oliver Deussen", "uri": "https://vimeo.com/299857124", "name": "InfoVis 2018: Structure-aware Fisheye Views for Efficient Large Graph Exploration", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:31:06+00:00", "description": "Authors: Fangzhou Guo, Wei Chen, Dongming Han, Jiacheng Pan, Xiaotao Nie, Jiazhi Xia, Xiaolong (Luke) Zhang", "uri": "https://vimeo.com/299856392", "name": "InfoVis 2018: Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:24:40+00:00", "description": "Authors: Carolina Nobre, Marc Streit, Alexander Lex", "uri": "https://vimeo.com/299855433", "name": "InfoVis 2018: Juniper: A Tree+Table Approach to Multivariate Graph Visualization", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:21:34+00:00", "description": "Authors: Wiebke K\u00f6pp, Tino Weinkauf", "uri": "https://vimeo.com/299855021", "name": "InfoVis 2018: Temporal Treemaps: Static Visualization of Evolving Trees", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-11-09T11:01:32+00:00", "description": "Authors: Carolina Nobre, Nils Gehlenborg, Hilary Coon, Alexander Lex", "uri": "https://vimeo.com/299852275", "name": "InfoVis 2018: Lineage: Visualizing Multivariate Clinical Data in Genealogy Graphs", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-10-10T00:11:16+00:00", "description": "VIS Arts Program\n\nAuthors: \n\nAbstract:", "uri": "https://vimeo.com/294263139", "name": "[VIS18 Preview] Arts Program Overview (VisAP Overview)", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-10-09T18:07:07+00:00", "description": "Authors: \n\nAbstract:", "uri": "https://vimeo.com/294204792", "name": "[VIS18 Preview] Poster session announcement (Posters cover)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-27T16:44:28+00:00", "description": "Poster \n\nAuthors: Ahmad Karawash, Sara Diamond, Marcus Gordon, Jad Al Rabbaa, Roxolyana Shepko-Hamilton, Lan-Xi Dong, Greice Mariano, Afrooz Samaei, Hugh Ritchie\n\nAbstract: This paper provides an overview of TasteGraph: an interactive data visualization tool for The Globe and Mail, Canadas largest national daily newspaper. The Globe and Mail is undertaking a significant move towards using data analytics tools. TasteGraph enables The Globe and Mail to better understand the psychographic profiles of their audiences, gain holistic knowledge about their readership, see the impact of certain taste correlations, and use these insights to inform advertising strategies.", "uri": "https://vimeo.com/292162295", "name": "[VIS18 Preview] TasteGraph: A Visual Analytics Tool for Profiling Media Audiences Tastes (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-27T16:40:11+00:00", "description": "Poster Honorable Mention\n\nAuthors: Jessica Magallanes-Castaneda, Lindsey van Gemeren, Steven Wood, Maria-Cruz Villa-Uriol \n\nAbstract: Current methods to visually explore event-data focus on discovering sequential patterns but present limitations when studying time attributes in event sequences. Time attributes are especially important when studying waiting times in patient flow analysis. We propose a visual methodology that allows the instant identification of trends and outliers with respect to duration and time of occurrence in event sequences. Moreover, we show how using Multiple Sequence Alignment (MSA) helps deriving conclusions that otherwise could not be easily reached using single event alignment. The proposed visualization has been applied to a dataset provided by Sheffield Teaching Hospitals (STH), for which four classes of conclusions were derived.", "uri": "https://vimeo.com/292161505", "name": "[VIS18 Preview] Visualizing Time in Temporal Event Sequences (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-18T19:57:57+00:00", "description": "(Panel) \n\nOrganizers / Panelists: Carla Schubert, Sheelagh Carpendale, Niklas Elmqvist, G. Elisabeta Marai, John Stasko, Daniel Archambault, Helwig Hauser\n\nAbstract: The academic competitive environment and its \u2018acceptance &amp; rejection\u2019 culture lead to high pressure and stress for researchers at all career stages. We will discuss such threats, coping mechanisms and ways how our community can help. Example topics include: work-life balance, dealing with rejections, providing supportive reviews, funding pressures, career prospects in times of uncertainty, and many more topics depending on the audience. Our panelists include: visualization professors with diverse expertise and seniority levels, and a psychologist/psychotherapist who also conducts research and treats academics with related psychological problems. Voice your first-hand experiences, questions and comments anonymously at http://www.luanamicallef.com/succeedingbyfailing and participate in our follow-up meetup!", "uri": "https://vimeo.com/290565157", "name": "[VIS18 Preview] Succeeding by Failing: The Iceberg in VIS Careers (Panel)", "year": "2018", "event": "PANEL, PREVIEW"}, {"created_time": "2018-09-18T19:57:43+00:00", "description": "(Panel) \n\nOrganizers / Panelists: Lisa Avila, Jeff Heer, Robert Kosara, Anders Ynnerman\n\nAbstract: Visualization is not just an academic pursuit, there are also a number of successful companies that have come out of visualization research or are closely related. This panel brings together some of the founders of these companies to discuss how start a small business based on research and how to sustain and keep building it as a business. We will talk about what were good and bad decisions, what was easier and harder than expected, and be open to questions from anybody who might be considering starting their own (or is just curious).", "uri": "https://vimeo.com/290565113", "name": "[VIS18 Preview] Meet the Founders: How to Start and Sustain a Business in the Visualization Space (Panel)", "year": "2018", "event": "PANEL, PREVIEW"}, {"created_time": "2018-09-18T19:57:31+00:00", "description": "(Panel) \n\nOrganizers / Panelists: Gerik Scheuerman, Roxana Bujack, Karen Schloss, Gael Obein, Lyn Bartram, Francesca Samsel\n\nAbstract: To date most of the design, construction, and evaluation of colormaps and tools have been developed within individual disciplines. Our panelists bring expertise that originates outside of computer science disciplines onto the visualization challenges. Each panelist will introduce their field, expertise, and its potential to contribute to scientific visualization. The panel has been designed to address our broader goals: highlighting a range of expertise beyond computer science; creating language toward common terminology to facilitate dialogue; listening to and understanding the domain sciences needs; and identifying opportunities for collaborative research avenues.", "uri": "https://vimeo.com/290565075", "name": "[VIS18 Preview] Perspectives in Color Research for Scientific Visualization: Understanding the Lenses and Languages (Panel)", "year": "2018", "event": "PANEL, PREVIEW"}, {"created_time": "2018-09-18T19:56:58+00:00", "description": "Authors: Tableau Research, Bosch, think-cell, Bayer,  Fraunhofer IGD\n\nAbstract: Supporter Session featuring our Top 5 supporters this year: Tableau Research, Bosch, think-cell, Bayer and Fraunhofer IGD.", "uri": "https://vimeo.com/290564961", "name": "[VIS18 Preview] Supporter Session (Supporters)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-18T19:56:27+00:00", "description": "Authors: John M Patchett, Thomas Wischgoll\n\nAbstract: Asteroids of various sizes, speeds, and compositions are zipping around the solar system with potential future Earth engagements. Most of the earth is covered in ocean and impacts would likely occur in deep ocean water. The IEEE SciVis Contest 2018 is dedicated to the visualization and analysis of simulations designed to study asteroid impacts in deep ocean water.", "uri": "https://vimeo.com/290564861", "name": "[VIS18 Preview] The 2018 IEEE SciVis Contest (SciVis Contest)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-18T19:55:57+00:00", "description": "Authors: Adam Perer, Rita Borgo, Aashish Chaudhary\n\nAbstract: Meetups\n- VIS Newcomers (Sunday)\n- VISKids Hello (Monday)\n- VISParents (Tuesday)\n- VisLies! (Tuesday)\n- Follow-up meetup to the panel \u2018Succeeding by Failing: The Iceberg in VIS Careers\u2019 (Tuesday)\n- Junior Faculty, Researchers, and Practitioners Happy Hour (Tuesday)\n- Building an Inclusive VIS Community (Thursday)\n- VIS Job-fair (Thursday)\n- Visualization Meets Vision Science (Thursday)\n- Velo Club de Vis (Thursday)\n- Broadening Intellectual Diversity of Visualization Research Papers (Thursday)\n- Good Food Meets Good Science (Thursday)", "uri": "https://vimeo.com/290564774", "name": "[VIS18 Preview] Meetups (Meetup)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:47:21+00:00", "description": "Poster \n\nAuthors: Vladimir Guchev, Paolo Buono, Cristina Gena\n\nAbstract: The analysis of structured complex data, such as clustered graph-based datasets, may use a variety of visual representation techniques and formats. Available tools and approaches to exploratory visualization are built on integrated schemes for simultaneous displaying of multiple aspects of objects and processes. Such schemes partition screen space in multiple views and adopt interaction patterns to focus on relevant items. From a technical point of view, the interpretation of widely known concepts, such as overview-plus-detail and focus-plus-context, is ambiguous. Therefore, their implementation by UI design practitioners needs reviews and a classification of the basic approaches to visual composition of graphical representation modules. We propose a description of components that characterize base, focus, context views, and provides an overview of their multiple combinations.", "uri": "https://vimeo.com/290332099", "name": "[VIS18 Preview] Combining Multiple View Components for Exploratory Visualization (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:47:10+00:00", "description": "Poster \n\nAuthors: Ricardo Langner, Raimund Dachselt\n\nAbstract: We present our work on the use of spatially-aware mobile devices in front of wall-sized displays for data exploration and navigation. The basic idea behind this work is to navigate data sets by walking and moving a mobile device within an interaction space. We describe mappings of different types of information spaces and report on results of a preliminary study regarding layered information spaces. We illustrate the potential of such a new data analysis interface by describing a prototype application that both visualizes traffic data and allows for performing comparison tasks.", "uri": "https://vimeo.com/290332059", "name": "[VIS18 Preview] Towards Visual Data Exploration at Wall-Sized Displays by Combining Physical Navigation with Spatially-Aware...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:46:55+00:00", "description": "Poster \n\nAuthors: Alex Bigelow, Carolina Nobre, Alexander Lex, Miriah Meyer\n\nAbstract: When interpreting data as a graph for visualization, an analyst first assigns semantic meaning to graph concepts. For example, they may choose to represent actors and movies as nodes, and roles as edges. Alternatively, they may wish to represent movies as edges, connecting actor nodes when they collaborate. Data abstraction choices such as these are critical, because different data abstractions can limit\u2014or inspire\u2014different analysis questions, approaches, perspectives, and visualizations. However, current network modeling frameworks, systems, and databases narrowly define graph abstraction constructs\u2014such as nodes, edges, node / edge classes, supernodes, hyperedges, etc.\u2014in terms of how the data is stored in memory or on disk, rather than semantic, human-driven abstractions. Consequently, the ability of an analyst to iterate on a data abstraction becomes fundamentally limited by the implementation details of data wrangling software. We present work-in-progress toward a broader framework for modeling network data that is less dependent on its underlying structure and storage. Our goal is to use semantic data abstraction constructs to inform how algorithms wrangle data, instead of allowing algorithmic concerns to define and constrain the semantics. We also present mure.js, a software library that represents an initial implementation of this framework, allowing users to map semantic graph constructs to arbitrary data structures as metadata, that can, in turn, be used to select, navigate, and reshape the underlying data. Additionally, we discuss an early software prototype of a visual graph wrangling system based on this library.", "uri": "https://vimeo.com/290332009", "name": "[VIS18 Preview] Mure.js: Toward Flexible Authoring and Reshaping of Networks (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:46:45+00:00", "description": "Poster Honorable Mention\n\nAuthors: Hyunjung Jenny Lee, Klaus Mueller \n\nAbstract: High-quality visual feedback plays a decisive role in the analysis of complex multivariate data. We present a framework that uses information abstraction to improve comprehensibility, reduce unnecessary complexity, and communicate data patterns more succinctly. Our framework uses scale-space filtering to create a multi-scale representation of the data, and then employs both data and user-driven illustrative abstraction within a level-of-detail design interface to help users in only visualizing those aspects and detail of the data they deem relevant at the current stage of the analysis.", "uri": "https://vimeo.com/290331973", "name": "[VIS18 Preview] A Scale-Space Filtering Approach for the Multi-Resolution Illustrative Visualization of Multivariate Data...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:46:28+00:00", "description": "Poster \n\nAuthors: Mithileysh Sathiyanarayanan, Cagatay Turkay, Jason Dykes\n\nAbstract: Electronic Discovery (E-discovery) is an investigation domain where electronic data is searched to find information and use it as an evidence in a legal case. One of the investigation areas in this domain is electronic mail (E-mail) communication. Lawyers and analysts involved in this activity are usually presented with a large E-mail dataset to manually comb through information in order to discover key information they need, expending large amounts of time, energy, effort and money in the process. We design and develop an interactive visualisation that will support our collaborators in an organisation specialising in E-discovery to unravel the multi-faceted information in the given communicated E-mails to find/discover pertinence, key information, points of interest (PoIs) and to develop evidence through which legal cases can be built.", "uri": "https://vimeo.com/290331925", "name": "[VIS18 Preview] Visualising E-mail Communication to Improve E-discovery (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:46:19+00:00", "description": "Poster \n\nAuthors: Vanessa Serrano Molinero, Jordi Cuadros, Javier Garcia-Zubia, Unai Hern\u00e1ndez-Jayo, Luis Momp\u00f3 \n\nAbstract: Remote laboratories operate by sending experimental conditions and results, back and forth, from an interface to an experiment setting placed away. A dashboard has been developed to analyze, fast and deep, the logs kept from this communication between the web interface and the real laboratory in the case of VISR, a remote lab for analog electronics. We expect this tool to be central to gain a better understanding on how learning of analogs circuits happens and to increase the use of remote labs in STEM education.", "uri": "https://vimeo.com/290331893", "name": "[VIS18 Preview] Design and Development of a Dashboard for the Visualization and Assessment of Students Work in a Remote Lab...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:46:05+00:00", "description": "Poster \n\nAuthors: Michail Schwab, James Tompkin, Jeff Huang, Michelle A. Borkin\n\nAbstract: The creation of data visualizations has become easier and can be authored by more people as the amount of tools increase that require fewer prerequisite skills. As creating visualizations becomes easier, the use of interactions on these visualizations should be simplified as well. Here, we introduce an open-source pan and zoom library---EasyPZ.js---for the creation of multi-scale visualizations across desktop and mobile devices, with flexible options for interaction design. EasyPZ can be enabled on visualizations without any code, yet is fully customizable and extendable. With many different methods of pan and zoom implemented, it is easy to choose ones compatible with other types of interactions on a visualization, such as scrolling or double clicking. EasyPZ pan and zoom can be enabled on any SVG-based visualization on the web simply by clicking a bookmark, providing visualization creators all the benefits of pan and zoom interactions without requiring commitment to code changes. With this library, we provide ways for the visualization community to easily harness the power of multi-scale visualizations.", "uri": "https://vimeo.com/290331851", "name": "[VIS18 Preview] EasyPZ.js: A Library For Pan and Zoom Visualizations (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:45:54+00:00", "description": "Poster \n\nAuthors: Anamaria Crisan, Jennifer Gardy, Tamara Munzner\n\nAbstract: New developments of data visualization programming libraries and software tools have made it easier for domain experts to visualize data. Yet, domain experts generally do not consult the data visualization literature resulting in ad hoc visualization designs of varying quality. Short of having a data visualization expert attached to many more research projects, which is an unrealistic goal given the small number of data visualization experts, it is necessary to create some vehicle for effectively translating visualization knowledge from the infovis community to domain experts. Here we argue that the best way to translate knowledge is through the systematic generation of explorable design spaces made up of visualizations drawn from the expert\u2019s own domain. We outline a method for systematically generating explorable design spaces and provide a concrete instantiation of this approach through an application to the infectious disease genomic epidemiology domain. We also provide preliminary impressions of domain experts toward our approach and resulting explorable design space, which we have called the GEViT gallery (http://gevit.net). Our findings have implications for both domain experts and infovis researchers engaged in visualization design and analysis.", "uri": "https://vimeo.com/290331808", "name": "[VIS18 Preview] Creating explorable visualization design spaces for domain experts: an example from infectious disease...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:45:43+00:00", "description": "Poster \n\nAuthors: Shah Rukh Humayoun, Dylan Cashman, Florian Heimerl, Remco Chang\n\nAbstract: A recent advancement in the machine learning community is the development of automated machine learning (autoML) technology, such as autoWeka or Googles Cloud AutoML. These systems automate the model selection and tuning process. For the field of visual analytics (VA), autoML can make VA systems more powerful by connecting a wide array of complex learning algorithms to the users data analysis tasks. However, integrating autoML with VA also comes with great challenges. In this paper, we examine the integration of autoML technology with VA systems. Our discussion is based on our experience from an ongoing research project about the integration challenges. We begin by approaching the roles of VA and the use of machine learning algorithms in VA systems from a theoretical standpoint. Based on this, we propose a VA framework that extends upon existing VA frameworks to include the consideration of autoML, which addresses the unique challenges of connecting a VA system to an autoML component.", "uri": "https://vimeo.com/290331776", "name": "[VIS18 Preview] A Visual Analytics Framework for Automated Machine Learning (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:45:32+00:00", "description": "Poster \n\nAuthors: Siming Chen, Jie Li, Gennady Andrienko, Natalia Andrienko, Phong H. Nguyen, Cagatay Turkay\n\nAbstract: Visual analytics usually deals with complex data and uses sophisticated algorithmic, visual, and interactive techniques supporting the analysis. Findings and results of the analysis often need to be communicated to the audience that lacks visual analytics knowledge and skills. This requires analysis outcomes to be presented in simpler ways than that are typically used in visual analytics systems. However, not only analytical visualizations may be too complex for target audience but also the information that needs to be presented. Analysis results may consist of multiple components, which may involve multiple heterogeneous facets. Hence, there exists a gap on the path from obtaining analysis results to communicating them, which involves two aspects: information complexity and display complexity. We address this problem by proposing a general framework in which the analyst employs story synthesis tools to assemble analytical findings and arrange them in various ways based on the information structure, i.e., according to the facets it involves. A specific feature of our work is the use of information structure and inherent properties of its components for organizing story contents. An essential difference of our work from the previous research is in its focus on assembling, organizing, and communicating finding, i.e., data constructs, rather than display states or analysis bookmarks. We demonstrate an implementation of our framework in application to social media data analysis, specifically, exploration of people'", "uri": "https://vimeo.com/290331742", "name": "[VIS18 Preview] Bridging the Gap between Visual Analytics and Storytelling: General Framework and Application to Social...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:45:21+00:00", "description": "Poster \n\nAuthors: Peter W. S. Butcher, Nigel W John, Panagiotis D. Ritsos\n\nAbstract: We present work-in-progress on the design and implementation of a Web framework for building Immersive Analytics (IA) solutions in Virtual Reality (VR). We outline the design of our prototype framework, &lt;", "uri": "https://vimeo.com/290331695", "name": "[VIS18 Preview] Towards a Framework for Immersive Analytics on the Web (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:45:07+00:00", "description": "Poster \n\nAuthors: Katherine Currier, Sheelagh Carpendale\n\nAbstract: We present Arctic Movement, a web-based visualization of the Arctic Institute of North Americas (AINA) photographic archive. This visualization was built to engage novices to explore the collection with relevant context while allowing domain experts from AINA to glean insight into the archives. Arctic Movement uses thumbnails of a photos primary color and moves them around the screen to show the year of photo, the location of photo, relevant expedition name, and content of the photo. With this visualization, we expand on existing work on visualizations of collections to encourage engagement and insight through serendipitous discovery.", "uri": "https://vimeo.com/290331647", "name": "[VIS18 Preview] Arctic Movement: Visualization of a Photographic Collection (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:44:54+00:00", "description": "Poster Honorable Mention\n\nAuthors: Kendra A Wannamaker, Lora Oehlberg, Sheelagh Carpendale, Wesley Willett\n\nAbstract: Our work explores the use of embroidery as a medium for personal data representation. Personal physicalization is at the intersection of personal visualization and physicalization. More precisely, personal physicalization uses tangible objects to represent data in a personal context. This unique design space inspired us to explore alternative mediums to integrate data with personal artifacts, as well as facilitate the exploration of a multi-sensory data encoding. We first developed a workflow for authoring embroidered physicalizations. Then, we used this process to create an embroidered blanket based on text message data.", "uri": "https://vimeo.com/290331608", "name": "[VIS18 Preview] Data Embroidery: Exploring Alternative Mediums for Personal Physicalization (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:44:39+00:00", "description": "Poster \n\nAuthors: John Dingliana, Salaheddin Alakkari\n\nAbstract: We present an image-based volume visualization approach based on Principal Component Analysis (PCA). Using PCA, a learned model is trained using pre-rendered images from spherically distributed viewing angles. The views are encoded into a compressed and more compact representation and then novel views can be synthesized by interpolating scores in the eigenspace. The main advantage of the PCA model is the low computational complexity in the encoding and decoding phases. Furthermore, the image encoding and reconstruction is independent of the rendering complexity. This is particularly important in the case of computationally demanding rendering techniques such as global illumination. Our technique has potential application in client-server volume visualization or where results of a computationally-complex 3D imaging process need to be interactively visualized on a display device of limited specification.", "uri": "https://vimeo.com/290331559", "name": "[VIS18 Preview] A Multi-View Image-Based Volume Visualization Technique (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:44:29+00:00", "description": "Poster \n\nAuthors: Zehua Zeng, Niklas Elmqvist, Catherine Plaisant\n\nAbstract: With the advancement of gene sequencing technology, it has become almost trivial to collect large numbers of microbiome samples from different environments during experiments. However, there exist few effective methods to view, analyze, and compare observations from microbiome data sampled over time. In this work, we use a phylogenetic hierarchy as an organizing principle for this data, and propose a vocabulary of tree editing operations to prune, filter, and aggregate this hierarchy both manually as well as automatically. Our sample implementation uses a treemap for the phylogenetic tree as well as time-series data to show microbiome experiments over time.", "uri": "https://vimeo.com/290331533", "name": "[VIS18 Preview] Manual and Automatic Tree Editing with Applications to Microbiome Time-series Data (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:44:01+00:00", "description": "Poster \n\nAuthors: Subhrajyoti Maji, John Dingliana\n\nAbstract: We propose an approach, called the Equilibrium Distribution Model(EDM), for automatically selecting colors with optimum perceptual contrast for scientific visualization. Given any number of features that need to be emphasized in a visualization task, our approach derives evenly distributed points in the CIELAB color space to assign colors to the features so that the minimum Euclidean Distance among the colors are optimized. Our approach can assign colors with high perceptual contrast even for very high numbers of features, where other color selection methods typically fail. We compare our approach with the widely used Harmonic color selection scheme and demonstrate that while the harmonic scheme can achieve reasonable color contrast for visualizing up to 20 different features, our Equilibrium scheme provides significantly better contrast and achieve perceptible contrast for visualizing even up to 100 unique features.", "uri": "https://vimeo.com/290331448", "name": "[VIS18 Preview] Perceptually Optimized Color Selection for Visualization (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:43:49+00:00", "description": "Poster \n\nAuthors: Halden Lin, Tongshuang Wu\n\nAbstract: Engineering, University of Washington, Seattle, Washington, United States - Kanit Wongsuphasawat Paul G. Allen School of Computer Science &amp;", "uri": "https://vimeo.com/290331414", "name": "[VIS18 Preview] Visualizing Attention in Sequence-to-Sequence Summarization Models (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:43:39+00:00", "description": "Poster \n\nAuthors: Yi Chen, Yue Li, Cheng Lv\n\nAbstract: Tree comparison which related to compare two hierarchically organized datasets both in structure and nodes attributes is still an unsolved problem. A typical visual comparison method is to merge two trees into a union tree and then use treemap to visualize it. This kind of space-filling visualization method can better reflect the differences in node attributes between two trees. However, it is hard to reflect the differences in structure. In this paper, we present a novel visual tree comparison method called Hyper Circular Treemap (HCT) which can support visual analysis on the difference in both structure and nodes attributes. Firstly, a union tree is designed to represent the differences between two compared trees", "uri": "https://vimeo.com/290331374", "name": "[VIS18 Preview] HCT: A Visual Analysis Method for Tree Comparison based on Circular Treemap (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:43:26+00:00", "description": "Poster \n\nAuthors: Micha\u00ebl Aupetit, Mosammat Samiha Sadeka, Ali Sajjad, Luis Gustavo Nonato\n\nAbstract: Multi-Dimensional Projections (MDP) are dimension reduction techniques that transform multivariate data into scatter plots to support visual analysis. Beyond the many MDP techniques and quality measures developed so far to quantify their distortions, researchers in Visual Analytics also explored analytic tasks to describe and categorize the type of tasks which are supported by MDP", "uri": "https://vimeo.com/290331345", "name": "[VIS18 Preview] Multi-Dimensional Projections Explorer: Searching through Techniques, Quality Measures, Analytic Tasks, and...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:43:16+00:00", "description": "Poster \n\nAuthors: James R Jackson, Panagiotis D. Ritsos, Jonathan C Roberts\n\nAbstract: Many modern day tasks involve the use of small screens, where users want to see a summary visualisation of an activity. For example, a runner using a smart watch needs to quickly view their progress, heart rate, comparison to previous races, etc. Subsequently, there is a need to portray data to users in small, yet well-defined, spaces. We define this space to be a single self-contained ``unit'", "uri": "https://vimeo.com/290331312", "name": "[VIS18 Preview] Creating Small Unit Based Glyph Visualisations (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:43:05+00:00", "description": "Poster \n\nAuthors: Tina Huynh, S\u00f8ren Knudsen, Sheelagh Carpendale\n\nAbstract: Visualization systems with multiple views are often used to represent data from different perspectives. There are many advantages in doing so, such as providing a way for users to understand a dataset by revealing its various attributes in separate views and offering additional information that could be more useful than if it was just one graph. However, this can often make data seem complicated and overwhelming as each view remains independent. We present a visualization that utilizes multiple views as a means for interaction and unifying the different data dimensions of a multi-view system. This creates a cohesive visualization that displays different aspects of the data, in this case, hierarchical text data and numerical data. With this personal data visualization, we offer a potential solution for relating additional data structures through interaction while maintaining different data types shown and linking multiple views to form a larger connected graph.", "uri": "https://vimeo.com/290331268", "name": "[VIS18 Preview] Views as Rich Menus for Other Views: A Case Study on Personal Data Visualization (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:42:54+00:00", "description": "Poster \n\nAuthors: Lisa Hynes, Tina Huynh, Sarah Storteboom, Jagoda Walny, Christian Frisson, Doris Kosminsky, Mieka West, Sheelagh Carpendale, Wesley Willett\n\nAbstract: This poster explores a data mapping exercise that we completed to extract a data mapping from an unfamiliar visualization and determine the process we followed in doing so. The exercise was completed using a visualization of imports and exports of energy products to and from Canada. We had no prior knowledge of the data set used. Our process included exploring the visualization, extracting the straightforward data mappings, collaboration, and visual structuring of the extracted mapping. Extracting the data mappings from complex visualizations can be a difficult process. By determining the steps through which a data mapping can be extracted and sharing these findings the community may help to determine the best approach to extracting data mappings from complex visualizations. This exercise contributes to our under-standing of how people extract meaning from the data mapping in a given visualization. This growing understanding can be used to influence design of data exploration and data extracting tools. The results may be applicable towards helping individuals read complex visualizations and obtain a fuller understanding of their data.", "uri": "https://vimeo.com/290331230", "name": "[VIS18 Preview] Discovering the Data Mapping of an Unfamiliar Visualization (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:42:43+00:00", "description": "Poster \n\nAuthors: Paul Parsons, Ya-Hsin Hung, Ali Baigelenov\n\nAbstract: Numerous online resources for supporting visualization design have been developed in recent years. Although many have become popular among practitioners, they have not received systematic analysis in the academic literature. Here we present a preliminary analysis focused on one subset of online practitioner-oriented resources\u2014those that aid in choosing visualization techniques based on a designers communicative intent. We report the results of a comprehensive search for such resources, and discuss \ufb01ndings of an analysis based on multiple characteristics including communicative intent. Finally, we discuss implications and future research directions.", "uri": "https://vimeo.com/290331193", "name": "[VIS18 Preview] Toward an Analysis of Practitioner-Oriented Resources for Visualization Design (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:42:32+00:00", "description": "Poster \n\nAuthors: Abigail Mosca, Shannon Robinson, Meredith Clarke, Rebecca Redelmeier, Sebastian Coates, Dylan Cashman, Remco Chang\n\nAbstract: There is a growing gap between the general public and advanced analytics. In an effort to better understand this gap, and potential ways to overcome it, we performed 14 semi-structured interviews with data scientists who work with clients with little to no expertise in data science. Interview recordings were coded and analyzed, resulting in three major data scientist - client tasks: (1) initialization, (2) communication, and (3) iteration. We find data scientists spend the majority of their time on task 1--defining analysis queries based on broad, high-level client questions and running the appropriate analysis. In this poster, we explore the tactics data scientists employ for this task and potential avenues for automation.", "uri": "https://vimeo.com/290331146", "name": "[VIS18 Preview] Towards Data Science for the Masses: A Study of Data Scientists and their Interactions with Clients (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:42:21+00:00", "description": "Poster \n\nAuthors: zana vosough\n\nAbstract: The big-data revolution has led to a steady accumulation of data in different fields and industries. However, assessing and analyzing this data has become increasingly challenging. Business intelligence applications are an example where the ability to analyze, validate and visualize large and complex multidimensional data plays a crucial role. In this work, we show how a real-world machine learning based product costing validation tool can be augmented with state-of-the-art visualization.", "uri": "https://vimeo.com/290331099", "name": "[VIS18 Preview] Visualizing the Results of Product Costing Plausibility Checks with Parallel Hierarchies (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:42:09+00:00", "description": "Poster \n\nAuthors: Mandy Keck, Dietrich Kammer, Rainer Groh\n\nAbstract: When working with a given high-dimensional data set, data analysts often use different machine-learning algorithms to calculate clusters and classifications. However, it is difficult to ascertain which differences occur across these algorithm versions. For a human-readable visualization, dimensionality reduction methods are used to achieve two-dimensional mappings. In previous work, we proposed to use glyphs to support analysis tasks in these two-dimensional plots. In this paper, we investigate novel glyph-based strategies that can be used for comparison tasks. We present two interface concepts that can be applied on fixed and varying position data to compare different versions of machine-learning algorithms.", "uri": "https://vimeo.com/290331063", "name": "[VIS18 Preview] Visual Version Comparison of Multidimensional Data Sets using Glyphs (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:41:59+00:00", "description": "Poster \n\nAuthors: Carlo Navarra, Tina Neset, Julie Wilk, Alena Bartosova, Ren\u00e9 Capell\n\nAbstract: This poster presents the design and implementation of the web-based visual analytics Bonus Miracle tool. Our aim has been to develop an open-access web-based application for the integrated analysis and communication of environmental modelling, socio-economic assessments, and governance analyses that supports the exploration and analysis of water quality related data.", "uri": "https://vimeo.com/290331029", "name": "[VIS18 Preview] Bonus Miracle Tool: Visual Exploration of Nutrients across the Baltic Sea Region (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:41:45+00:00", "description": "Poster \n\nAuthors: Carmen Hull, Wesley Willett, Sheelagh Carpendale\n\nAbstract: In this poster, we introduce an interactive prototype that integrates site-specific architectural models and tangible displays to compose multiple data representations in the same view. This vision of simultaneous worlds uses physical models as a substrate upon which visualizations of multiple data streams can be dynamically integrated. To explore the potential of this concept, we built a tangible tabletop system that uses scale models of a campus to visualize energy use and climate data. We believe that the metaphor of simultaneous worlds has the ability to unpack novel connections between datasets, supporting embodied exploration, critical thinking, and collaboration.", "uri": "https://vimeo.com/290330981", "name": "[VIS18 Preview] Simultaneous Worlds: Using Physical Models to Contextualize and Compose Visualizations (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:41:27+00:00", "description": "Poster \n\nAuthors: Ashley Suh, Christopher Salgado, Mustafa Hajij, Paul Rosen\n\nAbstract: We present TopoLines, a method for smoothing line charts by leveraging techniques from Topological Data Analysis. Common methods used to smooth line charts, such as rank filters, convolutional filters, frequency domain filters, and subsamples, optimize on various properties of the data -- many of which are not an ideal representation for people. We hypothesize TopoLines to be more visually accurate to the original graph when compared to conventional filtering techniques. Our approach uses a merge tree to capture the most topologically significant events of a scalar function and remove noise with a user-controlled simplification. In this paper, we briefly discuss and compare these frequently used filtering methods and TopoLines.", "uri": "https://vimeo.com/290330925", "name": "[VIS18 Preview] TopoLines: Topological Smoothing for Line Charts (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:41:16+00:00", "description": "Poster \n\nAuthors: Anjul Kumar Tyagi, Ayush Kumar, Anshul Gandhi, Klaus Mueller\n\nAbstract: Analysis of road accidents is crucial to understand the factors involved and their impact. Accidents usually involve multiple variables like time, weather conditions, age of driver etc. and hence it is challenging to analyze the data. To solve this problem, we use Multiple Correspondence Analysis (MCA) to first, filter out the most number of variables which can be visualized effectively in two dimensions and then study the correlations among these variables in a two dimensional scatter plot. Other variables, for which MCA cannot capture ample variance in the projected dimensions, we use hypothesis testing and time series analysis for the study.", "uri": "https://vimeo.com/290330896", "name": "[VIS18 Preview] Road Accidents in the UK (Analysis and Visualization) (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:41:06+00:00", "description": "Poster \n\nAuthors: Tom Horak, Ulrike Kister, Raimund Dachselt\n\nAbstract: In this work, we compared the established web-technologies SVG, Canvas, and WebGL regarding their performance for large visualizations. Specifically, we compared these technologies by analyzing the achievable frames per second (FPS) in exemplary implementations of a tree visualization with increasing number of elements. We found that SVG and Canvas almost perform on par, with performance drops starting at around 10,000 graphical elements, while WebGL performs slightly better when showing text elements and stays almost unaffected by increasing node quantities without text elements. Finally, we discuss additional strategies to improve the performance in certain situations.", "uri": "https://vimeo.com/290330867", "name": "[VIS18 Preview] Comparing Rendering Performance of Common Web Technologies for Large Graphs (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:40:56+00:00", "description": "Poster \n\nAuthors: Hayder Mahdi Al-maneea, Jonathan C Roberts\n\nAbstract: Multiple views is a popular strategy in information visualisation, but for many years researchers have asked questions such as how many views and what layout strategies do people use.\u009d Answering these questions would help developers create suitable multipleview systems, but to date there has been little research into these questions. In this short paper, we present initial results of a larger ongoing study looking at how multiple-views are used. For this study, we built a database of images containing screenshots of visualisation tools from articles presented at IEEE VIS from 2012 to 2017. We select suitable images across TVCG journal, conference, posters and workshop papers. We closely evaluate the layout of 340 images of multiple-view systems and consider the layout topology of each image. Our results show that in the past six years, developers use on average (just over) four views.", "uri": "https://vimeo.com/290330835", "name": "[VIS18 Preview] Study of Multiple View Layout Strategies in Visualisation (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:40:43+00:00", "description": "Poster \n\nAuthors: Edwin de Jonge, Peter-Paul de Wolf\n\nAbstract: Cartographic maps can have great analytic value, but may reveal private data of individuals. We formulate plotting a privacy protected map as a spatial disclosure risk problem, identifying which locations are unsafe to plot. We suggest and apply an adjusted kernel density estimation for continuous and dichotomous variables, that incorporates privacy protection and assess its dependency on the band width. This poster paper contains highlights of [5].", "uri": "https://vimeo.com/290330796", "name": "[VIS18 Preview] Privacy protected thematic maps (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:40:33+00:00", "description": "Poster \n\nAuthors: Erick Cuenca, Arnaud Sallaberry, Dino Ienco, Pascal Poncelet\n\nAbstract: Many real world data can be modeled by a graph with a set of nodes interconnected to each other by multiple relationships. Such a rich graph is called multilayer graph. We present a novel visual platform to query, explore and support the analysis of large multilayer graphs. Our approach provides coordinated views to navigate and explore the large set of retrieved results at different level of granularities. In addition, the proposed system supports the refinement of the query by visual suggestions to guide the user through the exploration process. A demo video is available at https://youtu.be/zray-iR8Hz0.", "uri": "https://vimeo.com/290330761", "name": "[VIS18 Preview] Visual Querying and Exploring of Large Multilayer Graphs (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:40:19+00:00", "description": "Poster \n\nAuthors: N. Philippe Rivi\u00e8re\n\nAbstract: We introduce the Voronoi projection, a novel construction method for polyhedral cartographic projections. The system allows the creation of maps that follow explicit design goals, encoded in the choice of a set of centers connected by an arbitrary spanning tree. From centers following a regular arrangement of points, this construction method can recreate the traditional polyhedral projections such as a cube or dodecahedron. However, as it does not depend on the rigid symmetries of the platonic solids, it continues to offer a solution when these centers are displaced by perturbations or by the addition of more centers. Furthermore, these centers and tree can be derived from arbitrary rules, which we demonstrate by specifying a cost function that prevents the fragmentation of land -- alternatively, ocean. Having specified these rules, we can push the number of faces higher, and observe the emergence of specific shapes of the world, thus giving a visual representation of a geographic computation. We offer a software package that builds these projections and integrates into the D3.js library, allowing in-browser and offline rendering of vector features and raster images, as well as interactivity. Finally, we provide distortion analysis.", "uri": "https://vimeo.com/290330705", "name": "[VIS18 Preview] The Voronoi Projection (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:40:02+00:00", "description": "Poster \n\nAuthors: Florian Spechtenhauser, Rastislav Hronsky, Torsten Moeller, Harald Piringer\n\nAbstract: The analysis of recurring patterns in time series plays a key role in many application domains, for example healthcare monitoring and predictive maintenance. This poster describes ongoing work towards a visual analytics pipeline for a holistic analysis of recurring temporal patterns, including segmentation, alignment, clustering, feature extraction, comparison and downstream tasks such as monitoring. The focus of this poster is on a novel interactive tool that covers parts of this pipeline, i.e., cluster analysis, interactive feature specification and extraction, and comparison of patterns. The work is motivated by the analysis of data from industrial batch production and was designed in close cooperation with domain experts in industrial quality management. The proposed dashboard is a first step towards a holistic visual analytics pipeline which is generally applicable for the analysis of recurring patterns in time series data.", "uri": "https://vimeo.com/290330651", "name": "[VIS18 Preview] Towards a Visual Analytics Pipeline for the Analysis of Recurring Patterns in Time Series Data (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:39:51+00:00", "description": "Poster \n\nAuthors: Marek Skokan, Jan Hreno\n\nAbstract: Proof of concept of ICT solution giving care professionals Visualisation tools for healthcare data was designed and developed. This poster gives overview of work-in-progress on two interactive GUIs for healthcare Information visualisation and healthcare data visualisation that were co-designed and developed by authors.", "uri": "https://vimeo.com/290330618", "name": "[VIS18 Preview] Interactive visual tools supporting effective Health state understanding of patient with comorbidities...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:39:39+00:00", "description": "Poster \n\nAuthors: Mina Alipour, Pierre Dragicevic, Tobias Isenberg, Petra Isenberg\n\nAbstract: In this student project, we explore how situated visualization of office noise can help desk workers become more aware of the noise they produce and are exposed to. More and more people spend a significant amount of time working in offices. Office work has been associated with a number of health issues and, in particular, noise has been identified as a cause of health concerns including the inability to concentrate and elevated stress. We contribute a beginning design study for a situated visualization of office noise. Specifically, we ran a design workshop session with office workers to elicit design requirements. From the workshop we then conducted a larger online survey on noise exposure in office spaces. From the results of the workshop and the survey, we finally derived first design mockups on which we also report here.", "uri": "https://vimeo.com/290330566", "name": "[VIS18 Preview] Situated Visualizations of Office Noise to Promote Personal Health (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:39:25+00:00", "description": "Poster \n\nAuthors: J\u00f6rg St\u00fcwe, Martin R\u00f6hlig, Heidrun Schumann, Ruby Kala Prakasam, Oliver Stachs\n\nAbstract: Optical coherence tomography (OCT) enables high-resolution 3D imaging of intraretinal layers to understand a variety of retinal and systemic disorders. Yet, discovering and relating localized changes of layers in complex OCT datasets is challenging. We propose a combination of systematic data reduction and interactive visualization to support the analysis of abnormal layer thickness. First, we classify and extract regions of abnormal thickness per retinal layer. Second, we compile the regions in a composite map and show their spatial distribution in a novel overview visualization. Third, we integrate coordinated interaction to quickly browse through the data and investigate selected regions in detail. Altogether, our solution enables a fast and targeted analysis of abnormal changes of intraretinal layers for assisting medical diagnoses.", "uri": "https://vimeo.com/290330521", "name": "[VIS18 Preview] Visual Analysis of Abnormal Thickness of Intraretinal Layers (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:39:15+00:00", "description": "Poster \n\nAuthors: Angelos Chatzimparmpas, Rafael M. Martins, Andreas Kerren\n\nAbstract: The use of t-Distributed Stochastic Neighborhood Embedding (t-SNE) for the visualization of multidimensional data has proven to be a popular approach, with applications published in a wide range of domains. Despite their usefulness, t-SNE plots can sometimes be hard to interpret or even misleading, which hurts the trustworthiness of the results. By opening the black box of the algorithm and showing insights into its behavior through visualization, we may learn how to use it in a more effective way. In this work, we present t-viSNE, a visual inspection tool that enables users to explore anomalies and assess the quality of t-SNE results by bringing forward aspects of the algorithm that would normally be lost after the dimensionality reduction process is finished.", "uri": "https://vimeo.com/290330473", "name": "[VIS18 Preview] t-viSNE: A Visual Inspector for the Exploration of t-SNE (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:39:00+00:00", "description": "Poster Honorable Mention\n\nAuthors: Philip Berger, Mohammad Chegini, Heidrun Schumann, Christian Tominski\n\nAbstract: An interesting question one may ask about multivariate graphs is if and how structural characteristics are reflected in multivariate attribute similarities and vice versa. In this work, we propose an integrated visualization of structure S and attribute similarity A in a single overview matrix. The idea is to show S and A in the lower and upper triangular matrix, respectively. Dynamic local rearrangement of matrix cells allows the user to create a side-by-side view of S and A for a detailed inspection of both aspects for a selected node. The approach is applied to a multivariate graph of soccer players.", "uri": "https://vimeo.com/290330426", "name": "[VIS18 Preview] Integrated Visualization of Structure and Attribute Similarity of Multivariate Graphs (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:38:49+00:00", "description": "Poster \n\nAuthors: Johannes Knittel, Steffen Koch, Thomas Ertl\n\nAbstract: Exploring large sets of textual documents to find relevant, new insights is a tedious task, particularly if interesting passages are sparsely scattered throughout the collection. In many cases, it is even not feasible to manually inspect every paragraph, either due to the massive amount of data at hand or when there is a need for quick assessments, e.g. journalistic investigations after leaks. We propose a new visual text analytics approach that relies on character-based Long Short-Term Memory (LSTM) recurrent neural networks to highlight text sections depending on their model probability. Thus, users can quickly dismiss sections that only contain common, redundant information. We trained three different LSTM models on subtitles of TV channels, tweets, and news reports. In this work we present analysis tasks that highlight the applicability and indicate the usefulness of our approach.", "uri": "https://vimeo.com/290330388", "name": "[VIS18 Preview] Highlighting Text Regions of Interest with Character-Based LSTM Recurrent Networks (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:38:35+00:00", "description": "Poster \n\nAuthors: Shridhar B Dandin, BKBIET-Pilani, Mireille Ducasse\n\nAbstract: Database information is multidimensional and often displayed in tabular format (row/column display). A Choropleth map is a thematic map in which areas are colored according to a variable of interest. They are used mostly for compact graphical representation of geographical information. We propose a system, ComVisMD, inspired by choropleth map, to visualize multidimensional data taking sets of 4 dimensions and projecting them on a compact 2D-display. The first dimension uses the attribute of main interest to color areas according to a 5-color scale. The next 2 dimensions define the displayed areas as square cells and give the horizontal and vertical axes. The fourth dimension is displayed in the form of varying-size holes in the cells. We illustrate our approach on cricket players data and show how ComVisMD's compact visualization can help analyze data and find correlations as well as explain the exceptions by the way of intuitive color observation, shape of the cells, information on cell, dynamic scaling, classification and clustering.", "uri": "https://vimeo.com/290330344", "name": "[VIS18 Preview] ComVisMD-A Visualization Tool for Compact Display of Multidimensional Data: An Illustration on Cricket...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:38:24+00:00", "description": "Poster \n\nAuthors: wenjun guo, Seungwook Kim, Seongmin Mun, Kyungwon Lee\n\nAbstract: Temporal event data such as diagnosis records of multiple dementia patients include both multivariate data features and temporal changes. Analyzing temporal event data can reveal changing patterns by time or by correlations between target data and others. It is challenging but to explore visually both of multivariate data features and temporal changes in one view. In this study, we present a novel visualization system named BubbleUp which can explore temporal event databased on machine learning methods to confirm the correlations among the data and visualizes the changing patterns by time. The usage of BubbleUp visualization system can be divided into four steps; Overall distribution, detail view, correlation base on similarity, and prediction. We evaluate the usage and effectiveness of BubbleUp visualization system through the usage scenarios.", "uri": "https://vimeo.com/290330310", "name": "[VIS18 Preview] BubbleUp: Toward better analysis for the temporal event data (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:38:13+00:00", "description": "Poster \n\nAuthors: Lijing Lin, Liwenhan Xie, Zhuo Zhang, Xiaoru Yuan\n\nAbstract: We propose Pixel Matrix Array, a novel visual analytic approach supporting interactive exploration of large dynamic networks with long sequences. In our work, group features of each time step of the dynamic network are automatically extracted according to the network community information, or predefined by users. The feature information of each time step is then mapped onto a pixel matrix while maintains the adjacent relation globally between features. The matrices are then merged and compressed to provide a compact representation of the whole dynamic networks in limited display space while still keep key information visible. Users are able to interact with the information in the network up to a few thousand time steps by splitting or zooming the matrices", "uri": "https://vimeo.com/290330277", "name": "[VIS18 Preview] Visualizing Dynamic Networks of Long Sequences with Pixel Matrix Array (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:38:02+00:00", "description": "Poster \n\nAuthors: Bon Adriel, Tony Tang, Sheelagh Carpendale\n\nAbstract: We conducted a qualitative study pilot to gather requirements for integrating health visualizations at home. We focused on finding dynamics between (1) people: who the visualizations are made for, and others who will see them, (2) visualization: what visual representations people expect for their homes, and (3) location: where people expect to find the visualizations. We describe our study methodology and the results. We found hints of concepts for consideration such as (1) privacy can be derived from visualization style, (2) visualizations can be installed at high traffic locations but data sensitivity should be considered, and (3) location of related tools at home can influence where visualizations should be located.", "uri": "https://vimeo.com/290330245", "name": "[VIS18 Preview] Health Visualizations at Home: Who Sees What Where (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:37:51+00:00", "description": "Poster \n\nAuthors: Bo Kang IDLab, Ghent University, Ghent, Belgium - Dylan Cashman Computer Science, Tufts University, Medford, Massachusetts, United States - Remco Chang Computer Science, Tufts University, Medford, Massachusetts, United States - Jefrey Lijffijt Ghent University, Ghent, Belgium - Tijl De Bie Ghent University , Gent, Belgium -\n\nAbstract: While there are various established methods used for projection, many projections fail to capture phenomena at different scales, due to occlusion or overplotting. A trade-off emerges between showing small and large-scale structure. In this work, we present an algorithm that parameterizes this tradeoff to calculate multiple projections that vary by the scale of the highlighted structure. By jointly optimizing both the information theoretic content of the projection and the clipped bounding region of the resulting view, we can empirically find relevant structure to show to a user. We describe how this method would be useful in a visual analytics system for providing a grand tour for both low- and high-dimensional datasets. By exposing a simple resolution parameter to the user, the user is able to guide their own path through their data, enabling them to glean multiple levels of insight in a way that other static projection techniques could not allow.", "uri": "https://vimeo.com/290330222", "name": "[VIS18 Preview] CLIPPR: Maximally Informative CLIPped PRojections with Bounding Regions (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:37:41+00:00", "description": "Poster \n\nAuthors: Alexander Koch, Tim De Meyer, Jana Jeschke, Wim Van Criekinge, Manon van Engeland\n\nAbstract: The recent growth in the number of publicly available cancer genomics databases has been accompanied by the development of various tools that allow researchers to visually explore these data. In 2015, we built such a tool, MEXPRESS, for the integration and visualization of gene expression, DNA methylation and clinical data from The Cancer Genome Atlas (TCGA), a large collection of publicly available cancer genomics data. The biological role of DNA methylation is tightly linked to its precise genomic location and MEXPRESS is unique in how it displays the two together. Given the large number of users MEXPRESS has managed to attract over the past three years and the recent migration of all TCGA data to a new data portal, we decided to update MEXPRESS with the latest TCGA data, additional data types and extra functionality.", "uri": "https://vimeo.com/290330195", "name": "[VIS18 Preview] MEXPRESS: 2018 Update (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:37:30+00:00", "description": "Poster \n\nAuthors: Martin Baumann, Harutyun Minasyan, Steffen Koch, Thomas Ertl\n\nAbstract: Text annotation data are complex along several dimensions and can be used in a series of demanding tasks. A central problem of a visualization tool dealing with these challenges is: How to navigate within long texts? The zooming-approach laid out here suggests three display modes that allow for a continuous movement between close and distant views upon the annotation data.", "uri": "https://vimeo.com/290330162", "name": "[VIS18 Preview] Zooming on Tokens: Seamless Display Modes for Annotation Analysis (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:37:16+00:00", "description": "Poster \n\nAuthors: Jinwook Bok Department of Computer Science and Engineering, Seoul National University, Seoul, Korea, Republic of - Bohyoung Kim Division of Biomedical Engineering, Hankuk University of Foreign Studies, Yongin-si, Korea, Republic of - Prof. Jinwook Seo Department of Computer Science and Engineering, Seoul National University, Seoul, Korea, Republic of -\n\nAbstract: The original visual encoding of parallel coordinates plot (PCP) backfires as limitations when the number of items and/or attributes increases. Polylines for individual items clutter with each other and the linear ordering of vertical PCP axes makes it difficult to interpret relationship between physically distant attributes. In this paper, we introduce a novel technique that overcomes the innate limitations of PCP by attaching stacked-bar histograms with discrete color schemes to PCP. The color-coded histograms enable users to grasp an overview of the whole data without cluttering or scalability issues. Each rectangle in the histograms is color-coded according to the ranking of data by a user-selected attribute. The color coding scheme allows users to perceptually examine relationships between attributes, even between the ones displayed far apart, without repositioning or reordering axes. We adopt the Visual Information Seeking Mantra so that the polylines of the original PCP can be used to show details of a small number of selected items when the cluttering problem subsides.", "uri": "https://vimeo.com/290330125", "name": "[VIS18 Preview] Scaling up Parallel Coordinate Plot with Color-coded Stacked Histograms (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:37:03+00:00", "description": "Poster \n\nAuthors: Dora Kiesel, Patrick Riehmann, Fan Fan, Yamen Ajjour, Henning Wachsmuth, Benno Stein, Bernd Froehlich\n\nAbstract: Topic modeling algorithms such as Latent Dirichlet Allocation (LDA) typically represent documents as a weighted combination of topics. Therefore, generalized barycentric coordinates are a natural fit for the visualization of a topic space. However, spatial positions in a planar barycentric coordinate system are ambiguous for more than three coordinates. Our glyphs for representing documents in combination with layout guidelines help to reduce the positional ambiguity. With an increasing number of documents, barycentric coordinate embeddings suffer from overplotting and visual clutter like other embeddings, possibly even more so since document positions are fully independent of each other. Our experiments with jittering, aggregating glyphs, and grids show potential to reduce these problems for barycentric and other layouts.", "uri": "https://vimeo.com/290330077", "name": "[VIS18 Preview] Improving Barycentric Embeddings of Topics Spaces (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:36:53+00:00", "description": "Poster \n\nAuthors: Charles Perin\n\nAbstract: Last year I presented a poster showing the past 18 months of my life. I updated it.", "uri": "https://vimeo.com/290330044", "name": "[VIS18 Preview] The Symmetry of My Life II (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:36:39+00:00", "description": "Poster \n\nAuthors: In Kwon Choi, Swati Mishra, Kyle Harris, Nirmal Kumar Raveendranath, Taylor Childers, Khairi Reda\n\nAbstract: Visualizations of data provide a proven method for analysts to explore and make data-driven discoveries. However, current visualization tools provide only limited support for hypothesis-driven analyses, and often lack capabilities that would allow users to visually test the fit of their conceptual models against the data. This imbalance could bias users to overly rely on exploratory visual analysis as the principal mode of inquiry, which can be detrimental to discovery. To address this gap, we propose a new paradigm for `concept-driven' visual analysis. In this style of analysis, analysts share their conceptual models and hypotheses with the system. The system then uses those inputs to drive the generation of visualizations, while providing plots and interactions to explore places where models and data disagree. We discuss key characteristics and design considerations for concept-driven visualizations, and report preliminary findings from a formative study.", "uri": "https://vimeo.com/290330004", "name": "[VIS18 Preview] Towards Concept-Driven Visual Analytics (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:36:27+00:00", "description": "Poster \n\nAuthors: David Borland, David Gotz\n\nAbstract: The display of multivariate data is a common task in data visualization. However as dimensionality increases, it becomes increasingly difficult to visualize all dimensions using standard multivariate visualization techniques, such as parallel coordinates. Dimension reduction is often used to show relationships between data objects using a lower-dimensional representation, but the relationships between data objects and the original dimensions are often unclear. We introduce Dual View, a visualization technique for high-dimensional datasets that directly represents both data objects and data dimensions in separate 2D layouts. Linked views, spatial aggregation, and iterative layout refinement enables the exploration of high-dimensional datasets. We present the underlying algorithms for layout and interaction, a prototype Dual View user interface, and some examples applying Dual View to multidimensional datasets.", "uri": "https://vimeo.com/290329966", "name": "[VIS18 Preview] Dual View: Multivariate Visualization Using Linked Layouts of Objects and Dimensions (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:36:14+00:00", "description": "Poster Best Poster\n\nAuthors: Maxim Spur, Vincent Tourre\n\nAbstract: Urban immersion is an important and recurring step in an urbanist's work. With the increasing availability of data regarding various aspects of a city, this immersion now involves getting a visual overview of said data within the urban context. Challenges arise from the desire to have a broad overview while remaining focused on the immediate surroundings of the area under study. We propose a novel method for visualizing geocoded data in virtual city models that uses stereographic projection of the data to a sphere surrounding the viewer. The data is separated and bent upwards from the ground plane, utilizing otherwise unused visual space above the horizon, creating an overview of more distant data, while keeping the city's geometry and close-by spatial relations intact. Seamless transitions to and from the familiar flat top-down view provide added interactivity and intuitive navigation.", "uri": "https://vimeo.com/290329936", "name": "[VIS18 Preview] Urban DataSphere: Exploring Immersive Multiview Visualizations In Cities (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:35:52+00:00", "description": "Poster \n\nAuthors: Jennifer Rogers, Nicholas Spina, Ashley Neese, Rachel Hess, Darrel Brodke, Alexander Lex\n\nAbstract: Visual cohort analysis utilizing electronic health record (EHR) data has become an important tool in clinical assessment of potential outcomes for patients. In this poster we introduce Composer, a visual analysis tool for Orthopaedic surgeons to analyze and compare the change in physical functions of a patient cohort following various spinal procedures. The goal of our project is to aid physicians and patients in making informed decisions about treatment alternatives by evaluating likely outcomes. In our tool, Composer, analysts can dynamically define a patient cohort using demographic information and events in their medical history and then analyze the physical function scores for the cohort over time. Methods to dynamically align and stratify the temporal data enable analysts to flexibly define relevant reference events and consider various scenarios.", "uri": "https://vimeo.com/290329865", "name": "[VIS18 Preview] Composer: Visual Cohort Analysis of Patient Outcomes (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:35:41+00:00", "description": "Poster Best Poster\n\nAuthors: Johannes Liem, Eirini Goudarouli, Steven Hirschorn, Jo Wood, Charles Perin\n\nAbstract: We introduce GeoBlob, an abstract representation of spatio-temporal data dedicated to conveying uncertain positions and uncertain temporal information of entities that move over time. We describe the preliminary design space of GeoBlobs by exploring variations of the technique to visualize data extracted from hand-written World War One diaries, which is our driving context and motivation for this project.", "uri": "https://vimeo.com/290329831", "name": "[VIS18 Preview] Conveying Uncertainty in Archived War Diaries with GeoBlobs (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:35:24+00:00", "description": "Poster \n\nAuthors: Qing Chen, Zhen Li, Ting-Chuen Pong, Huamin Qu\n\nAbstract: The practical power of data visualization and storytelling is currently attracting much attention in the e-learning domain. A growing number of studies have been conducted in recent years to help instructors better analyze learner behavior and reflect on their teaching. However, current e-learning dashboards and visualization systems usually require a lot of time and effort into the exploration process. Moreover, the lack of communication power of existing systems constrains users from organizing the narrative of information pieces into a compelling data story. In this paper, we have proposed a narrative visualization approach with an interactive slideshow that helps instructors and education experts explore potential learning patterns and convey data stories. This approach contains three key components: guided-tour concept, drill-down path, and dig-in exploration dimension. The case study further demonstrates the potential of employing this visual narrative approach in the e-learning context.", "uri": "https://vimeo.com/290329771", "name": "[VIS18 Preview] Designing Narrative Slideshows for Learning Analytics (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:35:09+00:00", "description": "Poster \n\nAuthors: Hasan Alp Boz, Rafiye Ke\u00e7eci, Yasemin Ya\u015faro\u011flu, Selim Balcisoy\n\nAbstract: This paper presents the results of the experiment that aims to observe the effects of declinism and distinction biases on decision-making process that are introduced to data visualizations. For a given dataset, biased and unbiased data visualizations were created and assigned with survey questions that aim to measure the effect of introduced biases. The responses for the prepared surveys were collected from the vast audience of Amazon Mechanical Turk. The results of the experiments show that declinism bias has a statistically significant effect on the given replies while the effect of distinction bias lacks the statistical significance.", "uri": "https://vimeo.com/290329719", "name": "[VIS18 Preview] Effects of Declinism and Distinction Biases on Data Visualization (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:34:59+00:00", "description": "Poster Best Poster\n\nAuthors: Johannes Novotny, Joshua Tveite, Morgan Turner, Stephen Gatesy, Fritz Drury, Peter L Falkingham, David H. Laidlaw\n\nAbstract: We present a two-year design study of developing virtual reality (VR) flow visualization tools for the analysis of dinosaur track creation using the Scientific Sketching design methodology. We involved 25 art and computer science students from a VR design course in a rapid visualization sketching cycle, guided by paleontologist collaborators through multiple critique sessions. This allowed us to explore a wide range of potential visualization methods and select the most promising methods for actual implementation. We introduce the resulting set of visual metaphors and discuss how the iterative Scientific Sketching process helped to solve visualization problems of our collaborators.", "uri": "https://vimeo.com/290329689", "name": "[VIS18 Preview] Developing Virtual Reality Visualizations of Dinosaur Track Creation with Scientific Sketching (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:34:45+00:00", "description": "Poster \n\nAuthors: Yuri Miyagi Ochanomizu, Daniel Weiskopf, Takayuki Itoh\n\nAbstract: Analysis of eye tracking data is helpful to understand human behavior and mentality. Drawing specific trajectories of eye tracking scan-paths makes us easy to understand such data. However, we may generate too complex visualization results if we simply draw original paths. We propose a technique to analyze eye tracking data and visualize the data as a directed graph. The system applies two types of processes using scan-paths and a result of region division on a stimulus. First, the system converts the scan-paths to run-length codes and divides the codes into shorter pieces. We apply clustering using these pieces to find patterns on the looking. Second, the system generates a directed graph using one scan-path and the region division result. Furthermore, we apply edge bundling to gather edges connected to the common nodes and direction. The graph implies features of the scan-path such as representative gaze points and transitions among areas of interest. As a use case, we analyzed and visualized eye-tracking data of a few participants. The result shows that the system can visualize features of eye tracking scan-paths in combination with a picture of the stimulus.", "uri": "https://vimeo.com/290329651", "name": "[VIS18 Preview] Analysis and Graph Visualization of Eye Tracking Data with a Static Stimulus (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:34:32+00:00", "description": "Poster \n\nAuthors: Kentaro Asai - Tsukasa Fukusato - Takeo Igarashi\n\nAbstract: This paper presents an interactive visualization tool for the analysis of outliers in multi-dimensional data. Each data sample consists of multiple variables (features). For a given outlier data sample, our tool helps the user to identify which variable of the data sample makes it an outlier. Our tool consists of a scatter plot view and variable relation graph view. The user first identifies an outlier data sample in a scatter plot. The user selects the outlier data sample, and the system updates the variable relation graph to visualize relationship between the variables of the outlier data sample. The user then examines the variable relation graph and the scatter plot based on the visualization to identify outlying variables.", "uri": "https://vimeo.com/290329614", "name": "[VIS18 Preview] An Interactive Tool for Feature Analysis of Outliers in Multi-Dimensional Data (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:34:21+00:00", "description": "Poster \n\nAuthors: Fateme Rajabiyazdi, Charles Perin, Lora Oehlberg, Sheelagh Carpendale\n\nAbstract: Patients with chronic conditions are usually advised or are self-motivated to track their health data at home and present this data to the healthcare providers during clinical visits. However, often these patient-generated data collections are large, complex and individual. These characteristics make it challenging and time-consuming for providers to understand this data during short clinical visits. We interviewed four diabetes patients and obtained a sample of their data collections to understand their personal lifestyle and perspectives on the process of tracking, recording, and presenting their data. Based on the information we gathered from patients in our study, we designed various personal visualizations tailored to them.", "uri": "https://vimeo.com/290329578", "name": "[VIS18 Preview] Personal Patient-Generated Data Visualizations for Diabetes Patients (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:34:10+00:00", "description": "Poster \n\nAuthors: Chanhee Park, SUNGJUN DO, eunjeong lee, Hanna Jang, SEUNGCHAN JEONG, hyunwoo han, Kyungwon Lee\n\nAbstract: GitHub Viz is helpful for developers to examine the work of major developers in areas similar to their interests for making decisions [1] [2]. Our Visualization is a web application that can visually explore the rapid trend changes in computer science and data science. Through this application, users can explore other technologies that are relevant to a specific technology, research the key developers through specific development areas and keywords, and view the changes in key developers technology interests over time. To solve these problems, we have developed a visualization application that provides three sub-views with RadViz, which combines several features.", "uri": "https://vimeo.com/290329551", "name": "[VIS18 Preview] GitHub Viz: An Interactive Visualization To Acquire Knowledge from Authoritative Developers (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:33:58+00:00", "description": "Poster Honorable Mention\n\nAuthors: Johannes Waschke, Katja Isabel Paul, Peter Lanzer, Mario Hlawitschka\n\nAbstract: Catheter-based interventions are crucial for diagnostics and therapy in cardiology, neurology and other medical disciplines. Operator skills heavily influence the outcome of these interventions, but there are no standards of teaching and it is unclear how these skills develop over time. Here, we present a visualization that summarizes the guide wire skill of an operator based on a performed intervention. The data is derived from videos of training sessions on a simulator for catheter-based interventions. Requirements on the visualization were formulated in dialogue with an expert. Accordingly, key properties visualized in this work include speed, smoothness, and locations of recurrent mistakes.", "uri": "https://vimeo.com/290329513", "name": "[VIS18 Preview] Evaluation of Guide Wire Proficiency During a Catheter-based Intervention (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:33:48+00:00", "description": "Poster \n\nAuthors: Nam Wook Kim, Hyejin Im,  Nathalie Henry Riche , Krzysztof Z. Gajos, Hanspeter Pfister\n\nAbstract: Georgia Lupi\u2019s manifesto on data humanism is advocating for leveraging data visualizations as catalysts to reflect upon ourselves and engage with others via data that matters in our lives. In this work, we present DataPortraits, an interface that enables people to design a personalized visual vocabulary to represent data. DataPortraits can support both individual and collaborative scenarios, empowering people to gather and represent personal data for self-reflection(e.g., create custom hand-drawn visualization of emotions and daily activities over time) or to generate visual artifacts that are shared in a group to foster engagement and discussion.", "uri": "https://vimeo.com/290329479", "name": "[VIS18 Preview] Fostering Data Humanism with DataPortraits: Empowering People to Create a Personalized Visual Vocabulary...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:33:34+00:00", "description": "Poster \n\nAuthors: Baran Koseoglu, Erdem Kaya, Selim Balcisoy\n\nAbstract: Analysis of event sequences recently gained much attention in both business world and academia. This is in part due to the fact that common practices of inferential and predictive analytics are mainly focused on locating the relationship between features of interest whereas the exploration of causal relationship is of more value, and literature suggests event sequences can facilitate clear understanding of causality. Analysis of event data, in particular those with references to both time and space, could form a challenging task due to the noisy and dirty nature of real world event sequences and the complexity of the understanding of event sequences accompanied with geographical locations. To address these challenges, we propose a visual analytics approach that incorporates temporal sequence pattern extraction and a pattern discovery guidance mechanism operating on geographic query and selection capabilities. We implemented our approach as a visual analytics tool, namely ST Sequence Miner, that enables exploration of time-location space introduced by the input dataset. Preliminary results of our case studies on credit card transaction type sequence analysis unveils that patterns of event sequences can better explain possible causality with proper visualization of time-location data.", "uri": "https://vimeo.com/290329436", "name": "[VIS18 Preview] ST Sequence Miner: Visual Event Sequence Pattern Mining with Spatio-temporal Log Data (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:33:23+00:00", "description": "Poster \n\nAuthors: Mandy Keck, Alexander Maasch, Romy B\u00fcrger, Rainer Groh \n\nAbstract: Conversation training is an e-learning method for the transfer of communication-related skills. With the increasing complexity of these conversations, such as length or structure, combined with the number of learners, the evaluation and analysis of the resulting data set become a challenging task. In this paper, we present a visualization approach based on Sankey diagrams that allows the analysis of complex conversation training data sets.", "uri": "https://vimeo.com/290329406", "name": "[VIS18 Preview] Visualizing Learning Experiences Using Conversation Flows (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:33:11+00:00", "description": "Poster \n\nAuthors: Sven Kluge, Stefan Gladisch, Uwe Freiherr von Lukas, Christian Tominski\n\nAbstract: Immersive analytics is an emerging research area that aims at exploiting the advantages of virtual environments for effective data analysis. Although interactive lenses have proven to be useful tools for supporting a variety of analytic tasks, only little research has addressed the development of lenses in the context of immersive analytics. In this work, we present first results of our work on lenses for immersive data analysis, where our focus is primarily on lens interaction. We implemented our ideas for the Oculus Rift and applied them for supporting the visual analysis of 3D sonar data.", "uri": "https://vimeo.com/290329362", "name": "[VIS18 Preview] Virtual Lenses for Immersive Analytics (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:33:00+00:00", "description": "Poster \n\nAuthors: Wouter Meulemans, Martijn Tennekes\n\nAbstract: Compositional geospatial data can be visualized as dot maps, where the color of each dot represents its class. For interactive dots maps, where it is possible to zoom out in order to see the global picture, it is often needed to aggregate the dots. Hence, we face the following aggregation problem: let M be an input matrix where each cell is assigned a class find an aggregated matrix A in which each cell aligns with k by k cells of M such that A is a good summary of M. We distinguish three dimensions of good summary : class balance, representation and presence. The first is holistic, whereas the other two capture spatial aspects.We propose a simple heuristic algorithm and explore the three quality dimensions with a visualization tool.", "uri": "https://vimeo.com/290329325", "name": "[VIS18 Preview] Assessing Dot-Map Aggregations (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:32:48+00:00", "description": "Poster \n\nAuthors: Martin Ennemoser, Peter Ruch, Holger Stitz, Hendrik Strobelt, Marc Streit\n\nAbstract: In order to monitor the learning process and track model quality, training of a neural network on a classification task is usually accompanied by accuracy and loss curves and the performance of the final model is summarized using a confusion matrix. However, showing the final result only completely disregards the change (flow) of the model confusion across epochs of the learning process. We propose ConfusionFlow, a generalization of the confusion matrix concept over time that enables the user to uncover the learning dynamics of the neural network model. As a first step towards a more informed design process for network architectures and selection of an optimization procedure and its hyperparameters, ConfusionFlow allows for interactive comparative, exploration of model confusion over the networks learning process.", "uri": "https://vimeo.com/290329299", "name": "[VIS18 Preview] ConfusionFlow: Interactive Faceting of Neural Network Performance Curves (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:32:37+00:00", "description": "Poster \n\nAuthors: Bing Wang, Klaus Mueller\n\nAbstract: High-dimensional point cloud projected into a generalized 3D subspace and visualized with the Subspace Voyager at an orientation selected using the trackball. The points have been divided into five clusters tagged by color.(a) Simple scatterplot projection (b) with opacity depth cueing -- points further back are rendered more transparent (c) shaded display after converting the point cloud clusters into a set of solid models using the framework described in this paper.", "uri": "https://vimeo.com/290329263", "name": "[VIS18 Preview] Subspace Shapes: Enhancing High-Dimensional Subspace Structures via Ambient Occlusion Shading (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:32:27+00:00", "description": "Poster Best Poster\n\nAuthors: Aditeya Pandey, Harsh Shukla, Geoffrey S. Young, Lei Qin, Cody Dunne, Michelle A. Borkin\n\nAbstract: Blood circulation to the human brain is provided through a network of cerebral arteries. A blockage or leakage of an artery in this system may be due to diseases such as a stroke or aneurysm. To identify and diagnose these conditions, doctors obtain and examine CTA or MRA radiological images. The doctor's diagnostic tasks include examination of artery branches for abnormalities and identification of paths of abnormal flow from a deformed artery. These tasks are in actuality the network analysis tasks of browsing and path following. In this work, we introduce the definition of the cerebral artery system as a network. This framing allowed us to develop a novel network representation of the cerebrovascular arteries. The layout uses a topology- and constraint-based technique to present the structure intuitively and preserve the spatial context. Experts validated the layout design, and robustness is demonstrated through testing with 56 MRA datasets.", "uri": "https://vimeo.com/290329227", "name": "[VIS18 Preview] CerebroVis: Topology- and Constraint-based Network Layout for the Visualization of Cerebrovascular Arteries...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:32:14+00:00", "description": "Poster \n\nAuthors: Yufei Zhang, David Gotz, David Borland\n\nAbstract: Surveys are a widely used tool for inferring information about a target population by collecting data from a smaller sample. However information is always lost, as the sample cannot in general fully represent the target population. Thus predictions made from surveys may contain biases. To mitigate these biases, sample subgroups can be re-weighted to match their known distributions in the target population. We introduce a web-based interactive tool to visualize the re-weighting process in surveys, with specific application to presidential election polls. A detailed description of the systems user interface and re-weighting algorithm are provided. In addition, the results of a twenty-person user study evaluating the system are presented and discussed.", "uri": "https://vimeo.com/290329194", "name": "[VIS18 Preview] Increasing Understanding of Survey Re-Weighting with Visualization (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:32:03+00:00", "description": "Poster \n\nAuthors: Jin Han, Jingyi Zhu, Ling Ma, Bindu Chib\n\nAbstract: Solving legal problems requires finding and analyzing the relevant legal data efficiently and effectively, such as the most relevant judicial decisions interpreting legislations. We developed an information visualization tool for a unique analysis of Chinese judicial decisions in intellectual property law. The purpose of this tool is to find the most relevant judicial decisions linked to the frequency of citations of any particular legislations in that decision. To render accurate results, the tool utilizes frequency analysis and presents the results in a visual and interactive way. The frequency analysis of any particular statutes in any judicial decision increases the likelihood of that decision being good authority on any statutes being researched. Our approach can be further adapted as a value-added feature on other legal research platforms in China.", "uri": "https://vimeo.com/290329166", "name": "[VIS18 Preview] A Visualization Tool for Intellectual Property Law Research in China (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:31:52+00:00", "description": "Poster \n\nAuthors: Tolga Uslu, Alexander Mehler\n\nAbstract: In this paper we present PolyViz, a new visualization system that can efficiently display a special kind of k-partite graphs with the benefit that the k groups themselves can also have internal links. PolyViz not only allows for the generation of the visualization, but also for the adaptation and the analysis of the underlying data. This was achieved by providing various means of interaction. We illustrate the visualization in the context of two conducted experiments. One of these experiments includes the analysis of the topic distribution of the German Wikipedia and the linkage of these topics. The other experiment is about the visual representation of sentence similarities including their analysis. In any event, PolyViz is not limited to these applications but can be used for visualizing any multipartite data.", "uri": "https://vimeo.com/290329139", "name": "[VIS18 Preview] PolyViz - a Visualization System for Special Kind of Multipartite Graphs (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:31:29+00:00", "description": "Poster \n\nAuthors: Nicolas Belmonte, Yang Wang\n\nAbstract: This year marks the 10-year anniversary of the birth of Myriahedral projection. We take this chance to present an interactive implementation of Myriahedral projection with multiple optimizations. We introduce the quad-edge data structure for storing and navigating between dual graphs for efficient graph cutting. Users can choose from various mesh models combined with different edge weighting strategies to explore new map projections on-the-fly. We also showcase how such interactive tool can help bridge digital prototyping and fabrication.", "uri": "https://vimeo.com/290329075", "name": "[VIS18 Preview] Refolding the Earth: Interactive Myriahedral Projection and Fabrication (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:31:15+00:00", "description": "Poster \n\nAuthors: Adhitya Kamakshidasan\n\nAbstract: There has been an increasing demand from the cricketing community to introduce newer metrics to analyze the game. Data from ball tracking and prediction have urged statisticians to re-look at existing comparative measures. Using methods from topological data analysis, we introduce a new technique to compare cricketers using spatial features. We use data from IPL seasons (2012 -2017) to compare our results with an existing ranking scheme.", "uri": "https://vimeo.com/290329030", "name": "[VIS18 Preview] Spatial Comparison of Cricketers (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:31:04+00:00", "description": "Poster \n\nAuthors: Wouter Meulemans\n\nAbstract: In the context of visualizing spatial data using proportional symbols, the following problem often arises: given a set of overlapping squares of varying sizes, reposition the squares as to remove the overlap while minimizing the displacement of the squares, constrained to maintain the orthogonal order. Though this problem is NP-hard, we show that rotating the squares by 45 degrees into diamonds allows for a linear or convex quadratic program and is thus efficiently solvable even for relatively large instances.", "uri": "https://vimeo.com/290329005", "name": "[VIS18 Preview] On Minimum-Displacement Overlap Removal (Poster)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:30:38+00:00", "description": "Authors: Naoko Sawada, Masanori Nakayama, Makoto Uemura, Prof. Issei Fujishiro\n\nAbstract: Blazars are attractive objects for astronomers to observe in order to demystify the relativistic jet. Astronomers need to classify characteristic temporal variation patterns and correlations of multi-dimensional time-dependent observed blazar datasets. Our visualization scheme, called TimeTubes, allows them to easily explore and analyze such datasets geometrically as a 3D volumetric tube. Even with TimeTubes, however, data analysis over such long-term datasets costs them so much labor and may cause a biased analysis. This paper, therefore, attempts to incorporate into the current prototype of TimeTubes, a new functionality: feature extraction, which supports astronomers\u2019 efficient data analysis by automatically extracting characteristic spatiotemporal subspaces.", "uri": "https://vimeo.com/290328934", "name": "[VIS18 Preview] TimeTubes: Automatic Extraction of Observable Blazar Features from Long-Term, Multi-Dimensional Datasets...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:30:31+00:00", "description": "Authors: Keshav Dasu, Takanori Fujiwara, Kwan-Liu Ma\n\nAbstract: Decisions made by domain experts, such as in healthcare and market research, are influenced by the conditional co-occurrence of different events. Learning about conditional co-occurrence is also beneficial for non-experts--the general public. By understanding the co-occurrences of diseases, it is easier to understand which diseases individuals are susceptible to. However, co-occurrence data is often complex. In order for a public understanding of conditional co-occurrence, there needs to be a simpler form to convey such complex information. We introduce an organic visual metaphor. Our model can provide a summary of the conditional co-occurrences within a large set of items and is accessible to the public with its organic shape. We develop a prototype application offering not only an overview for users to gain insights on how co-occurrence patterns evolve based on user-defined criteria (e.g., how do sex and age affect likelihood), but also functionality to explore the hierarchical data in-depth. We conducted two case studies with this prototype to demonstrate the effectiveness of our design.", "uri": "https://vimeo.com/290328908", "name": "[VIS18 Preview] An Organic Visual Metaphor for Public Understanding of Conditional Co-occurrences (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:30:21+00:00", "description": "Authors: Annie Preston, Kwan-Liu Ma\n\nAbstract: Scientific simulations are yielding increasing amounts of data; to visualize the full output from a simulation, one must first reduce clutter and obstruction. Clustering algorithms are common tools for condensing information and decreasing clutter when analyzing and visualizing simulation output. Often, simulation data have intuitive groupings. In some cases, though, such as merger trees from N-body dark matter simulations, there are limited expectations for clustering results. We investigate cluster-based visualization design for merger tree data, testing whether multidimensional encodings and opening the ``black box'' can allow for meaningful representation and exploration of these data.", "uri": "https://vimeo.com/290328886", "name": "[VIS18 Preview] Cluster-Based Visualization for Merger Tree Data: The Challenge of Missing Expectations (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:30:07+00:00", "description": "Authors: Anke Friederici, Habib Toye, Ibrahim Hoteit, Tino Weinkauf, Holger Theisel, Markus Hadwiger\n\nAbstract: Mesoscale ocean eddies play a major role for both the intermixing of water and the transport of biological mass. This renders their exact shape, location and deformation over time highly interesting for a number of applications. While eddies maintain a roughly circular shape in the free ocean, the narrow basins of the Red Sea and Gulf of Aden lead to the formation of irregular eddy shapes that existing methods struggle to define. We propose the following model: Inside an eddy, particles rotate around a common core and thereby remain at a constant distance in a certain parametrization. The transition to more unpredictable flow on the outside can thus be identified as the eddy boundary. We apply this algorithm on a simulation of the Red Sea, where we are able to identify the shape of irregular eddies robustly and more coherently than previous methods. We visualize the eddies as tubes in space-time to enable the analysis of their movement and deformation over several weeks.", "uri": "https://vimeo.com/290328836", "name": "[VIS18 Preview] A Lagrangian Method for Extracting Eddy Boundaries in the Red Sea and the Gulf of Aden (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:29:58+00:00", "description": "Authors: Rui Li, Jian Chen\n\nAbstract: We report results from a preliminary study exploring the memorability of spatial scientific visualizations, the goal which is to understand the intrinsic visualization features that contribute to memorability. The evaluation metrics include three objective measures (entropy, feature congestion, the number of edges), four subjective ratings (clutter, the number of distinct colors, familiarity, and realism), and two sentiment ratings (interestingness and happiness), all inspired by recent memorability studies in vision science and infographics. By constructing a scientific visualization dataset and conducting an online experiment, we collect memorability scores on Amazon Mechanical Turk (AMT) of 1142 images from the original 2231 images in published IEEE SciVis papers from 2008 to 2017. Results showed that the memorability of scientific visualizations is correlated with color and layout. We further investigate the differences between scientific visualization and infographics as a means to understand memorability differences of data attributes.", "uri": "https://vimeo.com/290328799", "name": "[VIS18 Preview] Toward A Deep Understanding of What Makes a Scientific Visualization Memorable (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:29:47+00:00", "description": "Authors: Thomas Wilde, Christian Roessl, Holger Theisel\n\nAbstract: We present an approach to the extraction of FTLE ridges for 2D unsteady vector fields under long integration times. This is a hard problem because such FTLE ridges tend to be sharp and close to each other. The main feature of our approach is that it does not only use an FTLE sampling at the desired final integration time but incorporates samples from prior integration times as well. With this additional information, the new method produces more and finer ridge lines than previous approaches. Based on this output, we can consider FTLE ridge statistics. We test the approach on synthetic benchmarks and real-world data sets", "uri": "https://vimeo.com/290328753", "name": "[VIS18 Preview] FTLE Ridge Lines for Long Integration Times (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:29:35+00:00", "description": "Authors: Siyuan Lin, Hao Jiang, Lingyun Sun\n\nAbstract: With the rapid progress of quantum computation recently, it attracts more people to learn quantum computation, which differs a lot from classical computation. To help novices learn quantum computation, we interviewed a set of people who are learning quantum computation and found that novices feel confused about 1) how quantum gates contribute to the final results in a complicated quantum circuit and 2) how the final results generate steps along the quantum circuits. Thus, we present QuFlow, an interactive visualization tool for teaching the fundamentals of quantum computation. First, users can build a quantum circuit, then QuFlow will simulate the quantum circuit in a classical computer. After simulation, QuFlow will not only present the final output results of quantum circuits but also shows us how the parameters change along the quantum circuits. To collect users\u2019 feedback, a user study was carried out among target users.", "uri": "https://vimeo.com/290328716", "name": "[VIS18 Preview] QuFlow: Visualizing Parameter Flow in Quantum Circuits for Understanding Quantum Computation (SciVis Short...", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:29:25+00:00", "description": "Authors: Dr. Roxana Bujack, Dr. Terece L Turton, David Rogers, James Ahrens\n\nAbstract: One of the most important properties that inherently defines a good colormap is perceptual order. In the literature, we find a wide range of recommendations and hypotheses regarding order. Properties such as monotonicity in luminance, saturation, or hue are/are not stated as necessary/sufficient to ensure perceptual order. In this paper, we gather the most common statements about perceptual order and, when possible, prove or disprove them.", "uri": "https://vimeo.com/290328692", "name": "[VIS18 Preview] Ordering Perceptions about Perceptual Order (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:29:14+00:00", "description": "Authors: Petra Gospodnetic, Divya Banesh, Philip Wolfram, Mark Petersen, Hans Hagen, James Ahrens, Markus Rauhut\n\nAbstract: When analyzing and interpreting results of an ocean simulation, the prevalent method in oceanography is to visualize the complete dataset. However, this can lead to data being missed or misinterpreted due to the distraction caused by the extraneous data of the simulation. Furthermore, when the data stretches over many layers in depth or over numerous time-steps, the ability to track attributes such as ocean currents becomes difficult due to the complexity of the data. We propose an image processing approach to simulation preprocessing for visualization purposes, which offers automation of ocean current tracking within a simulation and ocean current segmentation from the rest of the simulation data. Using the proposed approach, it is possible to automatically identify the most scientifically-relevant streams, extract them from the rest of the simulation and correlate their behavior with other simulation parameters.", "uri": "https://vimeo.com/290328665", "name": "[VIS18 Preview] Ocean Current Segmentation at Different Depths and Correlation with Temperature in a MPAS-Ocean Simulation...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:29:04+00:00", "description": "Authors: Ayan Biswas, Kelly R. Moran, Earl Lawrence, James Ahrens\n\nAbstract: Visualization of high-fidelity scientific simulations with high-dimensional inputs and outputs is an important task. Existing high-dimensional data visualization approaches generally assume a substantial amount of data is available or can be generated as needed. However, many of these simulations can be very computationally intensive, taking minutes or hours to run. Thus, visualization of such simulations with sparsely sampled data, poses a new challenge. In this work, we propose to use an emulator for the visualization of its corresponding simulation. Emulators are frequently used to approximate simulations for statistical analysis. In this work, we are introducing them for creating effective visualization systems. We choose Gaussian Spatial Process (GaSP) emulators for this purpose as it enables fast and accurate prediction with uncertainty information and using this, we design a system that enables visualization of high-dimensional input and output spaces of complex physics simulations. Using our system, users can get a detailed understanding of the uncertainties associated with the emulator predictions in both input and output space for a high-dimensional simulation.", "uri": "https://vimeo.com/290328642", "name": "[VIS18 Preview] Visualization of Uncertainty for Computationally Intensive Simulations Using High Fidelity Emulators (SciVis...", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:28:54+00:00", "description": "Authors: Max Zeyen, Tobias Post, Hans Hagen, James Ahrens, David Rogers, Dr. Roxana Bujack\n\nAbstract: Color interpolation is critical to many applications across a variety of domains, like color mapping or image processing. Due to the characteristics of the human visual system, color spaces whose distance measure is designed to mimic perceptual color differences tend to be non-Euclidean. In this setting, a generalization of established interpolation schemes is not trivial. This paper presents an approach to generalize linear interpolation to colors for color spaces equipped with an arbitrary non-Euclidean distance measure. It makes use of the fact that in Euclidean spaces, a straight line coincides with the shortest path between two points. Additionally, we provide an interactive implementation of our method for the CIELAB color space using the CIEDE2000 distance measure integrated into VTK and ParaView.", "uri": "https://vimeo.com/290328621", "name": "[VIS18 Preview] Color Interpolation for non-Euclidean Color Spaces (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:28:38+00:00", "description": "Authors: Xiangyang He, Yubo Tao, Qirui Wang, Hai Lin\n\nAbstract: This paper proposes a co-analysis framework based on biclusters, i.e., two subsets of variables and voxels with close scalar-value relationships, to guide the visual exploration process of multivariate data. We first automatically extract all meaningful biclusters, each of which only contains voxels with a similar scalar-value pattern over a subset of variables. These biclusters are organized according to their variable sets, and further grouped by a similarity metric to reduce redundancy and encourage diversity during visual exploration. Biclusters are visually represented in coordinated views to facilitate interactive exploration of multivariate data from the similarity between biclusters and the correlation of scalar values with different variables. Experiments demonstrate the effectiveness of our framework in exploring local relationships among variables, biclusters and scalar values in the data.", "uri": "https://vimeo.com/290328582", "name": "[VIS18 Preview] Biclusters based Visual Exploration of Multivariate Scientific Data (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:28:26+00:00", "description": "Authors: Mr Naif Alharbi, Michael Krone, Matthieu Chavent, Robert S. Laramee\n\nAbstract: Molecular visualization of molecular dynamics data commonly uses representative surfaces of varying resolution to depict protein molecules while a variety of geometric shapes, from lines to spheres, are often used to represent atoms and inter-atomic bonds. In general, the aim of molecular visualization is focused on efficiently rendering atoms or molecules themselves, while the interaction space between them is less explored. Furthermore, with naive approaches rendering every molecule, the particles overlap so that significant interaction details are obscured due to clutter. Contrary to common approaches, our work focuses on the interaction between lipids and proteins and the area in which this occurs, the - lipid - membrane. To do so, we introduce a novel abstract interaction space for Protein-Lipid Interaction (PLI). The cylindrical abstraction simplifies perception and computation of PLI. It does so by using a local projection of PLI onto a cylindrical geometry. This also addresses the challenge of visualizing complex, time-dependent interactions between these molecules. We propose a fast GPU-based implementation that maps lipid-constituents involved in the interaction onto the abstract protein interaction space. The tool provides interactive rendering of PLI for 336,260 particles over almost 2,000 time-steps. The result is a great simplification of this complex Protein-Lipid Interaction leading to both better perception and new insight for computational biologists.", "uri": "https://vimeo.com/290328550", "name": "[VIS18 Preview] VAPLI: Novel Visual Abstraction for Protein-Lipid Interactions (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:28:04+00:00", "description": "Authors: David A. B. Hyde, Tyler R. Hall, Jef Caers\n\nAbstract: The rapid onset of inexpensive, portable virtual reality (VR) devices has created opportunities for scientific visualization tools that harness this new, immersive modality. Researchers in the geological sciences, in particular those focused on earth resources (energy, water, minerals), are faced with significant challenges in building and understanding increasingly complex geological models. In this paper, we address these joint opportunities by introducing the Virtual Reality Geomodeling Environment (VRGE): a scientific visualization tool leveraging the Oculus Rift VR system, specialized for users involved in geological modeling. VRGE offers a number of features for viewing and interacting with geological models in VR, including human-centric navigation and manipulation, implicit surface editing, visual conditioning, and uncertainty analysis. Moreover, we examine how the design of VRGE meets current needs of the earth resources industry, in the context of reviewing the state-of-the-art, conducting an expert survey, and discussing performance.", "uri": "https://vimeo.com/290328472", "name": "[VIS18 Preview] VRGE: An Immersive Visualization Application for the Geosciences (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:27:43+00:00", "description": "Authors: Roberta Cabral Ramos Mota, Allan Rocha, Dr Julio Daniel Silva, Usman Raza Alim, Ehud Sharlin\n\nAbstract: We present 3De lens, a technique for focus+context visualization of multi-geometry data. It fuses two categories of lenses (3D and Decal) to become a versatile lens for seamlessly working on multiple geometric representations that commonly coexist in 3D visualizations. In addition, we incorporate our lens into virtual reality as it enables a natural style of direct spatial manipulation for exploratory 3D data analysis. To demonstrate its potential use, we discuss two domain examples in which our lens technique creates customized visualizations of both surfaces and streamlines.", "uri": "https://vimeo.com/290328419", "name": "[VIS18 Preview] 3De Interactive Lenses for Visualization in Virtual Environments (SciVis Short Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:27:17+00:00", "description": "Authors: Malik Olivier Boussejra, Dr. Kazuya Matsubayashi, Yuriko Takeshima, Shunya Takekawa, Rikuo Uchiki, Makoto Uemura, Prof. Issei Fujishiro\n\nAbstract: With the improvements of telescopes and proliferation of sky surveys there is always more astrophysical data to analyze, but not so many astronomers. We present aflak, a visualization environment to open astronomical datasets and analyze them. This paper\u2019s contribution lies in that we leverage visual programming techniques to conduct fine-grained, astronomical transformations, filtering and visual analyses on multi-spectral datasets with the possibility for", "uri": "https://vimeo.com/290328343", "name": "[VIS18 Preview] aflak: Pluggable Visual Programming Environment with Quick Feedback Loop Tuned for Astrophysical...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:26:47+00:00", "description": "Authors: Duong Hoang, Pavol Klacansky, Harsh Bhatia, Peer-Timo Bremer, Peter Lindstrom, Valerio Pascucci\n\nAbstract: There currently exist two dominant strategies to reduce data sizes in analysis and visualization: reducing the precision of the data, e.g., through compression, or reducing its resolution, e.g., by subsampling. Both have advantages and disadvantages and both face fundamental limits at which the reduced information ceases to be useful. The paper explores the additional gains that could be achieved by combining both strategies. In particular, we present a common framework that allows us to study the trade-off in reducing precision and/or resolution in a principled manner. We represent data reduction schemes as a progressive stream of bits, and study how various bit orderings such as by resolution, by precision, etc. impact the resulting approximation error across a wide range of test data and analysis tasks. Furthermore, we compute streams optimized for different tasks, to serve as lower bounds on the achievable error. Scientific data management systems can use the results presented in this paper as guidance on how to store and stream data to make efficient use of the limited storage and bandwidth in practice.", "uri": "https://vimeo.com/290328254", "name": "[VIS18 Preview] A Study of the Tradeoff between Reduced Precision and Resolution for Scientific Data Analysis and...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:26:36+00:00", "description": "Authors: Saeed Boorboor, Shreeraj Jadhav, Mala Ananth, David Talmage, Lorna W Role, Arie Kaufman\n\nAbstract: Wide-field microscopes are commonly used in neurobiology for experimental studies of brain samples. Available visualization tools are limited to electron, two-photon, and confocal microscopy datasets, and current volume rendering techniques do not yield effective results when used with wide-field data. We present a framework for the visualization of neuronal structures in wide-field microscopy images of brain samples. We introduce a novel image gradient transform that overcomes the out-of-focus blur caused due to the inherent design of wide-field microscopes. This is followed by the extraction of the 3D structure of axons and dendrites using a multi-scale curvilinear filter. The response from this filter is then applied as an opacity map to the raw data. Based on the visualization challenges faced by domain experts, our framework provides multiple rendering modes to enable qualitative analysis of neuronal structures, which includes separation of cell-bodies from neurons and dendrites, and an approximate classification of structures as strong or weak based on intensity strength in the raw images caused by experimental limitations. Additionally, we evaluate our rendering results against the standard image processing deconvolution technique to show that our method is significantly faster, and requires less computational resources, while producing comparable results. Finally, we provide feedback and qualitative evaluation of our framework by expert neurobiologists.", "uri": "https://vimeo.com/290328215", "name": "[VIS18 Preview] Visualization of neuronal structures in wide-field microscopy brain images (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:26:03+00:00", "description": "Authors: David Duran Rosich, Pedro Hermosilla Casajus, Timo Ropinski, Barbora Kozlikova, \u00c0lvar Vinacua, Pere-Pau V\u00e1zquez\n\nAbstract: The analysis of protein-ligand interactions is a time-intensive task. Researchers have to analyze multiple physico-chemical properties of the protein at once and combine them to make the correct conclusion about the protein-ligand interactivity. Typically, several charts are inspected, and 3D animations can be played side-by-side to provide deeper understanding on the data. With the improvement of simulation techniques, larger and larger datasets are available, with up to hundreds of thousands of steps. Such large trajectories are very difficult to investigate with traditional approaches. Therefore, the need for special tools that facilitate inspection of these large trajectories becomes substantial. In this paper, we present a novel system for visual exploration of very large trajectories in an interactive and user-friendly way. Several visualization motifs are automatically derived from the data to give the user the information about interactions between protein and ligand. Our system offers actionable widgets to ease and accelerate data inspection and navigation to interesting parts of the simulation. The system is suitable also for simulations where several ligands are interacting with the protein. We have tested the usefulness of our tool on a set of datasets obtained from protein engineers.", "uri": "https://vimeo.com/290328133", "name": "[VIS18 Preview] Visualization of large molecular trajectories (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:25:53+00:00", "description": "Authors: Feng Wang, Ingo Wald, Qi Wu, Will Usher, Chris R. Johnson\n\nAbstract: Adaptive mesh refinement (AMR) is a key technology for large-scale simulations that allows for adaptively changing the simulation mesh resolution, resulting in significant computational and storage savings. However, visualizing such AMR data poses a significant challenge due to the difficulties introduced by the hierarchical representation when reconstructing continuous field values. In this paper, we detail a comprehensive solution for interactive isosurface rendering of Block-Structured AMR data. We contribute a novel reconstruction strategy\u2014the octant method\u2014which is continuous, adaptive and simple to implement. Furthermore, we present a generally applicable hybrid implicit isosurface ray-tracing method, which provides better rendering quality and performance than the built-in sampling-based approach in OSPRay. Finally, we integrate our octant method and hybrid isosurface geometry into OSPRay as a module, providing the ability to create high-quality interactive visualizations combining the BS-AMR volumes and isosurfaces data. We evaluate the rendering performance, memory consumption and quality of our method on two gigascale Block-Structured AMR datasets.", "uri": "https://vimeo.com/290328099", "name": "[VIS18 Preview] CPU Iso-surface Ray Tracing of Adaptive Mesh Refinement Data (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:25:40+00:00", "description": "Authors: Timothy Basil Luciani, Andrew T Burks, Cassiano Sugiyama, Jonathan Komperda, Liz G.Elisabeta Marai\n\nAbstract: Visualization research often seeks designs that first establish an overview of the data, in accordance to the information seeking mantra: ``Overview first, zoom and filter, then details on demand ''. However, in computational fluid dynamics (CFD) there are many situations where such a spatial overview is not relevant or practical for users, for example when the experts already have a good mental overview of the data, or when an analysis of a large overall structure may not be related to the specific, information-driven tasks of users. Using scientific workflow theory and, as a vehicle, the problem of viscous finger evolution, we advocate an alternative model that allows users to explore features of interest first, then explore the context around those features, and finally move to a potentially unfamiliar overview, for example an abstracted diagram of a simulation ensemble. In a model instantiation, we show how a computational back-end can identify and track over time low-level, small features, then be used to filter the context of those features while controlling the complexity of the visualization, and finally to summarize and compare simulations. We demonstrate the effectiveness of this approach with an online web-based exploration of a total volume of data approaching half a billion seven-dimensional data points, and report supportive feedback provided by domain experts.", "uri": "https://vimeo.com/290328058", "name": "[VIS18 Preview] Details-First, Show Context, Overview Last: Supporting Exploration of Viscous Fingers in Large-Scale...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:25:30+00:00", "description": "Authors: Attila Gyulassy, Peer-Timo Bremer, Valerio Pascucci\n\nAbstract: Topological techniques have proven to be a powerful tool in the analysis and visualization of large-scale scientific data. In particular, the Morse-Smale complex and its various components provide a rich framework for robust feature definition and computation. Consequently there now exist a number of approaches to compute Morse-Smale complexes for large-scale data in parallel. However, existing technique are based on discrete concepts which produce the correct topological structure but are known to construct geometry highly biased by the underlying mesh.  Here, we present a new approach that combines parallel streamline computation with combinatorial methods to construct a high-quality discrete Morse-Smale complex. In addition to being invariant to the orientation of the underlying grid, this algorithm allows a user to tailor the computation to specifically build Morse-Smale complexes with specific features selected for high-quality geometry. This approach computes Morse-Smale complexes for larger data than previously feasible with significant speedups. We demonstrate and validate our approach using several examples from a variety of different scientific domains, and evaluate the performance of our method.", "uri": "https://vimeo.com/290328033", "name": "[VIS18 Preview] Shared Memory Parallel Morse-Smale Complexes with On-Demand Accuracy (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:25:02+00:00", "description": "Authors: Johanna Beyer, Haneen Mohammed, Marco Agus, Ali K. Al-Awami, Hanspeter Pfister, Markus Hadwiger\n\nAbstract: With the rapid increase in raw volume data sizes, such as terabyte-sized microscopy volumes, the corresponding segmentation label volumes have become extremely large as well. We focus on integer label data, whose efficient representation in memory, as well as fast random data access, pose an even greater challenge than the raw image data. Often, it is crucial to be able to rapidly identify which segments are located where, whether for empty space skipping for fast rendering, or for spatial proximity queries. We refer to this process as culling. In order to enable efficient culling of millions of labeled segments, we present a novel hybrid approach that combines deterministic and probabilistic representations of label data in a data-adaptive hierarchical data structure that we call the label list tree. In each node, we adaptively encode label data using either a probabilistic constant-time access representation for fast conservative culling, or a deterministic logarithmic-time access representation for exact queries. We choose the best data structures for representing the labels of each spatial region while building the label list tree. At run time, we further employ a novel query-adaptive culling strategy. While filtering a query down the tree, we prune it successively, and in each node adaptively select the representation that is best suited for evaluating the pruned query, depending on its size. We show an analysis of the efficiency of our approach with several large data sets from connectomics, including a brain scan with more than 16 million labeled segments, and compare our method to conventional culling approaches. Our approach achieves significant reductions in storage size as well as faster query times.", "uri": "https://vimeo.com/290327965", "name": "[VIS18 Preview] Culling for Extreme-Scale Segmentation Volumes: A Hybrid Deterministic and Probabilistic Approach (SciVis...", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:24:42+00:00", "description": "Authors: Felix Raith, Christian Blecha, Thomas Nagel, Francesco Parisio, Olaf Kolditz, Fabian G\u00fcnther, Markus Stommel, Gerik Scheuermann\n\nAbstract: Scientific visualization developed successful methods for scalar and vector fields. For tensor fields, however, effective, interactive visualizations are still missing despite progress over the last decades. We present a general approach for the generation of separating surfaces in symmetric, second-order, three-dimensional tensor fields. These surfaces are defined as fiber surfaces of the invariant space, i.e. as preimages of surfaces in the range of a complete set of invariants. This approach leads to a generalization of the fiber surface algorithm by Klacansky et al. [16] to three dimensions in the range. This is due to the fact that the invariant space is three dimensional for symmetric tensors over a spatial domain. We present an algorithm for surface construction for simplicial grids in the domain and simplicial surfaces in the invariant space. We demonstrate our approach by applying it to stress fields from component design in mechanical engineering.", "uri": "https://vimeo.com/290327914", "name": "[VIS18 Preview] Tensor Field Visualization using Fiber Surfaces of Invariant Space (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:24:15+00:00", "description": "Authors: Michael Ankele, Thomas Schultz\n\nAbstract: DT-MRI streamsurfaces, defined as surfaces that are everywhere tangential to the major and medium eigenvector fields, have been proposed as a tool for visualizing regions of predominantly planar behavior in diffusion tensor MRI. Even though it has long been known that their construction assumes that the involved eigenvector fields satisfy an integrability condition, it has never been tested systematically whether this condition is met in real-world data. We introduce a suitable and efficiently computable test to the visualization literature, demonstrate that it can be used to distinguish integrable from nonintegrable configurations in simulations, and apply it to whole-brain datasets of 15 healthy subjects. We conclude that streamsurface integrability is approximately satisfied in a substantial part of the brain, but not everywhere, including some regions of planarity. As a consequence, algorithms for streamsurface extraction should explicitly test local integrability. Finally, we propose a novel patch-based approch to streamsurface visualization that reduces visual artifacts, and is shown to more fully sample the extent of streamsurfaces.", "uri": "https://vimeo.com/290327825", "name": "[VIS18 Preview] DT-MRI Streamsurfaces Revisited (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:23:31+00:00", "description": "Authors: Tobias G\u00fcnther, Holger Theisel\n\nAbstract: Vortices are one of the most-frequently studied phenomena in fluid flows. The center of the rotating motion is called the vortex coreline and its successful detection strongly depends on the choice of the reference frame. The optimal frame moves with the center of the vortex, which incidentally makes the observed fluid flow steady and thus standard vortex coreline extractors such as Sujudi-Haimes become applicable. Recently, an objective optimization framework was proposed that determines a near-steady reference frame for tracer particles. In this paper, we extend this technique to the detection of vortex corelines of inertial particles. An inertial particle is a finite-sized object that is carried by a fluid flow. In contrast to the usual tracer particles, they do not move tangentially with the flow, since they are subject to gravity and exhibit mass-dependent inertia. Their particle state is determined by their position and own velocity, which makes the search for the optimal frame a high-dimensional problem. We demonstrate in this paper that the objective detection of an inertial vortex coreline can be reduced in 2D to a critical point search in 2D. For 3D flows, however, the vortex coreline criterion remains a parallel vectors condition in 6D. We propose a recursive subdivision approach to detect the vortex corelines that is tailored to the underlying structure of the 6D vectors. The resulting algorithm is objective, and we demonstrate the vortex coreline extraction in a number of 2D and 3D vector fields.", "uri": "https://vimeo.com/290327685", "name": "[VIS18 Preview] Objective Vortex Corelines of Finite-sized Objects in Fluid Flows (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:23:09+00:00", "description": "Authors: Min Shih, Charles Rozhon, Kwan-Liu Ma\n\nAbstract: This paper presents a declarative grammar for conveniently and effectively specifying advanced volume visualizations. Existing methods for creating volume visualizations either lack the flexibility to specify sophisticated visualizations or are difficult to use for those unfamiliar with volume rendering implementation and parameterization. Our design provides the ability to quickly create expressive visualizations without knowledge of the volume rendering implementation. It attempts to capture aspects of those difficult but powerful methods while remaining flexible and easy to use. As a proof of concept, our current implementation of the grammar allows users to combine multiple data variables in various ways and define transfer functions for diverse input data. The grammar also has the ability to describe advanced shading effects and create animations. We demonstrate the power and flexibility of our approach using multiple practical volume visualizations.", "uri": "https://vimeo.com/290327611", "name": "[VIS18 Preview] A Declarative Grammar of Flexible Volume Visualization Pipelines (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:22:44+00:00", "description": "Authors: Kaoji Cotrik Xu, Guoning Chen\n\nAbstract: Understanding hexahedral (hex-) mesh structures is important for a number of hex-mesh generation and optimization tasks. However, due to various configurations of the singularities in a valid pure hex-mesh, the structure (or base complex) of the mesh can be arbitrarily complex. There requires an effective way to characterize this complexity. In this work, we present an efficient tool to help meshing practitioners understand the possible configurations in a valid base complex. Our tool adopts the conventional visual analytic paradigm, that is, we first process the input valid pure hex-mesh and extract multi-level structure information, which is used to aid the subsequent visual exploration. In particular, we propose a strategy to decompose the complex hex-mesh structure into multi-level sub-structures so that they can be studied separately, from which we identify a small set of the sub-structures that can most efficiently represent the whole mesh structure. Furthermore, from this set of sub-structures, we attempt to define the first metric for the quantification of the complexity of hex-mesh structure. To aid the exploration of the extracted multi-level structure information, we devise a visual exploration system coupled with a matrix view to help alleviate the common challenge (e.g., clutter and occlusion) of 3D data exploration. We have applied our tool and metric to a large number of hex-meshes generated with different approaches to reveal different characteristics of these methods in terms of the mesh structures they can produce. We also use our metric to assess the existing structure simplification techniques in terms of their effectiveness.", "uri": "https://vimeo.com/290327525", "name": "[VIS18 Preview] Hexahedral Mesh Structure Visualization and Evaluation (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:22:14+00:00", "description": "Authors: Markus Hadwiger, Matej Mlejnek, Thomas Theussl, Peter Rautek\n\nAbstract: Flow fields are usually visualized relative to a global observer, i.e., a single frame of reference. However, often no global frame can depict all flow features equally well. Likewise, objective criteria for detecting features such as vortices often use either a global reference frame, or compute a separate frame for each point in space and time. We propose the first general framework that enables choosing a smooth trade-off between these two extremes. Using global optimization to minimize specific differential geometric properties, we compute a time-dependent observer velocity field that describes the motion of a continuous field of observers adapted to the input flow. This requires developing the novel notion of an observed time derivative. While individual observers are restricted to rigid motions, overall we compute an approximate Killing field, corresponding to almost-rigid motion. This enables continuous transitions between different observers. Instead of focusing only on flow features, we furthermore develop a novel general notion of visualizing how all observers jointly perceive the input field. This in fact requires introducing the concept of observation time, with respect to which a visualization is computed. We develop the corresponding notions of observed stream, path, streak, and time lines. For efficiency, these characteristic curves can be computed using standard approaches, by first transforming the input field accordingly. Finally, we prove that the input flow perceived by the observer field is objective. This makes derived flow features, such as vortices, objective as well.", "uri": "https://vimeo.com/290327432", "name": "[VIS18 Preview] Time-Dependent Flow seen through Approximate Observer Killing Fields (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:14:12+00:00", "description": "Authors: Marzieh Berenjkoub, Rodolfo Ostilla Monico, Robert S. Laramee, Guoning Chen\n\nAbstract: Despite significant advances in the analysis and visualization of unsteady flow, the interpretation of it's behavior still remains a challenge. In particular, one unsolved problem is the understanding of pairwise relations among different physical properties and how interpreting them together can help us understand their time-varying behavior from a new perspective. In this work, we focus on the linear correlation and non-linear dependency of different well-known physical attributes. Specifically, we extend the existing spatial correlation quantification method, i.e. the Local Correlation Coefficient (LCC), to the spatio-temporal domain and apply it to the study of the correlation of attribute-pairs from both the Eulerian and Lagrangian views. To study the dependency among attributes, which need not be linear, we extend and compute the mutual information (MI) among attributes over time. Furthermore, we adapt the concepts of joint probability and conditional probability to study the dependency among attributes. To help visualize and interpret the derived correlation and dependency among attributes associated with a particle (Lagrangian) or at a fixed location (Eulerian), we encode the correlation and dependency values on individual pathlines and construct a correlation glyph. Finally, we rank the strength of the relations among attributes, based on their pairwise correlation and dependency metrics, which yields a novel segmentation of the flow domain and particles. We have applied our correlation and dependency metrics to a number of 2D and 3D unsteady flows with varying spatio-temporal kernel size to demonstrate and assess their effectiveness.", "uri": "https://vimeo.com/290325964", "name": "[VIS18 Preview] Visual Analysis of Spatio-temporal Relations of Pairwise Attributes in Unsteady Flow (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:14:00+00:00", "description": "Authors: Martin Falk, Anders Ynnerman, Darren Treanor, Claes Lundstr\u00f6m\n\nAbstract: We present a visualization application that enables the effective interactive visual analysis of large scale 3D histopathology, that is, high resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large scale, in the order of 100,000 x 100,000 x 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user study employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.", "uri": "https://vimeo.com/290325921", "name": "[VIS18 Preview] Interactive Visualization of 3D Histopathology in Native Resolution (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:13:40+00:00", "description": "Authors: Bo Ma, Alireza Entezari\n\nAbstract: Numerical Weather Prediction (NWP) ensembles are commonly used to assess the uncertainty and confidence in weather forecasts. Spaghetti plots, while visualize isocontours of ensemble members simultaneously, are conventional tools for meteorologists to directly examine the uncertainty exhibited by the ensemble members. To avoid visual clutter in practical usages, one needs to select a small number of informative isovalues for visual analysis. Moreover, due to the complex topology and variation of ensemble isocontours, it is often a challenging task to interpret the spaghetti plot for even a single isovalue in large ensembles. In this paper, we propose an interactive framework for uncertainty visualization of weather forecast ensembles that significantly improves and expands the utility of spaghetti plots in ensemble analysis. Complementary to state-of-the-art methods, our approach provides a complete framework for visual exploration of ensemble isocontours, including isovalue selection, interactive isocontour variability exploration, and interactive sub-region selection and re-analysis. Our framework is built upon the high-density clustering paradigm, where the mode structure of the density function is represented as a hierarchy of nested subsets of the data. We generalize the high-density clustering for isocontours and propose a bandwidth selection method for estimating the density function of ensemble isocontours. We present novel visualizations based on high-density clustering results, called the mode plot and the simplified spaghetti plot. The proposed mode plot visually encodes the structure provided by the high-density clustering result and summarizes the distribution of ensemble isocontours. It also enables the selection of subsets of interesting isocontours, which are interactively highlighted in a linked spaghetti plot for providing spatial context. To provide an interpretable overview of the positional variability of isocontours, our system allows for selection of informative isovalues from the simplified spaghetti plot. Due to the spatial variability of ensemble isocontours, the system allows for interactive selection and focus on sub-regions for local uncertainty and clustering re-analysis. We examine a number of ensemble datasets to establish the utility of our approach and discuss its advantages over state-of-the-art visual analysis tools for ensemble data.", "uri": "https://vimeo.com/290325855", "name": "[VIS18 Preview] An Interactive Framework for Visualization of Weather Forecast Ensembles (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:13:26+00:00", "description": "Authors: Thomas Wilde, Christian Roessl, Holger Theisel\n\nAbstract: We present a formal approach to the visual analysis of recirculation in flows by introducing recirculation surfaces for 3D unsteady flow fields. Recirculation surfaces are the loci where massless particle integration returns to its starting point after some variable, finite integration. We study properties of recirculation surfaces and provide an approach to their extraction and visualization. The problem of finding isolated closed orbits in steady vector fields occurs as a special case of recirculation surfaces. We show recirculation surfaces for a number of artificial and real flow data sets.", "uri": "https://vimeo.com/290325803", "name": "[VIS18 Preview] Recirculation Surfaces for Flow Visualization (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:13:15+00:00", "description": "Authors: Jun Tao, Martin Imre, Chaoli Wang, Nitesh V Chawla, Hanqi Guo, G\u00f6khan Sever, Seung Hyun Kim\n\nAbstract: We present a novel visual representation and interface named MISM (Matrix of Isosurface Similarity Maps) for effective exploration of large time-varying multivariate volumetric data sets. MISM synthesizes three types of similarity maps (i.e., self, temporal, and variable similarity maps) to capture the essential relationships among isosurfaces of different variables and time steps. Additionally, it serves as the main visual mapping and navigation tool for examining the vast number of isosurfaces and exploring the underlying time-varying multivariate data set. We present temporal clustering, variable grouping, and interactive filtering to reduce the huge exploration space of MISM. In conjunction with the isovalue and isosurface views, MISM allows users to identify important isosurfaces or isosurface pairs and compare them over space, time, and value range. More importantly, we introduce path recommendation that suggests, animates, and compares traversal paths for effectively exploring MISM under varied criteria and at different levels-of-detail. A silhouette-based method is applied to render multiple surfaces of interest in a visually succinct manner. We demonstrate the effectiveness of our approach with case studies of several time-varying multivariate data sets and an ensemble data set, and evaluate our work with two domain experts.", "uri": "https://vimeo.com/290325769", "name": "[VIS18 Preview] Exploring Time-Varying Multivariate Volume Data Using Matrix of Isosurface Similarity Maps (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:13:03+00:00", "description": "Authors: Michael Alexander Kern, Timothy David Hewson, Andreas Sch\u00e4fler, R\u00fcdiger Westermann, Marc Rautenhaus\n\nAbstract: Atmospheric fronts play a central role in meteorology, as the boundaries between different air masses and as fundamental features of extra-tropical cyclones. They appear in numerous conceptual model depictions of extra-tropical weather systems. Conceptually, fronts are three-dimensional surfaces in space possessing an innate structural complexity, yet in meteorology, both manual and objective identification and depiction have historically focused on the structure in two dimensions. In this work, we --a team of visualization scientists and meteorologists-- propose a novel visualization approach to analyze the three-dimensional structure of atmospheric fronts and related physical and dynamical processes. We build upon existing approaches to objectively identify fronts as lines in two dimensions and extend these to obtain frontal surfaces in three dimensions, using the magnitude of temperature change along the gradient of a moist potential temperature field as the primary identifying factor. We introduce the use of normal curves in the temperature gradient field to visualize a frontal zone (i.e., the transitional zone between the air masses) and the distribution of atmospheric variables in such zones. To enable for the first time a statistical analysis of frontal zones, we present a new approach to obtain the volume enclosed by a zone, by classifying grid boxes that intersect with normal curves emanating on a selected front. We introduce our method by means of an idealized numerical simulation and demonstrate its use with two real-world cases using numerical weather prediction data.", "uri": "https://vimeo.com/290325737", "name": "[VIS18 Preview] Interactive 3D Visual Analysis of Atmospheric Fronts (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:12:49+00:00", "description": "Authors: Lawrence Roy, Prashant Kumar, Yue Zhang, Eugene Zhang\n\nAbstract: 3D symmetric tensor fields appear in many science and engineering domains, and topology-driven analysis is important in many of these application domains, such as solid mechanics and fluid dynamics. Degenerate curves and neutral surfaces are important topological features in that they are where the non-repeating eigenvector field becomes discontinuous. Existing methods to extract degenerate curves and neutral surfaces often miss parts of the curves and surfaces, respectively. Moreover, they are relatively expensive to extract. These issues are due to the lack of knowledge of their overall structures.\n \n In this paper, we provide theoretical analysis on the geometric and topological structures of degenerate curves and neutral surfaces of 3D linear tensor fields. These structures lead to parametrizations for degenerate curves and neutral surfaces that can not only provide more robust extraction of these features but at a reduced computational cost. \n  \n We demonstrate the benefits of our approach by applying our degenerate point and neutral surface detection to solid mechanics simulation data.", "uri": "https://vimeo.com/290325698", "name": "[VIS18 Preview] Robust and Fast Extraction of 3D Symmetric Tensor Field Topology (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:12:37+00:00", "description": "Authors: Subhashis Hazarika, Soumya Dutta, Han-Wei Shen, Jen-Ping Chen\n\nAbstract: CoDDA (Copula-based Distribution Driven Analysis) is a flexible framework for large-scale multivariate datasets. A common strategy to deal with large-scale scientific simulation data is to partition the simulation domain and create statistical data summaries. Instead of storing the high-resolution raw data from the simulation, storing the compact statistical data summaries results in reduced storage overhead and alleviated I/O bottleneck. Such summaries, often represented in the form of statistical probability distributions, can serve various post-hoc analysis and visualization tasks. However, for multivariate simulation data using standard multivariate distributions for creating data summaries is not feasible. They are either storage inefficient or are computationally expensive to be estimated in simulation time (in situ) for large number of variables. In this work, using copula functions, we propose a flexible multivariate distribution-based data modeling and analysis framework that offers significant data reduction and can be used in an \\textit{in situ} environment. Using the proposed multivariate data summaries, we perform various multivariate post-hoc analyses like query-driven visualization and sampling-based visualization. We evaluate our proposed method on multiple real-world multivariate scientific datasets. To demonstrate the efficacy of our framework in an in situ environment, we apply it on a large-scale flow simulation.", "uri": "https://vimeo.com/290325658", "name": "[VIS18 Preview] CoDDA: A Flexible Copula-based Distribution Driven Analysis Framework for Large-Scale Multivariate Data...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:12:27+00:00", "description": "Authors: Johannes Weissenb\u00f6ck, Bernhard Fr\u00f6hler, Eduard Gr\u00f6ller, Johann Kastner, Christoph Heinzl\n\nAbstract: The comparison of the many datasets of an ensemble is difficult, tedious, and error-prone aggravated by, often just subtle differences. In this paper, we introduce Dynamic Volume Lines for the interactive visual analysis and comparison of sets of 3D volumes. Each volume is depicted by a space-filling curve namely a nonlinearly scaled 1D Hilbert line plot which depicts the gray value intensities over the Hilbert indices. We present a nonlinear scaling of the Hilbert space-filling curve based on the gray value variations in the ensemble of 3D volumes, which enables a more effective use of the available screen space. The nonlinear scaling builds the basis for our interactive visualization techniques. An interactive 2D histogram of the gray value frequencies serves as overview visualization. When zooming in, the frequencies are replaced by detailed 1D Hilbert curves and optional functional boxplots. To focus on important regions of the volume ensemble, nonlinear scaling is incorporated into the plots. At each level of detail, an interactive scaling widget depicts the local ensemble variations by indicating the nonlinear scaling using a sequential color-coded gradient. Our brushing and linking interface reveals, for example, regions with a high ensemble variation by showing the affected voxels in a 3D spatial rendering view. We show the applicability of our concepts using two case studies on ensembles of 3D volumes resulting from tomographic reconstruction. In the first case study we evaluate an artificial specimen from simulated industrial 3D X-ray computed tomography (XCT), in the second case study, a real-world XCT foam specimen is investigated. Our results show that Dynamic Volume Lines can identify regions with high local gray value variations, allowing conclusions, for example, about the choice of reconstruction parameters. Furthermore, it is possible to detect ring artifacts in reconstructions volumes.", "uri": "https://vimeo.com/290325634", "name": "[VIS18 Preview] Dynamic Volume Lines: Visual Comparison of 3D Volumes through Space-filling Curves (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:12:16+00:00", "description": "Authors: Monique Meuschke, Tobias G\u00fcnther, Philipp Berg, Ralph Wickenhoefer, Markus Gross, Bernhard Preim, Kai Lawonn\n\nAbstract: This paper presents a framework to explore multi-field data of aneurysms occurring at intracranial and cardiac arteries by using statistical graphics. The rupture of an aneurysm often causes death, whereas its treatment also carries considerable risks for the patient. For rupture risk evaluation and treatment decision, both morphological and hemodynamic data have to be investigated. Medical researchers emphasize the importance of analyzing correlations between wall properties such as wall deformation and thickness, and hemodynamic attributes like wall shear stress and pressure. This is a time-consuming process, where suspicious wall regions are difficult to detect due to the time-dependent behavior of the data. Thus, we offer medical researchers an effective visual exploration tool for aneurysm risk and treatment assessment. We provide a set of views such as 2D and 3D depictions of the aneurysm morphology as\n well as statistical plots of different scalar fields. To support the exploration of parameter correlations, brushing and linking aids the user to identify interesting wall regions and to understand the influence of different parameters on the aneurysm\u2019s state. Moreover, a visual comparison of pre- and post-treatment as well as different treatment options is provided. Our analysis techniques are designed in collaboration with domain experts, e.g., physicians, and we provide details about the evaluation.", "uri": "https://vimeo.com/290325604", "name": "[VIS18 Preview] Visual Analysis of Aneurysm Data using Statistical Graphics (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:12:03+00:00", "description": "Authors: Michael traore, Christophe Hurter, Alexandru Telea\n\nAbstract: Occlusion is an issue in volumetric datasets visualization as it prevents direct visualization of the region of interest. To address this problem, many techniques have been developed such as transfer functions, volume segmentation or view distortion. Even if these techniques have proven their efficiency, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion int datasets where the target and the occluder are very similar density-wise. For these reasons, we investigated a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. As such, we propose a new focus+context technique. The user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts to operate and pushes at its border occluding objects (i.e. local deformation), thus revealing hidden parts of the volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings for the area under investigation within this lens. To develop this technique, we used a GPU accelerated ray-casting framework with a set of interactive tools to ease volumetric data exploration and real-time manipulation. We illustrated the efficiency of this technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.", "uri": "https://vimeo.com/290325542", "name": "[VIS18 Preview] Interactive obstruction-free lensing for volumetric data visualization (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:11:50+00:00", "description": "Authors: Sergej Stoppel, Magnus Paulson Erga, Stefan Bruckner\n\nAbstract: Automatic light placement in three dimensional scenes is a very complex problem that has been investigated over the last years. However users don't necessary consider lights as static and often move the lights to change the scene illumination. Hereby the simultaneous control of the camera and the light impose a high cognitive load on the user. To address this challenge, we introduce a novel approach for automatic scene illumination with moving lights, or Fireflies. Fireflies are intelligent light drones that illuminate the scene by traveling on a closed path. The Firefly path automatically adopts to changes in the scene based on an outcome oriented energy function on the fly. To achieve interactive performance, we employ a parallel rendering pipeline for the light path evaluations. We provide a catalog of energy functions for various application scenarios and discuss the applicability of our method on several examples.", "uri": "https://vimeo.com/290325505", "name": "[VIS18 Preview] Firefly: Illumination Drones for Interactive Visualization (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:11:37+00:00", "description": "Authors: Toni Sagrist\u00e0 Sell\u00e9s, Stefan Jordan, Thomas Mueller, Filip Sadlo\n\nAbstract: In this paper, we present Gaia Sky, a free and open-source multiplatform 3D universe system, developed since 2014 in the Data Processing and Analysis Consortium framework of ESA's Gaia mission. Gaia's data realease 2, scheduled for April 2018, will represent the largest catalog of the stars of our Galaxy, comprising 1.3 billion star positions, with parallaxes, proper motions, magnitudes, and colors. In this mission, Gaia Sky is the central tool for off-the-shelf visualization of these data, and for aiding production of outreach material. With its capabilities to effectively handle these data, to enable seamless navigation along the high dynamic range of distances, and at the same time to provide advanced visualization techniques including relativistic aberration and gravitational wave effects, no actively maintained cross-platform, modern, and open alternative exists.", "uri": "https://vimeo.com/290325474", "name": "[VIS18 Preview] Gaia Sky: Navigating the Gaia Catalog (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:11:19+00:00", "description": "Authors: Norbert Lindow, Daniel Baum, Morgan Leborgne, Hans-Christian Hege\n\nAbstract: The analysis and visualization of nucleic acids (RNA and DNA) plays an increasingly important role due to the growing number of known 3-dimensional structures of such molecules. The great complexity of these structures, in particular those of RNA, demands interactive visualization to get deeper insights into the relationship between the 2D secondary structure motifs and their 3D tertiary structures. Over the last decades, a lot of research in molecular visualization has focused on the visual exploration of protein structures while nucleic acids have only been marginally addressed. In contrast to proteins, which are composed of amino acids, the ingredients of nucleic acids are nucleotides. They form structuring patterns that differ from those of proteins and, hence, also require different visualization and exploration techniques. In order to support interactive exploration of nucleic acids, the computation of secondary structure motifs as well as their visualization in 2D and 3D must be fast. Therefore, in this paper we focus on the performance of both the computation and visualization of nucleic acid structure. For the first time, we present a ray casting-based visualization of RNA and DNA secondary and tertiary structures, which enables real-time visualization of even large molecular dynamics trajectories. Furthermore, we provide a detailed description of all important aspects to visualize nucleic acid secondary and tertiary structures. With this we close an important gap in molecular visualization.", "uri": "https://vimeo.com/290325417", "name": "[VIS18 Preview] Interactive Visualization of RNA and DNA Structures (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:11:01+00:00", "description": "Authors: Hui Zhang, Steffen Frey, Holger Steeb, David Uribe, Thomas Ertl, Wenping Wang\n\nAbstract: We present a visualization approach for the analysis of CO2 bubble-induced attenuation in porous rock formations. As a basis for this, we introduce customized techniques to extract CO2 bubbles and their surrounding porous structure from XRCT measurements. To understand how the structure of porous media influences the occurrence and the shape of formed bubbles, we automatically classify and relate them in terms of morphology and geometric features, and further directly support searching for promising porous structures. To allow for the meaningful direct visual comparison of bubbles and their structures, we propose a customized registration technique considering the bubble shape as well as its points of contact to the porous media surface. With our quantitative extraction of geometric bubble features, we further support the analysis as well as the creation of a physical model. We demonstrate that our approach was successfully used to answer several research questions in the domain, and discuss its high practical relevance to identify critical seismic characteristics of fluid-saturated rock that govern its capability to store CO2.", "uri": "https://vimeo.com/290325359", "name": "[VIS18 Preview] Visualization of Bubble Formation in Porous Media (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:10:48+00:00", "description": "Authors: Tushar Athawale, Chris R. Johnson\n\nAbstract: We present a framework for the analysis of uncertainty in isocontour extraction. The marching squares (MS) algorithm for isocontour reconstruction generates a linear topology that is consistent with hyperbolic curves of a piecewise bilinear interpolation. The saddle points of the bilinear interpolant cause topological ambiguity in isocontour extraction. The midpoint decider and the asymptotic decider are well-known mathematical techniques for resolving topological ambiguities. The latter technique investigates the data values at the cell saddle points for ambiguity resolution. The uncertainty in data, however, leads to uncertainty in underlying bilinear interpolation functions for the MS algorithm, and hence, their saddle points. In our work, we study the behavior of the asymptotic decider when data at grid \nvertices is uncertain. First, we derive closed-form distributions characterizing variations in the saddle point values for uncertain bilinear interpolants. The derivation assumes uniform and nonparametric noise models, and it exploits the concept of ratio distribution for analytic formulations. Next, the probabilistic asymptotic decider is devised for ambiguity resolution in uncertain data using distributions of the saddle point values derived in the first step. Finally, the confidence in probabilistic topological decisions is visualized using a colormapping technique. We demonstrate the higher accuracy and stability of the probabilistic asymptotic decider in uncertain data with regard to existing decision frameworks, such as deciders in the mean field and the probabilistic midpoint decider, through the isocontour visualization of synthetic and real datasets.", "uri": "https://vimeo.com/290325318", "name": "[VIS18 Preview] Probabilistic Asymptotic Decider for Topological Ambiguity Resolution in Isosurface Extraction for Uncertain...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-17T17:10:36+00:00", "description": "Authors: David Kou\u0159il, Ladislav \u010cmol\u00edk, Barbora Kozlikova, Hsiang-Yun Wu, Graham Johnson, David S. Goodsell, Arthur Olson Ph.D., Eduard Gr\u00f6ller, Ivan Viola\n\nAbstract: Labeling is intrinsically important for exploring and understanding complex environments and models in a variety of domains. We present a method for interactive labeling of crowded 3D scenes containing very many instances of objects spanning multiple scales in size. In contrast to previous labeling methods, we target cases where many instances of dozens of types are present and where the hierarchical structure of the objects in the scene presents an opportunity to choose the most suitable level for each placed label. Our solution builds on and goes beyond labeling techniques in medical 3D visualization, cartography, and biological illustrations from books and prints. In contrast to these techniques, the main characteristics of our new technique are: 1) a novel way of labeling objects as part of a bigger structure when appropriate, 2) visual clutter reduction by labeling only representative instances for each type of an object, and a strategy of selecting those.The appropriate level of label is chosen by analyzing the scene's depth buffer and the scene objects' hierarchy tree. We address the topic of communicating the parent-children relationship between labels by employing visual hierarchy concepts adapted from graphic design. Selecting representative instances considers several criteria tailored to the character of the data and is combined with a greedy optimization approach. We demonstrate the usage of our method with models from mesoscale biology where these two characteristics---multi-scale and multi-instance---are abundant, along with the fact that these scenes are extraordinarily dense.", "uri": "https://vimeo.com/290325281", "name": "[VIS18 Preview] Labels on Levels: Labeling of Multi-Scale Multi-Instance and Crowded 3D Environments (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:10:17+00:00", "description": "Authors: Andrey Krekhov, Jens Krueger\n\nAbstract: Preattentive visual features such as hue or flickering can effectively draw attention to an object of interest -- for instance, an important feature in a scientific visualization. These features appear to pop out and can be recognized by our visual system, independently from the number of distractors. Most cues do not take advantage of the fact that most humans have two eyes. In cases where binocular vision is applied, it is almost exclusively used to convey depth by exposing stereo pairs. \n \n We present Deadeye, a novel preattentive feature based on presenting different stimuli to each eye. The target object is rendered for one eye only and is instantly detected by our visual system. In contrast to existing cues, Deadeye does not modify any visual properties of the target and, thus, is particularly suited for visualization applications. Our evaluation confirms that Deadeye is indeed perceived preattentively. We also explore a conjunction search based on our technique and show that, in contrast to 3D depth, the task cannot be processed in parallel.", "uri": "https://vimeo.com/290325240", "name": "[VIS18 Preview] Deadeye: A Novel Preattentive Visual Feature Based on Dichoptic Presentation (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-17T17:10:05+00:00", "description": "Authors: Guillaume Favelier, Noura Faraj, Brian Summa, Julien Tierny\n\nAbstract: This paper presents a new approach for the visualization and analysis of the spatial variability of features of interest represented by critical points in ensemble data. Our framework, called Persistence Atlas, enables the visualization of the dominant spatial patterns of critical points, along with statistics regarding their occurrence in the ensemble. The persistence atlas represents in the geometrical domain each dominant pattern in the form of a confidence map for the appearance of critical points. As a by-product, our method also provides 2-dimensional layouts of the entire ensemble, highlighting the main trends at a global level. Our approach is based on the new notion of Persistence Map, a measure of the geometrical density in critical points which leverages the robustness to noise of topological persistence to better emphasize salient features. We show how to leverage spectral embedding to represent the ensemble members as points in a low-dimensional Euclidean space, where distances between points measure the dissimilarities between critical point layouts and where statistical tasks, such as clustering, can be easily carried out. Further, we show how the notion of mandatory critical point can be leveraged to evaluate for each cluster confidence regions for the appearance of critical points. Most of the steps of this framework can be trivially parallelized and we show how to efficiently implement them. Extensive experiments demonstrate the relevance of our approach. The accuracy of the confidence regions provided by the persistence atlas is quantitatively evaluated and compared to a baseline strategy using an off-the-shelf clustering approach. We illustrate the importance of the persistence atlas in a variety of real-life datasets, where clear trends in feature layouts are identified and analyzed. We provide a lightweight VTK-based C++ implementation of our approach that can be used for reproduction purposes.", "uri": "https://vimeo.com/290325207", "name": "[VIS18 Preview] Persistence Atlas for Critical Point Variability in Ensembles (SciVis Paper)", "year": "2018", "event": "SCIVIS, PREVIEW"}, {"created_time": "2018-09-14T00:38:35+00:00", "description": "Authors: Monique Meuschke, Steffen Oeltze-Jafra, Oliver Beuing, Bernhard Preim and Kai Lawonn\n\nAbstract: We present a Cerebral Aneurysm Vortex Classification (CAVOCLA) that allows to classify blood flow in cerebral aneurysms. Medical studies assume a strong relation between the progression and rupture of aneurysms and flow patterns. To understand how flow patterns impact the vessel morphology, they are manually classified according to predefined classes. However, manual classifica- tions are time-consuming and exhibit a high inter-observer variability. In contrast, our approach is more objective and faster than manual methods. The classification of integral lines, representing steady or unsteady blood flow, is based on a mapping of the aneurysm surface to a hemisphere by calculating polar-based coordinates. The lines are clustered and for each cluster a representative is calculated. Then, the polar-based coordinates are transformed to the representative as basis for the classification. Classes are based on the flow complexity. The classification results are presented by a detail-on-demand approach using a visual transition from the representative over an enclosing surface to the associated lines. Based on seven representative datasets, we conduct an informal interview with five domain experts to evaluate the system. They confirmed that CAVOCLA allows for a robust classification of intra-aneurysmal flow patterns. The detail-on-demand visualization enables an efficient exploration and interpretation of flow patterns.", "uri": "https://vimeo.com/289789890", "name": "[VIS18 Preview] Classification of Blood Flow Patterns in Cerebral Aneurysms (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:35:52+00:00", "description": "Authors: Fumeng Yang, Lane Harrison, Ronald A. Rensink , Steven Franconeri , Remco Chang\n\nAbstract: Recent visualization research efforts have incorporated experimental techniques and perceptual models from the vision science community. Perceptual models such as Weber\u2019s law, for example, have been used to model the perception of correlation in scatterplots. While this thread of research has progressively refined a model of the perception of correlation in scatterplots, it remains unclear as to why the perception of correlation in scatterplots can be modeled using relatively simple functions, e.g., linear and log-linear. In this paper, we investigate a longstanding hypothesis that people use visual features in a chart as a proxy for statistical measures like correlation. For a given scatterplot, we extract 49 candidate visual features and apply a set of metrics to evaluate which features best align with existing models and participant judgments. The results support the hypothesis that people attend to a small number of visual features when discriminating correlation in scatterplots. We discuss how this result may account for prior conflicting findings, and how visual features provide a baseline for future model-based approaches in visualization evaluation and design.", "uri": "https://vimeo.com/289789613", "name": "[VIS18 Preview] Correlation Judgment and Visualization Features: A Comparative Study (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:35:40+00:00", "description": "Authors: Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng (Polo) Chau\n\nAbstract: Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W\u2019 s and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.", "uri": "https://vimeo.com/289789596", "name": "[VIS18 Preview] Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:35:29+00:00", "description": "Authors: Shixia Liu, Xiting Wang, Christopher Collins, Wenwen Dou, Fangxin Ouyang, Mennatallah El-Assady, Liu Jiang, Daniel Keim\n\nAbstract: Visual text analytics has recently emerged as one of the most prominent topics in both academic research and the commercial world. To provide an overview of the relevant techniques and analysis tasks, as well as the relationships between them, we comprehensively analyzed 263 visualization papers and 4,346 mining papers published between 1992-2017 in two fields: visualization and text mining. From the analysis, we derived around 300 concepts (visualization techniques, mining techniques, and analysis tasks) and built a taxonomy for each type of concept. The co-occurrence relationships between the concepts were also extracted. Our research can be used as a stepping-stone for other researchers to 1) understand a common set of concepts used in this research topic; 2) facilitate the exploration of the relationships between visualization techniques, mining techniques, and analysis tasks; 3) understand the current practice in developing visual text analytics tools; 4) seek potential research opportunities by narrowing the gulf between visualization and mining techniques based on the analysis tasks; and 5) analyze other interdisciplinary research areas in a similar way. We have also contributed a web-based visualization tool for analyzing and understanding research trends and opportunities in visual text analytics.", "uri": "https://vimeo.com/289789579", "name": "[VIS18 Preview] Bridging Text Visualization and Mining: A Task-Driven Survey (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:35:19+00:00", "description": "Authors: Yong Wang, Daniel Archambault, Carlos E. Scheidegger, Huamin Qu\n\nAbstract: Animated transitions can be effective in explaining and exploring a small number of visualizations where there are drastic changes in the scene over a short interval of time. This is especially true if data elements cannot be visually distinguished by other means. Current research in animated transitions has mainly focused on linear transitions (all elements follow straight line paths) or enhancing coordinated motion through bundling of linear trajectories. In this paper, we introduce animated transition design, a technique to build smooth, non-linear transitions for clustered data with either minimal or no user involvement. The technique is flexible and simple to implement, and has the additional advantage that it explicitly enhances coordinated motion and can avoid crowding, which are both important factors to support object tracking in a scene. We investigate its usability, provide preliminary evidence for the effectiveness of this technique through metric evaluations and user study and discuss limitations and future directions.", "uri": "https://vimeo.com/289789555", "name": "[VIS18 Preview] A Vector Field Design Approach to Animated Transitions (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:35:08+00:00", "description": "Authors: Patrick Riehmann, Dora Kiesel, Martin Kohlhaas, and Bernd Froehlich\n\nAbstract: This paper presents a visualization framework that aids readers in understanding and analyzing the contents of medium-sized text collections that are typical for the opus of a single or few authors.We contribute several document-based visualization techniques to facilitate the exploration of the work of the German author Bazon Brock by depicting various aspects of its texts, such as the TextGenetics that shows the structure of the collection along with its chronology. The ConceptCircuit augments the TextGenetics with entities - persons and locations that were crucial to his work. All visualizations are sensitive to a wildcard-based phrase search that allows complex requests towards the author's work. Further development, as well as expert reviews and discussions with the author Bazon Brock, focused on the assessment and comparison of visualizations based on automatic topic extraction against ones that are based on expert knowledge.", "uri": "https://vimeo.com/289789544", "name": "[VIS18 Preview] Visualizing a Thinker\u2019s Life (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:34:58+00:00", "description": "Authors: Allan Rocha, Julio Daniel Silva, Usman R. Alim, Sheelagh Carpendale, and Mario Costa Sousa\n\nAbstract:", "uri": "https://vimeo.com/289789528", "name": "[VIS18 Preview] Decal-Lenses: Interactive Lenses on Surfaces for Multivariate Visualization (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:34:44+00:00", "description": "Authors: Bahador Saket, Alex Endert, Cagatay Demiralp\n\nAbstract: Visualizations of tabular data are widely used; understanding their effectiveness in different task and data contexts is fundamental to scaling their impact. However, little is known about how basic tabular data visualizations perform across varying data analysis tasks. In this paper, we report results from a crowdsourced experiment to evaluate the effectiveness of five small scale (5-34 data points) two-dimensional visualization types---Table, Line Chart, Bar Chart, Scatterplot, and Pie Chart---across ten common data analysis tasks using two datasets. We found the effectiveness of these visualization types significantly varies across task, suggesting that visualization design would benefit from considering context-dependent effectiveness. Based on our findings, we derive recommendations on which visualizations to choose based on different tasks.", "uri": "https://vimeo.com/289789506", "name": "[VIS18 Preview] Task-Based Effectiveness of Basic Visualizations (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:34:35+00:00", "description": "Authors: Michael Behrisch, DirkStreeb, Florian Stoffel, Daniel Seebacher, Stefan H agen Weber, Sebastian Mittelstaedt, Hanspeter Pfister, Daniel Keim\n\nAbstract: Five years after the first state-of-the-art report on Commercial Visual Analytics Systems we present a reevaluation of the Big Data Analytics field. We build on the success of the 2012 survey, which was influential even beyond the boundaries of the InfoVis and Visual Analytics (VA) community. While the field has matured significantly since the original survey, we find that innovation and research-driven development are increasingly sacrificed to satisfy a wide range of user groups. We evaluate new product versions on established evaluation criteria, such as available features, performance, and usability, to extend on and assure comparability with the previous survey. We also investigate previously unavailable products to paint a more complete picture of the commercial VA landscape. Furthermore, we introduce novel measures, like suitability for specific user groups and the ability to handle complex data types, and undertake a new case study to highlight innovative features. We explore the achievements in the commercial sector in addressing VA challenges and propose novel developments that should be on systems\u2019 roadmaps in the coming years.", "uri": "https://vimeo.com/289789498", "name": "[VIS18 Preview] Commercial Visual Analytics Systems \u2013 Advances in the Big Data Analytics Field (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:34:23+00:00", "description": "Authors: Richard Roberts, Robert S Laramee, Gary A Smith, Paul Brookes, and Tony D'Cruze\n\nAbstract: The Parallel Coordinates plot is a popular tool for the visualization of high-dimensional data. One of the main challenges when using parallel coordinates is occlusion and overplotting resulting from large data sets. Brushing is a popular approach to address these challenges. Since its conception, limited improvements have been made to brushing both in the form of visual design and functional interaction. We present a set of novel, smart brushing techniques that enhance the standard interactive brushing of a parallel coordinates plot. We introduce two new interaction concepts: Higher-order, sketch-based brushing, and smart, data-driven brushing. Higher-order brushes support interactive, flexible, n-dimensional pattern searches involving an arbitrary number of dimensions. Smart, data-driven brushing provides interactive, real-time guidance to the user during the brushing process based on derived meta-data. In addition, we implement a selection of novel enhancements and user options that complement the two techniques as well as enhance the exploration and analytical ability of the user. We demonstrate the utility and evaluate the results using a case study with a large, high-dimensional, real-world telecommunication data set and we report domain expert feedback from the data suppliers.", "uri": "https://vimeo.com/289789480", "name": "[VIS18 Preview] Smart Brushing for Parallel Coordinates (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:34:12+00:00", "description": "Authors: Matthew Berger, Jixian Li, and Joshua A. Levine\n\nAbstract: We present a technique to synthesize and analyze volume-rendered images using generative models. We use the Generative Adversarial Network (GAN) framework to compute a model from a large collection of volume renderings, conditioned on (1) viewpoint and (2) transfer functions for opacity and color. Our approach facilitates tasks for volume analysis that are challenging to achieve using existing rendering techniques such as ray casting or texture-based methods. We show how to guide the user in transfer function editing by quantifying expected change in the output image. Additionally, the generative model transforms transfer functions into a view-invariant latent space specifically designed to synthesize volume-rendered images. We use this space directly for rendering, enabling the user to explore the space of volume-rendered images. As our model is independent of the choice of volume rendering process, we show how to analyze volume-rendered images produced by direct and global illumination lighting, for a variety of volume datasets.", "uri": "https://vimeo.com/289789468", "name": "[VIS18 Preview] A Generative Model for Volume Rendering (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:34:00+00:00", "description": "Authors: Xiao Xie, Xiwen Cai, Junpei Zhou, Nan Cao, Yingcai Wu\n\nAbstract: Interactive visualization of large image collections is important and useful in many applications, such as personal album management and user profiling on images. However, most prior studies focus on using low-level visual features of images, such as texture and color histogram, to create visualizations without considering the more important semantic information embedded in images. This paper proposes a novel visual analytic system to analyze images in a semantic-aware manner. The system mainly comprises two components: a semantic information extractor and a visual layout generator. The semantic information extractor employs an image captioning technique based on convolutional neural network (CNN) to produce descriptive captions for images, which can be transformed into semantic keywords. The layout generator employs a novel co-embedding model to project images and the associated semantic keywords to the same 2D space. Inspired by the galaxy metaphor, we further turn the projected 2D space to a galaxy visualization of images, in which semantic keywords and images are visually encoded as stars and planets. Our system naturally supports multi-scale visualization and navigation, in which users can immediately see a semantic overview of an image collection and drill down for detailed inspection of a certain group of images. Users can iteratively refine the visual layout by integrating their domain knowledge into the co-embedding process. Two task-based evaluations are conducted to demonstrate the effectiveness of our system.", "uri": "https://vimeo.com/289789441", "name": "[VIS18 Preview] A Semantic-based Method for Visualizing Large Image Collections (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:33:50+00:00", "description": "Authors: Fabio Miranda, Harish Doraiswamy, Marcos Lage, Luc Wilson, Mondrian Hsieh, Claudio T. Silva\n\nAbstract: Large scale shadows from buildings in a city play an important role in determining the environmental quality of public spaces. They can be both beneficial, such as for pedestrians during summer, and detrimental, by impacting vegetation and by blocking direct sunlight. Determining the effects of shadows requires the accumulation of shadows over time across different periods in a year. In this paper, we propose a simple yet efficient class of approach that uses the properties of sun movement to track the changing position of shadows within a fixed time interval. We use this approach to extend two commonly used shadowing techniques, shadow maps and ray tracing, and demonstrate the efficiency of our approach. Our technique is used to develop an interactive visual analysis system, Shadow Profiler, targeted at city planners and architects that allows them to test the impact of shadows for different development scenarios. We validate the usefulness of this system through case studies set in Manhattan, a dense borough of New York City.", "uri": "https://vimeo.com/289789412", "name": "[VIS18 Preview] Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows over Time (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:33:35+00:00", "description": "Authors: Johanna Schmidt, Dominik Fleischmann, Bernhard Preim, Norbert Brandle, and Gabriel Mistelbauer\n\nAbstract:", "uri": "https://vimeo.com/289789393", "name": "[VIS18 Preview] Popup-Plots: Warping Temporal Data Visualization (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:33:18+00:00", "description": "Authors: Erick Cuenca, Arnaud Sallaberry, Florence Ying Wang and Pascal Poncelet\n\nAbstract: Multiple time series are a set of multiple quantitative variables occurring at the same interval. They are present in many domains such as medicine, finance, and manufacturing for analytical purposes. In recent years, streamgraph visualization (evolved from ThemeRiver) has been widely used for representing temporal evolution patterns in multiple time series. However, streamgraph as well as ThemeRiver suffer from scalability problems when dealing with several time series. To solve this problem, multiple time series can be organized into a hierarchical structure where individual time series are grouped hierarchically according to their proximity. In this paper, we present a new streamgraph-based approach to convey the hierarchical structure of multiple time series to facilitate the exploration and comparisons of temporal evolution. Based on a focus+context technique, our method allows time series exploration at different granularities (e. g., from overview to details). To illustrate our approach, two usage examples are presented.", "uri": "https://vimeo.com/289789365", "name": "[VIS18 Preview] A Multiresolution Streamgraph Approach to Explore Hierarchical Time Series (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:33:05+00:00", "description": "Authors: Vahan Yoghourdjian, Tim Dwyer, Karsten Klein, Kim Marriott, and Michael Wybrow\n\nAbstract: We propose Graph Thumbnails, small icon-like visualisations of the high-level structure of network data. Graph Thumbnails are designed to be legible in small multiples to support rapid browsing within large graph corpora. Compared to existing graph-visualisation techniques our representation has several advantages: (1) the visualisation can be computed in linear time; (2) it is canonical in the sense that isomorphic graphs will always have identical thumbnails; and (3) it provides precise information about the graph structure. We report the results of two user studies. The first study compares Graph Thumbnails to node-link and matrix views for identifying similar graphs. The second study investigates the comprehensibility of the different representations. We demonstrate the usefulness of this representation for summarising the evolution of protein-protein interaction networks across a range of species.", "uri": "https://vimeo.com/289789353", "name": "[VIS18 Preview] Graph Thumbnails: Identifying and Comparing Multiple Graphs at a Glance (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:32:55+00:00", "description": "Authors: Roba Binyahib, Tom Peterka, Matthew Larsen, Kwan-Liu Ma, and Hank Childs\n\nAbstract: We present an algorithm for parallel volume rendering that is a hybrid between classical object order and image order techniques. The algorithm operates on unstructured grids (and structured ones), and thus can deal with block boundaries interleaving in complex ways. It also deals effectively with cases that are prone to load imbalance, i.e., cases where cell sizes differ dramatically, either because of the nature of the input data, or because of the effects of the camera transformation. The algorithm divides work over resources such that each phase of its processing is bounded in the amount of computation it can perform. We demonstrate its efficacy through a series of studies, varying over camera position, data set size, transfer function, image size, and processor count. At its biggest, our experiments scaled up to 8,192 processors and operated on data sets with more than one billion cells. In total, we find that our hybrid algorithm performs well in all cases. This is because our algorithm naturally adapts its computation based on workload, and can operate like either an object order technique or an image order technique in scenarios where those techniques are efficient.", "uri": "https://vimeo.com/289789339", "name": "[VIS18 Preview] A Scalable Hybrid Scheme for Ray-Casting of Unstructured Volume Data (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:32:43+00:00", "description": "Authors: Christopher P. Kappe, Michael B\u00f6ttinger, Heike Leitte\n\nAbstract: Ensemble simulations are used in climate research to account for natural variability. For medium-term decadal predictions, each simulation run is initialized with real observations from a different day resulting in a set of possible climatic futures. Understanding the variability and the predictive power in this wealth of data is still a challenging task. In this paper, we introduce a visual analytics system to explore variability within ensembles of decadal climate predictions. We propose a new interactive visualization technique (clustering timeline) based on the Sankey diagram, which conveys a concise summary of data similarity and its changes over time. We augment the system with two additional visualizations, filled contour maps and heatmaps, to provide analysts with additional information relating the new diagram to raw data and automatic clustering results. The usefulness of the technique is demonstrated by case studies and user interviews.", "uri": "/channels/721847https://vimeo.com/289789327", "name": "[VIS18 Preview] Exploring Variability within Ensembles of Decadal Climate Predictions (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:32:32+00:00", "description": "Authors: Bo Zhou, Yi-Jen Chiang, and Cong Wang\n\nAbstract: Local histograms (i.e., point-wise histograms computed from local regions of mesh vertices) have been used in many data analysis and visualization applications. Previous methods for computing local histograms mainly work for regular or rectilinear grids only. In this paper, we develop theory and novel algorithms for computing local histograms in tetrahedral meshes and curvilinear grids. Our algorithms are theoretically sound and efficient, and work effectively and fast in practice. Our main focus is on scalar fields, but the algorithms also work for vector fields as a by-product with small, easy modifications. Our methods can benefit information theoretic and other distribution-driven analysis. The experiments demonstrate the efficacy of our new techniques, including a utility case study on tetrahedral vector field visualization.", "uri": "https://vimeo.com/289789310", "name": "[VIS18 Preview] Efficient Local Statistical Analysis via Point-Wise Histograms in Tetrahedral Meshes and Curvilinear Grids...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:32:21+00:00", "description": "Authors: Florian Windhager; Paolo Federico; G\u00fcnther Schreder; Katrin Glinka; Marian D\u00f6rk; Silvia Miksch; Eva Mayr\n\nAbstract: After decades of digitization, large cultural heritage collections have emerged on the web, which contain massive stocks of content from galleries, libraries, archives, and museums. This increase in digital cultural heritage data promises new modes of analysis and increased levels of access for academic scholars and casual users alike. Going beyond the standard representations of search-centric and grid-based interfaces, a multitude of approaches has recently started to enable visual access to cultural collections, and to explore them as complex and comprehensive information spaces by the means of interactive visualizations. In contrast to conventional web interfaces, we witness a widening spectrum of innovative visualization types specially designed for rich collections from the cultural heritage sector. This new class of information visualizations gives rise to a notable diversity of interaction and representation techniques while lending currency and urgency to a discussion about principles such as serendipity, generosity, and criticality in connection with visualization design. With this survey, we review information visualization approaches to digital cultural heritage collections and reflect on the state of the art in techniques and design choices. We contextualize our survey with humanist perspectives on the field and point out opportunities for future research.", "uri": "https://vimeo.com/289789286", "name": "[VIS18 Preview] Visualization of Cultural Heritage Collection Data: State of the Art and Future Challenges (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:32:08+00:00", "description": "Authors: Mar\u00eda-Jes\u00fas Lobo, Caroline Appert, Emmanuel Pietriga\n\nAbstract: Before-and-after image pairs show how entities in a given region have evolved over a specific period of time. Satellite images are a major source of such data, that capture how natural phenomena or human activity impact a geographical area. These images are used both for data analysis and to illustrate the resulting findings to diverse audiences. The simple techniques used to display them, including juxtaposing, swapping and monolithic blending, often fail to convey the underlying phenomenon in a meaningful manner. We introduce Baia, a framework to create advanced animated transitions, called animation plans, between before-and-after images. Baia relies on a pixel-based transition model that gives authors much expressive power, while keeping animations for common types of changes easy to create thanks to predefined animation primitives. We describe our model, the associated animation editor, and report on two user studies. In the first study, advanced transitions enabled by Baia were compared to monolithic blending, and perceived as more realistic and better at focusing viewer\u2019s attention on a region of interest than the latter. The second study aimed at gathering feedback about the usability of Baia\u2019s animation editor.", "uri": "https://vimeo.com/289789265", "name": "[VIS18 Preview] Animation Plans for Before-And-After Satellite images (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:31:55+00:00", "description": "Authors: Deokgun Park, Steven M. Drucker, Roland Fernandez, and Niklas Elmqvist\n\nAbstract: Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark\u2014a visual unit\u2014during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user\u2019s mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called ATOM, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.", "uri": "https://vimeo.com/289789243", "name": "[VIS18 Preview] ATOM: A Grammar for Unit Visualizations (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:31:45+00:00", "description": "Authors: Carolina Nobre, Nils Gehlenborg, Hilary Coon, and Alexander Lex\n\nAbstract: The majority of diseases that are a significant challenge for public and individual heath are caused by a combination of hereditary and environmental factors. In this paper we introduce Lineage, a novel visual analysis tool designed to support domain experts who study such multifactorial diseases in the context of genealogies. Incorporating familial relationships between cases with other data can provide insights into shared genomic variants and shared environmental exposures that may be implicated in such diseases. We introduce a data and task abstraction, and argue that the problem of analyzing such diseases based on genealogical, clinical, and genetic data can be mapped to a multivariate graph visualization problem. The main contribution of our design study is a novel visual representation for tree-like, multivariate graphs, which we apply to genealogies and clinical data about the individuals in these families. We introduce data-driven aggregation methods to scale to multiple families. By designing the genealogy graph layout to align with a tabular view, we are able to incorporate extensive, multivariate attributes in the analysis of the genealogy without cluttering the graph. We validate our designs by conducting case studies with our domain collaborators.", "uri": "https://vimeo.com/289789223", "name": "[VIS18 Preview] Lineage: Visualizing Multivariate Clinical Data in Genealogy Graphs (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:31:29+00:00", "description": "Authors: Pavel Govyadinov, Tasha Womack, Jason L. Eriksen, Guoning Chen, David Mayerich\n\nAbstract: Advances in high-throughput imaging allow researchers to collect three-dimensional images of whole organ microvascular networks. These extremely large images contain networks that are highly complex, time consuming to segment, and difficult to visualize. In this paper, we present a framework for segmenting and visualizing vascular networks from terabyte-sized three-dimensional images collected using high-throughput microscopy. While these images require terabytes of storage, the volume devoted to the fiber network is $\\approx$4\\% of the total volume size. While the networks themselves are sparse, they are tremendously complex, interconnected, and vary widely in diameter. We describe a parallel GPU-based predictor-corrector method for tracing filaments that is robust to noise and sampling errors common in these data sets. We also propose a number of visualization techniques designed to convey the complex statistical descriptions of fibers across large tissue sections - including commonly studied microvascular characteristics, such as orientation and volume.", "uri": "https://vimeo.com/289789196", "name": "[VIS18 Preview] Robust Tracing and Visualization of Heterogeneous Microvascular Networks (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:31:14+00:00", "description": "Authors: Akash Anil Valsangkar, Joy Merwin Monteiro, Vidya Narayanan, Ingrid Hotz, and Vijay Natarajan\n\nAbstract: Analyzing depressions plays an important role in meteorology, especially in the study of cyclones. In particular, the study of the temporal evolution of cyclones requires a robust depression tracking framework. To cope with this demand we propose a pipeline for the exploration of cyclones and their temporal evolution. This entails a generic framework for their identification and tracking. The fact that depressions and cyclones are not well-defined objects and their shape and size characteristics change over time makes this task especially challenging. Our method combines the robustness of topological approaches and the detailed tracking information from optical flow analysis. At first cyclones are identified within each time step based on well-established topological concepts. Then candidate tracks are computed from an optical flow field. These tracks are clustered within a moving time window to distill dominant coherent cyclone movements, which are then forwarded to a final tracking step. In contrast to previous methods our method requires only a few intuitive parameters. An integration into an exploratory framework helps in the study of cyclone movement by identifying smooth, representative tracks. Multiple case studies demonstrate the effectiveness of the method in tracking cyclones, both in the northern and southern hemisphere.", "uri": "https://vimeo.com/289789170", "name": "[VIS18 Preview] An Exploratory Framework for Cyclone Identification and Tracking (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:31:03+00:00", "description": "Authors: Luis Gustavo Nonato and Michael Aupetit\n\nAbstract: Visual analysis of multidimensional data requires effective ways to reduce data dimensionality to encode them visually. Multidimensional projections (MDP) figure among the most important visualization techniques in this context, transforming multidimensional data into scatter plots where patterns reflect some notion of similarity in the data. However, MDP come with distortions that make visual patterns not trustworthy. Moreover, the patterns present in scatter plots might not be enough to allow an understanding of multidimensional data, motivating the development of layout enrichment methodologies that operate with MDP. This survey attempts to cover the main aspects of MDP as a visualization and visual analytic tool, providing detailed analysis and taxonomies taht organize MDP techniques according to their main properties and traits. The survey also approaches the different types of distortions that can result from MDP mappings while overviewing existing mechanisms to quantitatively evaluate such distortions. A qualitative analysis of the impact of distortions on the different analytic tasks is also presented, providing guidelines for users to choose a proper MDP for an intended. Finally, layout enrichment schemes to debunk MDP distortions and/or reveal relevant information not directly inferable from the scatter plot are reviewed and discussed in the light of new taxonomies.", "uri": "https://vimeo.com/289789153", "name": "[VIS18 Preview] Multidimensional Projection for Visual Analytics: Linking Techniques with Distortions, Tasks, and Layout...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:30:54+00:00", "description": "Authors: Stefan Bruckner, Tobias Isenberg, Timo Ropinski, and Alexander Wiebel\n\nAbstract: We discuss the concept of directness in the context of spatial interaction with visualization. In particular, we propose a model that allows practitioners to analyze and describe the spatial directness of interaction techniques, ultimately to be able to better understand interaction issues that may affect usability. To reach these goals, we distinguish between different types of directness. Each type of directness depends on a particular mapping between different spaces, for which we consider the data space, the visualization space, the output space, the user space, the manipulation space, and the interaction space. In addition to the introduction of the model itself, we also show how to apply it to several real-world interaction scenarios in visualization, and thus discuss the resulting types of spatial directness, without recommending either more direct or more indirect interaction techniques. In particular, we will demonstrate descriptive and evaluative usage of the proposed model, and also briefly discuss its generative usage.", "uri": "https://vimeo.com/289789140", "name": "[VIS18 Preview] A Model of Spatial Directness in Interactive Visualization (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:30:44+00:00", "description": "Authors: Oleg Igouchkine, Yubo Zhang, Kwan-Liu Ma\n\nAbstract: Rendering techniques that increase realism in volume visualization help enhance perception of the 3D features in the volume data. While techniques focusing on high-quality global illumination have been extensively studied, few works handle the interaction of light with materials in the volume. Existing techniques for light-material interaction are limited in their ability to handle high-frequency real-world material data, and the current treatment of volume data poorly supports the correct integration of surface materials. In this paper, we introduce an alternative definition for the transfer function which supports surface-like behavior at the boundaries between volume components and volume-like behavior within. We show that this definition enables multi-material rendering with high-quality, real-world material data. We also show that this approach offers an efficient alternative to pre-integrated rendering through isosurface techniques. We introduce arbitrary spatially-varying materials to achieve better multi-material support for scanned volume data. Finally, we show that it is possible to map an arbitrary set of parameters directly to a material representation for the more intuitive creation of novel materials.", "uri": "https://vimeo.com/289789120", "name": "[VIS18 Preview] Multi-Material Volume Rendering with a Physically-Based Surface Reflection Model (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:30:31+00:00", "description": "Authors: Jun Tao, Chaoli Wang, Nitesh Chawla, Lei Shi, Seung Hyun Kim\n\nAbstract: Visual exploration of flow fields is important for studying dynamic systems. We introduce semantic flow graph (SFG), a novel graph representation and interaction framework that enables users to explore the relationships among key objects (i.e., field lines, features, and spatiotemporal regions) of both steady and unsteady flow fields. The objects and their relationships are organized as a heterogeneous graph. We assign each object a set of attributes, based on which a semantic abstraction of the heterogeneous graph is generated. This semantic abstraction is SFG. We design a suite of operations to explore the underlying flow fields based on this graph representation and abstraction mechanism. Users can flexibly reconfigure SFG to examine the relationships among groups of objects at different abstraction levels. Three linked views are developed to display SFG, its node split criteria and history, and the objects in the spatial volume. For simplicity, we introduce SFG construction and exploration for steady flow fields with critical points being the only features. Then we demonstrate that SFG can be naturally extended to deal with unsteady flow fields and multiple types of features. We experiment with multiple data sets and conduct an expert evaluation to demonstrate the effectiveness of our approach.", "uri": "https://vimeo.com/289789101", "name": "[VIS18 Preview] Semantic Flow Graph: A Framework for Discovering Object Relationships in Flow Fields (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:30:19+00:00", "description": "Authors: G.E. Marai, C. Ma, A. Burks, F. Pellolio, G. Canahuate, D. Vock, A.S.R. Mohamed, C.D. Fuller\n\nAbstract: We present the design and evaluation of an integrated problem solving environment for cancer therapy analysis. The environment intertwines a statistical martingale model and a K Nearest Neighbor approach with visual encodings, including novel interactive nomograms, in order to compute and explain a patient's probability of survival as a function of similar patient results. A coordinated views paradigm enables exploration of the multivariate, heterogeneous and few-valued data from a large head and neck cancer repository. A visual scaffolding approach further enables users to build from familiar representations to unfamiliar ones. Evaluation with domain experts show how this visualization approach and set of streamlined workflows enable the systematic and precise analysis of a patient prognosis in the context of cohorts of similar patients. We describe the design lessons learned from this successful, multi-site remote collaboration.", "uri": "https://vimeo.com/289789079", "name": "[VIS18 Preview] Precision Risk Analysis of Cancer Therapy with Interactive Nomograms and Survival Plots (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:30:08+00:00", "description": "Authors: J\u00fcrgen Bernard, David Sessler, J\u00f6rn Kohlhammer, and Roy A. Ruddle\n\nAbstract: In this design study, we present a visualization technique that segments patients\u2019 histories instead of treating them as raw event sequences, aggregates the segments using criteria such as the whole history or treatment combinations, and then visualizes the aggregated segments as static dashboards that are arranged in a dashboard network to show longitudinal changes. The static dashboards were developed in nine iterations, to show 15 important attributes from the patients\u2019 histories. The final design was evaluated with five non-experts, five visualization experts and four medical experts, who successfully used it to gain an overview of a 2,000 patient dataset, and to make observations about longitudinal changes and differences between two cohorts. The research represents a step-change in the detail of large-scale data that may be successfully visualized using dashboards, and provides guidance about how the approach may be generalized.", "uri": "https://vimeo.com/289789053", "name": "[VIS18 Preview] Using Dashboard Networks to Visualize Multiple Patient Histories: A Design Study on Post-operative Prostate...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:29:56+00:00", "description": "Authors: Tanja Blascheck, Lindsay MacDonald Vermeulen, Jo Vermeulen, Charles Perin, Wesley Willett, Thomas Ertl and Sheelagh Carpendale\n\nAbstract: We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization\u2019s functionality.", "uri": "https://vimeo.com/289789025", "name": "[VIS18 Preview] Exploration Strategies for Discovery of Interactivity in Visualizations (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:29:45+00:00", "description": "Authors: Yunhai Wang, Zeyu Wang, Lifeng Zhu, Jian Zhang, Chi-Wing Fu, Changhe Tu and Baoquan Chen\n\nAbstract: The aspect ratio of a line chart heavily influences the perception of the underlying data. Different methods explore different criteria in choosing aspect ratios, but so far, it was still unclear how to select aspect ratios appropriately for any given data. This paper provides a guideline for the user to choose aspect ratios for any input 1D curves by conducting an in-depth analysis of aspect ratio selection methods both theoretically and experimentally. By formulating several existing methods as line integrals, we explain their parameterization invariance. Moreover, we derive a new and improved aspect ratio selection method, namely the L1-LOR (local orientation resolution), with a certain degree of parameterization invariance. Furthermore, we connect different methods, including AL (arc length based method), the banking to 45 principle, RV (resultant vector) and AS (average absolute slope), as well as L1-LOR and AO (average absolute orientation). We verify these connections by a comparative evaluation involving various data sets, and show that the selections by RV and L1-LOR are complementary to each other for most data. Accordingly, we propose the dual-scale banking technique that combines the strengths of RV and L1-LOR, and demonstrate its practicability using multiple real-world data sets.", "uri": "https://vimeo.com/289789006", "name": "[VIS18 Preview] Is There a Robust Technique for Selecting Aspect Ratios in Line Charts? (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:29:35+00:00", "description": "Authors: Rafael Ballester-Ripoll and Renato Pajarola\n\nAbstract: Histograms are a fundamental tool for multidimensional data analysis and processing, and many applications in graphics and visualization rely on computing histograms over large regions of interest (ROI). Integral histograms (IH) greatly accelerate the calculation in the case of rectangular regions, but come at a large extra storage cost. Based on the tensor train decomposition model, we propose a new compression and approximate retrieval algorithm to reduce the overall IH memory usage by several orders of magnitude at a user-defined accuracy. To this end we propose an incremental tensor decomposition algorithm that allows us to compress integral histograms of hundreds of gigabytes. We then encode the borders of any desired rectangular ROI in the IH tensor-compressed domain and reconstruct the target histogram at a high speed which is independent of the region size. We furthermore generalize the algorithm to support regions of arbitrary shape rather than only rectangles, as well as histogram field computation, i.e. recovering many histograms at once. We test our method with several multidimensional data sets and demonstrate that it radically speeds up costly histogram queries while avoiding storing massive, uncompressed IHs.", "uri": "https://vimeo.com/289788992", "name": "[VIS18 Preview] Tensor Decompositions for Integral Histogram Compression and Look-Up (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:29:25+00:00", "description": "Authors: Markus Wagner, Djordje Slijepcevic, Brian Horsak, Alexander Rind, Matthias Zeppelzauer, and Wolfgang Aigner\n\nAbstract: In 2014, more than 10 million people in the US were affected by an ambulatory disability. Thus, gait rehabilitation is a crucial part of health care systems. The quantification of human locomotion enables clinicians to describe and analyze a patient\u2019s gait performance in detail and allows them to base clinical decisions on objective data. These assessments generate a vast amount of complex data which need to be interpreted in a short time period. We conducted a design study in cooperation with gait analysis experts to develop a novel Knowledge-Assisted Visual Analytics solution for clinical Gait analysis (KAVAGait). KAVAGait allows the clinician to store and inspect complex data derived during clinical gait analysis. The system incorporates innovative and interactive visual interface concepts, which were developed based on the needs of clinicians. Additionally, an explicit knowledge store (EKS) allows externalization and storage of implicit knowledge from clinicians. It makes this information available for others, supporting the process of data inspection and clinical decision making. We validated our system by conducting expert reviews, a user study, and a case study. Results suggest that KAVAGait is able to support a clinician during clinical practice by visualizing complex gait data and providing knowledge of other clinicians.", "uri": "https://vimeo.com/289788975", "name": "[VIS18 Preview] KAVAGait: Knowledge-Assisted Visual Analytics for Clinical Gait Analysis (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:29:15+00:00", "description": "Authors: Shenghui Cheng, Wei Xu and Klaus Mueller\n\nAbstract: A wide variety of color schemes have been devised for mapping scalar data to color. We address the challenge of color-mapping multivariate data. While a number of methods can map low-dimensional data to color, for example, using bilinear or barycentric interpolation for two or three variables, these methods do not scale to higher data dimensions. Likewise, schemes that take a more artistic approach through color mixing and the like also face limits when it comes to the number of variables they can encode. Our approach does not have these limitations. It is data driven in that it determines a proper and consistent color map from first embedding the data samples into a circular interactive multivariate color mapping display (ICD) and then fusing this display with a convex (CIE HCL) color space. The variables (data attributes) are arranged in terms of their similarity and mapped to the ICD\u2019s boundary to control the embedding. Using this layout, the color of a multivariate data sample is then obtained via modified generalized barycentric coordinate interpolation of the map. The system we devised has facilities for contrast and feature enhancement, supports both regular and irregular grids, can deal with multi-field as well as multispectral data, and can produce heat maps, choropleth maps, and diagrams such as scatterplots.", "uri": "https://vimeo.com/289788955", "name": "[VIS18 Preview] ColorMapND: A Data-Driven Approach and Tool for Mapping Multivariate Data to Color (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:29:04+00:00", "description": "Authors: Yunhai Wang, Kang Feng, Xiaowei Chu, Jian Zhang, Chi-Wing Fu, Michael Sedlmair, Xiaohui Yu, and Baoquan Chen\n\nAbstract: Dimensionality reduction (DR) is a common strategy for visual analysis of labeled high-dimensional data. Low-dimensional representations of the data help, for instance, to explore the class separability and the spatial distribution of the data. Widely-used unsupervised DR methods like PCA do not aim to maximize the class separation, while supervised DR methods like LDA often assume certain spatial distributions and do not take perceptual capabilities of humans into account. These issues make them ineffective for complicated class structures. Towards filling this gap, we present a perception-driven linear dimensionality reduction approach that maximizes the perceived class separation in projections. Our approach builds on recent developments in perception-based separation measures that have achieved good results in imitating human perception. We extend these measures to be density-aware and incorporate them into a customized simulated annealing algorithm, which can rapidly generate a near optimal DR projection. We demonstrate the effectiveness of our approach by comparing it to state-of-the-art DR methods on 93 datasets, using both quantitative measure and human judgments. We also provide case studies with class-imbalanced and unlabeled data.", "uri": "https://vimeo.com/289788938", "name": "[VIS18 Preview] A Perception-Driven Approach to Supervised Dimensionality Reduction for Visualization (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:28:53+00:00", "description": "Authors: Yuan Liang, Xiting Wang, Song-Hai Zhang, Shi-Min Hu, Shixia Liu\n\nAbstract: We present a visual analysis method for interactively recomposing a large number of photos based on example photos with high-quality composition. The recomposition method is formulated as a matching problem between photos. The key to this formulation is a new metric for accurately measuring the composition distance between photos. We have also developed an online earth-mover-distance-based metric learning algorithm to support the interactive adjustment of the composition distance based on user preferences. To better convey the compositions of a large number of example photos, we have developed a multi-level, example photo layout method to balance three factors: aspect ratio, composition similarity, and stability. By introducing an energy-based straightening method, the composition of each photos is clearly displayed. The effectiveness and usefulness of the method has been demonstrated by the experimental results, user study, and case studies.", "uri": "https://vimeo.com/289788924", "name": "[VIS18 Preview] PhotoRecomposer: Interactive Photo Recomposition by Cropping (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:28:41+00:00", "description": "Authors: Yingcai Wu, Zhutian Chen, Guodao Sun, Xiao Xie, Nan Cao, Shixia Liu, Weiwei Cui\n\nAbstract: Analyzing social streams is important for many applications, such as crisis management. However, the considerable diversity, increasing volume, and high dynamics of social streams of large events continue to be significant challenges that must be overcome to ensure effective exploration. We propose a novel framework by which to handle complex social streams on a budget PC. This framework features two components: 1) an online method to detect important time periods (i.e., subevents), and 2) a tailored GPU-assisted Self-Organizing Map (SOM) method, which clusters the tweets of subevents stably and efficiently. Based on the framework, we present StreamExplorer to facilitate the visual analysis, tracking, and comparison of a social stream at three levels. At a macroscopic level, StreamExplorer uses a new glyph-based timeline visualization, which presents a quick multi-faceted overview of the ebb and flow of a social stream. At a mesoscopic level, a map visualization is employed to visually summarize the social stream from either a topical or geographical aspect. At a microscopic level, users can employ interactive lenses to visually examine and explore the social stream from different perspectives. Two case studies and a task-based evaluation are used to demonstrate the effectiveness and usefulness of StreamExplorer.", "uri": "https://vimeo.com/289788908", "name": "[VIS18 Preview] StreamExplorer: A Multi-Stage System for Visually Exploring Events in Social Streams (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:28:19+00:00", "description": "Authors: Yafeng Lu, Hong Wang, Steven Landis, and Ross Maciejewski\n\nAbstract: Media data has been the subject of large scale analysis with applications of text mining being used to provide overviews of media themes and information flows. Such information extracted from media articles has also shown its contextual value of being integrated with other data, such as criminal records and stock market pricing. In this work, we explore linking textual media data with curated secondary textual data sources through user-guided semantic lexical matching for identifying relationships and data links. In this manner, critical information can be identified and used to annotate media timelines in order to provide a more detailed overview of events that may be driving media topics and frames. These linked events are further analyzed through an application of causality modeling to model temporal drivers between the data series. Such causal links are then annotated through automatic entity extraction which enables the analyst to explore persons, locations, and organizations that may be pertinent to the media topic of interest. To demonstrate the proposed framework, two media datasets and an armed conflict event dataset are explored.", "uri": "https://vimeo.com/289788885", "name": "[VIS18 Preview] A Visual Analytics Framework for Identifying Topic Drivers in Media Events (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:28:08+00:00", "description": "Authors: Yulong Guo, Xiaopei Liu, Chi Xiong, Xuemiao Xu, Chi-Wing Fu\n\nAbstract: Superfluidity is a special state of matter exhibiting macroscopic quantum phenomena and acting like a fluid with zero viscosity. In such a state, superfluid vortices exist as phase singularities of the model equation with unique distributions. This paper presents novel techniques to aid the visual understanding of superfluid vortices based on the state-of-the-art non-linear Klein-Gordon equation, which evolves a complex scalar field, giving rise to special vortex lattice/ring structures with dynamic vortex formation, reconnection, and Kelvin waves, etc. By formulating a numerical model with theoretical physicists in superfluid research, we obtain high-quality superfluid flow data sets without noise-like waves, suitable for vortex visualization. By further exploring superfluid vortex properties, we develop a new vortex identification and visualization method: a novel mechanism with velocity circulation to overcome phase singularity and an orthogonal-plane strategy to avoid ambiguity. Hence, our visualizations can help reveal various superfluid vortex structures and enable domain experts for related visual analysis, such as the steady vortex lattice/ring structures, dynamic vortex string interactions with reconnections and energy radiations, where the famous Kelvin waves and decaying vortex tangle were clearly observed. These visualizations have assisted physicists to verify the superfluid model, and further explore its dynamic behavior more intuitively.", "uri": "https://vimeo.com/289788863", "name": "[VIS18 Preview] Towards High-quality Visualization of Superfluid Vortices (TVCG Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:27:53+00:00", "description": "Authors: Yunhai Wang, Fubo Han, Lifeng Zhu, Oliver Deussen, Baoquan Chen\n\nAbstract: Line graphs are usually considered to be the best choice for visualizing time series data, whereas sometimes also scatter plots are used for showing main trends. So far there are no guidelines that indicate which of these visualization methods better display trends in time series for a given canvas. Assuming that the main information in a time series is its overall trend, we propose an algorithm that automatically picks the visualization method that reveals this trend best. This is achieved by measuring the visual consistency between the trend curve represented by a LOESS fit and the trend described by a scatter plot or a line graph. To measure the consistency between our algorithm and user choices, we performed an empirical study with a series of controlled experiments that show a large correspondence. In a factor analysis we furthermore demonstrate that various visual and data factors have effects on the preference for a certain type of visualization.", "uri": "https://vimeo.com/289788836", "name": "[VIS18 Preview] Line Graph or Scatter Plot? Automatic Selection of Methods for Visualizing Trends in Time Series (TVCG...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:18:40+00:00", "description": "Authors: Huan He, Qinghua Zheng, Bo Dong\n\nAbstract: Online Distance Education (ODE) provides massive course videos of various specialties for students across the country to learn professional knowledge anytime and anywhere. Analyzing the utilization of these videos from user log data can help academics better understand the learning process of students, evaluate the quality of service provided by regional learning centers, and improve the quality of program curriculum in the future. However, due to the lack of comparable indicators, it is a great challenge to discover the utilization patterns of massive videos and analyze the learning process of large-scale student population from learning log data. In this paper, we introduce a visual analytics system, called VUSphere, to explore the video utilization from multiple perspectives with two proposed indicators. This system offers three coordinated views: a spherical layout overview to depict the overall utilization distribution of videos, courses, and students; a detailed statistics view with four panels to present video utilization statistics of each element from multiple perspectives; and a comparison view to examine the differences in individual elements. Based on the real dataset from our ODE school, several patterns related to video utilization and enrollment are found in the case study with our domain experts.", "uri": "/channels/721847https://vimeo.com/289788052", "name": "[VIS18 Preview] VUSphere: Visual Analysis of Video Utilization in Online Distance Education (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:18:24+00:00", "description": "Authors: Po-Ming Law, Yanhong Wu, Rahul Basole\n\nAbstract: Getting the overall picture of how a large number of ego-networks evolve is a common yet challenging task. Existing techniques often require analysts to inspect the evolution patterns of individual ego-network one after another. In this study, we explore an approach that allows analysts to interactively create spatial layouts in which each dot is a dynamic ego-network. These spatial layouts provide overviews of the evolution patterns of ego-networks, thereby revealing different global patterns such as trends, clusters and outliers in evolution patterns. To let analysts interactively construct interpretable spatial layouts, we propose a data transformation pipeline, with which analysts can adjust the spatial layouts and convert dynamic ego-networks into event sequences to aid interpretations of the spatial positions. Based on this transformation pipeline, we develop Segue, a visual analysis system that supports thorough exploration of the evolution patterns of ego-networks. Through two usage scenarios, we demonstrate how analysts can gain insights into the overall evolution patterns of a large collection of ego-networks by interactively creating different spatial layouts.", "uri": "https://vimeo.com/289788023", "name": "[VIS18 Preview] Segue: Overviewing Evolution Patterns of Egocentric Networks by Interactive Construction of Spatial Layouts...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:18:11+00:00", "description": "Authors: Quan Li, Kristanto Sean Njotoprawiro, Hammad Haleem, Qiaoan Chen, Chris YI, Xiaojuan Ma\n\nAbstract: Constructing latent vector representation for nodes in a network through embedding models has shown its practicality in many graph analysis applications, such as node classification, clustering, and link prediction. However, despite the high efficiency and accuracy of learning an embedding model, people have little clue of what information about the original network is preserved in the embedding vectors. The abstractness of low-dimensional vector representation, stochastic nature of the construction process, and non-transparent hyper-parameters all obscure understanding of network embedding results. Visualization techniques have been introduced to facilitate embedding vector inspection, usually by projecting the embedding space to a two-dimensional display. Although the existing visualization methods allow simple examination of the structure of embedding space, they cannot support in-depth exploration of the embedding vectors. In this paper, we design an exploratory visual analytics system that supports the comparative visual interpretation of embedding vectors at the cluster, instance, and structural levels. To be more specific, it facilitates comparison of what and how node metrics are preserved across different embedding models and investigation of relationships between node metrics and selected embedding vectors. Several case studies confirm the efficacy of our system. Experts' feedback suggests that our approach indeed helps them better embrace the understanding of network embedding models.", "uri": "https://vimeo.com/289788000", "name": "[VIS18 Preview] EmbeddingVis: A Visual Analytics Approach to Comparative Network Embedding Inspection (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:17:56+00:00", "description": "Authors: Daniel Orban, Daniel Keefe, Ayan Biswas, James Ahrens, David Rogers\n\nAbstract: We present a direct manipulation technique that allows material scientists to interactively highlight relevant parameterized simulation instances located in dimensionally reduced spaces, enabling a user-defined understanding of a continuous parameter space. Our goals are two-fold: first, to build a user-directed intuition of dimensionally reduced data, and second, to provide a mechanism for creatively exploring parameter relationships in parameterized simulation sets, called ensembles. We start by visualizing ensemble data instances in dimensionally reduced scatter plots. To understand these abstract views, we employ user-defined virtual data instances that, through direct manipulation, search an ensemble for similar instances. Users can create multiple of these direct manipulation queries to visually annotate the spaces with sets of highlighted ensemble data instances. User-defined goals are therefore translated into custom illustrations that are projected onto the dimensionally reduced spaces. Combined forward and inverse searches of the parameter space follow naturally allowing for continuous parameter space prediction and visual query comparison in the context of an ensemble. The potential for this visualization technique is confirmed via expert user feedback for a shock physics application and synthetic model analysis.", "uri": "https://vimeo.com/289787986", "name": "[VIS18 Preview] Drag and Track: A Direct Manipulation Interface for Contextualizing Data Instances within a Continuous...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:17:45+00:00", "description": "Authors: Sara Alspaugh, Nava Zokaei, Andrea Liu, Cindy Jin, Marti Hearst\n\nAbstract: We report the results of interviewing thirty professional data analysts working in a range of industrial, academic, and regulatory environments. This study focuses on participants\u2019 descriptions of exploratory activities and tool usage in these activities. Highlights of the findings include distinctions between exploration as a precursor to more directed analysis versus truly open-ended exploration, confirmation that some analysts see \u201cfinding something interesting\u201d as a valid goal of data exploration while others explicitly disavow this goal, conflicting views about the role of intelligent tools in data exploration, and pervasive use of visualization for exploration, but only a subset using direct manipulation interfaces. These findings provide guidelines for future tool development, as well as a better understanding of the meaning of the term \u201cdata exploration\u201d based on the words of practitioners \u201cin the wild.\u201d", "uri": "https://vimeo.com/289787961", "name": "[VIS18 Preview] Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:17:34+00:00", "description": "Authors: Bum Chul Kwon, Min-Je Choi, Joanne Kim, Edward Choi, Young Bin Kim, Soonwook Kwon, Jimeng Sun, Jaegul Choo\n\nAbstract: In the past decade, we have seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients\u2019 diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often very challenging for users to understand why the model makes a particular prediction. Such black box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established method to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model.Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a recently proposed, interpretable RNN-based model called RETAIN and visualizations for users\u2019 exploration of EMR data in the context of prediction tasks.Our study shows the effective use of RetainVis for gaining insights into how RNN models EMR data, using real medical records of patients with heart failure, cataract, or dermatological symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers who aim to design more interpretable and interactive visual analytics tool for RNNs.", "uri": "https://vimeo.com/289787946", "name": "[VIS18 Preview] RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Network on Electronic...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:17:24+00:00", "description": "Honorable Mention\n\nAuthors: Cong Xie, Wei Xu, Klaus Mueller\n\nAbstract: Anomalous runtime behavior detection is one of the most important tasks for performance diagnosis in High Performance Computing (HPC). Most of the existing methods find anomalous executions based on the properties of individual functions, such as execution time. However, it is insufficient to identify abnormal behavior without taking into account the context of the executions, such as the invocations of children functions and the communications with other HPC nodes. We improve upon the existing anomaly detection approaches by utilizing the call stack structures of the executions, which record rich temporal and contextual information. With our call stack tree (CSTree) representation of the executions, we formulate the anomaly detection problem as finding anomalous tree structures in a call stack forest. The CSTrees are converted to vector representations using our proposed stack2vec embedding. Structural and temporal visualizations of CSTrees are provided to support users in the identification and verification of the anomalies during an active anomaly detection process. Three case studies of real-world HPC applications demonstrate the capabilities of our approach.", "uri": "https://vimeo.com/289787924", "name": "[VIS18 Preview] A Visual Analytics Framework for the Detection of Anomalous Call Stack Trees in High Performance Computing...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:17:15+00:00", "description": "Authors: Min Chen, Kelly Gaither, Nigel John, Brian McCann\n\nAbstract: Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theatre-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.", "uri": "https://vimeo.com/289787910", "name": "[VIS18 Preview] Cost-benefit Analysis of Visualization in Virtual Environments (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:17:01+00:00", "description": "Authors: Ying Zhao, Feng Luo, Minghui Chen, Yingchao Wang, Jiazhi Xia, Fangfang Zhou, Yunhai Wang, Yi Chen, Wei Chen\n\nAbstract: Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.", "uri": "https://vimeo.com/289787891", "name": "[VIS18 Preview] Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:16:51+00:00", "description": "Authors: Zhicong Lu, Mingming Fan, Yun Wang, Jian Zhao, Michelle Annett, Daniel Wigdor\n\nAbstract: Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners\u2019 needs and experts\u2019 recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget\u2014 NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.", "uri": "https://vimeo.com/289787883", "name": "[VIS18 Preview] InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:16:37+00:00", "description": "Authors: Po-Ming Law, Zhicheng Liu, Sana Malik, Rahul Basole\n\nAbstract: Exploring event sequences by defining queries alone or by using mining algorithms alone is often not sufficient to support analysis. Analysts often interweave querying and mining in a recursive manner during event sequence analysis: sequences extracted as query results are used for mining patterns, patterns generated are incorporated into a new query for segmenting the sequences, and the resulting segments are mined or queried again. To support flexible analysis, we propose a framework that describes the process of interwoven querying and mining. Based on this framework, we developed MAQUI, a Mining And Querying User Interface that enables recursive event sequence exploration. To understand the efficacy of MAQUI, we conducted two case studies with domain experts. The findings suggest that the capability of interweaving querying and mining helps the participants articulate their questions and gain novel insights from their data.", "uri": "https://vimeo.com/289787863", "name": "[VIS18 Preview] MAQUI: Interweaving Queries and Pattern Mining for Recursive Event Sequence Exploration (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:16:25+00:00", "description": "Authors: Po-Ming Law, Rahul Basole, Yanhong Wu\n\nAbstract: Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends the similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.", "uri": "https://vimeo.com/289787850", "name": "[VIS18 Preview] Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:16:16+00:00", "description": "Authors: Alexander Kumpf, Marc Rautenhaus, Michael Riemer, R\u00fcdiger Westermann\n\nAbstract: Ensemble sensitivity analysis (ESA) has been established in the atmospheric sciences as a correlation-based approach to determine the sensitivity of a scalar forecast quantity computed by a numerical weather prediction model on changes in another model variable at an different model state. Its applications include determining the origin of forecast errors and placing targeted observations to improve future forecasts. We---a team of visualization scientists and meteorologists---present a visual analysis framework to improve upon current practice of ESA. We support the user in selecting regions to compute a meaningful target forecast quantity by embedding correlation-based grid-point clustering to obtain statistically coherent regions. The evolution of sensitivity features computed via ESA are then traced through time, by integrating a quantitative measure of feature matching into optical-flow-based feature assignment, and displayed by means of a swipe-path showing the geo-spatial evolution of the sensitivities. Visualization of the internal correlation structure of computed features guides the user towards those features robustly predicting a certain weather event. We demonstrate the use of our method by application to real-world 2D and 3D cases that occurred during the 2016 NAWDEX field campaign, showing the interactive generation of hypothesis chains to explore how atmospheric processes sensitive to each other are interrelated.", "uri": "https://vimeo.com/289787833", "name": "[VIS18 Preview] Visual Analysis of the Temporal Evolution of Ensemble Forecast Sensitivities (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:16:04+00:00", "description": "Authors: Dominik Sacha, Matthias Kraus, Daniel Keim, Min Chen\n\nAbstract: While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely \u201cVA-assisted ML\u201d. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.", "uri": "https://vimeo.com/289787814", "name": "[VIS18 Preview] VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:15:53+00:00", "description": "Authors: Minsuk Kahng, Nikhil Thorat, Duen Horng Chau, Fernanda Viegas, Martin Wattenberg\n\nAbstract: Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.", "uri": "https://vimeo.com/289787794", "name": "[VIS18 Preview] GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:15:28+00:00", "description": "Authors: John Wenskovitch, Lauren Bradel, Michelle Dowling, Leanna House, Chris North\n\nAbstract: Completing text analysis tasks is a continuous sensemaking loop of foraging for information and incrementally synthesizing it into hypotheses. Past research has shown the advantages of using spatial workspaces as a means for synthesizing information through externalizing hypotheses and creating spatial schemas. However, spatializing the entirety of datasets becomes prohibitive as the number of documents available to the analysts grows, particularly when only a small subset are relevant to the task at hand. StarSPIRE is a visual analytics tool designed to explore collections of documents, leveraging users' semantic interactions to steer (1)~a synthesis model that aids in document layout, and (2)~a foraging model to automatically retrieve new relevant information. In contrast to traditional keyword-search foraging (KSF), \"semantic interaction foraging\" (SIF) occurs as a result of the user's synthesis actions. To quantify the value of semantic interaction foraging, we use StarSPIRE to evaluate its utility for an intelligence analysis sensemaking task. Semantic interaction foraging accounted for 26% of useful documents found, and it also resulted in increased synthesis interactions and improved sensemaking task performance by users in comparison to only using keyword-search.", "uri": "https://vimeo.com/289787752", "name": "[VIS18 Preview] The Effect of Semantic Interaction on Foraging in Text Analysis (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:15:13+00:00", "description": "Authors: Michael Blumenschein, Michael Behrisch, Stefanie Schmid, Simon Butscher, Deborah R. Wahl, Karoline Villinger, Britta Renner, Harald Reiterer, Daniel Keim\n\nAbstract: We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst\u2019s trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.", "uri": "https://vimeo.com/289787735", "name": "[VIS18 Preview] SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:15:03+00:00", "description": "Authors: Marlen Promann, Sabine Brunswicker\n\nAbstract: Social data charts \u2013 visual presentations of quantitative data about peer behaviors \u2013 may offer new means to motivate individuals to participate in group goals. However, to do so these charts need to create a semantic response of \u2018unity\u2019 among the chart viewers in order to overcome the problem of social loafing where people follow their self-interest. In this paper, we focus on two properties of a social data chart that may affect a viewer\u2019s perceptions of unity: (1) The skewness in the data structure \u2013 the statistical distribution of the social data, and (2) the proximity in the visual structure of the chart \u2013 the spatial organization of the data points. We performed a controlled perceptual experiment to examine the effect of proximity and skewness on four different semantic facets of perceived group unity: similarity, entitativity, rapport, and centrality. We exposed 179 workers on Amazon Mechanical Turk to different group charts using a 2x2 factorial design, varying both proximity and skewness. Our two-way ANCOVA analyses reveal three important findings: (1) Across all conditions, proximity has a strong positive effect on perceived group unity and conveys a social meaning of group entitativity, as well as member similarity and rapport (2) Skewness and proximity interact in a non-linear way, suggesting that skewness creates a negative force that modifies the semantic responses to higher proximity (3) the perceptual responses to proximity have different semantic facets that are either stable or sensitive to skewness. These findings contribute to perceptual as well as social InfoVis literature.", "uri": "https://vimeo.com/289787716", "name": "[VIS18 Preview] The Effect of Proximity in Social Data Charts on Perceived Unity (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:14:53+00:00", "description": "Authors: Mengchen Liu, Shixia Liu, Hang Su, Kelei Cao, Jun Zhu\n\nAbstract: Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.", "uri": "https://vimeo.com/289787703", "name": "[VIS18 Preview] Analyzing the Noise Robustness of Deep Neural Networks (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:14:42+00:00", "description": "Authors: Juri Buchmuller, Dominik Jackle, Eren Cakmak, Ulrik Brandes, Daniel Keim\n\nAbstract: Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatio temporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview on the development of group movements over time and thus, enables analysts to visually identify and explore group specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.", "uri": "https://vimeo.com/289787684", "name": "[VIS18 Preview] MotionRugs: Visualizing Collective Trends in Space and Time (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:14:30+00:00", "description": "Authors: Michelle Dowling, John Wenskovitch, J.T. Fry, Leanna House, Scotland Leman, Chris North\n\nAbstract: Much research has been done regarding how to visualize and interact with observations and attributes of high-dimensional data for exploratory data analysis. From the analyst\u2019s perceptual and cognitive perspective, current visualization approaches typically treat the observations of the high-dimensional dataset very differently from the attributes. Often, the attributes are treated as inputs (e.g., sliders), and observations as outputs (e.g., projection plots), thus emphasizing investigation of the observations. However, there are many cases in which analysts wish to investigate both the observations and the attributes of the dataset, suggesting a symmetry between how analysts think about attributes and observations. To address this, we define SIRIUS (Symmetric Interactive Representations In a Unified System), a symmetric, dual projection technique to support exploratory data analysis of high-dimensional data. We provide an example implementation of SIRIUS and demonstrate how this symmetry affords additional insights.", "uri": "https://vimeo.com/289787667", "name": "[VIS18 Preview] SIRIUS: Dual, Symmetric, Interactive Dimension Reductions (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:14:18+00:00", "description": "Honorable Mention\n\nAuthors: Hendrik Strobelt, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pfister, Alexander M. Rush\n\nAbstract: Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and \"what if\"-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.", "uri": "https://vimeo.com/289787650", "name": "[VIS18 Preview] Seq2Seq-Vis: A Visual Debugging Tool for Sequence to Sequence Models (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:14:07+00:00", "description": "Authors: John Goodall, Eric Ragan, Chad Steed, Joel Reed, Gregory Richardson, Kelly Huffer, Robert Bridges, Jason Laska\n\nAbstract: Despite the best efforts of cyber security analysts, networked computing assets are routinely compromised, resulting in the loss of intellectual property, the disclosure of state secrets, and major financial damages. Anomaly detection methods are beneficial for detecting new types of attacks and abnormal network activity, but such algorithms can be difficult to understand and trust. Network operators and cyber analysts need fast and scalable tools to help identify suspicious behavior that bypasses automated security systems, but operators do not want another automated tool with algorithms they do not trust. Experts need tools to augment their own domain expertise and to provide a contextual understanding of suspicious behavior to help them make decisions. In this paper we present Situ, a visual analytics system for discovering suspicious behavior in streaming network data. Situ provides a scalable solution that combines anomaly detection with information visualization. The system's visualizations enable operators to identify and investigate the most anomalous events and IP addresses, and the tool provides context to help operators understand why they are anomalous. Finally, operators need tools that can be integrated into their workflow and with their existing tools. This paper describes the Situ platform and its deployment in an operational network setting. We discuss how operators are currently using the tool in a large organization's security operations center and present the results of expert reviews with professionals.", "uri": "https://vimeo.com/289787635", "name": "[VIS18 Preview] Situ: Identifying and explaining suspicious behavior in networks (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:13:56+00:00", "description": "Authors: Marco Angelini, Graziano Blasilli, Tiziana Catarci, Simone Lenti, Giuseppe Santucci\n\nAbstract: Vulnerabilities represent one of the main weaknesses of IT systems and the availability of consolidated official data, like CVE (Common Vulnerabilities and Exposures), allows for using them to compute the paths an attacker is likely to follow. However, even if patches are available, business constraints or lack of resources create obstacles to their straightforward application. As a consequence, the security manager of a network needs to deal with a large number of vulnerabilities, making decisions on how to cope with them. This paper presents VULNUS (VULNerabilities visUal aSsessment), a visual analytics solution for dynamically inspecting the vulnerabilities spread on networks, allowing for a quick understanding of the network status and visually classifying nodes according to their vulnerabilities. Moreover, VULNUS computes the approximated optimal sequence of patches able to eliminate all the attack paths and allows for exploring sub-optimal patching strategies, simulating the effect of removing one or more vulnerabilities. VULNUS has been evaluated by domain experts using a lab-test experiment, investigating the effectiveness and efficiency of the proposed solution.", "uri": "https://vimeo.com/289787623", "name": "[VIS18 Preview] Vulnus: Visual Vulnerability Analysis for Network Security (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:13:42+00:00", "description": "Authors: Jarke van Wijk, Marcel van 't Veer, Patrick Houthuizen, Eveline H. J. Mestrom, Erik H.H.M. Korsten, Arthur R.A. Bouwman\n\nAbstract: We present RegressionExplorer, a Visual Analytics tool for the interactive exploration of logistic regression models. Our application domain is clinical biostatistics, where models are derived from patient data with the aim to obtain clinically meaningful insights and consequences. Development and interpretation of a proper model requires domain expertise and insight into model characteristics. Because of time constraints, often a limited number of candidate models is evaluated. RegressionExplorer enables experts to quickly generate, evaluate, and compare many different models, taking the workflow for model development as starting point. Global patterns in parameter values of candidate models can be explored effectively. In addition, experts are enabled to compare candidate models across multiple subpopulations. The insights obtained can be used to formulate new hypotheses or to steer model development. The effectiveness of the tool is demonstrated for two uses cases: prediction of a cardiac conduction disorder in patients after receiving a heart valve implant and prediction of hypernatremia in critically ill patients.", "uri": "https://vimeo.com/289787606", "name": "[VIS18 Preview] RegressionExplorer: Interactive Exploration of Logistic Regression Models with Subgroup Analysis (VAST...", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:13:31+00:00", "description": "Authors: Prithiviraj Kaliappa Gounder Muthumanickam, Katerina Vrotsou, Aida Nordman, Jimmy Johansson, Matthew Cooper\n\nAbstract: Eye-tracking has become an invaluable tool for the analysis of working practices in many technological fields of activity. Typically studies focus on short tasks and use static expected areas of interest (AoI) in the display to explore subjects' behaviour, making the analyst's task quite straightforward. In long-duration studies, where the observations may last several hours over a complete work session, the AoIs may change over time in response to altering workload, emergencies or other variables making the analysis more difficult. This work puts forward a novel method to automatically identify spatial AoIs changing over time through a combination of clustering and cluster merging in the temporal domain. A visual analysis system based on the proposed methods is also presented. Finally, we illustrate our approach within the domain of air traffic control, a complex task sensitive to prevailing conditions over long durations, though it is applicable to other domains such as monitoring of complex systems.", "uri": "https://vimeo.com/289787582", "name": "[VIS18 Preview] Identification of Temporally Varying Areas of Interest in Long Duration Eye Tracking Data Sets (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:13:20+00:00", "description": "Authors: Yingcai Wu, Xiao Xie, Jiachen Wang, Dazhen Deng, Hongye Liang, Hui Zhang, Shoubin Cheng, Wei Chen\n\nAbstract: Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.", "uri": "https://vimeo.com/289787566", "name": "[VIS18 Preview] ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:13:06+00:00", "description": "Authors: Akhilesh Camisetty, Chaitanya Chandurkar, Maoyuan Sun, David Koop\n\nAbstract: Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through Web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many Web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side Web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.", "uri": "https://vimeo.com/289787554", "name": "[VIS18 Preview] Enhancing Web-based Analytics Applications through Provenance (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:12:54+00:00", "description": "Authors: Holger Stitz, Samuel Gratzl, Harald Piringer, Thomas Zichner, Marc Streit\n\nAbstract: Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.", "uri": "https://vimeo.com/289787528", "name": "[VIS18 Preview] KnowledgePearls: Provenance-Based Visualization Retrieval (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:12:45+00:00", "description": "Authors: Jay Koven, Cristian Felix, Hossein Siadati, Enrico Bertini, Markus Jakobsson\n\nAbstract: The forensic investigation of communication datasets which contain unstructured text, social network information, and metadata is a complex task that is becoming more important due to the immense amount of data being collected. Currently there are limited approaches that allow an investigator to explore the network, text and metadata in a unified manner. We developed Beagle as a forensic tool for email datasets that allows investigators to flexibly form complex queries in order to discover important information in email data. Beagle was successfully deployed at a security firm which had a large email dataset that was difficult to properly investigate. We discuss our experience developing Beagle as well as the lessons we learned applying visual analytic techniques to a difficult real-world problem.", "uri": "https://vimeo.com/289787515", "name": "[VIS18 Preview] Lessons Learned Developing a Visual Analytics Solution for Investigative Analysis of Scamming Activities...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:12:34+00:00", "description": "Authors: Shahid Latif, Fabian Beck\n\nAbstract: Publication records and collaboration networks are important for assessing the expertise and experience of researchers. Existing digital libraries show the raw publication lists in author profiles, whereas visualization techniques focus on specific subproblems. Instead, we look at publication records from various perspectives mixing low-level publication data with high-level abstractions and background information. This work presents VIS Author Profiles, a novel approach to generate integrated textual and visual descriptions to highlight patterns in publication records. We leverage template-based natural language generation to summarize notable publication statistics, evolution of research topics, and collaboration relationships. Seamlessly integrated visualizations augment the textual description and are interactively connected with each other and the text. The underlying publication data and detailed explanations of the analysis are available on demand. We compare our approach to existing systems by taking into account information needs of users and demonstrate its usefulness in two realistic application examples.", "uri": "https://vimeo.com/289787504", "name": "[VIS18 Preview] VIS Author Profiles: Interactive Descriptions of Publication Records Combining Text and Visualization (VAST...", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:12:21+00:00", "description": "Authors: Xumeng Wang, Wei Chen, Huihua Guan, Wenlong Chen, Rusheng Pan, Jia-Kai Chou, Chris Bryan, Kwan-Liu Ma\n\nAbstract: Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysiscan also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph\u2019s structure based on hypothesized tactics that an adversary would employ.While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques\u2014along with evaluating how applying the strategies will affect the utility of the anonymized results\u2014remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphProtector, we report several case studies and feedback collected from interviews with expert users in various scenarios.", "uri": "https://vimeo.com/289787471", "name": "[VIS18 Preview] GraphProtector: a Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:12:09+00:00", "description": "Best Paper\n\nAuthors: Dongyu Liu, Panpan Xu, Liu Ren\n\nAbstract: Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which simultaneously slices the data into homogeneous partitions and extracts the latent patterns for each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. The system automatically recommends optimal ways to partition the data for effective pattern discovery. We demonstrate the general applicability and effectiveness of the approach through example usage scenarios with real-world ST data from three application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.", "uri": "https://vimeo.com/289787446", "name": "[VIS18 Preview] TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:11:57+00:00", "description": "Authors: Nicole Sultanum, Devin Singh, Michael Brudno, Fanny Chevalier\n\nAbstract: Before seeing a patient, physicians seek to obtain an overview of the patient\u2019s medical history. Text plays a major role in this activity since it represents the bulk of the clinical documentation, but reviewing it quickly becomes onerous when patient charts grow too large. Text visualization methods have been widely explored to manage this large scale through visual summaries that rely on information retrieval algorithms to structure text and make it amenable to visualization. However, the integration with such automated approaches comes with a number of limitations, including significant error rates and the need for healthcare providers to fine-tune algorithms without expert knowledge of their inner mechanics. In addition, several of these approaches obscure or substitute the original clinical text and therefore fail to leverage qualitative and rhetorical flavours of the clinical notes. These drawbacks have limited the adoption of text visualization and other summarization technologies in clinical practice. In this work we present Doccurate, a novel system embodying a semi-automatic, curation-based approach to the visualization of large clinical text datasets. Our approach enables better automation auditing and customization power to physicians while also preserving and extensively linking to the original text. We present use case scenarios and discuss findings of a formal qualitative evaluation of Doccurate conducted with 6 physicians, which helped shed light onto physician\u2019s information needs, perceived strengths and limitations of automated tools, and the importance of customization while balancing efficiency. We also present use case scenarios to showcase Doccurate\u2019s envisioned usage in practice.", "uri": "https://vimeo.com/289787431", "name": "[VIS18 Preview] Doccurate: A Curation-Based Approach for Clinical Text Visualization (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:11:44+00:00", "description": "Authors: Shunan Guo, Zhuochen Jin, David Gotz, Fan Du, Hongyuan Zha, Nan Cao\n\nAbstract: Event sequence data is common to a broad range of application domains, from security to health care to scholarly communication. This form of data captures information about the progression of events for an individual entity (e.g., a computer network device; a patient; an author) in the form of a series of time-stamped observations. Moreover, each event is associated with an event type (e.g., a computer login attempt, or a hospital discharge). Analyses of event sequence data have been shown to help reveal important temporal patterns, such as clinical paths resulting in improved outcomes, or an understanding of common career trajectories for scholars. Moreover, recent research has demonstrated a variety of techniques designed to overcome methodological challenges such as large volumes of data and high dimensionality. However, the effective identification and analysis of latent stages of progression, which can allow for variation within different but similarly evolving event sequences, remain a significant challenge with important real-world motivations. In this paper, we propose an unsupervised stage analysis algorithm to identify semantically meaningful progression stages as well as the critical events which help define those stages. The algorithm follows three key steps: (1) event representation estimation, (2) event sequence warping and alignment, and (3) sequence segmentation. We also present a novel visualization system, EventThread2, which interactively illustrates the results of the stage analysis algorithm to help reveal evolution patterns across stages. Finally, we report three forms of evaluation for EventThread2: (1) case studies with two real-world datasets, (2) interviews with domain expert users, and (3) a performance evaluation on the progression analysis algorithm and the visualization design.", "uri": "https://vimeo.com/289787414", "name": "[VIS18 Preview] Visual Progression Analysis of Event Sequence Data (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:11:33+00:00", "description": "Authors: Hong Wang, Yafeng Lu, Shade Shutters, Michael Steptoe, Feng Wang, Steven Landis, Ross Maciejewski\n\nAbstract: Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.", "uri": "https://vimeo.com/289787400", "name": "[VIS18 Preview] A Visual Analytics Framework for Spatiotemporal Trade Network Analysis (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:11:21+00:00", "description": "Authors: Changjian Chen, Shixia Liu, Yafeng Lu, Fangxin Ouyang, Bin Wang\n\nAbstract: In order to effectively infer correct labels from noisy crowdsourced annotations, learning-from-crowds models have introduced expert validation. However, little research has been done on facilitating the validation procedure. In this paper, we propose an interactive method to assist experts in verifying uncertain instance labels and unreliable workers. Given the instance labels and worker reliability inferred from a learning-from-crowds model, candidate instances and workers are selected for expert validation. The influence of verified results is propagated to relevant instances and workers through the learning-from-crowds model. To facilitate the validation of annotations, we have developed a confusion visualization to indicate the confusing classes for further exploration, a constrained projection method to show the uncertain labels in context, and a scatter-plot-based visualization to illustrate worker reliability. The three visualizations are tightly integrated with the learning-from-crowds model to provide an iterative and progressive environment for data validation. Two case studies were conducted that demonstrate our approach offers an efficient method for validating and improving crowdsourced annotations.", "uri": "https://vimeo.com/289787374", "name": "[VIS18 Preview] An Interactive Method to Improve Crowdsourced Annotations (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:11:10+00:00", "description": "Authors: Gromit Yeuk-Yin Chan, Panpan Xu, Zeng Dai, Liu Ren\n\nAbstract: Bipartite graphs model the key relations in many large scale real-world data: customers purchasing items, legislators voting for bills, people\u2019s affiliation with different social groups, faults occurring in vehicles, etc. However, it is challenging to visualize large scale bipartite graphs with tens of thousands or even more nodes or edges. In this paper, we propose a novel visual summarization technique for bipartite graphs based on the minimum description length (MDL) principle. The method simultaneously groups the two different set of nodes and constructs aggregated bipartite relations with balanced granularity and precision. It addresses the key trade-off that often occurs for visualizing large scale and noisy data: acquiring a clear and uncluttered overview while maximizing the information content in it. We formulate the visual summarization task as a co-clustering problem and propose an efficient algorithm based on locality sensitive hashing (LSH) that can easily scale to large graphs under reasonable interactive time constraints that previous related methods cannot satisfy. The method leads to the opportunity of introducing a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. In the framework, we also introduce a compact visual design inspired by adjacency list representation of graphs as the building block for a small multiples display to compare the bipartite relations for different subsets of data. We showcase the applicability and effectiveness of our approach by applying it on synthetic data with ground truth and performing case studies on real-world datasets from two application domains including roll-call vote record analysis and vehicle fault pattern analysis. Interviews with experts in the political science community and the automotive industry further highlight the benefits of our approach.", "uri": "https://vimeo.com/289787354", "name": "[VIS18 Preview] ViBr: Visualizing Bipartite Relations at Scale with the Minimum Description Length Principle (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:10:59+00:00", "description": "Authors: Ke Xu, Meng Xia, Xing Mu, Yun Wang, Nan Cao\n\nAbstract: The results of anomaly detection are sensitive to the choice of detection algorithms as they are specialized for different properties of data, especially for multidimensional data. Thus, it is vital to select the algorithm appropriately. To systematically select the algorithms, ensemble analysis techniques have been developed to support the assembly and comparison of heterogeneous algorithms. However, challenges remain due to the absence of the ground truth, interpretation, or evaluation of these anomaly detectors. In this paper, we present a visual analytics system named EnsembleLens that evaluates anomaly detection algorithms based on the ensemble analysis process. The system visualizes the ensemble processes and results by a set of novel visual designs and multiple coordinated contextual views to meet the requirements of correlation analysis, assessment and reasoning of anomaly detection algorithms. We also introduce an interactive analysis workflow that dynamically produces contextualized and interpretable data summaries that allow further refinements of exploration results based on user feedback. We demonstrate the effectiveness of EnsembleLens through a quantitative evaluation, three case studies with real-world data and interviews with two domain experts.", "uri": "https://vimeo.com/289787339", "name": "[VIS18 Preview] EnsembleLens: Ensemble-based Visual Exploration of Anomaly Detection Algorithms with Multidimensional Data...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:10:47+00:00", "description": "Authors: Xuanwu Yue, Xinhuan Shu, Xinyu ZHU, Xinnan DU, Zheqing Yu, Dimitrios Papadopoulos, Siyuan Liu\n\nAbstract: The emerging prosperity of cryptocurrencies, such as Bitcoin, has come into the spotlight during the past few years. Cryptocurrency exchanges, which act as the gateway to this world, now play a dominant role in the circulation of Bitcoin. Thus, delving into the analysis of the transaction patterns of exchanges can shed light on the evolution and trends in the Bitcoin market, and participants can gain hints for identifying credible exchanges as well. Not only Bitcoin practitioners but also researchers in the \ufb01nancial and statistics domains are interested in the business intelligence behind the curtain. However, the task of multiple exchanges exploration and comparisons has been limited owing to the lack of ef\ufb01cient tools. Previous methods of visualizing Bitcoin data have mainly concentrated on tracking suspicious transaction logs, but it is cumbersome to analyze exchanges and their relationships with existing tools and methods. In this paper, we present BitExTract, an interactive visual analytics system, which, to the best of our knowledge, is the \ufb01rst attempt to explore the evolutionary transaction patterns of Bitcoin exchanges from two perspectives, namely, exchange versus exchange and exchange versus client. The BitExTract system\u2019s analysis concentrates on three different levels: the overall-market level, the inter-exchange level, and the individual level. In particular, BitExTract summarizes the evolution of the Bitcoin market by observing the transactions between exchanges over time via a massive sequence view. A node-link diagram with ego-centered views depicts the trading network of exchanges and their temporal transaction distribution. Moreover, BitExTract embeds multiple parallel bars on a timeline to examine and compare the evolution patterns of transactions between different exchanges. The effectiveness and usability of BitExTract are demonstrated through three case studies with novel insights and further interviews with domain experts and senior Bitcoin practitioners.", "uri": "https://vimeo.com/289787312", "name": "[VIS18 Preview] BitExTract: Interactive Visualization for Extracting Bitcoin Exchange Intelligence (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:10:37+00:00", "description": "Authors: Yao Ming, Huamin Qu, Enrico Bertini\n\nAbstract: With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. We design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.", "uri": "https://vimeo.com/289787299", "name": "[VIS18 Preview] RuleMatrix: Visualizing and Understanding Classifiers with Rules (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:10:26+00:00", "description": "Authors: Natalia Andrienko, Gennady Andrienko, Jose Manuel Cordero Garcia, David Scarlatti\n\nAbstract: In movement data analysis, there exists a problem of comparing multiple trajectories of moving objects to common or distinct reference trajectories. We introduce a general conceptual framework for comparative analysis of trajectories and an analytical procedure, which consists of (1) finding corresponding points in pairs of trajectories, (2) computation of pairwise difference measures, and (3) interactive visual analysis of the distributions of the differences with respect to space, time, set of moving objects, trajectory structures, and spatio-temporal context. We propose a combination of visualisation, interaction, and data transformation techniques supporting the analysis and demonstrate the use of our approach for solving a challenging problem from the aviation domain.", "uri": "https://vimeo.com/289787283", "name": "[VIS18 Preview] Analysis of Flight Variability: a Systematic Approach (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:10:13+00:00", "description": "Authors: Mennatallah El-Assady, Fabian Sperrle, Oliver Deussen, Daniel Keim, Christopher Collins\n\nAbstract: To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-loop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.", "uri": "https://vimeo.com/289787264", "name": "[VIS18 Preview] Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:10:01+00:00", "description": "Honorable Mention\n\nAuthors: Junpeng Wang, Liang Gou, Han-Wei Shen, Hao Yang\n\nAbstract: Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.", "uri": "https://vimeo.com/289787246", "name": "[VIS18 Preview] DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:09:49+00:00", "description": "Authors: Zhiguang Zhou, Linhao Meng, Cheng Tang, Ying Zhao, Zhiyong Guo, Miaoxin Hu, Wei Chen\n\nAbstract: A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.", "uri": "https://vimeo.com/289787224", "name": "[VIS18 Preview] Visual Abstraction of the Large Scale Geospatial Origin-Destination Movement Data (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:09:37+00:00", "description": "Authors: Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, David Ebert\n\nAbstract: Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.", "uri": "https://vimeo.com/289787203", "name": "[VIS18 Preview] Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models (VAST...", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:09:27+00:00", "description": "Authors: Marco Cavallo, Cagatay Demiralp\n\nAbstract: Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The space of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large space of clusterings so as to improve the effectiveness of exploratory clustering analysis.\nWe introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. \n We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.", "uri": "https://vimeo.com/289787185", "name": "[VIS18 Preview] Clustrophile 2: Guided Visual Clustering Analysis (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:09:17+00:00", "description": "Authors: Xun Zhao, Yanhong Wu, Dik Lun Lee, Weiwei Cui\n\nAbstract: As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.", "uri": "https://vimeo.com/289787165", "name": "[VIS18 Preview] iForest: Interpreting Random Forests via Visual Analytics (VAST Paper)", "year": "2018", "event": "VAST, PREVIEW"}, {"created_time": "2018-09-14T00:03:50+00:00", "description": "VIS Arts Program\n\nAuthors: David Hunter\n\nAbstract: Data Walking is a research project exploring the potential of walking to gather environmental data. Through multiple walks and visualisations a rich picture and sense of identity of that area can be constructed. The project examines technology and design for creative data gathering and experimenting with data visualisation, to create tools, gain insight, and share knowledge. Contained in this annotated portfolio are details on the ideas and approaches to the project, and notes on the process and distinct phases, as well as change of perspective that have taken place on the project\u201a\u00c4\u00f4s life so far.", "uri": "https://vimeo.com/289786653", "name": "[VIS18 Preview] Data Walking AP (VISAP Annotated Portfolio)", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-09-14T00:03:40+00:00", "description": "VIS Arts Program\n\nAuthors: Francesca Samsel, Lyn Bartram, Annie Bares\n\nAbstract: As the complexity of scientific data and the needs to communicate the science have grown, the requirements for visualization design and use have become more sophisticated. We increasingly need more effective ways of communicating the science across multiple audiences, including non-experts in the field. The challenges of enriching the representation have moved from the more naive ideas of making it ''aesthetically attractive'' to more profound constructs of visual language: how to enhance nuances in the data, and how to support more expressive visualizations that elicit different cognitive and communicative affect to tell the science story. In this paper, we describe how artistic color techniques drawn from paintings can be operationally applied to produce more evocative and informative scientific visualization. We illustrate how the color use in a painting can reveal structure and information priority and elicit affect using examples from current work with our scientific visualization colleagues. Our results highlight the value of engaging with artists in long-term, multidisciplinary science teams, but also emphasize the comprehension gaps that exist across the disciplines and the need for methods and techniques that bridge them so they are accessible to a wider range of data scientists. Our color extraction method is a small example of such a bridging technique.", "uri": "https://vimeo.com/289786642", "name": "[VIS18 Preview] Art, Affect and Color: Creating Engaging Expressive Scientific Visualizations (VisAP Paper)", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-09-14T00:03:28+00:00", "description": "VIS Arts Program\n\nAuthors: Pascal Glissmann, Andreas Henrich, Olivier Arcioli\n\nAbstract: The increasing complexity of Big Data Practices reveals unseen societal patterns through computational processes. In an alternative approach, The Phaistos Project \u201a\u00c4\u00ee Forty-five Symbols is visualizing the tracks of our lives from a different \u201a\u00c4\u00ee rather qualitative and individual \u201a\u00c4\u00ee perspective: participants observe, experiment with, and speculate about data of their everyday to capture a meaningful fraction of their lives. They design ethnographic visualizations that stimulate a sociopolitical discourse and reflect a critical position.", "uri": "https://vimeo.com/289786621", "name": "[VIS18 Preview] The Phaistos Project \u2014 Forty-five Symbols (VISAP Annotated Portfolio)", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-09-14T00:03:17+00:00", "description": "VIS Arts Program\n\nAuthors: Pedro Cruz, John Wihbey, Avni Ghael, Steve Costa, Ruan Chao, Dr. Felipe Shibuya\n\nAbstract: This article presents the iterative design process of representing growths of population in the United States as tree rings. It explains why this metaphor was chosen, and how the iterative process went through several ideas and implementations in order to make the metaphor more visible. For that, several algorithmic approaches are discussed as their graphical results are presented.", "uri": "https://vimeo.com/289786600", "name": "[VIS18 Preview] Process of simulating tree rings for immigration in the U.S. (VISAP Annotated Portfolio)", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-09-14T00:03:06+00:00", "description": "VIS Arts Program\n\nAuthors: Heike Otten, Lennart Hildebrandt, Till Nagel, Marian D\u00f6rk, Boris M\u00fcller\n\nAbstract: We present a hybrid visualization technique that integrates maps into network visualizations to reveal and analyze diverse topologies in geospatial movement data. With the rise of GPS tracking in various contexts such as smartphones and vehicles there has been a drastic increase in geospatial data being collect for personal reflection and organizational optimization. The generated movement datasets contain both geographical and temporal information, from which rich relational information can be derived. Common map visualizations perform especially well in revealing basic spatial patterns, but pay less attention to more nuanced relational properties. In contrast, network visualizations represent the specific topological structure of a dataset through the visual connections of nodes and their positioning. So far there has been relatively little research on combining these two approaches. Shifted Maps aims to bring maps and network visualizations together as equals. The visualization of places shown as circular map extracts and movements between places shown as edges, can be analyzed in different network arrangements, which reveal spatial and temporal topologies of movement data. We implemented a web-based prototype and report on challenges and opportunities about a novel network layout of places gathered during a qualitative evaluation.", "uri": "https://vimeo.com/289786583", "name": "[VIS18 Preview] Shifted Maps: Revealing spatio-temporal topologies in movement data (VisAP Paper)", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-09-14T00:02:55+00:00", "description": "VIS Arts Program\n\nAuthors: David Bihanic\n\nAbstract: The present-day world is unduly complex to the point that we are no longer able to understand it, to read it (at both macro and micro levels), to embrace it in its essential dimensions, including political, economic, environmental and social ones. From this, as the philosopher Michael Huemer explains, it follows a sort of \"aphasia\" of Politics \u201a\u00c4\u00ee losing almost every reference point and thus any ability to act and govern with discernment, the highest representatives of Nation-States would be effectively condemned to passivity. According to analysts, the inextricable complexity of the world would come from an \"intertwining\" of cultural, religious, societal influences and geopolitical as well as geo-economic determinations. Let us observe that until the beginning of the 21st century (that is to say, at the early post-Cold War period), the world (the yesterday's world therefore) was still readable, comprehensible because it might be regarded through a dualistic interpretation. Indeed it used to be caught in a global polarization deliberately instigated and orchestrated by two superpowers both political and ideological: the United States (the proponents of a certain Liberalism) and the former USSR (the defenders of a certain Socialism); in the mid-1950s, the Third World countries as well as those of the \"Non-Aligned Movement\" (NAM) have attempted to act as a sort of \"counterbalance\". Following the collapse of the Soviet bloc in 1991 (thereby signalled the end of the USSR), the United States then held a hegemonic position. Then, the 1990s marked the new era of Globalization leading to a techno-economic revolution \u201a\u00c4\u00ee towards a global networked economy. The consequence was a complete reorganization of the socio-economic-political \"chessboard\" fostering (among much other things) an acceleration of Capitalism (\u201a\u00c4\u00faTurbo-Capitalism\u201a\u00c4\u00f9) whose \"human and social cost\" would henceforth appear fairly significant. In the face of such upheavals, we should be worried about the means at our disposal to remain \"present-in-the-world\", in other words to keep seeing and thinking the world in an holistic and global way. Indeed we are forced to recognize today that the \"classical\" cartographic representation models (static or printed, in particular) \u201a\u00c4\u00ee those we used until now to keep up with the evolving world \u201a\u00c4\u00ee are not fully \"effective\" to reflect, depict or describe its current complexity. On closer observation, we notice that the recent interactive techniques and methods for massive data representation-visualization (frequently map-based) would be in a position to take over. Strongly helpful for decision-making (political, strategic first and foremost), analysis and heuristic exploration, these software productions (using often idiomatic formalisms) would allow us to apprehend some of the main facts, phenomena and realities of our contemporary world, several of whom are largely ignored and unknown. Available in countless formats and updated in realtime, this new type of \u201a\u00c4\u00faimages\u201a\u00c4\u00f9 (in the broadest sense of the word) might preserve the whole strength and beauty of the obvious \u201a\u00c4\u00ee the Beautiful Evidence, as wrote Edward Tufte, through which we can clearly grasp, by sight and mind, the major issues and challenges facing the planet. In my presentation, I will illustrate (with supporting data design project examples) how data visualizations manage to reveal some of today's societal ills and social traumas, making them both perceptible and intelligible.", "uri": "https://vimeo.com/289786549", "name": "[VIS18 Preview] Designing beautiful evidences in an era of complexity. When the graphics can reveal profound societal and...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-14T00:02:39+00:00", "description": "VIS Arts Program\n\nAuthors: Aaron Hill, Clare Churchouse, Michael Schober\n\nAbstract: In data visualization, the representation of uncertainty and error estimation is often difficult to display effectively. Constraints on the number of dimensions that can be expressed visually as well as limitations of statistical graphing software often lead to data visualizations that inadvertently omit and/or poorly convey the uncertainty and vulnerability of the underlying data. This research is based on more than 400 works of fine art from museum collections and galleries across several countries, curated and analyzed for inspiration and information on effective ways to visually communicate uncertainty, ambiguity, and vulnerability. The selected artworks were chosen for their unique ability to convey uncertainty using a range of approaches and techniques. This paper includes key findings from the analysis, examples of compelling works of art from the research, and an exploration of ways these works can inform data visualization practice, specifically for the visual display of uncertainty.", "uri": "https://vimeo.com/289786519", "name": "[VIS18 Preview] Seeking New Ways to Visually Represent Uncertainty in Data: What We Can Learn from the Fine Arts (VisAP...", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-09-14T00:02:25+00:00", "description": "VIS Arts Program\n\nAuthors: Charles Perin\n\nAbstract: Personal data is increasingly seen as a political and economic weapon, used by evil industries against the will of individuals. But personal data is also a resource of great value as it provides a medium to reminisce, to reflect and to share personal stories that shape our identities. I explore with this visualization design the peculiarities of visualizing personal data for the purpose of private reminiscing and public sharing.", "uri": "https://vimeo.com/289786488", "name": "[VIS18 Preview] Cycles and (A)Symmetry Exploring the Design of Shareable Personal Visualizations (VISAP Annotated Portfolio)", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-09-14T00:02:13+00:00", "description": "VIS Arts Program\n\nAuthors: Raphael Arar\n\nAbstract: Nostalgia is an installation that draws attention to the computational challenges of understanding human emotion. Through affective computing and machine learning, the underlying system attempts to translate the components of the sentiment\u201a\u00c4\u00f4s qualitative makeup in quantitative terms. In Nostalgia, participants are asked to submit text-based memories, which are then used to calculate, predict and ultimately visualize relative nostalgia scores based on the aggregate of stories collected. However, given the ambiguities and complexity of human self-expression and the necessary precision of computational intelligence, Nostalgia highlights the entanglements of achieving emotional understanding between humans and machines.", "uri": "https://vimeo.com/289786468", "name": "[VIS18 Preview] Nostalgia: A Human-Machine Transliteration (VisAP Paper)", "year": "2018", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2018-09-13T23:59:44+00:00", "description": "Authors: Jason Dykes\n\nAbstract: The Velo Club de VIS encourages colleagues to ride together on the weekend following IEEE VIS. This movie includes some clips from the 2017 ride in Arizona.\n\nJoin us for a road ride out of Berlin on Sat 27th October 2018 \u2013 7am start, home for pm flights.\nTerms of Reference, information on rides, bikes, club kit and other details are at :\n\nhttp://gicentre.net/velo-club-de-vis/\n\n@veloclubdevis\n\n#LeTourdeVIS\n\nContact veloclubdevis@gmail.com\n\nMeetup Thursday", "uri": "https://vimeo.com/289786226", "name": "[VIS18 Preview] Velo Club de VIS 2018 (Meetup)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:55:58+00:00", "description": "Authors: Shin-Ting Wu, Raphael Voltoline, Wallace S. Loos, J.A. Iva\u0301n Rubianes, Lionis S. Watanabe, Ba\u0301rbara J. Amorim, A. Carolina Coan, Fernando Cendes, Clarissa L. Yasuda\n\nAbstract: Focal cortical dysplasia (FCD) is a malformation of cortical development and a common cause of pharmacoresistant epilepsy. Resective surgery of clear-cut lesions may be curative. However, the localization of the seizure focus and the evaluation of its spatial extent can be challenging in many situations. For concordance assessment, medical studies show the relevance of accurate correlation of multisource imaging sequences to improve the sensitivity and specificity of the evaluation. In this paper, we share the process we went through to reach our simple, but effective, solution for integrating multi-volume rendering into an exploratory visualization environment for the diagnosis of FCD. We focus on fetching of multiple data assigned to a sample when they are rendered. Knowing that the major diagnostic role of multiple volumes is to complement information, we demonstrate that appropriate geometric transformations in the texture space are sufficient for accomplishing this task. This allows us to fully implement our proposal in the OpenGL rendering pipeline and to easily integrate it into the existing visual diagnostic application. Both time performance and the visual quality of our proposal were evaluated with a set of clinical data volumes for assessing the potential practical impact of our solution in routine diagnostic use.", "uri": "https://vimeo.com/289785856", "name": "[VIS18 Preview] Toward a Multimodal Diagnostic Exploratory Visualization of Focal Cortical Dysplasia (CG&amp;A Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:55:46+00:00", "description": "Authors: Monique Meuschke, Tobias Gu\u0308nther, Ralph Wickenho\u0308fer, Markus Gross, Bernhard Preim, Kai Lawonn\n\nAbstract: We present a framework to manage cerebral aneurysms. Rupture risk evaluation is based on manually extracted descriptors, which is timeconsuming. Thus, we provide an automatic solution by considering several questions: How can expert knowledge be integrated? How should metadata be defined? Which interaction techniques are needed for data exploration?", "uri": "https://vimeo.com/289785840", "name": "[VIS18 Preview] Management of Cerebral Aneurysm Descriptors based on an Automatic Ostium Extraction (CG&amp;A Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:55:36+00:00", "description": "Authors: Li Liu, Deborah Silver, Karen Bemis\n\nAbstract: Understanding how college preparation relates to employment prospects is crucial in positioning a student for successful post-college employment. Yet few tools for job seeking or job market assessment utilize visualizations. Visualizing job postings and helping students gain a better sense of the job market require both the representation of hundreds of majors and thousands of jobs and the display of the intangible relationships between them. To address these challenges, we present JobViz, a novel visualization designed for interactive exploration of job postings from a university career portal and for use with career counseling. The application-driven design combines the intuitiveness of node-link diagrams and the scalability of aggregation-based techniques to provide an overview of the entire database and to allow users to individualize and explore the data. We demonstrate the effectiveness of JobViz with a case study and two questionnaire-based user studies.", "uri": "https://vimeo.com/289785826", "name": "[VIS18 Preview] Application-Driven Design: Help Students Understand Employment and See the \u201cBig Picture\u201d (CG&amp;A Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:55:25+00:00", "description": "Authors: Zach Duer, Leo Piilonen, and George Glasson\n\nAbstract: Belle2VR is an interactive virtual-reality visualization of subatomic particle physics, designed by an interdisciplinary team as an educational tool for learning about and exploring subatomic particle collisions. This article describes the tool, discusses visualization design decisions, and outlines our process for collaborative development.", "uri": "https://vimeo.com/289785809", "name": "[VIS18 Preview] Belle2VR: A Virtual-Reality Visualization of Subatomic Particle Physics in the Belle II Experiment (CG&amp;A...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:55:11+00:00", "description": "Authors: Alexander Bock, Emil Axelsson, Carter Emmart, Masha Kuznetsova, Charles Hansen, Anders Ynnerman\n\nAbstract: We present the development of an open-source software called OpenSpace bridging the gap between scientific discoveries and public dissemination and thus paves the way for the next generation of science communication and data exploration. We describe how the platform enables interactive presentations of dynamic and time-varying processes, by domain experts, to the general public. The concepts are demonstrated through four cases: Image acquisitions of the New Horizons and Rosetta spacecraft, the dissemination of space weather phenomena, and the display of high-resolution planetary images. Each case has been presented at public events with great success. These cases highlight the details of data acquisition, rather than presenting the final results, showing the audience the value of supporting the efforts of the scientific discovery.", "uri": "https://vimeo.com/289785793", "name": "[VIS18 Preview] OpenSpace: Changing the Narrative of Public Dissemination in Astronomical Visualization from What to How...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:54:51+00:00", "description": "Authors: Feng Wang, Brett Hansen, Ryan Simmons, Ross Maciejewski\n\nAbstract: The Name Profiler Toolkit is a visual analytics system designed to enable the interactive exploration and analysis of forename and surname geographical distributions across the United States. Using demographic data from the US Census Bureau and Zillow, the toolkit lets users interactively compare distributions of names and name attributes.", "uri": "https://vimeo.com/289785766", "name": "[VIS18 Preview] The Name Profiler Toolkit (CG&amp;A Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:54:39+00:00", "description": "Authors: Alireza Karduni, Isaac Cho, Ginette Wessel, William Ribarsky, Eric Sauda, Wenwen Dou\n\nAbstract: The study of human activity in cities is integral to many professions such as urban planning, design, and transit management. By conducting a survey of practitioners within these fields, we generated a set of elements vital to their work. We used the findings as a guideline to design Urban Space Explorer, a visual analytics system that features interlinked maps of tweet density and Twitter user flows, a timeline of flow and density, and semantics and language views responding to space, time and topical focus of users. We conducted a user study with 21 practicing professionals. There was unanimous support for the usefulness of geolocated social media data and our interface. However, most users needed ways to search for specific content and keywords. Two affordances, in particular, were found to be useful; the analysis of human flows and activities and the ability to explore space at varying scales.", "uri": "https://vimeo.com/289785751", "name": "[VIS18 Preview] Urban Space Explorer: A Visual Analytics System for Urban Planning (CG&amp;A Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:54:27+00:00", "description": "Authors: Wei Luo, Michael Steptoe, Zheng Chang, Robert Link, Leon Clarke, Ross Maciejewski\n\nAbstract: Intercomparison and similarity analysis of different climate scenarios based on multiple simulation runs remain challenging. The proposed visual analytics system lets users perform similarity analysis of climate scenarios from the Global Change Assessment Model at world, continental, and country scales over time.", "uri": "https://vimeo.com/289785725", "name": "[VIS18 Preview] Impact of Spatial Scales on the Intercomparison of Climate Scenarios (CG&amp;A Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:54:16+00:00", "description": "Authors: Tatiana von Landesberger, Sebastian Bremm, Marcel Wunderlich\n\nAbstract: Static geolocated graphs have nodes connected by edges that can have geographic location and associated attributes. This article proposes a typology of uncertainty in static geolocated graphs, which can affect the existence, location, attributes, or grouping of nodes and edges. The authors also summarize available techniques for visualizing such uncertainty.", "uri": "https://vimeo.com/289785707", "name": "[VIS18 Preview] Typology of Uncertainty in Static Geolocated Graphs for Visualization (CG&amp;A Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:54:06+00:00", "description": "Authors: Hessam Djavaherpour, Ali Mahdavi-Amiri, Faramarz F. Samavati\n\nAbstract: Geospatial datasets are too complex to easily visualize and understand on a computer screen. Combining digital fabrication with a discrete global grid system (DGGS) can produce physical models of the Earth for visualizing multiresolution geospatial datasets. This proposed approach includes a mechanism for attaching a set of 3D printed segments to produce a scalable model of the Earth. The authors have produced two models that support the attachment of different datasets both in 2D and 3D format.", "uri": "https://vimeo.com/289785692", "name": "[VIS18 Preview] Physical Visualization of Geospatial Datasets (CG&amp;A Paper)", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:50:45+00:00", "description": "Authors: Brian David Ondov, Nicole Jardine, Niklas Elmqvist, Steven Franconeri\n\nAbstract: Data are often viewed as a single set of values, but those values frequently must be compared with another set. The existing evaluations of designs that facilitate these comparisons tend to be based on intuitive reasoning, rather than quantifiable measures. We build on this work with a series of crowdsourced experiments that use low-level perceptual comparison tasks that arise frequently in comparisons within data visualizations (e.g., which value changes the most between the two sets of data?). Participants completed these tasks across a variety of layouts: superposition, two arrangements of juxtaposed small multiples, mirror-symmetric small multiples, and animated transitions. A staircase procedure sought the difficulty level (e.g., value change delta) that led to equivalent accuracy for each layout. Confirming prior intuition, we observe high levels of performance for superposition versus standard small multiples. However, we also find performance improvements for both mirror symmetric small multiples and animated transitions. While some results are incongruent with common wisdom in data visualization, they align with previous work in perceptual psychology, and thus have potentially strong implications for visual comparison designs.", "uri": "https://vimeo.com/289785372", "name": "[VIS18 Preview] Face to Face: Evaluating Visual Comparison (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:50:35+00:00", "description": "Honorable Mention\n\nAuthors: Jo Wood, Alexander Kachkaev, Jason Dykes\n\nAbstract: We propose a new approach to the visualization design and communication process, literate visualization, based upon and extending, Donald Knuth's idea of literate programming. It integrates the process of writing data visualization code with description of the design choices that led to the implementation (design exposition). We develop a model of design exposition characterised by four visualization designer architypes: the evaluator, the autonomist, the didacticist and the rationalist. The model is used to justify the key characteristics of literate visualization: 'notebook' documents that integrate live coding input, rendered output and textual narrative; low cost of authoring textual narrative; guidelines to encourage structured visualization design and its documentation. We propose narrative schemas for structuring and validating a wide range of visualization design approaches and models, and branching narratives for capturing alternative designs and design views. We describe a new open source literate visualization environment, litvis, based on a declarative interface to Vega and Vega-Lite through the functional programming language Elm combined with markdown for formatted narrative. We informally assess the approach, its implementation and potential by considering three examples spanning a range of design abstractions: new visualization idioms; validation though visualization algebra; and feminist data visualization. We argue that the rich documentation of the design process provided by literate visualization offers the potential to improve the validity of visualization design and so benefit both academic visualization and visualization practice.", "uri": "https://vimeo.com/289785349", "name": "[VIS18 Preview] Design Exposition with Literate Visualization (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:50:25+00:00", "description": "Authors: Tan Tang, Sadia Rubab, Jiewen Lai, Weiwei Cui, Lingyun Yu, Yingcai Wu\n\nAbstract: Storyline visualization techniques have progressed significantly to generate illustrations of complex stories automatically. However, the visual layouts of storylines are not enhanced accordingly despite improvement in performance and the extension of its application area. The existing methods attempt to achieve several shared optimization goals, such as reducing empty space and minimizing line crossings and wiggles. However, these goals are not always conclusive. We conducted a preliminary study to learn how users translate a narrative into a hand-drawn storyline and to check whether the visual elements found in hand-drawn illustrations can be mapped back to appropriate narrative contexts. Moreover, we also compared the hand-drawn storylines with the storylines generated by the state-of-the-art methods and found they have significant differences. Our findings and discussion lead to a design space that summarizes how artists utilize narrative elements and the sequence of actions that artists follow to portray expressive and attractive storylines. We developed an authoring tool, iStoryline, to integrate high-level user interactions into optimization algorithms and achieve a balance between hand-drawn storylines and automatic layouts. iStoryline allows users to easily create a new storyline visualization according to their preferences by modifying the automatically-generated layout. The effectiveness and usability of the authoring tool are studied with qualitative evaluations.", "uri": "https://vimeo.com/289785328", "name": "[VIS18 Preview] iStoryline: Effective Convergence to Hand-drawn Storylines (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:50:15+00:00", "description": "Authors: Alper Sarikaya, Michael Correll, Lyn Bartram, Melanie Tory, Danyel A Fisher\n\nAbstract: Dashboards are one of the most common use cases for data visualization, and their design and contexts of use are considerably different from exploratory visualization tools. In this paper, we look at the broad scope of how dashboards are used in practice through an analysis of dashboard examples and documentation about their use. We systematically review the literature surrounding dashboard use, construct a design space for dashboards, and identify major dashboard types. We characterize dashboards by their design goals, levels of interaction, and the practices around them. Our framework and literature review suggest a number of fruitful research directions to better support dashboard design, implementation, and use.", "uri": "https://vimeo.com/289785315", "name": "[VIS18 Preview] What Do We Talk About When We Talk About Dashboards? (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:50:04+00:00", "description": "Authors: Michael Correll, Mingwei Li, Gordon L Kindlmann, Carlos Scheidegger\n\nAbstract: Famous examples such as Anscombe's Quartet highlight that one of the core benefits of visualizations is allowing people to discover visual patterns that might otherwise be hidden by summary statistics. This visual inspection is particularly important in exploratory data analysis, where analysts can use visualizations such as histograms and dot plots to identify data quality issues. Yet, these visualizations are driven by parameters such as histogram bin size or mark opacity that have a great deal of impact on the final visual appearance of the chart, but are rarely optimized to make important features visible. In this paper, we show that data flaws have varying impact on the visual features of visualizations, and that the adversarial or merely uncritical setting of design parameters of visualizations can obscure the visual signatures of these flaws. Drawing on the framework of Algebraic Visualization Design, we present the results of a crowdsourced study that shows that common visualization types can appear to reasonably summarize distributional data while hiding large and important flaws such as missing data and extraneous modes. We make use of these results to propose additional best practices for visualizations of distributions for data quality tasks.", "uri": "https://vimeo.com/289785290", "name": "[VIS18 Preview] Looks Good To Me: Visualizations As Sanity Checks (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:49:54+00:00", "description": "Authors: Jessica Hullman, Xiaoli Qiao, Michael Correll, Alex Kale, Matthew Kay\n\nAbstract: Understanding the uncertainty present in data is critical to effectively reasoning about visualized data. However, evaluating the impact of an uncertainty visualization is complex due to the difficulties that people have interpreting uncertainty and the challenge of defining correct behavior with uncertainty information. Currently, evaluators of uncertainty visualization must rely on general purpose visualization evaluation frameworks which can be ill-equipped to provide guidance with the unique difficulties of assessing response to uncertainty. To help evaluators navigate these complexities, we present a taxonomy for characterizing decisions related to evaluating the impact of uncertainty visualizations. Our taxonomy differentiates six levels of decisions that comprise an uncertainty visualization evaluation: the research questions, expected effects from an uncertainty visualization, evaluation goals, measures, elicitation techniques, and analysis paradigms. Combining insights from research in uncertainty comprehension and an analysis of uncertainty visualization evaluation practice in visualization, psychology, and related fields, our taxonomy presents a detailed view of how different evaluation methods can serve high-level research questions and goals related to visualizing uncertainty. Applying our taxonomy to 86 user studies of uncertainty visualizations, we find that existing evaluation practice, particularly in visualization research, focus on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and the disparity between evaluating decision making versus accuracy. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization of uncertainty in visualization versus other fields.", "uri": "https://vimeo.com/289785271", "name": "[VIS18 Preview] In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:49:44+00:00", "description": "Authors: Alex Kale, Francis Nguyen, Matthew Kay, Jessica Hullman\n\nAbstract: Animated representations of outcomes drawn from distributions (hypothetical outcome plots, or HOPs) are used in the media and other public venues to communicate uncertainty. HOPs greatly improve multivariate probability estimation over conventional static uncertainty visualizations and leverage the ability of the visual system to quickly, accurately, and automatically process the summary statistical properties of ensembles. However, it is unclear how well HOPs support realistic judgment tasks posed in uncertainty communication. We identify and motivate an appropriate task to investigate realistic judgments of uncertainty in the public domain through a qualitative analysis of uncertainty visualizations in the news. We contribute two crowdsourced experiments comparing the effectiveness of HOPs, error bars, and line ensembles for supporting perceptual decision-making from visualized uncertainty. Participants infer which of two possible underlying trends is more likely to have produced a sample of time series data by referencing uncertainty visualizations which depict the two trends with variability due to sampling error. By modeling each participant's accuracy as a function of the level of evidence presented over many repeated judgments, we find that observers are able to correctly infer the underlying trend in samples conveying a lower level of evidence when using HOPs rather than static aggregate uncertainty visualizations as a decision aid. Modeling approaches like ours contribute theoretically grounded and richly descriptive accounts of user perceptions to visualization evaluation.", "uri": "https://vimeo.com/289785258", "name": "[VIS18 Preview] Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:49:34+00:00", "description": "Authors: Yunhai Wang, Yanyan Wang, Yinqi Sun, Haifeng Zhang, Chi-Wing Fu, Michael Sedlmair, Baoquan Chen, Oliver Deussen\n\nAbstract: Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.", "uri": "https://vimeo.com/289785238", "name": "[VIS18 Preview] Structure-aware Fisheye Views for Efficient Large Graph Exploration (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:49:24+00:00", "description": "Authors: Yunhai Wang, Xin Chen, Tong Ge, Chen Bao, Michael Sedlmair, Chi-Wing Fu, Oliver Deussen, Baoquan Chen\n\nAbstract: Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.", "uri": "https://vimeo.com/289785222", "name": "[VIS18 Preview] Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:49:14+00:00", "description": "Authors: Nina McCurdy, Julie Gerdes, Miriah Meyer\n\nAbstract: This paper presents a framework for externalizing and analyzing expert knowledge about discrepancies in data through the use of visualization. Grounded in an 18-month design study with global health experts, the framework formalizes the notion of data discrepancies as implicit error, both in global health data and more broadly. We use the term implicit error to describe measurement error that is inherent to and pervasive throughout a dataset, but that isn't explicitly accounted for or defined. Instead, implicit error exists in the minds of experts, is mainly qualitative, and is accounted for subjectively during expert interpretation of the data. Externalizing knowledge surrounding implicit error can assist in synchronizing, validating, and enhancing interpretation, and can inform error analysis and mitigation. The framework consists of a description of implicit error components that are important for downstream analysis, along with a process model for externalizing and analyzing implicit error using visualization. As a second contribution, we provide a rich, reflective, and verifiable description of our research process as an exemplar summary toward the ongoing inquiry into ways of increasing the validity and transferability of design study research.", "uri": "https://vimeo.com/289785202", "name": "[VIS18 Preview] A Framework for Externalizing Implicit Error Using Visualization (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:49:04+00:00", "description": "Authors: Hayeong Song, Danielle Albers Szafir\n\nAbstract: Many real-world datasets are incomplete due to factors such as data collection failures or misalignments between fused datasets. Visualizations of incomplete datasets should allow analysts to draw conclusions from their data while effectively reasoning about the quality of the data and resulting conclusions. We conducted a pair of crowdsourced studies to measure how the methods used to impute and visualize missing data may influence analysts' perceptions of data quality and their confidence in their conclusions. Our experiments used different design choices for line graphs and bar charts to estimate averages and trends in incomplete time series datasets. Our results provide preliminary guidance for visualization designers to consider when working with incomplete data in different domains and scenarios.", "uri": "https://vimeo.com/289785184", "name": "[VIS18 Preview] Where is my data? Evaluating Visualizations with Missing Data (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:48:54+00:00", "description": "Authors: Mi Feng, Evan Peck, Lane Harrison\n\nAbstract: The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples\u00ed open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples\u00ed explorations of visualizations. In this paper, we address this challenge by identifying needs for visualization behavior analysis, and by developing corresponding candidate features that can be inferred from users\u00ed interaction data with visualization. We then propose metrics that capture novel aspects of peoples\u00ed open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples\u00ed use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing a design space of metrics for visualization engagement.", "uri": "https://vimeo.com/289785167", "name": "[VIS18 Preview] Patterns and Pace: Quantifying Diverse Exploration Behavior with Visualizations on the Web (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:48:42+00:00", "description": "Authors: Vladimir Molchanov, Lars Linsen\n\nAbstract: Dimensionality reduction is commonly applied to multidimensional data to reduce the complexity of their analysis. In visual analysis systems, projections embed multidimensional data into 2D or 3D spaces for graphical representation. To facilitate a robust and accurate analysis, essential characteristics of the multidimensional data shall be preserved when projecting. Orthographic star coordinates is a state-of-the-art linear projection method that avoids distortion of multidimensional clusters by restricting interactive exploration to orthographic projections. However, existing numerical methods for computing orthographic star coordinates have a number of limitations when putting them into practice. We overcome these limitations by proposing the novel concept of shape-preserving star coordinates where shape preservation is assured using a superset of orthographic projections. Our scheme is explicit, exact, simple, fast, parameter-free, and stable. To maintain a valid shape-preserving star-coordinates configuration during user interaction with one of the star-coordinates axes, we derive an algorithm that only requires us to modify the configuration of one additional compensatory axis. Different design goals can be targeted by using different strategies for selecting the compensatory axis. We propose and discuss four strategies including a strategy that approximates orthographic star coordinates very well and a data-driven strategy. We further present shape-preserving morphing strategies between two shape-preserving configurations, which can be adapted for the generation of data tours. We apply our concept to multiple data analysis scenarios to document its applicability and validate its desired properties.", "uri": "https://vimeo.com/289785145", "name": "[VIS18 Preview] Shape-preserving Star Coordinates (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:48:31+00:00", "description": "Authors: Ali Sarvghad, Bahador Saket, Alex Endert, Nadir Weibel\n\nAbstract: Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge &amp; Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.", "uri": "https://vimeo.com/289785130", "name": "[VIS18 Preview] Embedded Merge &amp; Split: Visual Adjustment of Data Grouping (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:48:20+00:00", "description": "Authors: Yixuan Zhang, Kartik Chanana, Cody Dunne\n\nAbstract: Type 1 diabetes is a chronic, incurable autoimmune disease affecting millions of Americans in which the body stops producing insulin and blood glucose levels rise. The goal of intensive diabetes management is to lower average blood glucose through frequent adjustments to insulin protocol, diet, and behavior. Manual logs and medical device data are collected by patients, but these multiple sources are presented in disparate visualization designs to the clinician\u00f3making temporal inference difficult. We conducted a design study over 18 months with clinicians performing intensive diabetes management. We present a data abstraction and novel hierarchical task abstraction for this domain. We also contribute IDMVis: a visualization tool for temporal event sequences with multidimensional, interrelated data. IDMVis includes a novel technique for folding and aligning records by dual sentinel events and scaling the intermediate timeline. We validate our design decisions based on our domain abstractions, best practices, and through a qualitative evaluation with six clinicians. The results of this study indicate that IDMVis accurately reflects the workflow of clinicians. Using IDMVis, clinicians are able to identify issues of data quality such as missing or conflicting data, reconstruct patient records when data is missing, differentiate between days with different patterns, and promote educational interventions after identifying discrepancies.", "uri": "https://vimeo.com/289785100", "name": "[VIS18 Preview] IDMVis: Temporal Event Sequence Visualization in Support of Type 1 Diabetes Treatment Decision (InfoVis...", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:48:10+00:00", "description": "Authors: Anna Gogolou, Theophanis Tsandilas, Themis Palpanas, Anastasia Bezerianos\n\nAbstract: A common challenge faced by many domain experts working with time series data is how to identify and compare similar patterns. This operation is fundamental in high-level tasks, such as detecting recurring phenomena or creating clusters of similar temporal sequences. While automatic measures exist to compute time series similarity, human intervention is often required to visually inspect these automatically generated results. The visualization literature has examined similarity perception and its relation to automatic similarity measures for line charts, but has not yet considered if alternative visual representations, such as horizon graphs and colorfields, alter this perception. Motivated by how neuroscientists evaluate epileptiform patterns, we conducted two experiments that study how these three visualization techniques affect similarity perception in EEG signals. We seek to understand if the time series results returned from automatic similarity measures are perceived in a similar manner, irrespective of the visualization technique; and if what people perceive as similar with each visualization aligns with different automatic measures and their similarity constraints. Our findings indicate that horizon graphs align with similarity measures that allow local variations in temporal position or speed (i.e., dynamic time warping) more than the two other techniques. On the other hand, horizon graphs do not align with measures that are insensitive to amplitude and y-offset scaling (i.e., measures based on z-normalization), but the inverse seems to be the case for line charts and colorfields. Overall, our work indicates that the choice of visualization affects what temporal patterns we consider as similar, i.e., the notion of similarity in time series is not visualization independent.", "uri": "https://vimeo.com/289785077", "name": "[VIS18 Preview] Comparing Similarity Perception in Time Series Visualizations (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:47:59+00:00", "description": "Authors: Daniel Haehn, James Tompkin, Hanspeter Pfister\n\nAbstract: Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill's seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks, and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance, we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.", "uri": "https://vimeo.com/289785061", "name": "[VIS18 Preview] Evaluating 'Graphical Perception' with CNNs (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:47:49+00:00", "description": "Authors: Fangzhou Guo, Wei Chen, Dongming Han, JIACHENG PAN, Xiaotao Nie, Jiazhi Xia, Xiaolong (Luke) Zhang\n\nAbstract: When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.", "uri": "https://vimeo.com/289785045", "name": "[VIS18 Preview] Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks (InfoVis...", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:47:40+00:00", "description": "Authors: Jaemin Jo, Fr\u00e9d\u00e9ric Vernier, Pierre Dragicevic, Jean-Daniel Fekete\n\nAbstract: Multiclass maps are scatterplots, multidimensional projections, or thematic geographic maps where data points have a categorical attribute in addition to two quantitative attributes. This categorical attribute is often rendered using shape or color, which does not scale when overplotting occurs. When the number of data points increases, multiclass maps must resort to data aggregation to remain readable. We present multiclass density maps: multiple 2D histograms computed for each of the category values. Multiclass density maps are meant as a building block to improve the expressiveness and scalability of multiclass map visualization. In this article, we first present a short survey of aggregated multiclass maps, mainly from cartography. We then introduce a declarative model\u00f3a simple yet expressive JSON grammar associated with visual semantics\u00f3that specifies a wide design space of visualizations for multiclass density maps. Our declarative model is expressive and can be efficiently implemented in visualization front-ends such as modern web browsers. Furthermore, it can be reconfigured dynamically to support data exploration tasks without recomputing the raw data. Finally, we demonstrate how our model can be used to reproduce examples from the past and support exploring data at scale.", "uri": "https://vimeo.com/289785026", "name": "[VIS18 Preview] A Declarative Rendering Model for Multiclass Density Maps (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:47:29+00:00", "description": "Authors: Tanja Blascheck, Lonni Besan\u00e7on, Anastasia Bezerianos, Bongshin Lee, Petra Isenberg\n\nAbstract: We present the result of two studies to assess how quickly people can perform a simple data comparison task for small-scale visualizations on a smartwatch. The main goal of these studies is to extend our understanding of design constraints for smartwatch visualizations. Previous work has shown that a vast majority of smartwatch interactions last under 5 s. It is still unknown what people can actually perceive from visualizations during short glances, in particular in light of the limited display space of smartwatches. To shed light on this question, we conducted two studies that assessed the lower bounds of task time for a simple data comparison task. We tested three data representations common on smartwatches: bar charts, donut charts, and radial bar charts with three different data sizes: 7, 12, and 24 data values. In our first study, we controlled the differences of the two target bars to be compared, while the second study varied the difference randomly. For both studies we found that participants performed the task on average in &lt;220 ms for the donut chart, &lt;300 ms for the bar chart, and in &lt;1780 ms for the radial bar chart. Thresholds in the second study per chart type were on average 1.14-1.35x higher than in the first study. Our results show that bar and donut charts should be preferred on smartwatch displays when quick data comparisons are necessary.", "uri": "https://vimeo.com/289785013", "name": "[VIS18 Preview] Glanceable Visualization: Studies of Data Comparison Performance on Smartwatches (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:47:18+00:00", "description": "Authors: Sriram Karthik Badam, Andreas Mathisen, Roman R\u00e4dle, Clemens Nylandsted Klokmose, Niklas Elmqvist\n\nAbstract: Visualization tools are often specialized for specific tasks, which turns the user\u00eds analytical workflow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components\u00f3the building blocks of this model\u00f3can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch. To realize our model, we introduce Vistrates, a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present use cases of Vistrates that span the full range of the classic \u00ecanytime\u00ee and \u00ecanywhere\u00ee motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices.", "uri": "https://vimeo.com/289784994", "name": "[VIS18 Preview] Vistrates: A Component Model for Ubiquitous Analytics (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:47:04+00:00", "description": "Authors: Sriram Karthik Badam, Zhicheng Liu, Niklas Elmqvist\n\nAbstract: Today\u00eds data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, figures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader\u00eds current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.", "uri": "https://vimeo.com/289784966", "name": "[VIS18 Preview] Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:46:52+00:00", "description": "Authors: Arjun Srinivasan, Steven Drucker, Alex Endert, John Stasko\n\nAbstract: Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder\u00eds design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "uri": "https://vimeo.com/289784943", "name": "[VIS18 Preview] Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:46:43+00:00", "description": "Authors: Emily Wall, Meeshu Agnihotri, Laura Matzen, Kristin Divis, Michael Haass, Alex Endert, John Stasko\n\nAbstract: Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization\u00eds value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of a course in which they were developed.", "uri": "https://vimeo.com/289784928", "name": "[VIS18 Preview] A Heuristic Approach to Value-Driven Evaluation of Visualizations (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:46:32+00:00", "description": "Honorable Mention\n\nAuthors: Karen Schloss, Connor C Gramazio, A. Taylor Silverman, Madeline L. Parker, Audrey S Wang\n\nAbstract: To interpret data visualizations, people must determine how visual features map onto concepts. For example, to interpret colormaps, people must determine how dimensions of color (e.g., lightness, hue) map onto quantities of a given measure (e.g., brain activity, correlation magnitude). This process is easier when the encoded mappings in the visualization match people\u2019s predictions of how visual features will map onto concepts, their inferred mappings. To harness this principle in visualization design, it is necessary to understand what factors determine people\u2019s inferred mappings. In this study, we investigated how inferred color-quantity mappings for colormap data visualizations were influenced by the background color. Prior literature presents seemingly conflicting accounts of how the background color affects inferred color-quantity mappings. The present results help resolve those conflicts, demonstrating that sometimes the background has an effect and sometimes it does not, depending on whether the colormap appears to vary in opacity. When there is no apparent variation in opacity, participants infer that darker colors map to larger quantities (dark-is-more bias). As apparent variation in opacity increases, participants become biased toward inferring that more opaque colors map to larger quantities (opaque-is-more bias). These biases work together on light backgrounds and conflict on dark backgrounds. Under such conflicts, the opaque-is-more bias can negate, or even supersede the dark-is-more bias. The results suggest that if a design goal is to produce colormaps that match people\u2019s inferred mappings and are robust to changes in background color, it is beneficial to use colormaps that will not appear to vary in opacity on any background color, and to encode larger quantities in darker colors.", "uri": "https://vimeo.com/289784916", "name": "[VIS18 Preview] Mapping Color to Meaning in Colormap Data Visualizations (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:46:21+00:00", "description": "Authors: Carolina Nobre, Marc Streit, Alexander Lex\n\nAbstract: Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize.In this paper, we introduce a novel, scalable, tree+table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out in a linear layout, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link/adjacency matrix technique. We implement this concept in Juniper, and complement it with a set of interaction techniques that enable analysts to dynamically grow, re-structure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics, and a multitype network of story characters, places, books, etc.", "uri": "https://vimeo.com/289784894", "name": "[VIS18 Preview] Juniper: A Tree+Table Approach to Multivariate Graph Visualization (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:46:10+00:00", "description": "Authors: Timothy Major, Rahul C. Basole\n\nAbstract: Many real-world datasets are large, multivariate, and relational in nature and relevant associated decisions frequently require a simultaneous consideration of both attributes and connections. Existing visualization systems and approaches, however, often make an explicit trade-off between either affording rich exploration of individual data units and their attributes or exploration of the underlying network structure. In doing so, important analysis opportunities and insights are potentially missed. In this study, we aim to address this gap by (1) considering visualizations and interaction techniques that blend the spectrum between unit and network visualizations, (2) discussing the nature of different forms of contexts and the challenges in implementing them, and (3) demonstrating the value of our approach for visual exploration of multivariate, relational data for a real-world use case. Specifically, we demonstrate through a system called Graphicle how network structure can be layered on top of unit visualization techniques to create new opportunities for visual exploration of physician characteristics and referral data. We report on the design, implementation, and evaluation of the system and effectiveness of our blended approach.", "uri": "https://vimeo.com/289784877", "name": "[VIS18 Preview] Graphicle: Exploring Units, Networks, and Context in a Blended Visualization Approach (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:45:59+00:00", "description": "Authors: Ronell Sicat, Jiabao Li, JunYoung Choi, Maxime Cordeil, Won-Ki Jeong, Benjamin Bach, Hanspeter Pfister\n\nAbstract: This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.", "uri": "https://vimeo.com/289784861", "name": "[VIS18 Preview] DXR: A Toolkit for Building Immersive Data Visualizations (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:45:43+00:00", "description": "Honorable Mention\n\nAuthors: Donghao Ren, Bongshin Lee, Matthew Brehmer\n\nAbstract: We present Charticulator, an interactive authoring tool that enables the creation of bespoke and reusable chart layouts. Charticulator is our response to most existing chart construction interfaces that require authors to choose from predefined chart layouts, thereby precluding the construction of novel charts. In contrast, Charticulator transforms a chart specification into mathematical layout constraints and automatically computes a set of layout attributes using a constraint-solving algorithm to realize the chart. It allows for the articulation of compound marks or glyphs as well as links between these glyphs, all without requiring any coding or knowledge of constraint satisfaction. Furthermore, thanks to the constraint-based layout approach, Charticulator can export chart designs into reusable templates that can be imported into other visualization tools. In addition to describing Charticulator's conceptual framework and design, we present three forms of evaluation: a gallery to illustrate its expressiveness, a user study to verify its usability, and a click-count comparison between Charticulator and three existing tools. Finally, we discuss the limitations and potentials of Charticulator as well as directions for future research.", "uri": "https://vimeo.com/289784837", "name": "[VIS18 Preview] Charticulator: Interactive Construction of Bespoke Chart Layouts (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:45:31+00:00", "description": "Authors: Mathieu Le Goc, Charles Perin, Sean Follmer, Jean-Daniel Fekete, Pierre Dragicevic\n\nAbstract: This paper introduces dynamic composite physicalizations, a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.", "uri": "https://vimeo.com/289784806", "name": "[VIS18 Preview] Dynamic Composite Data Physicalization Using Wheeled Micro-Robots (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:45:16+00:00", "description": "Authors: Di Weng, Ran Chen, Zikun Deng, Feiran Wu, Jingmin Chen, Yingcai Wu\n\nAbstract: Interactive ranking techniques have substantially promoted analysts' ability in making judicious and informed decisions effectively based on multiple criteria. However, the existing techniques cannot satisfactorily support the analysis tasks involved in ranking large-scale spatial alternatives, such as selecting optimal locations for chain stores, where the complex spatial contexts involved are essential to the decision-making process. Limitations observed in the prior attempts of integrating rankings with spatial contexts motivate us to develop a context-integrated visual ranking technique. Based on a set of generic design requirements we summarized by collaborating with domain experts, we propose SRVis, a novel spatial ranking visualization technique that supports efficient spatial multi-criteria decision-making processes by addressing three major challenges in the aforementioned context integration, namely, a) the presentation of spatial rankings and contexts, b) the scalability of rankings' visual representations, and c) the analysis of context-integrated spatial rankings. Specifically, we encode massive rankings and their cause with scalable matrix-based visualizations and stacked bar charts based on a novel two-phase optimization framework that minimizes the information loss, and the flexible spatial filtering and intuitive comparative analysis are adopted to enable the in-depth evaluation of the rankings and assist users in selecting the best spatial alternative. The effectiveness of the proposed technique has been evaluated and demonstrated with an empirical study of optimization methods, two case studies, and expert interviews.", "uri": "https://vimeo.com/289784783", "name": "[VIS18 Preview] SRVis: Towards Better Spatial Integration in Ranking Visualization (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:45:04+00:00", "description": "Authors: Christophe Hurter, Nathalie Henry Riche, Steven Drucker, Maxime Cordeil, Richard Alligier, Romain Vuillemot\n\nAbstract: Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.", "uri": "https://vimeo.com/289784766", "name": "[VIS18 Preview] FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:44:46+00:00", "description": "Authors: Shusen Liu, Zhimin Li, Tao Li, Vivek Srikumar, Valerio Pascucci, Peer-Timo Bremer\n\nAbstract: With the recent advances in deep learning, neural network models have obtained state-of-the-art performances for many linguistic tasks in natural language processing. However, this rapid progress also brings enormous challenges. The opaque nature of neural network model leads to hard-to-debug-systems and difficult to interpret mechanisms. Here, we introduce a visualization system that through a tight yet flexible integration between visualization elements and the underlying model allows a user to interrogate the model by perturbing the input, the internal state, and prediction while observing changes in other parts of the pipeline. We use the natural language inference problem as an example to illustrate how a perturbation-driven paradigm can help domain experts assess the potential limitation of a model, probe its inner states, and interpret and form hypothesize about fundamental model mechanisms such as attention.", "uri": "https://vimeo.com/289784737", "name": "[VIS18 Preview] NLIZE: A Perturbation-Driven Visual Interrogation Tool for Analyzing and Interpreting Natural Language...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:44:36+00:00", "description": "Authors: Hariharan Subramonyam, Eytan Adar\n\nAbstract: Details-on-demand is a crucial feature in the visual information-seeking process but are often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). \u00ebQueries\u00ed to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. From the end-user perspective, SmartCues supports multitouch interactions to construct complex queries and detail types, and a predictive approach to determine which details are and will be requested. We demonstrate how the approach can be implemented across a wide array of visualization types and, through a lab study, show that end-users can easily and effectively use SmartCues. We reflect on how designers can best integrate SmartCues in the presence of other interaction strategies.", "uri": "https://vimeo.com/289784720", "name": "[VIS18 Preview] SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays (InfoVis...", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:44:25+00:00", "description": "Authors: Qianwen Wang, Zhen Li, Siwei Fu, Weiwei Cui, Huamin Qu\n\nAbstract: Visual designs can be complex in modern data visualization systems, which poses special challenges for explaining them to the general non-experts. However, a few, if any, presentation tools are tailored for this purpose. In this study, we present Narvis, a slideshow authoring tool designed for introducing data visualizations to non-experts. Narvis targets two types of end users: teachers, experts in data visualization who produce tutorials for explaining a data visualization, and students, non-experts who try to understand visualization designs through tutorials. We present an analysis of requirements through close discussions with the two types of end users. The resulting considerations guide the design and implementation of Narvis. Additionally, to help teachers better organize their introduction slideshows, we specify a data visualization as a hierarchical combination of components, which are automatically detected and extracted by Narvis. The teachers craft an introduction slideshow through first organizing these components, and then explaining them sequentially. A series of templates are provided for adding annotations and animations to improve efficiency during the authoring process. We evaluate Narvis through a qualitative analysis of the authoring experience, and a preliminary evaluation of the generated slideshow.", "uri": "https://vimeo.com/289784711", "name": "[VIS18 Preview] Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:44:14+00:00", "description": "Authors: Yalong Yang, Tim Dwyer, Bernhard Jenny, Kim Marriott, Maxime Cordeil, Haohui Chen\n\nAbstract: Immersive virtual- and augmented-reality headsets can overlay a flat image against any surface or hang virtual objects in the space around the user. The technology is rapidly improving and may, in the long term, replace traditional flat panel displays in many situations. When displays are no longer intrinsically flat, how should we use the space around the user for abstract data visualisation? In this paper, we ask this question with respect to origin-destination flow data in a global geographic context. We report on the findings of three studies exploring different spatial encodings for flow maps. The first experiment focuses on different 2D and 3D encodings for flows on flat maps. We find that participants are significantly more accurate with raised flow paths whose height is proportional to flow distance but fastest with traditional straight line 2D flows. In our second and third experiment we compared flat maps, 3D globes and a novel interactive design we call MapsLink, involving a pair of linked flat maps. We find that participants took significantly more time with MapsLink than other flow maps while the 3D globe with raised flows was the fastest, most accurate, and most preferred method. Our work suggests that careful use of the third spatial dimension can resolve visual clutter in complex flow maps.", "uri": "https://vimeo.com/289784692", "name": "[VIS18 Preview] Origin-Destination Flow Maps in Immersive Environments (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:44:01+00:00", "description": "Authors: Evanthia Dimara, Gilles Bailly, Anastasia Bezerianos, Steven Franconeri\n\nAbstract: Human decisions are prone to biases, and this is no less true for decisions made within data visualizations. Bias mitigation strategies often focus on the person, by educating people about their biases, typically with little success. We focus instead on the system, presenting the first evidence that altering the design of an interactive visualization tool can mitigate a strong bias \u00f1 the attraction effect. Participants viewed 2D scatterplots where choices between superior alternatives were affected by the placement of other suboptimal points. We found that highlighting the superior alternatives weakened the bias, but did not eliminate it. We then tested an interactive approach where participants completely removed locally dominated points from the view, inspired by the elimination by aspects strategy in the decision-making literature. This approach strongly decreased the bias, leading to a counterintuitive suggestion: tools that allow removing inappropriately salient or distracting data from a view may help lead users to make more rational decisions.", "uri": "https://vimeo.com/289784657", "name": "[VIS18 Preview] Mitigating the Attraction Effect with Visualizations (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:43:50+00:00", "description": "Authors: Matthew Brehmer, Bongshin Lee, Petra Isenberg, Eun Kyoung Choe\n\nAbstract: In the first crowdsourced visualization experiment conducted exclusively on mobile phones, we experimentally compare approaches to visualizing ranges over time on small displays. People routinely consume such data via a mobile phone, from temperatures in weather forecasting apps to sleep and blood pressure readings in personal health apps. However, we lack guidance on how to effectively visualize ranges on small displays in the context of different value retrieval and comparison tasks, or with respect to different data characteristics such as periodicity, seasonality, or the cardinality of ranges. Central to our experiment is a comparison between two ways to lay out ranges: a more conventional linear layout strikes a balance between quantitative and chronological scale resolution, while a less conventional radial layout emphasizes the cyclicality of time and may prioritize discrimination between values at its periphery. With results from 87 crowd workers, we found that while participants completed tasks more quickly with linear layouts than with radial ones, there were few differences in terms of error rate between layout conditions. We also found that participants performed similarly with both layouts in tasks that involved comparing superimposed observed and average ranges.", "uri": "https://vimeo.com/289784625", "name": "[VIS18 Preview] Visualizing Ranges over Time on Mobile Phones: A Task-Based Crowdsourced Evaluation (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:43:38+00:00", "description": "Authors: Le Liu, Lace M. K. Padilla, Sarah Creem-Regehr, Donald House\n\nAbstract: A common approach to sampling the space of a prediction is the generation of an ensemble of potential outcomes, where the ensemble's distribution reveals the statistical structure of the prediction space. For example, the US National Hurricane Center generates multiple day predictions for a storm's path, size, and wind speed, and then uses a Monte Carlo approach to sample this prediction into a large ensemble of potential storm outcomes. Various forms of summary visualizations are generated from such an ensemble, often using spatial spread to indicate its statistical characteristics. However, studies have shown that changes in the size of such summary glyphs, representing changes in the uncertainty of the prediction, are frequently confounded with other attributes of the phenomenon, such as its size or strength. In addition, simulation ensembles typically encode multivariate information, which can be difficult or confusing to include in a summary display. This problem can be overcome by directly displaying the ensemble as a set of annotated trajectories, however this solution will not be effective if ensembles are densely overdrawn or structurally disorganized. We propose to overcome these difficulties by selectively sampling the original ensemble, constructing a smaller representative and spatially well organized ensemble. This can be drawn directly as a set of paths that implicitly reveals the underlying spatial uncertainty distribution of the prediction. Since this approach does not use a visual channel to encode uncertainty, additional information can more easily be encoded in the display without leading to visual confusion. To demonstrate our argument, we describe the development of a visualization for ensembles of tropical cyclone forecast tracks, explaining how their spatial and temporal predictions, as well as other crucial storm characteristics such as size and intensity, can be clearly revealed. We verify the effectiveness of this visualization approach through a cognitive study exploring how storm damage estimates are affected by the density of tracks drawn, and by the presence or absence of annotating information on storm size and intensity.", "uri": "https://vimeo.com/289784601", "name": "[VIS18 Preview] Visualizing Uncertain Tropical Cyclone Predictions using Representative Samples from Ensembles of Forecast...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:43:27+00:00", "description": "Authors: Rebecca Faust, David Glickenstein, Carlos Scheidegger\n\nAbstract: Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are popular with visualization researchers and experienced data analysts, but present serious problems of interpretation. In this paper, we present DimReader, a technique that recovers readable axes from such techniques. DimReader is based on analyzing infinitesimal perturbations of the dataset with respect to variables of interest. The perturbations define exactly how we want to change each point in the original dataset and we measure the effect that these changes have on the projection. The recovered axes are in direct analogy with the axis lines (grid lines) of traditional scatterplots. We also present methods for discovering perturbations on the input data that change the projection the most. The calculation of the perturbations is efficient and easily integrated into programs written in modern programming languages. We present results of DimReader on a variety of NDR methods and datasets both synthetic and real-life, and show how it can be used to compare different NDR methods. Finally, we discuss limitations of our proposal and situations where further research is needed.", "uri": "https://vimeo.com/289784574", "name": "[VIS18 Preview] DimReader: Axis lines that explain non-linear projections (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:43:17+00:00", "description": "Authors: Ricardo Langner, Ulrike Kister, Raimund Dachselt\n\nAbstract: Interactive wall-sized displays benefit data visualization. Due to their sheer display size, they make it possible to show large amounts of data in multiple coordinated views (MCV) and facilitate collaborative data analysis. In this work, we propose a set of important design considerations and contribute a fundamental input vocabulary and interaction mapping for MCV functionality. We also developed a fully functional application with more than 45 coordinated views visualizing a real-world, multivariate data set of crime activities, which we used in a comprehensive qualitative user study investigating how pairs of users behave. Most importantly, we found that flexible movement is essential and\u00f3depending on user goals\u00f3is connected to collaboration, perception, and interaction. Therefore, we argue that for future systems interaction from the distance is required and needs good support. We show that our consistent design for both direct touch at the large display and distant interaction using mobile phones enables the seamless exploration of large-scale MCV at wall-sized displays. Our MCV application builds on design aspects such as simplicity, flexibility, and visual consistency and, therefore, supports realistic workflows. We believe that in the future, many visual data analysis scenarios will benefit from wall-sized displays presenting numerous coordinated visualizations, for which our findings provide a valuable foundation.", "uri": "https://vimeo.com/289784543", "name": "[VIS18 Preview] Multiple Coordinated Views at Large Displays for Multiple Users: Empirical Findings on User Behavior,...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:43:04+00:00", "description": "Best Paper\n\nAuthors: Dominik Moritz, Chenglong Wang, Greg L. Nelson, Halden Lin, Adam M. Smith, Bill Howe, Jeffrey Heer\n\nAbstract: There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "uri": "https://vimeo.com/289784521", "name": "[VIS18 Preview] Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco...", "year": "2018", "event": "PREVIEW"}, {"created_time": "2018-09-13T23:42:54+00:00", "description": "Authors: Biswaksen Patnaik, Andrea Batch, Niklas Elmqvist\n\nAbstract: Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present viScent: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general visualization beyond the examples in this paper.", "uri": "https://vimeo.com/289784509", "name": "[VIS18 Preview] Information Olfactation: Harnessing Scent to Convey Data (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:42:37+00:00", "description": "Authors: Eugene Wu, Remco Chang, Abigail Mosca, Gabriel Ryan\n\nAbstract: When inspecting information visualizations under time critical settings, such as emergency response or monitoring the heart rate in a surgery room, the user only has a small amount of time to view the visualization \u00ecat a glance\u00ee. In these settings, it is important to provide a quantitative measure of the visualization to understand whether or not the visualization is too \u00eccomplex\u00ee to accurately judge at a glance. This paper proposes Pixel Approximate Entropy (PAE), which adapts the approximate entropy statistical measure commonly used to quantify regularity and unpredictability in time-series data, as a measure of visual complexity for line charts. We show that PAE is correlated with user-perceived chart complexity, and that increased chart PAE correlates with reduced judgement accuracy. \u00ebWe also find that the correlation between PAE values and participants\u00ed judgment increases when the user has less time to examine the line charts.", "uri": "https://vimeo.com/289784475", "name": "[VIS18 Preview] At a Glance: Approximate Entropy as a Measure of Line Chart Visualization Complexity (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:42:26+00:00", "description": "Authors: Wiebke K\u00f6pp, Tino Weinkauf\n\nAbstract: We consider temporally evolving trees with changing topology and data: tree nodes may persist for a time range, merge or split, and the associated data may change. Essentially, one can think of this as a time series of trees with a node correspondence per hierarchy level between consecutive time steps. Existing visualization approaches for such data include animated 2D treemaps, where the dynamically changing layout makes it difficult to observe the data in its entirety. We present a method to visualize this dynamic data in a static, nested, and space-filling visualization. This is based on two major contributions: First, the layout constitutes a graph drawing problem. We approach it for the entire time span at once using a combination of a heuristic and simulated annealing. Second, we propose a rendering that emphasizes the hierarchy through an adaption of the classic cushion treemaps. We showcase the wide range of applicability using data from feature tracking in time-dependent scalar fields, evolution of file system hierarchies, and world population.", "uri": "https://vimeo.com/289784452", "name": "[VIS18 Preview] Temporal Treemaps: Static Visualization of Evolving Trees (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:42:16+00:00", "description": "Authors: Yunhai Wang, Zeyu Wang, Chi-Wing Fu, Hansj\u00f6rg Schmauder, Oliver Deussen, Daniel Weiskopf\n\nAbstract: Selecting a good aspect ratio is crucial for effective 2D diagrams. There are several aspect ratio selection methods for function plots and line charts, but only few can handle general, discrete diagrams such as 2D scatter plots. However, these methods either lack a perceptual foundation or heavily rely on intermediate isoline representations, which depend on choosing the right isovalues and are time-consuming to compute. This paper introduces a general image-based approach for selecting aspect ratios for a wide variety of 2D diagrams, ranging from scatter plots and density function plots to line charts. Our approach is derived from Federer\u00eds co-area formula and a line integral representation that enable us to directly construct image-based versions of existing selection methods using density fields. In contrast to previous methods, our approach bypasses isoline computation, so it is faster to compute, while following the perceptual foundation to select aspect ratios. Furthermore, this approach is complemented by an anisotropic kernel density estimation to construct density fields, allowing us to more faithfully characterize data patterns, such as the subgroups in scatterplots or dense regions in time series. We demonstrate the effectiveness of our approach by quantitatively comparing to previous methods and revisiting a prior user study. Finally, we present extensions for ROI banking, multi-scale banking, and the application to image data.", "uri": "https://vimeo.com/289784435", "name": "[VIS18 Preview] Image-based Aspect Ratio Selection (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-09-13T23:42:07+00:00", "description": "Authors: Ethan Kerzner, Sarah Goodwin, Jason Dykes, Sara V Jones, Miriah Meyer\n\nAbstract: Applied visualization researchers often work closely with domain collaborators to explore new and useful applications of visualization. The early stages of collaborations are typically time consuming for all stakeholders as researchers piece together an understanding of domain challenges from disparate discussions and meetings. A number of recent projects, however, report on the use of creative visualization-opportunities (CVO) workshops to accelerate the early stages of applied work, eliciting a wealth of requirements in a few days of focused work. Yet, there is no established guidance for how to use such workshops effectively. In this paper, we present the results of two-year collaboration in which we analyzed the use of 17 workshops in 10 visualization contexts. Its primary contribution is a framework for CVO workshops that 1) identifies a process model for using workshops; 2) describes a structure of what happens within effective workshops; 3) recommends 25 actionable guidelines for future workshops; and 4) presents an example workshop and workshop methods. The creation of this framework exemplifies the use of critical reflection to learn about visualization in practice from diverse studies and experience", "uri": "https://vimeo.com/289784415", "name": "[VIS18 Preview] A Framework for Creative Visualization-Opportunities Workshops (InfoVis Paper)", "year": "2018", "event": "INFOVIS, PREVIEW"}, {"created_time": "2018-06-02T22:12:23+00:00", "description": "Authors: Artem Konev, J\u00fcrgen Waser, Bernhard Sadransky, Daniel Cornel, Rui A. P. Perdig\u00e3o, Zsolt Horv\u00e1th, Eduard Gr\u00f6ller", "uri": "https://vimeo.com/273133628", "name": "VAST 2014: Run Watchers: Automatic Simulation-Based Decision Support in Flood Management", "year": "2018", "event": "VAST"}, {"created_time": "2018-06-02T22:06:40+00:00", "description": "Authors: Abish Malik, Ross Maciejewski, Sherry Towers, Sean McCullough, David Ebert", "uri": "https://vimeo.com/273133290", "name": "VAST 2014: The Problem of Scale in Visual Analytics: Proactive Spatiotemporal Resource Allocation and Predictive Visual Analytic", "year": "2018", "event": "VAST"}, {"created_time": "2018-06-02T16:24:01+00:00", "description": "Authors: Sungahn Ko, Jieqiong Zhao, Jing Xia, Xiaoyu Wang, Greg Abram, Niklas Elmqvist, Shaun Kennedy, Kelly Gaither, William Tolone, William Ribarsky, David Ebert", "uri": "https://vimeo.com/273108861", "name": "VAST 2014: VASA: Interactive Computational Steering of Large Asynchronous Simulation Pipelines for Societal Infrastructure", "year": "2018", "event": "VAST"}, {"created_time": "2018-06-02T16:15:48+00:00", "description": "Authors: Jiawan Zhang, Yanli E, Jing Ma, Yahui Zhao, Binghan Xu, Liting Sun, Jinyan Chen, Xiaoru Yuan", "uri": "https://vimeo.com/273108178", "name": "VAST 2014: Visual Analysis of Public Utility Service Problems in a Metropolis", "year": "2018", "event": "VAST"}, {"created_time": "2018-06-02T16:03:37+00:00", "description": "Authors: Wei Zeng, Chi-Wing Fu, Stefan M\u00fcller Arisona, Alexander Erath, Huamin Qu", "uri": "https://vimeo.com/273107166", "name": "VAST 2014: Visualizing Mobility of Public Transportation System", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T17:27:31+00:00", "description": "Authors: Jian Zhao, Liang Gou, Fei Wang, Michelle Zhou", "uri": "https://vimeo.com/272408293", "name": "VAST 2014: PEARL: An Interactive Visual Analytic Tool for Understanding Personal Emotion Style Derived from Social Media", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T17:21:21+00:00", "description": "Authors: Yafeng Lu, Robert Krueger, Dennis Thom, Feng Wang, Steffen Koch, Thomas Ertl, Ross Maciejewski", "uri": "https://vimeo.com/272407209", "name": "VAST 2014: Integrating Predictive Analytics and Social Media", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T17:15:29+00:00", "description": "Authors: Shixia Liu, Xiting Wang, Jianfei Chen, Jun Zhu, Baining Guo", "uri": "https://vimeo.com/272406070", "name": "VAST 2014: TopicPanorama: a Full Picture of Relevant Topics", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T17:12:04+00:00", "description": "Authors: Eric Alexander, Joe Kohlmann, Michael Witmore, Robin Valenza, Michael Gleicher", "uri": "https://vimeo.com/272405267", "name": "VAST 2014: Serendip: Topic Model-Driven Visual Exploration of Text Corpora", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T17:06:57+00:00", "description": "Authors: Lauren Bradel, Chris North, Leanna House, Scotland Leman", "uri": "https://vimeo.com/272404441", "name": "VAST 2014: Multi-Model Semantic Interaction for Text Analytics", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T17:00:32+00:00", "description": "Authors: Johanna Schmidt, Reinhold Preiner, Thomas Auzinger, Michael Wimmer, Eduard Gr\u00f6ller, Stefan Bruckner", "uri": "https://vimeo.com/272403271", "name": "VAST 2014: YMCA: Your Mesh Comparison Application", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T16:57:13+00:00", "description": "Authors: Wenchao Wu, Yixian Zheng, Huamin Qu, Wei Chen, Eduard Gr\u00f6ller, Lionel Ni", "uri": "https://vimeo.com/272402399", "name": "VAST 2014: BoundarySeer: Visual Analysis of 2D Boundary Changes", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T16:51:21+00:00", "description": "Authors: Jie Li, Kang Zhang, Zhao-Peng Meng", "uri": "https://vimeo.com/272400688", "name": "VAST 2014: Vismate: Interactive Visual Analysis of Station-Based Observation Data on Climate Changes", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T16:36:48+00:00", "description": "Authors: Pierre Accorsi, Micka\u00ebl Fabr\u00e8gue, Arnaud Sallaberry, Flavie Cernesson, Nathalie Lalande, Agn\u00e8s Braud, Sandra Bringay, Florence Le Ber, Pascal Poncelet, Maguelonne Teisseire", "uri": "https://vimeo.com/272398129", "name": "VAST 2014: HydroQual: Visual Analysis of River Water Quality", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T16:30:09+00:00", "description": "Authors: Krist Wongsuphasawat and Jimmy Lin", "uri": "https://vimeo.com/272396855", "name": "VAST 2014: Using Visualizations to Monitor Changes and Harvest Insights from Log Data at Twitter", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T14:08:41+00:00", "description": "Authors: Zuchao Wang, Tangzhi Ye, Min Lu, Xiaoru Yuan, Huamin Qu, Jacky Yuan, Qianliang Wu", "uri": "https://vimeo.com/272368868", "name": "VAST 2014: Visual Exploration of Sparse Traffic Trajectory Data", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T13:58:59+00:00", "description": "Authors: Ellen Isaacs, Kelly Domico, Shane Ahern, Eugene Bart, Mudita Singhal", "uri": "https://vimeo.com/272367023", "name": "VAST 2014: Footprints: A Visual Search Tool that Supports Discovery and Coverage Tracking", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T13:54:07+00:00", "description": "Authors: David Gotz and Harry Stavropoulos", "uri": "https://vimeo.com/272366015", "name": "VAST 2014: DecisionFlow: Visual Analytics for High-Dimensional Temporal Event Sequence Data", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T13:51:54+00:00", "description": "Authors: Jorge Poco, Aritra Dasgupta, Yaxing Wei, William Hargrove, Christopher Schwalm, Deborah Huntzinger, Robert Cook, Enrico Bertini, Claudio Silva", "uri": "https://vimeo.com/272365561", "name": "VAST 2014: Visual Reconciliation of Alternative Similarity Spaces in Climate Modeling", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T13:48:08+00:00", "description": "Authors: Bowen Yu, Harish Doraiswamy, Xi Chen, Emily Miraldi, Mario Luis Arrieta-Ortiz, Christoph Hafemeister, Aviv Madar, Richard Bonneau, Claudio Silva", "uri": "https://vimeo.com/272364879", "name": "VAST 2014: Genotet: An Interactive Web-based Visual Exploration Framework to Support Validation of Gene Regulatory Networks", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T13:42:22+00:00", "description": "Authors: Patrick K\u00f6thur, Mike Sips, Henryk Dobslaw, Doris Dransch", "uri": "https://vimeo.com/272363784", "name": "VAST 2014: Visual Analytics for Comparison of Ocean Model Output with Reference Data: Detecting and Analyzing Geophysical Proces", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-29T13:37:24+00:00", "description": "Authors: Christian Partl, Alexander Lex, Marc Streit, Hendrik Strobelt, Anne-Mai Wassermann, Hanspeter Pfister, Dieter Schmalstieg", "uri": "https://vimeo.com/272362853", "name": "VAST 2014: ConTour: Data-Driven Exploration of Multi-Relational Datasets for Drug Discovery", "year": "2018", "event": "VAST"}, {"created_time": "2018-05-27T15:35:59+00:00", "description": "Authors: Jeremy Boy, Ronald Rensink, Enrico Bertini, Jean-Daniel Fekete", "uri": "https://vimeo.com/272094936", "name": "InfoVIS 2014: A Principled Way of Assessing Visualization Literacy", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T15:34:19+00:00", "description": "Authors: Connor Gramazio, Karen Schloss, David Laidlaw", "uri": "https://vimeo.com/272094806", "name": "InfoVIS 2014: The relation between visualization size, grouping, and user performance", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T15:31:14+00:00", "description": "Authors: Lane Harrison, Fumeng Yang, Steven Franconeri, Remco Chang", "uri": "https://vimeo.com/272094561", "name": "InfoVIS 2014: Ranking Visualization of Correlation Using Weber\u2019s Law", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T15:29:56+00:00", "description": "Authors: \u00c7a\u011fatay Demiralp, Michael Bernstein, Jeffrey Heer", "uri": "https://vimeo.com/272094478", "name": "InfoVIS 2014: Learning Perceptual Kernels for Visualization Design", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T15:17:24+00:00", "description": "Authors: Rita Borgo, Joel Dearden, Mark W. Jones", "uri": "https://vimeo.com/272093532", "name": "InfoVIS 2014: Order of Magnitude Markers: An Empirical Study on Large Magnitude Number Detection", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T15:13:40+00:00", "description": "Authors: Johannes Fuchs, Petra Isenberg, Anastasia Bezerianos, Fabian Fischer, Enrico Bertini", "uri": "https://vimeo.com/272093234", "name": "InfoVIS 2014: The Influence of Contour on Similarity Perception of Star Glyphs", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T15:12:12+00:00", "description": "Authors: Fanny Chevalier, Pierre Dragicevic, Steven Franconeri", "uri": "https://vimeo.com/272093119", "name": "InfoVIS 2014: The Not-so-Staggering Effect of Staggered Animated Transitions on Visual Tracking", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T15:10:19+00:00", "description": "Authors: Bahador Saket, Paolo Simonetto, Stephen Kobourov, Katy Borner", "uri": "https://vimeo.com/272092999", "name": "InfoVIS 2014: Node, Node-Link, and Node-Link-Group Diagrams: An Evaluation", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T15:07:52+00:00", "description": "Authors: Rudolf Netzel, Michael Burch, Daniel Weiskopf", "uri": "https://vimeo.com/272092820", "name": "InfoVIS 2014: Comparative Eye Tracking Study on Node-Link Visualizations of Trajectories", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:56:29+00:00", "description": "Authors: Ali Al-Awami, Johanna Beyer, Hendrik Strobelt, Narayanan Kasthuri, Jeff W. Lichtman, Hanspeter Pfister, Markus Hadwiger", "uri": "https://vimeo.com/272092000", "name": "InfoVIS 2014: NeuroLines: A Subway Map Metaphor for Visualizing Nanoscale Neuronal Connectivity", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:47:05+00:00", "description": "Authors: Gregorio Palmas, Myroslav Bachynskyi, Antti Oulasvirta, Hans-Peter Seidel, Tino Weinkauf", "uri": "https://vimeo.com/272091309", "name": "InfoVIS 2014: MovExp: A Versatile Visualization Tool for Human-Computer Interaction Studies with 3D Performance and Biomechanica", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:41:32+00:00", "description": "Authors: Katherine Isaacs, Peer-Timo Bremer, Ilir Jusufi, Todd Gamblin, Abhinav Bhatele, Martin Schulz, Bernd Hamann", "uri": "https://vimeo.com/272090890", "name": "InfoVIS 2014: Combing the Communication Hairball: Visualizing Large-Scale Parallel Execution Traces using Logical Time", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:37:08+00:00", "description": "Authors: Tom Polk, Jing Yang, Yueqi Hu, Ye Zhao", "uri": "https://vimeo.com/272090553", "name": "InfoVIS 2014: TenniVis: Visualization for Tennis Match Analysis", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:33:21+00:00", "description": "Authors: Jaemin Jo, Jaeseok Huh, Jonghun Park, Bohyoung Kim, Jinwook Seo", "uri": "https://vimeo.com/272090277", "name": "InfoVIS 2014: LiveGantt: Interactively Visualizing a Large Manufacturing Schedule", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:30:01+00:00", "description": "Authors: Paul van der Corput, Jarke J. van Wijk", "uri": "https://vimeo.com/272090036", "name": "InfoVIS 2014: Effects of Presentation Mode and Pace Control on Performance in Image Classification", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:28:05+00:00", "description": "Authors: Pascal Goffin, Wesley Willett, Jean-Daniel Fekete, Petra Isenberg", "uri": "https://vimeo.com/272089910", "name": "InfoVIS 2014: Exploring the Placement and Design of Word-Scale Visualizations", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:24:34+00:00", "description": "Authors: Weiwei Cui, Shixia Liu, Zhuofeng Wu, Hao Wei", "uri": "https://vimeo.com/272089665", "name": "InfoVIS 2014: How Hierarchical Topics Evolve in Large Text Corpora", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-27T14:22:26+00:00", "description": "Authors: Matthew Brehmer, Stephen Ingram, Jonathan Stray, Tamara Munzner", "uri": "https://vimeo.com/272089501", "name": "InfoVIS 2014: Overview: The Design, Adoption, and Analysis of a Visual Document Mining Tool for Investigative Journalists", "year": "2018", "event": "INFOVIS"}, {"created_time": "2018-05-18T16:49:43+00:00", "description": "Authors: Mahsa Mirzargar, Ross Whitaker, Mike Kirby", "uri": "https://vimeo.com/270716877", "name": "SciVIS 2014: Curve Boxplot: Generalization of Boxplot for Ensembles of Curves", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:46:52+00:00", "description": "Authors: David Schroeder, Fedor Korsakov, Carissa Mai-Ping Knipe, Lauren Thorson, Arin M. Ellingson, David Nuckley, John Carlis, Daniel F. Keefe", "uri": "https://vimeo.com/270716272", "name": "SciVIS 2014: Trend-Centric Motion Visualization: Designing and Applying a new Strategy for Analyzing Scientific Motion Collectio", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:45:21+00:00", "description": "Authors: Harish Doraiswamy, Nivan Ferreira, Theodoros Damoulas, Juliana Freire, Claudio Silva", "uri": "https://vimeo.com/270715966", "name": "SciVIS 2014: Using Topological Analysis to Support Event-Guided Exploration in Urban Data", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:38:37+00:00", "description": "Authors: Sean Arietta, Alexei Efros, Ravi Ramamoorthi, Maneesh Agrawala", "uri": "https://vimeo.com/270714506", "name": "SciVIS 2014: City Forensics: Using Visual Elements to Predict Non-Visual City Attributes", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:33:38+00:00", "description": "Author: Jarke J. van Wijk", "uri": "https://vimeo.com/270713428", "name": "SciVIS 2014: Visualization of Regular Maps: The Chase Continues", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:31:03+00:00", "description": "Authors: Gustavo Mello Machado, Filip Sadlo, Thomas M\u00fcller, Thomas Ertl", "uri": "https://vimeo.com/270712865", "name": "SciVIS 2014: Escape Maps", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:27:14+00:00", "description": "Authors: Attila Gyulassy, David Guenther, Joshua A. Levine, Julien Tierny, Valerio Pascucci", "uri": "https://vimeo.com/270712034", "name": "SciVIS 2014: Conforming Morse-Smale Complexes", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:20:23+00:00", "description": "Authors: David Guenther, Alec Jacobson, Jan Reininghaus, Hans-Peter Seidel, Olga Sorkine-Hornung, Tino Weinkauf", "uri": "https://vimeo.com/270710546", "name": "SciVIS 2014: Fast and Memory-Efficient Topological Denoising of 2D and 3D Scalar Fields", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:17:21+00:00", "description": "Authors: Hui Zhang, Jianguang Weng, Guangchen Ruan", "uri": "https://vimeo.com/270709829", "name": "SciVIS 2014: Visualizing 2-dimensional Manifolds with Curve Handles in 4D", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:10:29+00:00", "description": "Authors: Lars Huettenberger, Christian Heine, Christoph Garth", "uri": "https://vimeo.com/270708402", "name": "SciVIS 2014: Decomposition and Simplification of Multivariate Data using Pareto Sets", "year": "2018", "event": "SCIVIS"}, {"created_time": "2018-05-18T16:02:52+00:00", "description": "Author: Peter Lindstrom", "uri": "https://vimeo.com/270706722", "name": "SciVIS 2014: Fixed-Rate Compressed Floating-Point Arrays", "year": "2018", "event": "SCIVIS"}, {"created_time": "2017-12-05T02:35:48+00:00", "description": "Authors: David L\u00f3pez, Lora Oehlberg, Candemir Doger, Tobias Isenberg", "uri": "https://vimeo.com/245846750", "name": "SciVis 2015: Towards an Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-11-29T23:34:53+00:00", "description": "Authors: Kai Lawonn, Sylvia Glasser, Anna Vilanova, Bernhard Preim, Tobias Isenberg", "uri": "https://vimeo.com/245103942", "name": "SciVis 2015: Occlusion-free Blood Flow Animation with Wall Thickness Visualization", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-11-27T22:33:50+00:00", "description": "Robert S. Laramee (organizer), Swansea University; Thomas Ertl, University of Stuttgart; Chris Johnson, University of Utah; Robert Moorhead, Mississippi State University; Penny Rheingans, University of Maryland; William Ribarsky, University of North Carolina", "uri": "https://vimeo.com/244734094", "name": "VIS 2015: [Panel] Solved Problems in Visualization", "year": "2017", "event": "PANEL"}, {"created_time": "2017-11-16T21:27:36+00:00", "description": "Organizers: Bernd Hentschel, RWTH Aachen University; Daniela Oelke, Siemens AG; Justin Talbot, Tableau Research", "uri": "https://vimeo.com/243196574", "name": "VIS 2017: [Workshop] VIP: Visualization Solutions in the Wild", "year": "2017", "event": "WORKSHOP"}, {"created_time": "2017-11-13T23:11:00+00:00", "description": "Authors: Zhongyuan Yu, Kara Pepe, George Rust, Jose Emmanuel Ramirez-Marquez, Shun Zhang, and Bryan Bonnet", "uri": "https://vimeo.com/242662517", "name": "VAHC 2017: Patient-Provider Geographic Map: An Interactive Visualization Tool of Patients' Selection of Health Care Providers", "year": "2017", "event": "VAHC"}, {"created_time": "2017-11-13T22:43:53+00:00", "description": "Hadi Kharrazi, MD, PhD, MHI (John Hopkins Bloomberg School of Public Health/Johns Hopkins School of Medicine)", "uri": "https://vimeo.com/242659279", "name": "VAHC 2017: [Keynote] Population Health Informatics and Visualization: Challenges and Opportunities", "year": "2017", "event": "KEYNOTE"}, {"created_time": "2017-11-13T01:44:53+00:00", "description": "Authors: Qiao Gu, Lian Chen, Hang Yin, Haotian Li, Chengzhong Liu, Xuanwu Yue, and Huamin Qu", "uri": "https://vimeo.com/242503907", "name": "VAST Challenge 2017: PreserVis, a Visual Analytic System for Traffic and Pollution Patterns", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-13T01:04:41+00:00", "description": "Authors: Chufan Lai, Geozheng Li, and Shuai Chen", "uri": "https://vimeo.com/242500472", "name": "VAST Challenge 2017: Interactive and Collaborative Visual Analysis on Traffic Sensor Data, and Temporal Pattern Analysis...", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-13T00:50:50+00:00", "description": "Authors: Zheng Zhou, Hui Tang, and Wenjie Wu", "uri": "https://vimeo.com/242499465", "name": "VAST Challenge 2017: ClockPetals: Interactive Sequential Analysis of Traffic Patterns and WindNebula: Vectorial-Tempor", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-13T00:46:06+00:00", "description": "Authors: V. Mahida, B. Kupiec, A. Burks, T. Luciani, and G. E. Marai", "uri": "https://vimeo.com/242499136", "name": "VAST Challenge 2017: A Web-Based Interactive Image Explorer for Temporal Analysis of Satellite Images", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-13T00:39:10+00:00", "description": "Authors: Ghulam Quadri, Sulav Malla, Anwehst Tuladhar, and Paul Rosen", "uri": "https://vimeo.com/242498603", "name": "VAST Challenge 2017: Multi-Spectral Satellite Image Analysis for Feature Identification and Change Detection", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-12T22:47:42+00:00", "description": "Authors: Joseph Borowicz, J. Castor, A. Burks, M. Thomas, T. Luciani, and G. E. Marai", "uri": "https://vimeo.com/242490082", "name": "VAST Challenge 2017: Mining Factory Pollution Data through a Spatial-Nonspatial Flow Approach", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-12T22:41:18+00:00", "description": "Authors: Johannes Liem and Jo Wood", "uri": "https://vimeo.com/242489529", "name": "VAST Challenge 2017: Visual Analytic Design for Detecting Airborne Pollution Sources", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-12T22:38:54+00:00", "description": "Authors: Wooil Kim, Changboem Shim, Ilhyun Suh, and Yon Dohn Chung", "uri": "https://vimeo.com/242489334", "name": "VAST Challenge 2017: A Visual Explorer for Analyzing Trajectory Patterns", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-12T22:30:01+00:00", "description": "Authors: Juri Buchm\u00fcller, Wolfgang Jentner, Dirk Streeb, and Daniel A. Keim", "uri": "https://vimeo.com/242488559", "name": "VAST Challenge 2017: ODIX: A Rapid Hypotheses Testing System for Origin-Destination Data", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-12T22:20:00+00:00", "description": "Author: Guan Yifei, Singapore Management University", "uri": "https://vimeo.com/242487755", "name": "VAST Challenge 2017: Interactive Visual Analytic Application for Spatiotemporal Movement Data", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-12T22:12:19+00:00", "description": "Author: Bram Cappers, Eindhoven University of Technology", "uri": "https://vimeo.com/242487107", "name": "VAST Challenge 2017: Exploring Lekagul Sensor Events using Rules, Aggregations, and Selections", "year": "2017", "event": "VAST"}, {"created_time": "2017-11-09T19:56:48+00:00", "description": "Presenter: Theresa-Marie Rhyne", "uri": "https://vimeo.com/242122443", "name": "VIS 2017: [Tutorial] Applying Color Theory to VIS", "year": "2017", "event": "TUTORIAL"}, {"created_time": "2017-11-08T01:30:36+00:00", "description": "Presenter: Tamara Munzner, University of British Columbia", "uri": "https://vimeo.com/241799817", "name": "VIS 2017: [Tutorial] Visualization Analysis & Design", "year": "2017", "event": "TUTORIAL"}, {"created_time": "2017-11-07T01:12:46+00:00", "description": "Giorgia Lupi (PhD, Politecnico di Milano), co-founder Accurat", "uri": "https://vimeo.com/241617101", "name": "VIS 2017: [Capstone] Data Humanism: The Revolution will be Visualized", "year": "2017", "event": "CAPSTONE"}, {"created_time": "2017-11-06T21:40:01+00:00", "description": "Jacqueline H. Chen, Sandia National Laboratories", "uri": "https://vimeo.com/241590688", "name": "VIS 2017: [Keynote] Analytics Inspired Visualization: a Holistic In-situ Scientific Workflow at Extreme Scale", "year": "2017", "event": "KEYNOTE"}, {"created_time": "2017-10-31T21:17:50+00:00", "description": "Panelists: Steven M. Drucker, Microsoft Research; Adam Perer, IBM Research; Daniela Oelke, Siemens; Melanie Tory, Tableau Research; Krist Wongsuphasawat, Twitter", "uri": "https://vimeo.com/240741714", "name": "VIS 2017: [Panel] Increasing the Impact of Visualization Research", "year": "2017", "event": "PANEL"}, {"created_time": "2017-10-31T20:41:34+00:00", "description": "Panelists: Ronald Rensink, University of British Columbia; Steven Franconeri, Northwestern University; Karen Schloss, University of Wisconsin-Madison; Ruth Rosenholtz, Masachusetts Institute of Technology", "uri": "https://vimeo.com/240736387", "name": "VIS 2017: [Panel] Vision Science meets Visualization", "year": "2017", "event": "PANEL"}, {"created_time": "2017-10-30T23:21:38+00:00", "description": "Robert S. Laramee (Organizer), Swansea University; Rita Borgo, Kings College; Vetria Byrd, Purdue Polytechnic Institute; Aviva Frank, State University of New York, Purchase; Kelly Gaither, University of Texas, Austin; Ronald Metoyer, University of Notre Dame; Erica Yang, Science and Technology Facilities Council (STFC)", "uri": "https://vimeo.com/240582265", "name": "VIS 2017: [Panel] Diversity in Visualization", "year": "2017", "event": "PANEL"}, {"created_time": "2017-10-29T21:43:52+00:00", "description": "Author: Vasant Dhar (New York University)", "uri": "https://vimeo.com/240401180", "name": "VDS 2017: [Keynote] When Should We Trust Autonomous Learning Systems with Decision Making?", "year": "2017", "event": "KEYNOTE"}, {"created_time": "2017-10-29T21:39:08+00:00", "description": "Author: Hadley Wickham (RStudio)", "uri": "https://vimeo.com/240400798", "name": "VDS 2017: [Keynote] You can't do data science in a GUI", "year": "2017", "event": "KEYNOTE"}, {"created_time": "2017-10-28T21:54:38+00:00", "description": "Timo Ropinski (Organizer), Ulm University; Daniel Archambault, Swansea University; Min Chen, Oxford University; Ross Maciejewski, Arizona State University; Klaus Mueller, Stony Brook University; Alexandru Telea, University of Groningen; Martin Wattenberg, Google", "uri": "https://vimeo.com/240317738", "name": "VIS 2017: [Panel] How do Recent Machine Learning Advances Impact the Data Visualization Research Agenda?", "year": "2017", "event": "PANEL"}, {"created_time": "2017-10-18T22:58:42+00:00", "description": "Authors: Oliver R\u00fcbel, Benjamin P. Bowen", "uri": "https://vimeo.com/238855116", "name": "SciVis 2017: BASTet: Shareable and reproducible analysis and visualization of mass spectrometry imaging data via OpenMSI", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-18T22:55:32+00:00", "description": "Authors: Qiaomu Shen, Wei Zeng, Yu Ye, Stefan M\u00fcller Arisona, Simon Schubiger, Remo Burkhard, Huamin Qu", "uri": "https://vimeo.com/238854784", "name": "SciVis 2017: StreetVizor: Visual Exploration of Human-Scale Urban Forms Based on Street Views", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-18T22:45:36+00:00", "description": "Authors: Haichao Miao, Elisa De Llano, Johannes Sorger, Yasaman Ahmadi, Tadija Kekic, Tobias Isenberg, M. Eduard Gr\u00f6ller, Ivan Bari\u0161i\u0107, Ivan Viola", "uri": "https://vimeo.com/238853739", "name": "SciVis 2017: Multiscale Visualization and Scale-Adaptive Modification of DNA Nanostructures", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-18T22:43:59+00:00", "description": "Authors: Zening Qu, Jessica Hullman", "uri": "https://vimeo.com/238853591", "name": "InfoVis 2017: Keeping Multiple Views Consistent: Constraints, Validations, and Exceptions in Visualization Authoring", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-18T22:37:40+00:00", "description": "Authors: Benjamin Bach, Ronell Sicat, Johanna Beyer, Maxime Cordeil, Hanspeter Pfister", "uri": "https://vimeo.com/238852930", "name": "InfoVis 2017: The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augme", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-18T22:33:17+00:00", "description": "Authors: Jessica Hullman, Matthew Kay, Yea-Seul Kim, Samana Shrestha", "uri": "https://vimeo.com/238852435", "name": "InfoVis 2017: Imagining Replications: Graphical Prediction & Discrete Visualizations Improve Recall & Estimation of Effect Uncer", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-18T22:31:04+00:00", "description": "Authors: Josua Krause, Aritra Dasgupta, Jordan Swartz, Yindalon Aphinyanaphongs, Enrico Bertini", "uri": "https://vimeo.com/238852188", "name": "VAST 2017: A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-18T22:29:05+00:00", "description": "Authors: Shixia Liu, Jiannan Xiao, Junlin Liu, Xiting Wang, Jing Wu, Jun Zhu", "uri": "https://vimeo.com/238851978", "name": "VAST 2017: Visual Diagnosis of Tree Boosting Methods", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-18T22:26:09+00:00", "description": "Authors: Bilal Alsallakh, Amin Jourabloo, Mao Ye, Xiaoming Liu, Liu Ren", "uri": "https://vimeo.com/238851698", "name": "VAST 2017: Do Convolutional Neural Networks Learn Class Hierarchy?", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-18T22:24:21+00:00", "description": "Authors: Peter Mindek, David Kou\u0159il, Johannes Sorger, Daniel Toloudis, Blair Lyons, Graham Johnson, M. Eduard Gr\u00f6ller, Ivan Viola", "uri": "https://vimeo.com/238851516", "name": "SciVis 2017: Visualization Multi-Pipeline for Communicating Biology", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-18T22:22:25+00:00", "description": "Authors: Julian Kreiser, Alexander Hann, Eugen Zizer, Timo Ropinski", "uri": "https://vimeo.com/238851294", "name": "SciVis 2017: Decision Graph Embedding for High-Resolution Manometry Diagnosis", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-18T22:17:55+00:00", "description": "Authors: Tobias Klein, Ludovic Autin, Barbora Kozl\u00edkov\u00e1, David S. Goodsell, Arthur Olson, M. Eduard Gr\u00f6ller, Ivan Viola", "uri": "https://vimeo.com/238850791", "name": "SciVis 2017: Instant Construction and Visualization of Crowded Biological Environments", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-18T22:13:12+00:00", "description": "Authors: Haneen Mohammed, Ali K. Al-Awami, Johanna Beyer, Corrado Cali, Pierre Magistretti, Hanspeter Pfister, Markus Hadwiger", "uri": "https://vimeo.com/238850198", "name": "SciVis 2017: Abstractocyte: A Visual Tool for Exploring Nanoscale Astroglial Cells", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-17T00:47:40+00:00", "description": "Authors: Dustin Arendt, Megan Pirrung", "uri": "https://vimeo.com/238503149", "name": "VAST 2017: The y of it Matters: Even for Storyline Visualization", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-17T00:46:31+00:00", "description": "Authors: Xumeng Wang, Jia-Kai Chou, Wei Chen, Huihua Guan, Wenlong Chen, Tianyi Lao, Kwan-Liu Ma", "uri": "https://vimeo.com/238503046", "name": "VAST 2017: A Utility-aware Visual Approach for Anonymizing Multi-attribute Tabular Data", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-17T00:44:58+00:00", "description": "Authors: Yan-Ting Kuan, Yu-Shuen Wang, Jung-Hong Chuang", "uri": "https://vimeo.com/238502893", "name": "VAST 2017: Visualizing Real-Time Strategy Games: The Example of StarCraft II", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-17T00:43:52+00:00", "description": "Authors: Takanori Fujiwara, Preeti Malakar, Khairi Reda, Venkatram Vishwanath, Michael Papka, Kwan-Liu Ma", "uri": "https://vimeo.com/238502779", "name": "VAST 2017: A Visual Analytics System for Optimizing Communications in Massively Parallel Applications", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-17T00:43:01+00:00", "description": "Authors: Nan-Chen Chen, Been Kim", "uri": "https://vimeo.com/238502693", "name": "VAST 2017: QSAnglyzer: Visual Analytics for Prismatic Analysis of Question Answering System Evaluations", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-17T00:42:00+00:00", "description": "Authors: Fritz Lekschas, Benjamin Bach, Peter Kerpedjiev, Nils Gehlenborg, Hanspeter Pfister", "uri": "https://vimeo.com/238502606", "name": "InfoVis 2017: HiPiler: Visual Exploration of Large Genome Interaction Matrices with Interactive Small Multiples", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:40:35+00:00", "description": "Authors: Arjun Srinivasan, John Stasko", "uri": "https://vimeo.com/238502489", "name": "InfoVis 2017: Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:38:45+00:00", "description": "Authors: Christophe Hurter, St\u00e9phane Puechmorel, Florence Nicol, Alexandru Telea", "uri": "https://vimeo.com/238502320", "name": "InfoVis 2017: Functional Decomposition for Bundled Simplification of Trail Sets", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:37:52+00:00", "description": "Authors: Yunhai Wang, Yanyan Wang, Yingqi Sun, Lifeng Zhu, Kecheng Lu, Chi-Wing Fu, Michael Sedlmair, Oliver Deussen, Baoquan Chen", "uri": "https://vimeo.com/238502264", "name": "InfoVis 2017: Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:37:00+00:00", "description": "Authors: Oh-Hyun Kwon, Tarik Crnovrsanin, Kwan-Liu Ma", "uri": "https://vimeo.com/238502205", "name": "InfoVis 2017: What Would a Graph Look Like in This Layout? A Machine Learning Approach to Large Graph Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:36:07+00:00", "description": "Authors: Jonathan C. Roberts, Panagiotis D. Ritsos, James R. Jackson, Christopher Headleand", "uri": "https://vimeo.com/238502119", "name": "InfoVis 2017: The Explanatory Visualization Framework: An active learning framework for teaching creative computing using explan", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:35:24+00:00", "description": "Authors: Pierre Dragicevic, Yvonne Jansen", "uri": "https://vimeo.com/238502056", "name": "InfoVis 2017: Blinded with Science or Informed by Charts? A Replication Study", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:34:06+00:00", "description": "Authors: Jagoda Walny, Samuel Huron, Charles Perin, Tiffany Wun, Richard Pusch, Sheelagh Carpendale", "uri": "https://vimeo.com/238501936", "name": "InfoVis 2017: Active Reading of Visualizations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:33:30+00:00", "description": "Authors: Yea-Seul Kim, Katharina Reinecke, Jessica Hullman", "uri": "https://vimeo.com/238501884", "name": "InfoVis 2017: Data Through Others\u2019 Eyes: The Impact of Visualizing Others\u2019 Expectations on Visualization Interpretation", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-17T00:32:42+00:00", "description": "Authors: Evanthia Dimara, Anastasia Bezerianos, Pierre Dragicevic", "uri": "https://vimeo.com/238501814", "name": "InfoVis 2017: Conceptual and Methodological Issues in Evaluating Multidimensional Visualizations for Decision Support", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-16T20:09:08+00:00", "description": "Authors: Bum Chul Kwon, Ben Eysenbach, Janu Verma, Kenney Ng, Christopher deFilippi, Walter F. Stewart, Adam Perer", "uri": "https://vimeo.com/238468808", "name": "VAST 2017: Clustervision: Visual Supervision of Unsupervised Clustering", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T20:05:07+00:00", "description": "Authors: John Wenskovitch, Ian Crandell, Naren Ramakrishnan, Leanna House, Scotland Leman, Chris North", "uri": "https://vimeo.com/238468223", "name": "VAST 2017: Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T20:03:48+00:00", "description": "Authors: Dominik Sacha, Matthias Kraus, J\u00fcrgen Bernard, Michael Behrisch, Tobias Schreck, Yuki Asano, Daniel A. Keim", "uri": "https://vimeo.com/238468013", "name": "VAST 2017: SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T20:02:44+00:00", "description": "Authors: Alexander Kumpf, Bianca Tost, Marlene Baumgart, Michael Riemer, R\u00fcdiger Westermann, Marc Rautenhaus", "uri": "https://vimeo.com/238467837", "name": "VAST 2017: Visualizing Confidence in Cluster-based Ensemble Weather Forecast Analyses", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T20:01:21+00:00", "description": "Authors: Stefan J\u00e4nicke, David Wrisley", "uri": "https://vimeo.com/238467627", "name": "VAST 2017: Interactive Visual Alignment of Medieval Text Versions", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T20:00:18+00:00", "description": "Authors: Jie Liu, Tim Dwyer, Kim Marriott, Jeremy Millar, Annette Haworth", "uri": "https://vimeo.com/238467466", "name": "VAST 2017: Understanding the Relationship between Interactive Optimisation and Visual Analytics in the Context of Prostate Brach", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T19:59:29+00:00", "description": "Authors: Enamul Hoque, Vidya Setlur, Melanie Tory, Isaac Dykeman", "uri": "https://vimeo.com/238467322", "name": "VAST 2017: Applying Pragmatics Principles for Interaction with Visual Analytics", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T19:58:04+00:00", "description": "Authors: J\u00fcrgen Bernard, Marco Hutter, Matthias Zeppelzauer, Dieter Fellner, Michael Sedlmair", "uri": "https://vimeo.com/238467126", "name": "VAST 2017: Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T19:57:19+00:00", "description": "Authors: Emily Wall, Subhajit Das, Ravish Chawla, Bharath Kalidindi, Eli T. Brown, Alex Endert", "uri": "https://vimeo.com/238467013", "name": "VAST 2017: Podium: Ranking Data Using Mixed-Initiative Visual Analytics", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T04:38:08+00:00", "description": "Authors: Andrea Batch, Niklas Elmqvist", "uri": "https://vimeo.com/238344637", "name": "VAST 2017: The Interactive Visualization Gap in Initial Exploratory Data Analysis", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T04:37:24+00:00", "description": "Authors: Emily Wall, Leslie Blaha, Lyndsey Franklin, Alex Endert", "uri": "https://vimeo.com/238344585", "name": "VAST 2017: Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T04:36:24+00:00", "description": "Authors: Paolo Federico, Markus Wagner, Alexander Rind, Albert Amor-Amoros, Silvia Miksch, Wolfgang Aigner", "uri": "https://vimeo.com/238344488", "name": "VAST 2017: The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T04:34:58+00:00", "description": "Authors: Darren Edge, Nathalie Henry Riche, Jonathan Larson, Christopher White", "uri": "https://vimeo.com/238344371", "name": "VAST 2017: Beyond Tasks: An Activity Typology for Visual Analytics", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T04:33:48+00:00", "description": "Authors: Jun Wang, Klaus Mueller", "uri": "https://vimeo.com/238344268", "name": "VAST 2017: Visual Causality Analysis Made Practical", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T04:32:53+00:00", "description": "Authors: Jian Zhao, Michael Glueck, Petra Isenberg, Fanny Chevalier, Azam Khan", "uri": "https://vimeo.com/238344210", "name": "VAST 2017: Supporting Handoff in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-16T04:32:00+00:00", "description": "Authors: Roger A. Leite, Theresia Gschwandtner, Silvia Miksch, Simone Kriglstein, Margit Pohl, Erich Gstrein, Johannes Kuntner", "uri": "https://vimeo.com/238344119", "name": "VAST 2017: EVA: Visual Analytics to Identify Fraudulent Events", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T22:22:59+00:00", "description": "Authors: Thomas H\u00f6llt, Nicola Pezzotti, Vincent van Unen, Frits Koning, Boudewijn P.F. Lelieveldt, Anna Vilanova", "uri": "https://vimeo.com/238315291", "name": "InfoVis 2017: CyteGuide: Visual Guidance for Hierarchical Single-Cell Analysis", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-15T22:22:30+00:00", "description": "Authors: Max Sondag, Bettina Speckmann, Kevin Verbeek", "uri": "https://vimeo.com/238315239", "name": "InfoVis 2017: Stable Treemaps via Local Moves", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-15T22:21:37+00:00", "description": "Authors: Jochen G\u00f6rtler, Christoph Schulz, Daniel Weiskopf, Oliver Deussen", "uri": "https://vimeo.com/238315152", "name": "InfoVis 2017: Bubble Treemaps for Uncertainty Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-15T22:20:28+00:00", "description": "Authors: Yingcai Wu, Ji Lan, Xinhuan Shu, Chenyang Ji, Kejian Zhao, Jiachen Wang, Hui Zhang", "uri": "https://vimeo.com/238315055", "name": "InfoVis 2017: iTTVis: Interactive Visualization of Table Tennis Data", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-15T22:18:22+00:00", "description": "Authors: Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander M. Rush", "uri": "https://vimeo.com/238314901", "name": "InfoVis 2017: LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-15T22:17:46+00:00", "description": "Authors: Cristian Felix, Enrico Bertini, Steven Franconeri", "uri": "https://vimeo.com/238314856", "name": "InfoVis 2017: Taking Word Clouds Apart: An Empirical Investigation of the Design Space for Keyword Summaries", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-15T22:15:38+00:00", "description": "Authors: Yunhai Wang, Xiaowei Chu, Chen Bao, Lifeng Zhu, Oliver Deussen, Baoquan Chen, Michael Sedlmair", "uri": "https://vimeo.com/238314701", "name": "InfoVis 2017: EdWordle: Consistency-preserving Word Cloud Editing", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-15T22:14:47+00:00", "description": "Authors: Jorge Poco, Angela Mayhua, Jeffrey Heer", "uri": "https://vimeo.com/238314618", "name": "InfoVis 2017: Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-15T22:13:33+00:00", "description": "Authors: Lawrence Roy, Prashant Kumar, Sanaz Golbabaei, Yue Zhang, Eugene Zhang", "uri": "https://vimeo.com/238314519", "name": "SciVis 2017: Interactive Design and Visualization of Branched Covering Spaces", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-15T22:11:40+00:00", "description": "Authors: Julien Tierny, Guillaume Favelier, Joshua A. Levine, Charles Gueunet, Michael Michaux", "uri": "https://vimeo.com/238314372", "name": "SciVis 2017: The Topology ToolKit", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-15T22:10:18+00:00", "description": "Authors: Bastian Rieck, Ulderico Fugacci, Jonas Lukasczyk, Heike Leitte", "uri": "https://vimeo.com/238314273", "name": "SciVis 2017: Clique Community Persistence: A Topological Visual Analysis Approach for Complex Networks", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-15T22:08:57+00:00", "description": "Authors: Alexander Bock, Harish Doraiswamy, Adam Summers, Cl\u00e1udio Silva", "uri": "https://vimeo.com/238314153", "name": "SciVis 2017: TopoAngler: Interactive Topology-based Extraction of Fishes", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-15T22:07:41+00:00", "description": "Authors: Andrea Unger, Nadine Dr\u00e4ger, Mike Sips, Dirk J. Lehmann", "uri": "https://vimeo.com/238314041", "name": "VAST 2017: Understanding a sequence of sequences: Visual exploration of categorical states in lake sediment cores", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T22:06:17+00:00", "description": "Authors: Siming Chen, Shuai Chen, Lijing Lin, Xiaoru Yuan, Jie Liang, Xiaolong (Luke) Zhang", "uri": "https://vimeo.com/238313935", "name": "VAST 2017: E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T22:05:21+00:00", "description": "Authors: Shunan Guo, Ke Xu, Rongwen Zhao, David Gotz, Hongyuan Zha, Nan Cao", "uri": "https://vimeo.com/238313875", "name": "VAST 2017: EventThread: Visual Summarization and Stage Analysis of Event Sequence Data", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T22:02:30+00:00", "description": "Authors: Yuanzhe Chen, Panpan Xu, Liu Ren", "uri": "https://vimeo.com/238313645", "name": "VAST 2017: Sequence Synopsis: Optimize Visual Summary of Temporal Event Data", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T22:01:27+00:00", "description": "Authors: Nicola Pezzotti, Thomas H\u00f6llt, Jan van Gemert, Boudewijn P.F. Lelieveldt, Elmar Eisemann, Anna Vilanova", "uri": "https://vimeo.com/238313549", "name": "VAST 2017: DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T22:00:38+00:00", "description": "Authors: Yao Ming, Shaozu CAO, Ruixiang Zhang, Zhen LI, Yuanzhe Chen, Yangqiu Song, Huamin Qu", "uri": "https://vimeo.com/238313481", "name": "VAST 2017: Understanding Hidden Memories of Recurrent Neural Networks", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T21:59:48+00:00", "description": "Authors: Mengchen Liu, Jiaxin Shi, Kelei Cao, Jun Zhu, Shixia Liu", "uri": "https://vimeo.com/238313430", "name": "VAST 2017: Analyzing the Training Processes of Deep Generative Models", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T21:57:21+00:00", "description": "Author: Leland Wilkinson", "uri": "https://vimeo.com/238313232", "name": "VAST 2017: Visualizing Big Data Outliers through Distributed Aggregation", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T21:56:15+00:00", "description": "Authors: Xun Zhao, Yanhong Wu, Weiwei Cui, Xinnan Du, Yuan Chen, Yong Wang, Dik Lun Lee, Huamin Qu", "uri": "https://vimeo.com/238313153", "name": "VAST 2017: SkyLens: Visual Analysis of Skyline on Multi-dimensional Data", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T21:55:32+00:00", "description": "Authors: Dominik J\u00e4ckle, Michael Hund, Michael Behrisch, Daniel Keim, Tobias Schreck", "uri": "https://vimeo.com/238313104", "name": "VAST 2017: Pattern Trails: Visual Analysis of Pattern Transitions in Subspaces", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-15T21:54:19+00:00", "description": "Authors: Jiazhi Xia, Fenjin Ye, Wei Chen, Yusi Wang, Weifeng Chen, Yuxin Ma, Anthony K.H. Tung", "uri": "https://vimeo.com/238313004", "name": "VAST 2017: LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-14T21:39:15+00:00", "description": "Authors: Ashok Jallepalli, Julia Docampo-S\u00e1nchez, Jennifer K. Ryan, Bob Haimes, Robert M. Kirby", "uri": "https://vimeo.com/238223919", "name": "SciVis 2017: On the Treatment of Field Quantities and Elemental Continuity in FEM Solutions", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-14T21:35:36+00:00", "description": "Authors: Michael Kern, Tim Hewson, Filip Sadlo, R\u00fcdiger Westermann, Marc Rautenhaus", "uri": "https://vimeo.com/238223674", "name": "SciVis 2017: Robust Detection and Visualization of Jet-stream Core Lines in Atmospheric Flow", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-14T21:34:15+00:00", "description": "Authors: Mennatallah El-Assady, Rita Sevastjanova, Fabian Sperrle, Daniel Keim, Christopher Collins", "uri": "https://vimeo.com/238223588", "name": "VAST 2017: Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-14T21:33:27+00:00", "description": "Authors: Michael Glueck, Mahdi Pakdaman Naeini, Finale Doshi-Velez, Fanny Chevalier, Azam Khan, Daniel Wigdor, Michael Brudno", "uri": "https://vimeo.com/238223548", "name": "VAST 2017: PhenoLines: Phenotype Comparison Visualizations for Disease Subtyping via Topic Models", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-14T21:32:12+00:00", "description": "Authors: Deokgun Park, Seungyeon Kim, Jurim Lee, Jaegul Choo, Nicholas Diakopoulos, Niklas Elmqvist", "uri": "https://vimeo.com/238223471", "name": "VAST 2017: ConceptVector: Text Visual Analytics via Interactive Lexicon Building using Word Embedding", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-14T21:31:16+00:00", "description": "Authors: Petra Isenberg, Florian Heimerl, Steffen Koch, Tobias Isenberg, Panpan Xu, Charles Stolper, Michael Sedlmair, Jian Chen, Torsten Moller, John T. Stasko", "uri": "https://vimeo.com/238223405", "name": "VAST 2017: vispubdata.org: A Metadata Collection about IEEE Visualization (VIS) Publications", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-12T20:42:29+00:00", "description": "Authors: Andr\u00e9 Calero Valdez, Martina Ziefle, Michael Sedlmair", "uri": "https://vimeo.com/237973912", "name": "InfoVis 2017: Priming and Anchoring Effects in Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:40:46+00:00", "description": "Authors: David Burlinson, Kalpathi Subramanian, Paula Goolkasian", "uri": "https://vimeo.com/237973669", "name": "InfoVis 2017: Open vs. Closed Shapes: New Perceptual Categories?", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:39:43+00:00", "description": "Authors: Laura E. Matzen, Michael J. Haass, Kristin M. Divis, Zhiyuan Wang, Andrew T. Wilson", "uri": "https://vimeo.com/237973524", "name": "InfoVis 2017: Data Visualization Saliency Model: A Tool for Evaluating Abstract Data Visualizations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:38:54+00:00", "description": "Authors: Heidi Lam, Melanie Tory, Tamara Munzner", "uri": "https://vimeo.com/237973396", "name": "InfoVis 2017: Bridging From Goals to Tasks with Design Study Analysis Reports", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:37:39+00:00", "description": "Authors: Romain Vuillemot, Jeremy Boy", "uri": "https://vimeo.com/237973229", "name": "InfoVis 2017: Structuring Visualization Mock-ups at a Graphical Level by Dividing the Display Space", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:36:31+00:00", "description": "Author: Michael Gleicher", "uri": "https://vimeo.com/237973076", "name": "InfoVis 2017: Considerations for Visualizing Comparison", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:35:35+00:00", "description": "Authors: Alper Sarikaya, Michael Gleicher", "uri": "https://vimeo.com/237972937", "name": "InfoVis 2017: Scatterplots: Tasks, Data, and Designs", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:33:24+00:00", "description": "Authors: Shusen Liu, Peer-Timo Bremer, Jayaraman J. Thiagarajan, Vivek Srikumar, Bei Wang, Yarden Livnat, Valerio Pascucci", "uri": "https://vimeo.com/237972626", "name": "InfoVis 2017: Visual Exploration of Semantic Relationships in Neural Word Embeddings", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:31:23+00:00", "description": "Authors: Jos\u00e9 Matute, Alexandru C. Telea, Lars Linsen", "uri": "https://vimeo.com/237972314", "name": "InfoVis 2017: Skeleton-based Scagnostics", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-12T20:30:24+00:00", "description": "Authors: Bram C.M. Cappers, Jarke J. van Wijk", "uri": "https://vimeo.com/237972151", "name": "InfoVis 2017: Exploring Multivariate Event Sequences using Rules, Aggregations, and Selections", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-11T01:53:20+00:00", "description": "Authors: Will Usher, Pavol Klacansky, Frederick Federer, Peer-Timo Bremer, Aaron Knoll, Alessandra Angelucci, Valerio Pascucci", "uri": "https://vimeo.com/237674422", "name": "SciVis 2017: A Virtual Reality Visualization Tool for Neuron Tracing", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:51:40+00:00", "description": "Authors: Jens G. Magnus, Stefan Bruckner", "uri": "https://vimeo.com/237674324", "name": "SciVis 2017: Interactive Dynamic Volume Illumination with Refraction and Caustics", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:49:28+00:00", "description": "Authors: Markus Hadwiger, Ali K. Al-Awami, Johanna Beyer, Marco Agus, Hanspeter Pfister", "uri": "https://vimeo.com/237674161", "name": "SciVis 2017: SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:41:23+00:00", "description": "Authors: Tran Minh Quan, Junyoung Choi, Haejin Jeong, Won-Ki Jeong", "uri": "https://vimeo.com/237673536", "name": "SciVis 2017: An Intelligent System Approach for Probabilistic Volume Rendering using Hierarchical 3D Convolutional Sparse Coding", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:39:10+00:00", "description": "Authors: Jiang Zhang, Hanqi Guo, Fan Hong, Xiaoru Yuan, Tom Peterka", "uri": "https://vimeo.com/237673362", "name": "SciVis 2017: Dynamic Load Balancing Based on Constrained K-D Tree Decomposition for Parallel Particle Tracing", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:37:07+00:00", "description": "Authors: Mohamed Ibrahim, Patrick Wickenh\u00e4user, Peter Rautek, Guido Reina, Markus Hadwiger", "uri": "https://vimeo.com/237673207", "name": "SciVis 2017: Screen-Space Normal Distribution Function Caching for Consistent Multi-Resolution Rendering of Large Particle Data", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:34:13+00:00", "description": "Authors: Subhashis Hazarika, Ayan Biswas, Han-Wei Shen", "uri": "https://vimeo.com/237672953", "name": "SciVis 2017: Uncertainty Visualization Using Copula-Based Analysis in Mixed Distribution Models", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:31:04+00:00", "description": "Authors: Roxana Bujack, Terece L. Turton, Francesca Samsel, Colin Ware, David H. Rogers, James Ahrens", "uri": "https://vimeo.com/237672669", "name": "SciVis 2017: The Good, the Bad, and the Ugly: A Theoretical Framework for the Assessment of Continuous Colormaps", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:28:07+00:00", "description": "Author: G. Elisabeta Marai", "uri": "https://vimeo.com/237672418", "name": "SciVis 2017: Activity-Centered Domain Characterization for Problem-Driven Scientific Visualization", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-10-11T01:25:28+00:00", "description": "Authors: Charles Perin, Tiffany Wun, Richard Pusch, Sheelagh Carpendale", "uri": "https://vimeo.com/237672181", "name": "InfoVis 2017: Assessing the Graphical Perception of Time and Speed on 2D + Time Trajectories", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-11T01:20:06+00:00", "description": "Authors: Paulo Ivson, Daniel Nascimento, Waldemar Celes, Simone DJ Barbosa", "uri": "https://vimeo.com/237671706", "name": "InfoVis 2017: CasCADe: A Novel 4D Visualization System for Virtual Construction Planning", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-11T01:18:13+00:00", "description": "Authors: Christina Niederer, Holger Stitz, Reem Hourieh, Florian Grassinger, Wolfgang Aigner, Marc Streit", "uri": "https://vimeo.com/237671556", "name": "InfoVis 2017: TACO: Visualizing Changes in Tables Over Time", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-11T01:14:54+00:00", "description": "Authors: Ricardo Langner, Tom Horak, Raimund Dachselt", "uri": "https://vimeo.com/237671288", "name": "InfoVis 2017: VisTiles: Coordinating and Combining Co-located Mobile Devices for Visual Data Exploration", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-11T01:13:44+00:00", "description": "Authors: Nils Rodrigues, Daniel Weiskopf", "uri": "https://vimeo.com/237671207", "name": "InfoVis 2017: Nonlinear Dot Plots", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-11T01:11:41+00:00", "description": "Authors: Philipp Koytek, Charles Perin, Jo Vermeulen, Elisabeth Andr\u00e9, Sheelagh Carpendale", "uri": "https://vimeo.com/237671005", "name": "InfoVis 2017: MyBrush: Brushing and Linking with Personal Agency", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-11T01:09:17+00:00", "description": "Authors: Nam Wook Kim, Benjamin Bach, Hyejin Im, Sasha Schriber, Markus Gross, Hanspeter Pfister", "uri": "https://vimeo.com/237670804", "name": "InfoVis 2017: Visualizing Nonlinear Narratives with Story Curves", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-11T01:07:34+00:00", "description": "Authors: Arjun Srinivasan, Hyunwoo Park, Alex Endert, Rahul C. Basole", "uri": "https://vimeo.com/237670662", "name": "VAST 2017: Graphiti: Interactive Specification of Attribute-based Edges for Network Modeling and Visualization", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-11T01:05:23+00:00", "description": "Authors: Robert Pienta, Fred Hohman, Alex Endert, Acar Tamersoy, Kevin Roundy, Chris Gates, Shamkant Navathe, Duen Horng Chau", "uri": "https://vimeo.com/237670479", "name": "VAST 2017: VIGOR: Interactive Visual Exploration of Graph Query Results", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-11T01:03:12+00:00", "description": "Authors: Siwei Fu, Hao Dong, Weiwei Cui, Jian Zhao, Huamin Qu", "uri": "https://vimeo.com/237670285", "name": "VAST 2017: How Do Ancestral Traits Shape Family Trees over Generations?", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-11T01:00:31+00:00", "description": "Authors: Jian Zhao, Maoyuan Sun, Francine Chen, Patrick Chiu", "uri": "https://vimeo.com/237670040", "name": "VAST 2017: BiDots: Visual Exploration of Weighted Biclusters", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-11T00:57:56+00:00", "description": "Authors: Angus G. Forbes, Andrew Burks, Kristine Lee, Xing Li, Pierre Boutillier, Jean Krivine, Walter Fontana", "uri": "https://vimeo.com/237669826", "name": "VAST 2017: Dynamic Influence Networks for Rule-based Models", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-11T00:54:45+00:00", "description": "Authors: Seokyeon Kim, Seongmin Jeong, Insoo Woo, Yun Jang, Ross Maciejewski, David Ebert", "uri": "https://vimeo.com/237669562", "name": "VAST 2017: Data Flow Analysis and Visualization for Spatiotemporal Statistical Data without Trajectory Information", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-11T00:50:34+00:00", "description": "Authors: Gennady Andrienko, Natalia Andrienko, Georg Fuchs, Jo Wood", "uri": "https://vimeo.com/237669248", "name": "VAST 2017: Revealing Patterns and Trends of Mass Mobility through Spatial and Temporal Abstraction of Origin-Destination Movemen", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-11T00:44:35+00:00", "description": "Authors: Gennady Andrienko, Natalia Andrienko, Georg Fuchs, Jose Manuel Cordero Garcia", "uri": "https://vimeo.com/237668753", "name": "VAST 2017: Clustering Trajectories by Relevant Parts for Air Traffic Analysis", "year": "2017", "event": "VAST"}, {"created_time": "2017-10-02T20:52:11+00:00", "description": "Authors: Julian Stahnke, Marian D\u00f6rk, Boris Muller, Andreas Thom", "uri": "https://vimeo.com/236482290", "name": "InfoVis 2015: Probing Projections: Interaction Techniques for Interpreting Arrangements and Errors of Dimensionality Reductions", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-02T20:50:10+00:00", "description": "Authors: Manuel Rubio-Sanchez, Francisco Diaz, Laura Raya, Alberto Sanchez", "uri": "https://vimeo.com/236481968", "name": "InfoVis 2015: A comparative study between RadViz and Star Coordinates", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-10-02T20:49:13+00:00", "description": "Authors: Dirk Joachim Lehmann, Holger Theisel", "uri": "https://vimeo.com/236481812", "name": "InfoVis 2015: Optimal Sets of Projections of High-Dimensional Data", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T20:04:36+00:00", "description": "Authors: Hendrik Strobelt, Daniela Oelke, Bum Chul Kwon, Tobias Schreck, Hanspeter Pfister", "uri": "https://vimeo.com/236237549", "name": "InfoVis 2015: Guidelines for Effective Usage of Text Highlighting Techniques", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T20:03:43+00:00", "description": "Authors: Yvonne Jansen, Kasper Hornbaek", "uri": "https://vimeo.com/236237488", "name": "InfoVis 2015: A Psychophysical Investigation of Size as a Physical Variable", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T19:58:47+00:00", "description": "Authors: Matthew Kay, Jeffrey Heer", "uri": "https://vimeo.com/236237171", "name": "InfoVis 2015: Beyond Weber's Law: A Second Look at Ranking Visualizations of Correlation", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T19:57:50+00:00", "description": "Authors: Susan VanderPlas, Heike Hofmann", "uri": "https://vimeo.com/236237108", "name": "InfoVis 2015: Spatial Reasoning and Data Displays", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:47:19+00:00", "description": "Authors: Vidya Setlur, Maureen Stone", "uri": "https://vimeo.com/236172848", "name": "InfoVis 2015: A Linguistic Approach to Categorical Color Assignment for Data Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:45:12+00:00", "description": "Authors: M. Adil Yalcin, Niklas Elmqvist, Benjamin B. Bederson", "uri": "https://vimeo.com/236172686", "name": "InfoVis 2015: AggreSet: Rich and Scalable Set Exploration using Visualizations of Element Aggregations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:43:08+00:00", "description": "Authors: Paolo Simonetto, Daniel Archambault, Carlos Scheidegger", "uri": "https://vimeo.com/236172553", "name": "InfoVis 2015: A Simple Approach for Boundary Improvement of Euler Diagrams", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:42:26+00:00", "description": "Authors: Anushka Anand, Justin Talbot", "uri": "https://vimeo.com/236172509", "name": "InfoVis 2015: Automatic Selection of Partitioning Variables for Small Multiple Displays", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:41:13+00:00", "description": "Authors: Sarah Goodwin, Jason Dykes, Aidan Slingsby, Cagatay Turkay", "uri": "https://vimeo.com/236172434", "name": "InfoVis 2015: Visualizing Multiple Variables Across Scale and Geography", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:40:04+00:00", "description": "Authors: Renata Raidou, Martin Eisemann, Marcel Breeuwer, Elmar Eisemann, Anna Vilanova", "uri": "https://vimeo.com/236172350", "name": "InfoVis 2015: Orientation-Enhanced Parallel Coordinate Plots", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:39:08+00:00", "description": "Authors: Jimmy Johansson, Camilla Forsell", "uri": "https://vimeo.com/236172299", "name": "InfoVis 2015: Evaluation of Parallel Coordinates: Overview, Categorization, and Guidelines for Future Research", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:37:35+00:00", "description": "Authors: Yael Albo, Joel Lanir, Peter Bak, Sheizaf Rafaeli", "uri": "https://vimeo.com/236172188", "name": "InfoVis 2015: Off the Radar: Comparative Evaluation of Radial Visualization Solutions for Composite Indicators", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:31:23+00:00", "description": "Authors: Benjamin Bach, Conglei Shi, Nicolas Heulot, Tara Madhayastha, Tom Grabowski, Pierre Dragicevic", "uri": "https://vimeo.com/236171787", "name": "InfoVis 2015: Time Curves: Folding Time to Visualize Patterns of Temporal Evolution in Data", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:28:48+00:00", "description": "Authors: James Walker, Rita Borgo, Mark Jones", "uri": "https://vimeo.com/236171594", "name": "InfoVis 2015: TimeNotes: A Study on Effective Chart Visualization and Interaction Techniques for Time-Series Data", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:27:11+00:00", "description": "Authors: Theresia Gschwandtner, Markus Bogl, Paolo Federico, Silvia Miksch", "uri": "https://vimeo.com/236171472", "name": "InfoVis 2015: Visual Encodings of Temporal Uncertainty: A Comparative User Study", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:21:50+00:00", "description": "Authors: Arvind Satyanarayan, Ryan Russell, Jane Hoffswell, Jeffrey Heer", "uri": "https://vimeo.com/236171108", "name": "InfoVis 2015: Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:19:33+00:00", "description": "Authors: Kanit Wongsuphasawat, Dominik Moritz, Anushka Anand, Jock Mackinlay, Bill Howe, Jeffrey Heer", "uri": "https://vimeo.com/236170947", "name": "InfoVis 2015: Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:16:43+00:00", "description": "Authors: Jeremy Boy, Louis Eveillard, Francoise Detienne, Jean-Daniel Fekete", "uri": "https://vimeo.com/236170748", "name": "InfoVis 2015: Suggested Interactivity: Seeking Perceived Affordances for Information Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:15:20+00:00", "description": "Authors: Mona Hosseinkhani Loorak, Charles Perin, Noreen Kamal, Michael Hill, Sheelagh Carpendale", "uri": "https://vimeo.com/236170646", "name": "InfoVis 2015: TimeSpan: Using Visualization to Explore Temporal Multidimensional Data of Stroke Patients", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:14:02+00:00", "description": "Authors: Hendrik Strobelt, Bilal Alsallakh, Joseph Botros, Brant Peterson, Mark Borowsky, Hanspeter Pfister, Alexander Lex", "uri": "https://vimeo.com/236170531", "name": "InfoVis 2015: Vials: Visualizing Alternative Splicing of Genes", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:12:41+00:00", "description": "Authors: P. Samuel Quinan, Miriah Meyer", "uri": "https://vimeo.com/236170428", "name": "InfoVis 2015: Visually Comparing Weather Features in Forecasts", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:07:57+00:00", "description": "Authors: Roeland Scheepens, Christophe Hurter, Huub van de Wetering, Jarke J. van Wijk", "uri": "https://vimeo.com/236170094", "name": "InfoVis 2015: Visualization, Selection, and Analysis of Traffic Flows", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:05:12+00:00", "description": "Authors: Alice Thudt, Dominikus Baur, Samuel Huron, Sheelagh Carpendale", "uri": "https://vimeo.com/236169885", "name": "InfoVis 2015: Visual Mementos: Reflecting Memories with Personal Data", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-30T00:02:56+00:00", "description": "Authors: Matthew Brehmer, Jocelyn Ng, Kevin Tate, Tamara Munzner", "uri": "https://vimeo.com/236169723", "name": "InfoVis 2015: Matches, Mismatches, and Methods: Multiple-View Workflows for Energy Portfolio Analysis", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-29T23:58:58+00:00", "description": "Authors: Nina McCurdy, Julie Lein, Katharine Coles, Miriah Meyer", "uri": "https://vimeo.com/236169439", "name": "InfoVis 2015: Poemage: Visualizing the Sonic Topology of a Poem", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-29T23:57:26+00:00", "description": "Authors: Uta Hinrichs, Stefania Forlini, Bridget Moynihan", "uri": "https://vimeo.com/236169311", "name": "InfoVis 2015: Speculative Practises: Utilizing InfoVis to Explore Untapped Literary Collections", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-29T23:55:15+00:00", "description": "Authors: Jonathan C. Roberts, Chris Headleand, Panagiotis D. Ritsos", "uri": "https://vimeo.com/236169123", "name": "InfoVis 2015: Sketching designs using the Five Design-Sheet methodology", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-29T23:53:26+00:00", "description": "Authors: Alvitta Ottley, Evan M. Peck, Lane T. Harrison, Daniel Afergan, Caroline Ziemkiewicz, Holly A. Taylor, Paul K. J. Han, Remco Chang", "uri": "https://vimeo.com/236168967", "name": "InfoVis 2015: Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-29T23:52:11+00:00", "description": "Authors: Michelle A. Borkin, Zoya Bylinskii, Nam Wook Kim, Constance May Bainbridge, Chelsea S. Yeh, Daniel Borkin, Hanspeter Pfister, Aude Oliva", "uri": "https://vimeo.com/236168873", "name": "InfoVis 2015: Beyond Memorability: Visualization Recognition and Recall", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-29T23:51:13+00:00", "description": "Authors: Lydia Byrne, Daniel Angus, Janet Wiles", "uri": "https://vimeo.com/236168798", "name": "InfoVis 2015: Acquired Codes of Meaning in Data Visualization and Infographics: Beyond Perceptual Primitives", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-29T23:43:55+00:00", "description": "Authors: Sukwon Lee, Sung-Hee Kim, Ya-Hsin Hung, Heidi Lam, Youn-ah Kang, Ji Soo Yi", "uri": "https://vimeo.com/236168224", "name": "InfoVis 2015: How do People Make Sense of Unfamiliar Visualization?: A Grounded Model of Novice's Information Visualization Sens", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-09-26T21:48:00+00:00", "description": "Authors: Lingyun Yu, Konstantinos Efstathiou, Petra Isenberg, Tobias Isenberg", "uri": "https://vimeo.com/235642712", "name": "SciVis 2015: CAST: Effective and Efficient User Interaction for Context-Aware Selection in 3D Particle Clouds", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-09-25T23:22:47+00:00", "description": "Poster\n\nAuthors: Cindy Xiong, Lisanne van Weelden, Steven Franconeri\n\nAbstract: The curse of knowledge, the inability to separate one\u2019s own knowledge and expertise from that of his/her audience, is a well-studied psychological phenomenon. We test the idea that it can make visual data communication more difficult \u2013 and less clear \u2013 than one realizes. Because a viewer can extract many potential relationships and patterns from any set of visualized data values, when the visualization entails intricate patterns and features, the viewer may see one pattern in the data as more visually salient than others. We demonstrate this phenomenon in lab, showing that when people are given background information, they see the pattern in the data corresponding to the background information as more visually salient. Critically, they also believe that other viewers will experience the same visual salience, even when they are explicitly told that other viewers are na\u00efve to the background information. The present findings show that the curse of knowledge does plague the visual perception of data, explaining why presenters, paper authors, and data analysts can fail to connect with audiences when they communicate patterns in data. Because curse of knowledge is tough for a viewer to detect and inhibit, we encourage people to visualize their data a variety of designs and use critiques and feedback to improve their visualizations.", "uri": "https://vimeo.com/235438771", "name": "[VIS17 Preview] The Curse of Knowledge in Visual Data Communication (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-09-25T18:52:10+00:00", "description": "Authors: Maarten H. Everts, Henk Bekker, Jos B.T.M. Roerdink, Tobias Isenberg", "uri": "https://vimeo.com/235396780", "name": "SciVis 2015: Exploration of the Brain's White Matter Structure through Visual Abstraction and Multi-Scale Local Fiber Tract Cont", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-24T03:55:43+00:00", "description": "Poster \n\nAuthors: Shengzhou Luo, John Dingliana\n\nAbstract: We present an interactive approach for intuitively editing colors and opacity values in transfer functions for volume visualization. We introduce the concept of a relative visibility histogram, which represents the difference between the global visibility distribution across the full volume and the local visibility distribution within a user-selected region in the viewport. From this measure we can infer what the user intends to select when they click on a specific region in the viewport and use this result to directly modify the relevant parts of the transfer function. The approach is lightweight compared to similar techniques and performs in real-time.", "uri": "https://vimeo.com/230875337", "name": "[VIS17 Preview] Intuitive Transfer Function Editing Using Relative Visibility Histograms (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:34:50+00:00", "description": "Authors: Philip A. Legg, Eamonn Maguire, Simon Walton, Min Chen\n\nAbstract: In many spatial and temporal visualization applications, glyphs provide an effective means for encoding multivariate data. However, because glyphs are typically small, they are vulnerable to various perceptual errors. This article introduces the concept of a quasi-Hamming distance in the context of glyph design and examines the feasibility of estimating the quasi-Hamming distance between a pair of glyphs and the minimal Hamming distance for a glyph set. The authors demonstrate the design concept by developing a file-system event visualization that can depict the activities of multiple users.", "uri": "https://vimeo.com/230841589", "name": "[VIS17 Preview] Glyph Visualization: A Fail-Safe Design Scheme Based on Quasi-Hamming Distances (CG&amp;A Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:34:37+00:00", "description": "Authors: Jimmy Johansson, Tomasz Opach, Erik Glaas, Tina-Simone Neset, Carlo Navarra, Bj\u00f6rn-Ola Linn\u00e9r, Jan Ketil R\u00f8d\n\nAbstract: The web-based visualization VisAdapt tool was developed to help laypeople in the Nordic countries assess how anticipated climate change will impact their homes. The tool guides users through a three-step visual process that helps them explore risks and identify adaptive actions specifically modified to their location and house type. This article walks through the tool's multistep, user-centered design process. Although VisAdapt's target end users are Nordic homeowners, the insights gained from the development process and the lessons learned from the project are applicable to a wide range of domains.", "uri": "https://vimeo.com/230841562", "name": "[VIS17 Preview] VisAdapt: A Visualization Tool to Support Climate Change Adaptation (CG&amp;A Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:34:28+00:00", "description": "Authors: Antonio G. Losada, Roberto Ther\u00f3n, Alejandro Benito\n\nAbstract: The amount of data available in the sports field is difficult for coaches, analysts, and players to comprehend using classic analytics methods. Thus, new methods are necessary to help users break down that information and analyze it at a deeper level. The BKViz visual analytics system focuses on individual basketball games using classic and novel methods to reveal how players perform together and as individuals. The information is presented in interactive visualizations that allow immediate user feedback.", "uri": "https://vimeo.com/230841541", "name": "[VIS17 Preview] BKViz: A Basketball Visual Analysis Tool (CG&amp;A Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:34:16+00:00", "description": "Authors: Marcos Lage, Jorge Piazentin Ono, Daniel Cervone, Justin Chiang, Carlos Dietrich, Claudio T. Silva\n\nAbstract: This article presents a visualization and analytics infrastructure to help query and facilitate the analysis of this new tracking data. The goal is to go beyond descriptive statistics of individual plays, allowing analysts to study diverse collections of games and game events. The StatCast Dashboard visual interface helps users query, filter, and analyze the tracking data gathered by the Major League Baseball (MLB) StatCast spatiotemporal data-tracking system. The proposed system enables the exploration of the data using a simple querying interface and a set of flexible interactive visualization tools.", "uri": "https://vimeo.com/230841515", "name": "[VIS17 Preview] StatCast Dashboard: Exploration of Spatiotemporal Baseball Data. (CG&amp;A Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:34:08+00:00", "description": "Authors: Charles Perin, Jeremy Boy, Frederic Vernier\n\nAbstract: To address the limitations of traditional line chart approaches, in particular rank charts (RCs) and score charts (SCs), a novel class of line charts called gap charts (GCs) show entries that are ranked over time according to a performance metric. The main advantages of GCs are that entries never overlap (only changes in rank generate limited overlap between time steps) and gaps between entries show the magnitude of their score difference. The authors evaluate the effectiveness of GCs for performing different types of tasks and find that they outperform standard time-dependent ranking visualizations for tasks that involve identifying and understanding evolutions in both ranks and scores. They also show that GCs are a generic and scalable class of line charts by applying them to a variety of different datasets.", "uri": "https://vimeo.com/230841502", "name": "[VIS17 Preview] Using a Gap Strategy to Visualize the Temporal Evolution of Ranks and Scores (CG&amp;A Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:33:55+00:00", "description": "Authors: Romain Vuillemot, Charles Perin\n\nAbstract: An advanced interface for sports tournament predictions uses direct manipulation to allow users to make nonlinear predictions. Unlike previous interface designs, the interface helps users focus on their prediction tasks by enabling them to first choose a winner and then fill out the rest of the bracket. In real-world tests of the proposed interface (for the 2014 FIFA World Cup tournament and 2015/2016 UEFA Champions League), the authors validated the use of direct manipulation as an alternative to widgets. Using visitor interaction logs, they were able to determine the strategies people use to perform predictions and identify potential areas of improvement for further prediction interfaces.", "uri": "https://vimeo.com/230841474", "name": "[VIS17 Preview] Sport Tournament Predictions Using Direct Manipulation (CG&amp;A Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:33:29+00:00", "description": "Authors: Manuel Stein, Halld\u00f3r Janetzko, Thorsten Breitkreutz, Daniel Seebacher, Tobias Schreck, Michael Grossniklaus, Iain Couzin, Daniel A Keim\n\nAbstract: For development and alignment of tactics and strategies, professional soccer analysts spend up to three working days manually analyzing and annotating professional soccer matches. In an effort to improve soccer player and match analysis, a visual-interactive and data-analysis support system focuses on key situations by using rule-based filtering and automatically annotating key types of soccer match elements. The authors evaluate the proposed approach by analyzing real-world soccer matches and several expert studies. Quantitative measures show the proposed methods can significantly outperform naive solutions.", "uri": "https://vimeo.com/230841435", "name": "[VIS17 Preview] Director's Cut: Analysis and Annotation of Soccer Matches (CG&amp;A Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:33:19+00:00", "description": "Authors: Lhaylla Crissaff, Louisa Ruby, Samantha Deutch, Luke DuBois, Jean-Daniel Fekete, Juliana Freire, and Claudio T. Silva\n\nAbstract: Art historians have traditionally used physical light boxes to prepare exhibits or curate collections. On a light box, they can place slides or printed images, move the images around at will, group them as desired, and visually compare them. The transition to digital images has rendered this workflow obsolete. Now, art historians lack well-designed, unified interactive software tools that effectively support the operations they perform with physical light boxes.  To address this problem, we designed ARIES (ARt Image Exploration Space), an interactive image manipulation system that enables the exploration and orga of fine digital art. The system allows images to be compared in multiple ways, offering dynamic overlays analogous to a physical light box, and supporting advanced image comparisons and feature-matching functions, available through computational image processing. We demonstrate the effectiveness of our system to support art historians' tasks through real use cases.", "uri": "https://vimeo.com/230841414", "name": "[VIS17 Preview] ARIES: Enabling Visual Exploration and Organization of Art Image Collections (CG&amp;A Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:32:04+00:00", "description": "Authors: Jorge Poco, Angela Mayhua, Jeffrey Heer\n\nAbstract: Visualization designers regularly use color to encode quantitative or categorical data. However, visualizations \"in the wild\" often violate perceptual color design principles and may only be available as bitmap images. In this work, we contribute a method to semi-automatically extract color encodings from a bitmap visualization image. Given an image and a legend location, we classify the legend as describing either a discrete or continuous color encoding, identify the colors used, and extract legend text using OCR methods. We then combine this information to recover the specific color mapping. Users can also correct interpretation errors using an annotation interface. We evaluate our techniques using a corpus of images extracted from scientific papers and demonstrate accurate automatic inference of color mappings across a variety of chart types. In addition, we present two applications of our method: automatic recoloring to improve perceptual effectiveness, and interactive overlays to enable improved reading of static visualizations.", "uri": "https://vimeo.com/230841267", "name": "[VIS17 Preview] Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:31:56+00:00", "description": "(Best Paper Award)\n\nAuthors: Danielle Albers Szafir\n\nAbstract: Color is frequently used to encode values in visualizations. For color encodings to be effective, the mapping between colors and values must preserve important differences in the data. However, most guidelines for effective color choice in visualization are based on either color perceptions measured using large, uniform fields in optimal viewing environments or on qualitative intuitions. These limitations may cause data misinterpretation in visualizations, which frequently use small, elongated marks. Our goal is to develop quantitative metrics to help people use color more effectively in visualizations. We present a series of crowdsourced studies measuring color difference perceptions for three common mark types: points, bars, and lines. Our results indicate that peoples' abilities to perceive color differences varies significantly across mark types. Probabilistic models constructed from the resulting data can provide objective guidance for designers, allowing them to anticipate viewer perceptions to inform effective encoding design.", "uri": "https://vimeo.com/230841251", "name": "[VIS17 Preview] Modeling Color Difference for Visualization Design (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:31:48+00:00", "description": "(Best Paper Honorable Mention)\n\nAuthors: Heidi Lam, Melanie Tory, Tamara Munzner\n\nAbstract: Visualization researchers and practitioners engaged in generating or evaluating designs are faced with the difficult problem of transforming the questions asked and actions taken by target users from domain-specific language and context into more abstract forms. Existing abstract task classifications aim to provide support for this endeavour by providing a carefully delineated suite of actions. Our experience is that this bottom-up approach is part of the challenge: low-level actions are difficult to interpret without a higher-level context of analysis goals and the analysis process. To bridge this gap, we propose a framework based on analysis reports derived from open-coding 20 design study papers published at IEEE InfoVis 2009-2015, to build on the previous work of abstractions that collectively \\ encompass a broad variety of domains. The framework is organized in two axes illustrated by nine analysis goals. It helps situate the analysis goals by placing each goal under axes of specificity (Explore, Describe, Explain, Confirm) and number of data populations \\ (Single, Multiple). The single-population types are Discover Observation, Describe Observation, Identify Main Cause, and Collect Evidence. The multiple-population types are Compare Entities, Explain Differences, and Evaluate Hypothesis. Each analysis goal is scoped by an input and an output and is characterized by analysis steps reported in the design study papers. We provide examples of how we and others have used the framework in a top-down approach to abstracting domain problems: visualization designers or \\ researchers first identify the analysis goals of each unit of analysis in an analysis stream, and then encode the individual steps using existing task classifications with the context of the goal, the level of specificity, and the number of populations involved in the analysis.", "uri": "https://vimeo.com/230841235", "name": "[VIS17 Preview] Bridging From Goals to Tasks with Design Study Analysis Reports (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:31:40+00:00", "description": "Authors: Jose Matute, Alex Telea, Lars Linsen\n\nAbstract: Scatterplot matrices (SPLOMs) are widely used for exploring multidimensional data. Scatterplot diagnostics (scagnostics) approaches measure characteristics of scatterplots to automatically find potentially interesting plots, thereby making SPLOMs more scalable with the dimension count. While statistical measures such as regression lines can capture orientation, and graph-theoretic scagnostics measures can capture shape, there is no scatterplot characterization measure that uses both descriptors. Based on well-known results in shape analysis, we propose a scagnostics approach that captures both scatterplot shape and orientation using skeletons (or medial axes). Our representation can handle complex spatial distributions, helps discovery of principal trends in a \\ multiscale way, scales visually well with the number of samples, is robust to noise, and is automatic and fast to compute. We define skeleton-based similarity metrics for the visual exploration and analysis of SPLOMs. We perform a user study to measure the human perception of scatterplot similarity and compare the outcome to our results as well as to graph-based scagnostics and other visual \\ quality metrics. Our skeleton-based metrics outperform previously defined measures both in terms of closeness to perceptually-based similarity and computation time efficiency.", "uri": "https://vimeo.com/230841215", "name": "[VIS17 Preview] Skeleton-based Scagnostics (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:31:30+00:00", "description": "Authors: Christophe Hurter, Stephane Puechmorel, Florence Nicol, Alex Telea\n\nAbstract: Bundling visually aggregates curves to reduce clutter and help \ufb01nding important data patterns in spatial trail-sets or graph drawings. We propose a new approach to bundling based on functional decomposition of the underling dataset. We recover the functional nature of the curves by representing them as linear combinations of piecewise-polynomial basis functions with associated expansioncoef\ufb01cients. Next, weexpressallcurvesinagivenclusterintermsofacentroidcurveandacomplementaryterm, viaaset of so-called principal component functions. Based on the above, we propose a two-fold contribution: First, we use cluster centroids to design a new bundling method for 2D and 3D curve-sets. Secondly, we deform the cluster centroids and generate new curves along them,whichenablesustomodifytheunderlyingdatainastatistically-controlledwayviaitssimpli\ufb01ed(bundled)view. Wedemonstrate our method by applications on real-world 2D and 3D datasets for graph bundling, trajectory analysis, and vector \ufb01eld and tensor \ufb01eld visualization. \\", "uri": "https://vimeo.com/230841191", "name": "[VIS17 Preview] Functional Decomposition for Bundled Simpli\ufb01cation of Trail Sets (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:31:17+00:00", "description": "Authors: Thomas H\u00f6llt, Nicola Pezzotti, Vincent van Unen, Frits Koning, Boudewijn P. F. Lelieveldt, Anna Vilanova\n\nAbstract: Single-cell analysis through mass cytometry has become an increasingly important tool for immunologists to study the immune system in health and disease. Mass cytometry creates a high-dimensional description vector for single cells by time-of-flight measurement. Recently, t-Distributed Stochastic Neighborhood Embedding (t-SNE) has emerged as one of the state-of-the-art techniques for the visualization and exploration of single-cell data. Ever increasing amounts of data lead to the adoption of Hierarchical Stochastic Neighborhood Embedding (HSNE), enabling the hierarchical representation of the data. Here, the hierarchy is explored selectively by the analyst, who can request more and more detail in areas of interest. Such hierarchies are usually explored by visualizing disconnected plots of selections in different levels of the hierarchy. This poses problems for navigation, by imposing a high cognitive load on the analyst. In this work, we present an interactive summary-visualization to tackle this problem. CyteGuide guides the analyst through the exploration of hierarchically represented single-cell data, and provides a complete overview of the current state of the analysis. We conducted a two-phase user study with domain experts that use HSNE for data exploration. We first studied their problems with their current workflow using HSNE and the requirements to ease this workflow in a field study. These requirements have been the basis for our visual design. In the second phase, we verified our proposed solution in a user evaluation.", "uri": "https://vimeo.com/230841169", "name": "[VIS17 Preview] CyteGuide: Visual Guidance for Hierarchical Single-Cell Analysis (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:31:06+00:00", "description": "Authors: Pierre Dragicevic, Yvonne Jansen\n\nAbstract: We provide a reappraisal of Tal and Wansink's study \"Blinded with Science\", where seemingly trivial charts were shown to increase belief in drug efficacy, presumably because charts are associated with science. Through a series of four replications conducted on two crowdsourcing platforms, we investigate an alternative explanation, namely, that the charts allowed participants to better assess the drug's efficacy. Considered together, our experiments suggest that the chart seems to have indeed promoted understanding, although the effect is likely very small. Meanwhile, we were unable to replicate the original study\u2019s findings, as text with chart appeared to be no more persuasive -- and sometimes less persuasive -- than text alone. This suggests that the effect may not be as robust as claimed and may need specific conditions to be reproduced. Regardless, within our experimental settings and considering our study as a whole (N = 623), the chart\u2019s contribution to understanding was clearly larger than its contribution to persuasion.", "uri": "https://vimeo.com/230841144", "name": "[VIS17 Preview] Blinded with Science or Informed by Charts? A Replication Study (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:30:57+00:00", "description": "Authors: Yingcai Wu, Ji Lan, Xinhuan Shu, Chenyang Ji, Kejian Zhao, Jiachen Wang, Hui Zhang\n\nAbstract: The rapid development of information technology paved the way for the recording of fine-grained data, such as stroke techniques and stroke placements, during a table tennis match. This data recording creates opportunities to analyze and evaluate matches from new perspectives. Nevertheless, the increasingly complex data poses a significant challenge to make sense of and gain insights into. Analysts usually employ tedious and cumbersome methods which are limited to watching videos and reading statistical tables. However, existing sports visualization methods cannot be applied to visualizing table tennis competitions due to different competition rules and particular data attributes. In this work, we collaborate with data analysts to understand and characterize the sophisticated domain problem of analysis of table tennis data. We propose iTTVis, a novel interactive table tennis visualization system, which to our knowledge, is the first visual analysis system for analyzing and exploring table tennis data. iTTVis provides a holistic visualization of an entire match from three main perspectives, namely, time-oriented, statistical, and tactical analyses. The proposed system with several well-coordinated views not only supports correlation identification through statistics and pattern detection of tactics with a score timeline but also allows cross analysis to gain insights. Data analysts have obtained several new insights by using iTTVis. The effectiveness and usability of the proposed system are demonstrated with four case studies.", "uri": "https://vimeo.com/230841126", "name": "[VIS17 Preview] iTTVis: Interactive Visualization of Table Tennis Data (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:30:48+00:00", "description": "Authors: Michael Gleicher\n\nAbstract: Supporting comparison is a common and diverse challenge in visualization. \\ Such support is difficult to design because solutions must address both the specifics of their scenario as well as the general issues of comparison. \\ This paper aids designers by providing a strategy for considering those general issues. \\ It presents four considerations that abstract comparison. \\ These considerations identify issues and categorize solutions in a domain independent manner. \\ The first considers how the common elements of comparison---a target set of items that are related and an action the user wants to perform on that relationship---are present in an analysis problem. \\ The second considers why these elements lead to challenges because of their scale, in  number of items, complexity of items, or complexity of relationship. \\ The third considers what strategies address the identified scaling challenges, grouping solutions into three broad categories.  \\ The fourth considers which visual designs map to these strategies to provide solutions for a comparison analysis problem. \\ In sequence, these considerations provide a process for developers to consider support for comparison in the design of visualization tools.  \\ Case studies show how these considerations can help in the design and evaluation of visualization solutions for comparison problems. \\", "uri": "https://vimeo.com/230841111", "name": "[VIS17 Preview] Considerations for Visualizing Comparison (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:30:40+00:00", "description": "(Best Paper Honorable Mention)\n\nAuthors: Zening Qu, Jessica Hullman\n\nAbstract: Visualizations often appear in multiples, either in a single display (e.g., small multiples, dashboard) or across time or space (e.g., slideshow, set of dashboards). However, existing visualization design guidelines typically focus on single rather than multiple views. Solely following these guidelines can lead to effective yet inconsistent views (e.g., the same field has different axes ranges across charts), making interpretation slow and error-prone. Moreover, little is known how consistency balances with other design considerations, making it difficult to incorporate consistency mechanisms in visualization authoring software. We present a wizard-of-oz study in which we observed how Tableau users achieve and sacrifice consistency in an exploration-to-presentation visualization design scenario. We extend (from our prior work) a set of encoding-specific constraints defining consistency across multiple views. Using the constraints as a checklist in our study, we observed cases where participants spontaneously maintained consistent encodings and warned cases where consistency was overlooked. In response to the warnings, participants either revised views for consistency or stated why they thought consistency should be overwritten. We categorize participants' actions and responses as constraint validations and exceptions, depicting the relative importance of consistency and other design considerations under various circumstances (e.g., data cardinality, available encoding resources, chart layout). We discuss automatic consistency checking as a constraint-satisfaction problem and provide design implications for communicating inconsistencies to users.", "uri": "https://vimeo.com/230841093", "name": "[VIS17 Preview] Keeping Multiple Views Consistent: Constraints, Validations, and Exceptions in Visualization Authoring...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:30:31+00:00", "description": "Authors: Alper Sarikaya, Michael Gleicher\n\nAbstract: Traditional scatterplots fail to scale as the complexity and amount of data increases.  In response, there exist many design options that modify or expand the traditional scatterplot design to meet these larger scales. This breadth of design options creates challenges for designers and practitioners who must select appropriate designs for particular analysis goals. In this paper, we help designers in making design choices for scatterplot visualizations.  We survey the literature to catalog scatterplot-specific analysis tasks.  We look at how data characteristics influence design decisions. We then survey scatterplot-like designs to understand the range of design options.  Building upon these three organizations, we connect data characteristics, analysis tasks, and design choices in order to generate challenges, open questions, and example best practices for the effective design of scatterplots. \\", "uri": "https://vimeo.com/230841068", "name": "[VIS17 Preview] Scatterplots: Tasks, Data, and Designs (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:30:22+00:00", "description": "Authors: Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander Rush\n\nAbstract: Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks. Long-term usage data after putting the tool online revealed great interest in the machine learning community.", "uri": "https://vimeo.com/230841046", "name": "[VIS17 Preview] LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:30:13+00:00", "description": "Authors: Andre Calero Valdez, Martina Ziefle, Michael Sedlmair\n\nAbstract: We investigate priming and anchoring effects on perceptual tasks in visualization. Priming or anchoring effects depict the phenomena that a stimulus might influence subsequent human judgments on a perceptual level, or on a cognitive level by providing a frame of reference. Using visual class separability in scatterplots as an example task, we performed a set of five studies to investigate the potential existence of priming and anchoring effects. Our findings show that-under certain circumstances-such effects indeed exist. In other words, humans judge class separability of the same scatterplot differently depending on the scatterplot(s) they have seen before. These findings inform future work on better understanding and more accurately modeling human perception of visual patterns.", "uri": "https://vimeo.com/230841025", "name": "[VIS17 Preview] Priming and Anchoring Effects in Visualizations (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:30:05+00:00", "description": "Authors: Nam Wook Kim, Benjamin Bach, Hyejin Im, Sasha Schriber, Markus Gross, Hanspeter Pfister\n\nAbstract: In this paper, we present story curves, a visualization technique for exploring and communicating nonlinear narratives in movies. A nonlinear narrative is a storytelling device that portrays events of a story out of chronological order, e.g., in reverse order or going back and forth between past and future events. Many acclaimed movies employ unique narrative patterns which in turn have inspired other movies and contributed to the broader analysis of narrative patterns in movies. However, understanding and communicating nonlinear narratives is a difficult task due to complex temporal disruptions in the order of events as well as no explicit records specifying the actual temporal order of the underlying story. Story curves visualize the nonlinear narrative of a movie by showing the order in which events are told in the movie and comparing them to their actual chronological order, resulting in possibly meandering visual patterns in the curve. We also present Story Explorer, an interactive tool that visualizes a story curve together with complementary information such as characters and settings. Story Explorer further provides a script curation interface that allows users to specify the chronological order of events in movies. We used Story Explorer to analyze 10 popular nonlinear movies and describe the spectrum of narrative patterns that we discovered, including some novel patterns not previously described in the literature. Feedback from experts highlights potential use cases in screenplay writing and analysis, education and film production. A controlled user study shows that users with no expertise are able to understand visual patterns of nonlinear narratives using story curves.", "uri": "https://vimeo.com/230841007", "name": "[VIS17 Preview] Visualizing Nonlinear Narratives with Story Curves (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:29:54+00:00", "description": "Authors: Fritz Lekschas, Benjamin Bach, Peter Kerpedjiev, Nils Gehlenborg, Hanspeter Pfister\n\nAbstract: This paper presents an interactive visualization interface\u2014HiPiler\u2014for the exploration and visualization of regions-of-interest in large genome interaction matrices. Genome interaction matrices approximate the physical distance of pairs of regions on the genome to each other and can contain up to 3 million rows and columns with many sparse regions. Regions of interest (ROIs) can be defined, e.g., by sets of adjacent rows and columns, or by specific visual patterns in the matrix. However, traditional matrix aggregation or pan-and-zoom interfaces fail in supporting search, inspection, and comparison of ROIs in such large matrices. In HiPiler, ROIs are first-class objects, represented as thumbnail-like \u201csnippets\u201d. Snippets can be interactively explored and grouped or laid out automatically in scatterplots, or through dimension reduction methods. Snippets are linked to the entire navigable genome interaction matrix through brushing and linking. The design of HiPiler is based on a series of semi-structured interviews with 10 domain experts involved in the analysis and interpretation of genome interaction matrices. We describe six exploration tasks that are crucial for analysis of interaction matrices and demonstrate how HiPiler supports these tasks. We report on a user study with a series of data exploration sessions with domain experts to assess the usability of HiPiler as well as to demonstrate respective findings in the data.", "uri": "https://vimeo.com/230840987", "name": "[VIS17 Preview] HiPiler: Visual Exploration of Large Genome Interaction Matrices with Interactive Small Multiples (InfoVis...", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:29:42+00:00", "description": "Authors: Jagoda Walny, Samuel Huron, Charles Perin, Tiffany Wun, Richard Pusch, Sheelagh Carpendale\n\nAbstract: We investigate whether the notion of active reading for text might be usefully applied to visualizations. Through a qualitative study we explored whether people apply observable active reading techniques when reading paper-based node-link visualizations. Participants used a range of physical actions while reading, and from these we synthesized an initial set of active reading techniques for visualizations. To learn more about the potential impact such techniques may have on visualization reading, we implemented support for one type of physical action from our observations (making freeform marks) in an interactive node-link visualization. Results from our quantitative study of this implementation show that interactive support for active reading techniques can improve the accuracy of performing low-level visualization tasks. Together, our studies suggest that the active reading space is ripe for research exploration within visualization and can lead to new interactions that make for a more flexible and effective visualization reading experience.", "uri": "https://vimeo.com/230840961", "name": "[VIS17 Preview] Active Reading of Visualizations (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:29:28+00:00", "description": "Authors: Benjamin Bach, Ronell Sicat, Maxime Cordeil, Johanna Beyer, Hanspeter Pfister\n\nAbstract: We report on a controlled user study comparing three visualization environments for common 3D exploration. Our environments differ in how they exploit natural human perception and interaction capabilities. We compare an augmented-reality head-mounted display (Microsoft HoloLens), a handheld tablet, and a desktop setup. The novel head-mounted HoloLens display projects stereoscopic images of virtual content into a user\u2019s real world and allows for interaction in-situ at the spatial position of the 3D hologram. The tablet is able to interact with 3D content through touch, spatial positioning, and tangible markers, however, 3D content is still presented on a 2D surface. Our hypothesis is that visualization environments that match human perceptual and interaction capabilities better to the task at hand improve understanding of 3D visualizations. To better understand the space of display and interaction modalities in visualization environments, we first propose a classification based on three dimensions: perception, interaction, and the spatial and cognitive proximity of the two. Each technique in our study is located at a different position along these three dimensions. We asked 15 participants to perform four tasks, each task having different levels of difficulty for both spatial perception and degrees of freedom for interaction. Our results show that each of the tested environments is more effective for certain tasks, but that generally the desktop environment is still fastest and most precise in almost all cases.", "uri": "https://vimeo.com/230840940", "name": "[VIS17 Preview] The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:29:15+00:00", "description": "Authors: yunhai wang, Yanyan Wang, Yingqi Sun, Lifeng Zhu, Chi-Wing Fu, Michael Sedlmair, Oliver Deussen, Baoquan Chen, Kecheng Lu\n\nAbstract: We present an improved stress majorization method that incorporates various constraints including directional constraints \\ without the necessity of solving a constraint optimization problem. This is achieved by reformulating the stress function to impose \\ constraints on both the edge vectors and lengths instead of just on the edge lengths (node distances). This is a unified framework for \\ both constrained and unconstrained graph visualizations, where we can model most existing layout constraints, as well as develop \\ new ones such as the star shapes and cluster separation constraints within stress majorization. This improvement also allows us to \\ parallelize the computation with an efficient GPU conjugant gradient solver, which yields fast and stable solutions, even for large graphs. \\ As a result, we can support constraint based exploration of large graphs with 10K nodes, to which previous methods cannot support.", "uri": "https://vimeo.com/230840913", "name": "[VIS17 Preview] Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:29:06+00:00", "description": "Authors: Jonathan Roberts, Panagiotis Ritsos, James Jackson, Christopher Headleand\n\nAbstract: Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.", "uri": "https://vimeo.com/230840888", "name": "[VIS17 Preview] The Explanatory Visualization Framework: An active learning framework for teaching creative computing using...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:28:58+00:00", "description": "Authors: yunhai wang, Xiaowei Chu, Chen Bao, Lifeng Zhu, Oliver Deussen, Baoquan Chen, Michael Sedlmair\n\nAbstract: We present EdWordle, a method for consistently editing word clouds. At its heart, EdWordle allows users to move and edit words while preserving the neighborhoods of other words. To do so, we combine a constrained rigid body simulation with a neighborhood-aware local Wordle algorithm to update the cloud and to create very compact layouts. The consistent and stable behavior \\ of EdWordle enables users to create new forms of word clouds such as storytelling clouds in which the position of words is carefully \\ edited. We compare our approach with state-of-the-art methods and show that we can improve user performance, user satisfaction, as \\ well as the layout itself.", "uri": "https://vimeo.com/230840872", "name": "[VIS17 Preview] EdWordle: Consistency-preserving Word Cloud Editing (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:28:48+00:00", "description": "Authors: Christina Niederer, Holger Stitz, Reem Hourieh, Florian Grassinger, Wolfgang Aigner, Marc Streit\n\nAbstract: Multivariate, tabular data is one of the most common data structures used in many different domains. Over time, tables can undergo changes in both structure and content, which results in multiple versions of the same table. A challenging task when working with such derived tables is to understand what exactly has changed between versions in terms of additions/deletions, reorder, merge/split, and content changes. For textual data, a variety of commonplace \u201cdiff\u201d tools exist that support the task of investigating changes between revisions of a text. Although there are some comparison tools which assist users in inspecting differences between multiple table instances, the resulting visualizations are often difficult to interpret or do not scale to large tables with thousands of rows and columns. To address these challenges, we developed TACO, an interactive comparison tool that visualizes effectively the differences between multiple tables at various levels of detail. With TACO we show (1) the aggregated differences between multiple table versions over time, (2) the aggregated changes between two selected table versions, and (3) detailed changes between the selection. To demonstrate the effectiveness of our approach, we show its application by means of two usage scenarios.", "uri": "https://vimeo.com/230840851", "name": "[VIS17 Preview] TACO: Visualizing Changes in Tables Over Time (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:28:38+00:00", "description": "Authors: Bram Cappers, Jarke van Wijk\n\nAbstract: Multivariate event sequences are ubiquitous: travel history, telecommunication conversations, and server logs are some examples. Besides standard properties such as type and timestamp, events often have other associated multivariate data. Current exploration and analysis methods either focus on the temporal analysis of a single attribute or the structural analysis of the multivariate data only. We present an approach where users can explore event sequences at multivariate and sequential level simultaneously by interactively defining a set of rewrite rules using multivariate regular expressions. Users can store resulting patterns as new types of events or attributes to interactively enrich or simplify event sequences for further investigation. In Eventpad we provide a bottom-up glyph-oriented approach for multivariate event sequence analysis by searching, clustering, and aligning them according to newly defined domain specific properties. We illustrate the effectiveness of our approach with real-world data sets including telecommunication traffic and hospital treatments.", "uri": "https://vimeo.com/230840824", "name": "[VIS17 Preview] Exploring Multivariate Event Sequences using Rules, Aggregations, and Selections (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:28:29+00:00", "description": "Authors: Yea-Seul Kim, Katharina Reinecke, Jessica Hullman\n\nAbstract: In addition to visualizing input data, interactive visualizations have the potential to be social artifacts that reveal other people's perspectives on the data. However, how such social information embedded in a visualization impacts a viewer's interpretation of the data remains unknown. Inspired by recent interactive visualizations that display people's expectations of data against the data, we conducted a controlled experiment to evaluate the effect of showing social information in the form of other people's expectations on people's ability to recall the data, the degree to which they adjust their expectations to align with the data, and their trust in the accuracy of the data. We found that social information that exhibits a high degree of consensus lead participants to recall the data more accurately relative to participants who were exposed to the data alone. Additionally, participants trusted the accuracy of the data less and were more likely to maintain their initial expectations when other people's expectations aligned with their own initial expectations but not with the data. We conclude by characterizing the design space for visualizing others' expectations alongside data.", "uri": "https://vimeo.com/230840801", "name": "[VIS17 Preview] Data Through Others' Eyes: The Impact of Visualizing Others' Expectations on Visualization Interpretation...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:28:20+00:00", "description": "Authors: Cristian Felix, Enrico Bertini, Steven Franconeri\n\nAbstract: In this paper we present a set of four user studies aimed at exploring the visual design space of what we call keyword summaries: lists of words with associated quantitative values used to help people derive an intuition of what information a given document collection (or part of it) may contain. We seek to systematically study how different visual representations may affect people\u2019s performance in extracting information out of keyword summaries. To this purpose, we first create a design space of possible visual representations and then compare the possible solutions in this design space through a variety of representative tasks and performance metrics. Other researchers have, in the past, studied some aspects of effectiveness with word clouds, however, the existing literature is somewhat scattered and do not seem to address the problem in a sufficiently systematic and holistic manner. The results of our studies showed a strong dependency on the tasks users are performing. We present details of our methodology, the results, as well as, guidelines on how to design effective keyword summaries base in our discoveries.", "uri": "https://vimeo.com/230840786", "name": "[VIS17 Preview] Taking Word Clouds Apart: An Empirical Investigation of the Design Space for Keyword Summaries (InfoVis...", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:28:12+00:00", "description": "Authors: Arjun Srinivasan, John Stasko\n\nAbstract: Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from a preliminary evaluation of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.", "uri": "https://vimeo.com/230840776", "name": "[VIS17 Preview] Orko: Facilitating Multimodal Interaction for Visual Network Exploration and Analysis (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:28:00+00:00", "description": "Authors: Shusen Liu, Peer-Timo Bremer, Jayaraman J. Thiagarajan, Vivek Srikumar, Bei Wang, Yarden Livnat, Valerio Pascucci\n\nAbstract: Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP). However, despite their widespread application, little is known about the structure and properties of these spaces. To gain insights into the relationship between words, the NLP community has begun to adapt high-dimensional visualization techniques. \\ In particular, researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (e.g., word analogies), respectively. \\ Unfortunately, these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings. \\ Here, we introduce new embedding techniques for visualizing semantic \\ and syntactic analogies, and the corresponding tests to determine whether \\ the resulting views capture salient structures. \\ Additionally, we introduce two novel views for a \\ comprehensive study of analogy relationships.  Finally, we augment \\ t-SNE embeddings to convey uncertainty information in order to allow a \\ reliable interpretation. \\ Combined, the different views address a number of domain-specific \\ tasks difficult to solve with existing tools.", "uri": "https://vimeo.com/230840739", "name": "[VIS17 Preview] Visual Exploration of Semantic Relationships in Neural Word Embeddings (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:27:51+00:00", "description": "Authors: Max Sondag, Bettina Speckmann, Kevin Verbeek\n\nAbstract: Treemaps are a popular tool to visualize hierarchical data: items are represented by nested rectangles and the area of each rectangle corresponds to the data being visualized for this item. The visual quality of a treemap is commonly measured via the aspect ratio of the rectangles. If the data changes, then a second important quality criterion is the stability of the treemap: how much does the treemap change as the data changes. We present a novel stable treemapping algorithm that has very high visual quality. Whereas existing treemapping algorithms generally recompute the treemap every time the input changes, our algorithm changes the layout of the treemap using only local modifications. This approach not only gives us direct control over stability, but it also allows us to use a larger set of possible layouts, thus provably resulting in treemaps of higher visual quality compared to existing algorithms. We further prove that we can reach all possible treemap layouts using only our local modifications. Furthermore, we introduce a new measure for stability that better captures the relative positions of rectangles. We finally show via experiments on real-world data that our algorithm outperforms existing treemapping algorithms also in practice on either visual quality and/or stability. Our algorithm scores high on stability regardless of whether we use an existing stability measure or our new measure.", "uri": "https://vimeo.com/230840715", "name": "[VIS17 Preview] Stable Treemaps via Local Moves (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:27:42+00:00", "description": "Authors: Evanthia Dimara, Anastasia Bezerianos, Pierre Dragicevic\n\nAbstract: We explore how to rigorously evaluate multidimensional visualizations for their ability to support decision making. We first define multi-attribute choice tasks, a type of decision task commonly performed with such visualizations. We then identify which of the existing multidimensional visualizations are compatible with such tasks, and set out to evaluate three elementary visualizations: parallel coordinates, scatterplot matrices and tabular visualizations. Our method consists in first giving participants low-level analytic tasks, in order to ensure that they properly understood the visualizations and their interactions. Participants are then given multi-attribute choice tasks consisting of choosing holiday packages. We assess decision support through multiple objective and subjective metrics, including a decision accuracy metric based on the consistency between the choice made and self-reported preferences for attributes. We found the three visualizations to be comparable on most metrics, with a slight advantage for tabular visualizations. In particular, tabular visualizations allow participants to reach decisions faster. Thus, although decision time is typically not central in assessing decision support, it can be used as a tie-breaker when visualizations achieve similar decision accuracy. Our results also suggest that indirect methods for assessing choice confidence may allow to better distinguish between visualizations than direct ones. We finally discuss the limitations of our methods and directions for future work, such as the need for more sensitive metrics of decision support.", "uri": "https://vimeo.com/230840695", "name": "[VIS17 Preview] Conceptual and Methodological Issues in Evaluating Multidimensional Visualizations for Decision Support...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:27:34+00:00", "description": "Authors: David Burlinson, Kalpathi Subramanian, Paula Goolkasian\n\nAbstract: Effective communication using visualization relies in part on the use of viable encoding strategies. For example, a viewer\u2019s ability to rapidly and accurately discern between two or more categorical variables in a chart or figure is contingent upon the distinctiveness of the encodings applied to each variable. Research in perception suggests that color is a more salient visual feature when compared to shape and although that finding is supported by visualization studies, characteristics of shape also yield meaningful differences in distinctiveness. We propose that open or closed shapes (that is, whether shapes are composed of line segments that are bounded across a region of space or not) represent a salient characteristic that influences perceptual processing. Three experiments were performed to test the reliability of the open/closed category; the first two from the perspective of attentional allocation, and the third experiment in the context of multi-class scatterplot displays. In the first, a flanker paradigm was used to test whether perceptual load and open/closed feature category would modulate the effect of the flanker on target processing. Results showed an influence of both variables. The second experiment used a Same/Different reaction time task to replicate and extend those findings. Results from both show that responses are faster and more accurate when closed rather than open shapes are processed as targets, and there is more processing interference when two competing shapes come from the same rather than different open or closed feature categories. The third experiment employed three commonly used visual analytic tasks - perception of average value, numerosity, and linear relationships with both single and dual displays of open and closed symbols. Our findings show that for numerosity and trend judgments, in particular, that different symbols from the same open or closed feature category cause more perceptual interference when they are presented together in a plot than symbols from different categories. Moreover, the extent of the interference appears to depend upon whether the participant is focused on processing open or closed symbols.", "uri": "https://vimeo.com/230840681", "name": "[VIS17 Preview] Open vs Closed Shapes: New Perceptual Categories? (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:27:13+00:00", "description": "Authors: Paulo Ivson, Daniel Nascimento, Waldemar Celes, Simone DJ Barbosa\n\nAbstract: Building Information Modeling (BIM) provides an integrated 3D environment to manage large-scale engineering projects. The Architecture, Engineering and Construction (AEC) industry explores 4D visualizations over these datasets for virtual construction planning. However, existing solutions lack adequate visual mechanisms to inspect the underlying schedule and make inconsistencies readily apparent. The goal of this paper is to apply best practices of information visualization to improve 4D analysis of construction plans. We first present a review of previous work that identifies common use cases and limitations. We then consulted with AEC professionals to specify the main design requirements for such applications. These guided the development of CasCADe, a novel 4D visualization system where task sequencing and spatio-temporal simultaneity are immediately apparent. This unique framework enables the combination of diverse analytical features to create an information-rich analysis environment. We also describe how engineering collaborators used CasCADe to review the real-world construction plans of an Oil &amp; Gas process plant. The system made evident schedule uncertainties, identified work-space conflicts and helped analyze other constructability issues. The results and contributions of this paper suggest new avenues for future research in information visualization for the AEC industry.", "uri": "https://vimeo.com/230840635", "name": "[VIS17 Preview] CasCADe: A Novel 4D Visualization System for Virtual Construction Planning (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:27:00+00:00", "description": "Authors: Ricardo Langner, Tom Horak, Raimund Dachselt\n\nAbstract: We present VisTiles, a conceptual framework that uses a set of mobile devices to distribute and coordinate visualization views for the exploration of multivariate data. In contrast to desktop-based interfaces for information visualization, mobile devices offer the potential to provide a dynamic and user-defined interface supporting co-located collaborative data exploration with different individual workflows. As part of our framework, we contribute concepts that enable users to interact with coordinated &amp; multiple views (CMV) that are distributed across several mobile devices. The major components of the framework are: (i) dynamic and flexible layouts for CMV focusing on the distribution of views and (ii) an interaction concept for smart adaptations and combinations of visualizations utilizing explicit side-by-side arrangements of devices. As a result, users can benefit from the possibility to combine devices and organize them in meaningful spatial layouts. Furthermore, we present a web-based prototype implementation as a specific instance of our concepts. This implementation provides a practical application case enabling users to explore a multivariate data collection. We also illustrate the design process including feedback from a preliminary user study, which informed the design of both the concepts and the final prototype.", "uri": "https://vimeo.com/230840608", "name": "[VIS17 Preview] VisTiles: Coordinating and Combining Co-located Mobile Devices for Visual Data Exploration (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:26:50+00:00", "description": "Authors: Nils Rodrigues, Daniel Weiskopf\n\nAbstract: Conventional dot plots use a constant dot size and are typically applied to show the frequency distribution of small data sets. \\ Unfortunately, they are not designed for a high dynamic range of frequencies. \\ We address this problem by introducing nonlinear dot plots. Adopting the idea of nonlinear scaling from logarithmic bar charts, our plots allow for dots of varying size so that columns with a large number of samples are reduced in height. For the construction of these diagrams, we introduce an efficient two-way sweep algorithm that leads to a dense and symmetrical layout. \\ We compensate aliasing artifacts at high dot densities by a specifically designed low-pass filtering method. Examples of nonlinear dot plots are compared to conventional dot plots as well as linear and logarithmic histograms. \\ Finally, we include feedback from an expert review.", "uri": "https://vimeo.com/230840580", "name": "[VIS17 Preview] Nonlinear Dot Plots (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:26:40+00:00", "description": "Authors: Romain Vuillemot, Jeremy Boy\n\nAbstract: Mock-ups are rapid, low fidelity prototypes, that are used in many design-related fields to generate and share ideas. While their creation is supported by many mature methods and tools, surprisingly little are suited for the needs of information visualization. In this article, we introduce a novel approach to creating visualizations mock-ups, based on a dialogue between graphic design and parametric toolkit explorations. Our approach consists in iteratively subdividing the display space, while progressively informing each division with realistic data. We show that a wealth of mock-ups can easily be created using only temporary data attributes, as we wait for more realistic data to become available. We describe the implementation of this approach in a D3-based toolkit, which we use to highlight its generative power, and we discuss the potential for transitioning towards higher fidelity prototypes.", "uri": "https://vimeo.com/230840552", "name": "[VIS17 Preview] Structuring Visualization Mock-ups at a Graphical Level by Dividing the Display Space (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:26:25+00:00", "description": "Authors: Jochen Goertler, Christoph Schulz, Daniel Weiskopf, Oliver Deussen\n\nAbstract: We present a novel type of circular treemap, where we intentionally allocate extra space for additional visual variables. With this extended visual design space, we encode hierarchically structured data along with their uncertainties in a combined diagram. We introduce a hierarchical and force-based circle-packing algorithm to compute Bubble Treemaps, where each node is visualized using nested contour arcs. Bubble Treemaps do not require any color or shading, which offers unrestricted design choices. We explore uncertainty visualization as an application of our treemaps using standard error and Monte Carlo-based statistical models. To this end, we discuss how uncertainty propagates within hierarchies. Furthermore, we show the effectiveness of our visualization using three different examples: the package structure of Flare, the S&amp;P 500 index, and the US consumer expenditure survey.", "uri": "https://vimeo.com/230840520", "name": "[VIS17 Preview] Bubble Treemaps for Uncertainty Visualization (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:26:16+00:00", "description": "Authors: Laura Matzen, Michael Haass, Kristin Divis, Zhiyuan Wang, Andrew Wilson\n\nAbstract: Evaluating the effectiveness of data visualizations is a challenging undertaking and often relies on one-off studies that test a visualization in the context of one specific task. Researchers across the fields of data science, visualization, and human-computer interaction are calling for foundational tools and principles that could be applied to assessing the effectiveness of data visualizations in a more rapid and generalizable manner. One possibility for such a tool is a model of visual saliency for data visualizations. Visual saliency models are typically based on the properties of the human visual cortex and predict which areas of a scene have visual features (e.g. color, luminance, edges) that are likely to draw a viewer\u2019s attention. While these models can accurately predict where viewers will look in a natural scene, they typically do not perform well for abstract data visualizations. In this paper, we discuss the reasons for the poor performance of existing saliency models when applied to data visualizations. We introduce the Data Visualization Saliency (DVS) model, a saliency model tailored to address some of these weaknesses, and we test the performance of the DVS model and existing saliency models by comparing the saliency maps produced by the models to eye tracking data obtained from human viewers. Finally, we describe how modified saliency models could be used as general tools for assessing the effectiveness of visualizations, including the strengths and weaknesses of this approach.", "uri": "https://vimeo.com/230840505", "name": "[VIS17 Preview] Data Visualization Saliency Model: A Tool for Evaluating Abstract Data Visualizations (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:26:08+00:00", "description": "Authors: Charles Perin, Tiffany Wun, Richard Pusch, Sheelagh Carpendale\n\nAbstract: We empirically evaluate the extent to which people perceive non-constant time and speed encoded on 2D paths. In our graphical perception study, we evaluate nine encodings from the literature for both straight and curved paths. Visualizing time and speed information is a challenge when the x and y axes already encode other data dimensions, for example when plotting a trip on a map. This is particularly true in disciplines such as time-geography and movement analytics that often require visualizing spatio-temporal trajectories. A common approach is to use 2D+time trajectories, which are 2D paths for which time is an additional dimension. However, there are currently no guidelines regarding how to represent time and speed on such paths. Our study results provide InfoVis designers with clear guidance regarding which encodings to use and which ones to avoid; in particular, we suggest using color value to encode speed and segment length to encode time whenever possible.", "uri": "https://vimeo.com/230840492", "name": "[VIS17 Preview] Assessing the Graphical Perception of Time and Speed on 2D + Time Trajectories (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:26:00+00:00", "description": "Authors: Jessica Hullman, Matthew Kay, Yea-Seul Kim, Samana Shrestha\n\nAbstract: People often have erroneous intuitions about the results of uncertain processes, such as scientific experiments. Many uncertainty visualizations assume considerable statistical knowledge, but have been shown to prompt erroneous conclusions even when users possess this knowledge. Active learning approaches as well as discrete (frequency) formats for probability information have been shown to improve statistical reasoning, but are rarely applied in visualizing uncertainty in scientific reports. We present a controlled study to evaluate the impact of an alternative, interactive graphical prediction technique for communicating uncertainty in experiment results. Using our technique, users sketch their prediction of the uncertainty in experimental effects prior to viewing the true sampling distribution from an experiment. We find that having a user graphically predict the possible effects from experiment replications is an effective way to improve one\u2019s ability to make predictions about replications of new experiments. Additionally, visualizing uncertainty as a set of discrete outcomes, as opposed to a continuous probability distribution, can improve recall of a sampling distribution from a single experiment. Our work has implications for various applications where it is important to elicit peoples\u2019 estimates of probability distributions and to communicate uncertainty effectively. \\", "uri": "https://vimeo.com/230840470", "name": "[VIS17 Preview] Imagining Replications: Graphical Prediction and Discrete Visualizations Improve Recall and Estimation of...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:25:35+00:00", "description": "Authors: Philipp Koytek, Charles Perin, Jo Vermeulen, Elisabeth Andre, Sheelagh Carpendale\n\nAbstract: We extend the popular brushing and linking technique by incorporating personal agency in the interaction. We map existing \\ research related to brushing and linking into a design space that deconstructs the interaction technique into three components: source \\ (what is being brushed), link (the expression of relationship between source and target), and target (what is revealed as related to the \\ source). Using this design space, we created MyBrush, a unified interface that offers personal agency over brushing and linking by \\ giving people the flexibility to configure the source, link, and target of multiple brushes. The results of three focus groups demonstrate \\ that people with different backgrounds leveraged personal agency in different ways, including performing complex tasks and showing \\ links explicitly. We reflect on these results, paving the way for future research on the role of personal agency in information visualization.", "uri": "https://vimeo.com/230840428", "name": "[VIS17 Preview] MyBrush: Brushing and Linking with Personal Agency (InfoVis Paper)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:25:24+00:00", "description": "Authors: Oh-Hyun Kwon, Tarik Crnovrsanin, Kwan-Liu Ma\n\nAbstract: Using different methods for laying out a graph can lead to very different visual appearances, with which the viewer perceives different information. Selecting a \"good\" layout method is thus important for visualizing a graph. The selection can be highly subjective and dependent on the given task. A common approach to selecting a good layout is to use aesthetic criteria and visual inspection. However, fully calculating various layouts and their associated aesthetic metrics is computationally expensive. In this paper, we present a machine learning approach to large graph visualization based on computing the topological similarity of graphs using graph kernels. For a given graph, our approach can show what the graph would look like in different layouts and estimate their corresponding aesthetic metrics. An important contribution of our work is the development of a new framework to design graph kernels. Our experimental study shows that our estimation calculation is considerably faster than computing the actual layouts and their aesthetic metrics. Also, our graph kernels outperform the state-of-the-art ones in both time and accuracy. In addition, we conducted a user study to demonstrate that the topological similarity computed with our graph kernel matches perceptual similarity assessed by human users.", "uri": "https://vimeo.com/230840405", "name": "[VIS17 Preview] What Would a Graph Look Like in This Layout? A Machine Learning Approach to Large Graph Visualization...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:24:51+00:00", "description": "Poster \n\nAuthors: \n\nAbstract:", "uri": "https://vimeo.com/230840335", "name": "[VIS17 Preview] Posters (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:24:24+00:00", "description": "Poster \n\nAuthors: Damon Crockett, Joe Walsh, Klaus Ackermann, Andrea Navarrete, Rayid Ghani\n\nAbstract: The recent spread of machine learning methods into critical decision-making, especially in public policy domains, has necessitated a focus on their intelligibility and transparency. The literature on intelligibility in machine learning offers a range of methods for identifying model variables important for making predictions, but measures of predictor importance may be poorly understood by human users, leaving the crucial matter unexplained \u2013 viz., why the predictor in question is important. There is a critical need for tools that can interpret predictor importances in such a way as to help users understand, trust, and take action on model predictions. We describe a prototype system for achieving these goals and discuss a particular use case \u2013 early intervention systems for police departments, which model officers' risk of having \"adverse incidents'' with the public.", "uri": "https://vimeo.com/230840271", "name": "[VIS17 Preview] Visualizing Meta-Explanation in Early Intervention Systems for Police Departments (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:21:38+00:00", "description": "Poster \n\nAuthors: Vinh Nguyen, Tuan Dang\n\nAbstract: Interactive visualization tools are highly desirable to biologist and cancer researchers to explore the complex structures, detect patterns and find out the relationships among biomolecules responsible for a cancer type. A pathway contains various biomolecules in different layers of the cell which are responsible for specific cancer type. Researchers are highly interested in understanding the relationships among the proteins of different pathways and furthermore want to know how those proteins are interacting in different pathways for various cancer types. We introduce CancerMapper, a visual analytics system that helps researchers to explore cancer study interaction network. To fully understand the role of proteins in different cancers, twenty-six cancer studies are merged together. CancerMapper also helps biologists to drill down the cancer network based on the commonly mutated proteins and their frequencies. Proteins which are highly interacted are clustered together. A bubble graph visualizes common protein based on its frequency and biological assemblies. Parallel coordinates highlight patterns of patient profiles (obtained from cBioportal by WebAPI services) on different attributes for a specified cancer study", "uri": "https://vimeo.com/230839900", "name": "[VIS17 Preview] CancerMapper: Explorations of Cancer Study Network (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:21:22+00:00", "description": "Poster \n\nAuthors: David Gotz, Rashnil Chaturvedi\n\nAbstract: As organizations gather ever larger and more detailed datasets, predictive modeling is becoming a widely used technology in support of data-driven decision making. In a diverse set of disciplines, ranging from advertising to medicine, temporal event data (such as click streams and electronic health records) are increasingly being used as the basis for training these predictive models. In these cases, temporal relationships between events (e.g., one event occurring before another vs. the same events in opposite order) can be highly predictive. However, existing methods for feature construction make it difficult to incorporate this sort of information, and often require domain experts to manually specify patterns of interest. This poster introduces Interactive Temporal Feature Construction (ITFC), a visual analytics technique designed to enable more effective, data-driven temporal feature construction. The primary contributions for this work include a new interactive workflow for model refinement, a set of algorithms and visual representations designed to support that workflow, and a use case which demonstrates how ITFC can result in more accurate predictive models when applied to complex cohorts of electronic health data.", "uri": "https://vimeo.com/230839867", "name": "[VIS17 Preview] Interactive Temporal Feature Construction: A User-Driven Approach to Predictive Model Development (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:21:14+00:00", "description": "Poster \n\nAuthors: Vinh Nguyen, Hoang Long Nguyen, Tuan Dang\n\nAbstract: Detecting community structure in networks is highly desirable in many application domains, such as finding proteins with similar functionality in a biological pathway or automatically grouping relevant people in a social network. However, this is still a daunting task due to the network sizes, as well as the complicated relations between entities. This paper provides a study on algorithms to find communities in a network using edge betweenness centrality. We have implemented the modularity in order to compute the suitable network structure on weighted undirected networks. We also discuss the pros and cons of existing techniques in detecting community structures. To highlight the benefits of the selected techniques, we demonstrate their applications on various datasets, including Victor Hugo's Les Mis\u00e9rables, the movies's network of actors, the author's collaboration network in visualization publications, and the protein interaction network", "uri": "https://vimeo.com/230839847", "name": "[VIS17 Preview] A study of algorithms for detecting community in networks (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:21:05+00:00", "description": "Poster \n\nAuthors: John Hwong, Pierre Amelot, Kathryn McManus\n\nAbstract: Art has not traditionally been quantified in large datasets available to the public, but this has changed over the last several years thanks to the \\ digitization of many collections. Our team utilized this opportunity to create four unique web-based \"exhibits\" that help users explore and \\ interact with the Rijksmuseum collection in ways previously unavailable.", "uri": "https://vimeo.com/230839825", "name": "[VIS17 Preview] Rijks Viz (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:20:53+00:00", "description": "Poster \n\nAuthors: Hamid Mansoor, Kartik Vasu, Lane Harrison\n\nAbstract: When it comes to graphical perception tasks, are we all equally skilled? Research in visualization literacy has established methods for objectively measuring a person's literacy level. However, the literacy thread of research has not yet assessed how literacy measures are related to well-established graphical perception tasks. To bridge this gap and contribute to our understanding of how visualization literacy may impact low-level performance with visualizations, we contribute a study replicating a graphical perception study in conjunction with a visualization literacy assessment. Namely, participants with high literacy scores tended to perform not necessarily accurately on low-level graphical perception tasks, but consistently. These preliminary results suggest a new consideration, consistency, in graphical perception studies, as well as a new dimension relating to the value of visualization literacy.", "uri": "https://vimeo.com/230839793", "name": "[VIS17 Preview] Linking Performance on Graphical Perception tasks to Visualization Literacy (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:20:42+00:00", "description": "Poster \n\nAuthors: Marzieh Berenjkoub, Lei Zhang, Guoning Chen\n\nAbstract: In this work, we extend the existing spatial correlation quantification metrics, i.e. the Local Correlation Coefficient (LCC), to spatio-temporal domain, and introduce a new metric for the quantification of the correlation of vector-valued attributes for unsteady flows. In addition, we compute the mutual information between pairs of attributes over time to analyze their temporal correlations. We have applied our correlation quantification metrics to a number of 2D and 3D unsteady flows to assess its usefulness.", "uri": "https://vimeo.com/230839763", "name": "[VIS17 Preview] Correlation Study on Attributes of Unsteady Flows (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:20:34+00:00", "description": "Poster \n\nAuthors: Christine Nothelfer, Steven Franconeri\n\nAbstract: The power of data visualization is not to convey absolute values of individual data points, but to allow the exploration of relations among them. We explored how 7 different ways of encoding pairs of data values can lead to vast differences in the efficiency of visually processing the relations between those pairs. In Exp. 1a and 1b, participants located a pair of data values with a given relation (e.g., small bar to the left of a tall bar) among pairs of the opposite relation. In Exp. 2, participants judged which of two relation types was more prevalent in a briefly presented display of data pairs (e.g., 30 positively sloped lines and 20 negatively sloped lines). Across all experiments, the choice of data depiction led to strong differences in efficiency of processing relations between values, with slope graphs, benchmark graphs, connected dashes performing the best. The ranking is also strikingly unrelated to prescriptions stemming from measurements of perceptual precision of visual encodings [1] (e.g., while an orientation encoding is only moderately precise, it provided a highly efficient encoding for relation perception here).", "uri": "https://vimeo.com/230839741", "name": "[VIS17 Preview] Ranking encodings for efficient perceptual processing of data relations (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:20:25+00:00", "description": "Poster \n\nAuthors: Gabriel Ryan, Abigail Mosca, Eugene Wu, Remco Chang\n\nAbstract: We propose to use approximate entropy as a measure of perceived complexity of line charts. We analytically demonstrate a positive correlation between the noise levels in a line chart and measured approximate entropy values. We also perform an experiment to show that users perceive line charts with high approximate entropy measures as more complex and vice versa.", "uri": "https://vimeo.com/230839714", "name": "[VIS17 Preview] Approximate Entropy as a Measure of Line Chart Complexity (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:20:12+00:00", "description": "Poster \n\nAuthors: Chris Bartlett, Brett Klamer, Annalisa Hartlaub, William Ray\n\nAbstract: Displaying multi-variate data is made more difficult by the fact that arbitrary glyph systems used to denote data values require either user training, or repeated reference to a figure key, especially if multiple glyph systems are used in a single visualization. We demonstrate that pictographic glyphs that evoke cultural or natural responses that are congruent with the desired variable mapping can be used with minimal user training, and that moreover, choosing a glyph system with an inappropriate (for example incongruent) mapping, is detrimental to user understanding and performance. This suggests that certain pictographic glyph systems and mappings possess a Stroop- like\\\\citep{stroop} over-learned characteristic, wherein they have transitioned from conventional symbols requiring conscious reading, to sensory-like symbols with mappings that are processed pre-consciously.", "uri": "https://vimeo.com/230839677", "name": "[VIS17 Preview] Culturally Meaningful Glyphs Contain Information About Data as Elucidated Through a Stroop Task (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:20:02+00:00", "description": "Poster \n\nAuthors: Zhenge Zhao, Youhao Wei, Joshua Levine, Matthew Berger, Danilo Motta, Carlos Scheidegger\n\nAbstract: Computer simulations of the effect of earthquakes on built structures promise to let engineers understand different tradeoffs and designs at an attractively low cost. In this scenario, the bottleneck for the expert is one of data understanding: how do different building designs respond to different earthquakes? What do the building failure modes have in common, and how does this compare to theoretical predictions? The way in which a building responds to an earthquake is complex and controlled by different factors. In this poster, we present ongoing work in building a system for interactive visualization of earthquake simulation data, in collaboration with civil engineers who have run thousands of simulations, varying the height of the simulated buildings, ground acceleration, and building structure design. We describe the challenges in the visualization of such multivariate time series data, and some of our proposed solutions.", "uri": "https://vimeo.com/230839650", "name": "[VIS17 Preview] Using visualization To Understand Earthquake Simulation Data (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:19:54+00:00", "description": "Poster \n\nAuthors: Filip Dabek, Jian Chen, Jesus Caban\n\nAbstract: Summarizing a collection of temporal sequences is a difficult task given the irregular and variable patterns often found in longitudinal events. Across a wide array of domains, researchers and analysts seek to determine ways to gain an overview of a dataset through the identification of the common temporal paths as well as the trajectories that exist between individual events. While these tasks can be difficult on small and structured datasets, they are increased tenfold on temporal sequences that are noisy, irregular, and voluminous in size. This poster presents an approach that has been designed to visually explore large collections of temporal sequences by combining event mining algorithms with visualization techniques to overcome the challenges and complexities of analyzing longitudinal data.", "uri": "https://vimeo.com/230839626", "name": "[VIS17 Preview] TrajectoryFlow: Visual Summarization of Temporal Sequences (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:19:46+00:00", "description": "Poster \n\nAuthors: Juliano Franz, Joseph Malloch, Derek Reilly\n\nAbstract: Mission planning and decision making often require a large amount of complex, heterogeneous data that comes from multiple distributed sources. Data specialists are responsible for exploring and extracting vital information that can lead to a successful operation from complex datasets. Augmented-reality can help experts to explore and understand data by adding an extra volume of 3D information to familiar data representations and interfaces such as maps and interactive tables. We present the beginning stages of a long term research project focusing on immersive interfaces for data analysis in a naval mission setting. The long term research goals include the development of approaches and techniques for data exploration and interaction, query creation and management, virtual co-presence and the integration of heterogeneous devices into a single data session.", "uri": "https://vimeo.com/230839607", "name": "[VIS17 Preview] Exploring Shared Immersive Visualization in AR (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:19:35+00:00", "description": "Poster \n\nAuthors: Mingwei Li, Andreina Siri, Asher Haug-Baltzell, Eric Lyons, Carlos Scheidegger\n\nAbstract: Interactive visualization has become a powerful means to explore syntenic relationships among two genomes, with a variety of available tools for domain scientists to employ. However, these tools do not tend to scale well in the case where many genomes are com- pared against one another. This poster describes ongoing efforts to build techniques and tools to help geneticists understand sets of genomes and their syntenic relationships. Our main contribution is a mechanism that defines set distances: this can be used to compare entire genomes to one another, as well as sets of genomes to each other. Currently, we use this mechanism to generate dimensionality reduction visualizations. We discuss limitations of this approach, as well as future directions.", "uri": "https://vimeo.com/230839587", "name": "[VIS17 Preview] SynMapN: Interactive Visual Comparison for Multiple Genomes (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:19:27+00:00", "description": "Poster \n\nAuthors: Anamaria Crisan, Geoffrey McKee, Tamara Munzner, Jennifer L. Gardy\n\nAbstract: Design study methodologies developed by infovis researchers are not adopted beyond this community despite a growing repertoire of visualization tools created by domain specialists. While better knowledge translation between infovis and domain specialists is needed, it is also arguable that visualization tools, which can have many complex interacting parts, can make it challenging to demonstrate design study methodology in action. Here, we present our application of a design study methodology to a simpler problem of information design in a tuberculosis (TB) clinical report that presents results derived from TB whole genome sequencing (WGS). Clinical reports are a common and foundational element of medical and public health practice and as such are a good, and relatively simple, application context in which to demonstrate the value of applying design study methodology. Using an existing TB clinical WGS report as a base, we collected relevant tasks and data, linked those to alternative report designs, and finally compared those alternatives to the original report with stakeholders. The evidence gathered through our project demonstrated how the original, ad hoc, report design contained elements that were unnecessary, difficult to interpret, or insufficient. We also demonstrated how a number of procedural constraints around current reporting practices, such as how stakeholders received reports and how much time they had to review them, affected the report's design. Taking into consideration the evidence gathered and regulatory guidelines, we produced a new TB WGS clinical report that is currently under more detailed assessment and awaiting deployment. By using a simple and relatable design challenge, providing a concrete framework for data gathering and analysis, and through participatory engagement of the relevant domain specialist community, our work aimed to introduce infovis evidence-based design methodologies to the health sciences community", "uri": "https://vimeo.com/230839573", "name": "[VIS17 Preview] Showcasing the design study methodology through simpler design challenges: An application to a microbial...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:19:19+00:00", "description": "Poster \n\nAuthors: Sina Mohseni, Eric Ragan, Alyssa M. Pena\n\nAbstract: Our work aims to generate visualizations to enable meta-analysis of analytic provenance and aid better understanding of analysts' strategies during exploratory text analysis. We introduce ProvThreads, a visual analytics approach that incorporates interactive topic modeling outcomes to illustrate relationships between user interactions and the data topics under investigation. ProvThreads uses a series of continuous analysis paths called topic threads to demonstrate both topic coverage and the progression of an investigation over time. As an analyst interacts with different pieces of data during the analysis, interactions are logged and used to track user interests in topics over time. A line chart shows different amounts of interest in multiple topics over the duration of the analysis. We discuss how different configurations of ProvThreads can be used to reveal changes in focus throughout an analysis.", "uri": "https://vimeo.com/230839558", "name": "[VIS17 Preview] ProvThreads: Analytic Provenance Visualization and Segmentation (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:19:11+00:00", "description": "Poster \n\nAuthors: Charles D. Stolper, Will Price, Matt Sanford, Duen Horng Chau, John Stasko\n\nAbstract: While there are a variety of distinct graph visualization techniques described in the visualization literature and used in practice, many of these techniques have similarities. Graph-Level Operations (GLOs) use these similarities as building blocks for specifying graph visualization techniques. We present two GLO-based models, each consisting of a visual element model and an operation set. GLOv2 enables specifying interactive static-graph visualization techniques with multiple displays and DGLOs enables specifying dynamic-graph visualization techniques.", "uri": "https://vimeo.com/230839532", "name": "[VIS17 Preview] Atomic Operations for Specifying Graph Visualization Techniques (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:19:03+00:00", "description": "Poster \n\nAuthors: Felipe Sarmiento, Hemanth Pidaparthy, Peter Coppin\n\nAbstract: With a focus on online streaming sites, in this work, we have developed an intuitive real-time visual summary to recap the key events in a match. Using state-of-the-art computer vision algorithms, we visualize game-flow to enable a tactical understanding of the match. We develop an approach that we intend to extend to other non-visual modalities in the future.", "uri": "https://vimeo.com/230839509", "name": "[VIS17 Preview] Real-time Visual Recap and Game-flow Visualization (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:18:55+00:00", "description": "Poster \n\nAuthors: Brian Ondov, Adam Phillippy\n\nAbstract: Interactive implementations of hierarchical, radial space-filling displays, such as Sunburst or Krona charts, allow complex datasets to be displayed and explored intuitively. However, to date, these methods have lacked the facility to compare two datasets in a unified visualization, which has limited their usefulness for correlative exploration and assessment of controlled experiments. Here, we address these use cases with bipartite Krona charts, which allow two hierarchical datasets to be explored simultaneously. This allows differences between datasets to be observed easily while preserving the depth and detail inherent in these types of charts.", "uri": "https://vimeo.com/230839486", "name": "[VIS17 Preview] Juxtaposition of hierarchical quantities with bipartite Krona charts (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:18:40+00:00", "description": "Poster \n\nAuthors: Sierra Shell, Lisa Everdyke, Joseph Hines, Shaun Kurian, Rajiv Ramarajan, Sunny Su, Jordan Benson\n\nAbstract: We have extended the work of Heer and Robertson [1], which evaluated animated transitions in statistical graphics and established guidelines for effective use of staggering and staging to improve perception of changes in data. Our objective is to create a generalizable and testable framework which is applicable to additional visualization types and data transitions not covered by previous research. The framework defines how individual data transitions should be composed into a complete animation including individual animation methods, pacing, and staging. Our initial evaluation efforts test which staging arrangements allow for the most accurate understanding of the change in the data as well as which are most preferred by end users.", "uri": "https://vimeo.com/230839443", "name": "[VIS17 Preview] Toward a General Animated Transition Framework (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:18:31+00:00", "description": "Poster \n\nAuthors: Ji Won Chung, Isha Raut, Ji Young Yun, Kelly Pien, Subshini Sridhar, Morganne Crouser, R. Jordan Crouser\n\nAbstract: Issues regarding mental illness and substance abuse are some of the most rapidly growing segments of the global disease burden today. Despite this, collaboration between computational scientists and mental health practitioners has been nearly nonexistent. Catalyzed by participating in recent Symposia on Computing and Mental Health, the Human Computation and Visualization Laboratory at Smith College has established an ongoing collaboration with community-based clinicians at the Justice Resource Institute (JRI). In this paper, we present DSMVis, a co-designed interactive visual exploration system designed to help mitigate diagnostic bias.", "uri": "https://vimeo.com/230839419", "name": "[VIS17 Preview] DSMVis: Interactive Visual Exploration of the DSM-5 for Mental HealthProviders (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:18:22+00:00", "description": "Poster \n\nAuthors: Mi Feng, Cheng Deng, Evan Peck, Lane Harrison\n\nAbstract: Designers occasionally add text-based search to visualizations to enable users to quickly identify parts of the visualization they are interested in. Yet precisely how this functionality augments the experience of the visualization as well as the goals of the user remains unknown. To bridge this gap, we contribute a study on text-based search in visualization. We report the results of an experiment on two search-augmented visualizations across 357 online participants, which demonstrate that text-based search influences users' intention to identify personally relevant information in visualizations, and can lead to increased engagement metrics such as more time spent viewing the data elements found using the search box. We then provide a discussion on the broader implications of including search in visualization. We establish a link to existing information seeking taxonomies, showing that search enables users to more diverse information seeking goals with a given visualization.", "uri": "https://vimeo.com/230839396", "name": "[VIS17 Preview] The Impact of Text-based Search in Interactive Data Visualizations on the Web (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:18:13+00:00", "description": "Poster \n\nAuthors: Nathan Garrett\n\nAbstract: It is challenging to visualize the time component of eye-tracking data. Scanpaths can show where a single user looked, and in what order, but multiple users' scanpaths can easily overwhelm viewers. This paper's approach shows larger trends without hiding short duration fixations. Each user's fixations are plotted in a separate space-time cube, where fixation x- &amp; y-coordinates are plotted normally, but the z-axis is used to represent time. The fixations are joined by a line, which is color-coded when it intersects areas of interest (AOIs). The resulting cubes, one per user, are then placed into a 3-dimensional space side-by-side. The result can be viewed close up to see an individual user's gaze, or zoomed out to see larger patterns. When viewed from above, the result looks similar to Sparklines. This design is demonstrated on the eye movements of users watching training videos. It is able to show patterns not visible through other techniques.", "uri": "https://vimeo.com/230839372", "name": "[VIS17 Preview] Plotting Eye Tracking Data in Space-Time Cubes (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:18:02+00:00", "description": "Poster \n\nAuthors: Xiyao Wang, Lonni Besan\u00e7on, Mehdi Ammi, Tobias Isenberg\n\nAbstract: We present a pressure-augmented tactile interaction technique to improve 3D object/view manipulation tasks on mobile devices. Existing tactile techniques for mobile data exploration either make use of up to four fingers to control all the needed degrees of freedom (DOF) for 3D manipulation or simultaneously adjust multiple DOF together to reduce the number of fingers needed for interaction. Yet the small display size of mobile devices limits the number of fingers that should simultaneously be used. Controlling each DOF for 3D data exploration separately, however, gives users more control. We address this contradiction by combining tactile and pressure input. We thus use pressure to intuitively switch between different tactile interaction modes. In this extended abstract we describe our interaction design as well as our rationale for the input mappings.", "uri": "https://vimeo.com/230839347", "name": "[VIS17 Preview] Augmenting Tactile 3D Data Exploration With Pressure Sensing (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:17:37+00:00", "description": "Poster \n\nAuthors: Ali Baigelenov, Michael Saenz, Ya-Hsin Hung, Paul Parsons\n\nAbstract: Visualizations act as cognitive aids by making reasoning tractable. To choose an appropriate visualization, designers need to know about the cognitive advantages and disadvantages of different visualization techniques. While considerable research has focused on low-level perceptual issues related to visual encodings and judgments, less research has focused on the cognitive operations that are supported by visualization techniques. We conducted a pilot study using mixed methods to uncover properties of some common visualization tech- niques that allow propositional statements to be directly observed rather than indirectly inferred. We describe the results of our study and discuss potential benefits of this line of research to visualization designers and researchers.", "uri": "https://vimeo.com/230839289", "name": "[VIS17 Preview] Toward an Understanding of Observational Advantages in Information Visualization (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:15:48+00:00", "description": "Poster \n\nAuthors: Sakshi Sanjay Pratap, Dhruv Chand Muttaraju\n\nAbstract: Information Visualization as a field has been growing tremendously because of the greater need to obtain insights and answer questions on the large number of massive and complex data sets available. The visual representation and encoding component receives the majority of attention in Infovis research, but interaction is often relegated to a secondary role. In interaction research, developing techniques such as brushing, panning, and zooming to visually query and filter data are active areas. However, in comparison, developing novel input techniques to support interactions with visualizations is lesser explored. In this poster, we describe a gesture based input method to control data visualizations. Our motivation for this project is threefold: to develop a way of interacting with data visualizations in exhibits and presentation environments without on-screen manipulation; to explore an innovative interaction technique that is intuitive and ubiquitous and does not rely on the use of complicated external devices; and to create a library that can be integrated with existing web-based visualizations.", "uri": "https://vimeo.com/230839044", "name": "[VIS17 Preview] Viseract: Computer Vision based Gesture Control for InfoVis (Poster)", "year": "2017", "event": "INFOVIS, PREVIEW"}, {"created_time": "2017-08-23T21:15:25+00:00", "description": "Poster \n\nAuthors: Antoine Laumond, Bruno Pinaud, Guy Melancon\n\nAbstract: The \"zoom and filter'' operations dear to the Shneiderman mantra are in reality not well specified in a data exploration context. There is a need to support users and suggest potential directions for exploration, and automatically selects a subset of nodes of interest based on a minimum initial user input. Filtering the data additionally avoids visual cluttering, especially overplotting, often resulting from laying out large networks. This paper explores how the multilayer properties of networks can be used to design a DOI (Degree of Interest) approach. Layers in a multilayer network bring nuances and must not be considered on an equal basis. We propose a method for extracting, expanding and displaying a sequence of sub-networks to guide users when exploring a multilayer network. Nodes and edges are selected based on an interest score computed for each node specifically taking the layer structure into account.", "uri": "https://vimeo.com/230838983", "name": "[VIS17 Preview] eDOI: Exploratory Degree of Interest, Visual User Interest Based Exploration of Multilayer Networks (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:12:50+00:00", "description": "Poster \n\nAuthors: Chris Rooney, Roger Beecham, Jason Dykes, William Wong\n\nAbstract: A common characteristic of applied visualization is collaboration between visualization researcher and domain expert \u2013 where the visualization researcher attempts to assimilate sufficient detail around data, task and requirements to design a visualization tool that is manifestly useful. We report on a method for enabling such a collaboration that can be used throughout the design process to gather and develop requirements and continually evaluate and support iterative design. We do so using highly interactive web-pages that we term dynamic design documents. Applied during a four-year visual data analysis project for crime research, these documents enabled a series of data mappings to be explored by our collaborators (crime analysts) remotely \u2013 in a flexible and continuous way. We argue that they engendered a level of engagement that is qualitatively distinct from more traditional methods of feedback elicitation, offered a solution to limited and intermittent contact between analyst and visualization researcher and speculate that they provided a means of partially addressing certain intractable deficiencies, such as social desirability-bias, that are common to evaluation in applied data visualization.", "uri": "https://vimeo.com/230838650", "name": "[VIS17 Preview] Dynamic Design Documents for supporting applied visualization (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:12:40+00:00", "description": "Poster \n\nAuthors: Xan Gregg\n\nAbstract: We explore a new chart type, called \"packed bars\", especially suited to displaying data sets with a categorical factor variable having high cardinality (typically hundreds or thousands of unique values) and with a continuous response variable that may have a wide range of values. A packed bars chart combines features of standard bar charts and treemaps to support a Focus+Context view of the entire data set. We examine examples using packed bars and discuss alternative data visualizations for such data sets.", "uri": "https://vimeo.com/230838625", "name": "[VIS17 Preview] Introducing the Packed Bars Chart Type (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:12:30+00:00", "description": "Poster \n\nAuthors: Houda Lamqaddam, Jan Aerts\n\nAbstract: The design process is a critical part of creating visualisation tools. Different datasets, target users, work contexts can mean requiring drastically different designs. Several methods have been devised to get designers to explore the design space more consciously. In this work, we look at the methods used to assist with the ideation and overall creation process in data visualization and in other fields. We have used analysis axes found in the field of design research to compare existing design methods.", "uri": "https://vimeo.com/230838602", "name": "[VIS17 Preview] Design Methods in Data Visualization and Elsewhere: Towards Aiding Novice Designers (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:12:17+00:00", "description": "Poster \n\nAuthors: Carolina Nobre, Nils Gehlenborg, Hilary Coon, Alexander Lex\n\nAbstract: The majority of diseases that are a significant challenge for public and individual heath are caused by a combination of hereditary and environmental factors. In this poster, we introduce Lineage, a novel visual analysis tool, designed to support domain experts that study such multifactorial diseases in the context of genealogies. We also introduce data-driven aggregation methods to effectively visualize genealogies of multiple families with hundreds of members across several generations.", "uri": "https://vimeo.com/230838580", "name": "[VIS17 Preview] Lineage: Visualizing Multivariate Clinical Data in Genealogy Graphs (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:12:06+00:00", "description": "Poster \n\nAuthors: Xiaoxue Zhang, Alex Godwin, John Stasko\n\nAbstract: With a focus on urban health, this project intended to elicit the magnitude of health inequities in Atlanta using Millennium Development Goals (MDGs) indicators. We sought to highlight those sections of the urban population who are disadvantaged in accessing services and achieving healthy outcomes. Therefore, the project focused on providing a quick and clear understanding of the current health inequities in Atlanta. A pooled dataset was applied to obtain MDGs indicators, and then we visualized the Atlanta health profile, helping the public to gain insights about this information. We tried to expose as much statistical data as possible, but to avoid visual encodings that may bias the perception of that data. The developed visualization harbors the potential to explore the hidden inequity of different social and economic aspects, such as education, violence, transportation and so forth. Also, the visualization could potentially be extended to display city profiles besides Atlanta.", "uri": "https://vimeo.com/230838545", "name": "[VIS17 Preview] Equity Monitor: Visualizing Attributes of Health Inequity in Atlanta (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:11:57+00:00", "description": "Poster \n\nAuthors: Nicolas M\u00e9doc, Colas Picard, Beno\u00eet Otjacques, Mohammad Ghoniem\n\nAbstract: State-of-the-art dynamic labeling techniques, e.g. excentric labeling, have successfully been used to inspect the immediate neighborhood of a target object. Yet, they seem to underperform with compact visualizations of large term hierarchies, where the analyst focuses on long term sequences along root-to-leaf paths. In this work we propose ray labeling, a dynamic labeling technique designed to support the exploration of large term hierarchies using compact adjacency-based tree visualizations such as Icicle Plots and Sunbursts. We also describe the protocol of a controlled experiment we are about to conduct in order to compare ray labeling to excentric labeling, for the specific tasks of identifying common and specific term sub-sequences.", "uri": "https://vimeo.com/230838525", "name": "[VIS17 Preview] Ray labeling: dynamic directional labeling for compact term hierarchies (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:11:49+00:00", "description": "Poster \n\nAuthors: Jun Yuan, Siyao Fang, Xiang Huang, Cao Nan\n\nAbstract: With the development of portable electrocardiogram (ECG) monitors, a huge amount of high-quality ECG data have been collected every day in real time, leaving doctors a much heavier workload to read the ECG diagrams for making a diagnosis. By observing the classification result of a series of consecutive heartbeats and their order, doctors can better know the rhyme of the signal and then detect an arrhythmia. However, the present classification algorithms still have an accuracy gap to industrial production. A system with manual correction is desired for the current ECG classification analysis. Therefore, we propose ECGLens, as the first attempt to interactively interpret and improve the heartbeat classification results and support arrhythmia identification with visualization techniques.", "uri": "https://vimeo.com/230838506", "name": "[VIS17 Preview] ECGLens : Interactive ECG Classification and Exploration (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:11:37+00:00", "description": "Poster \n\nAuthors: Erick Cuenca, Arnaud Sallaberry, Florence Ying Wang, Pascal Poncelet\n\nAbstract: Multiple time series are present in many domains such as medicine, finance, and manufacturing for analytical purposes. When dealing with several time series scalability problem overcome. To solve this problem, multiple time series can be organized into a hierarchical structure. In this work, we introduce a Streamgraph-based approach to convey this hierarchical structure. Based on a focus+context technique, our visualization allows time series exploration at different granularities (e. g., from overview to details). A demo is available at http://advanse.lirmm.fr/hierarchical/.", "uri": "https://vimeo.com/230838484", "name": "[VIS17 Preview] Visualizing Hierarchical Time Series with a Focus+Context Approach (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:11:27+00:00", "description": "Poster \n\nAuthors: James Jackson, Jonathan Roberts\n\nAbstract: Visualization researchers perform user studies to evaluate the effectiveness and usability of produced visualisations. \\ While there are different styles of evaluation, some popular techniques involve comparing the result of several parameter changes, looking how results have changed and if a change is noticeable. Developers need to display many different pictures, to a wide variety of users, while guaranteeing full coverage of all potential designs across all participants. One solution is to use static screen shots of their outputs along with popular on line surveying tools, but such a solution omits any system interactivity. Alternatively they can create their own surveying application on a chosen platform. We introduce VisSurvey.js, a JavaScript library that helps developers create evaluation studies. Developers can create user studies of their application in a web browser, and easily capture the results.", "uri": "https://vimeo.com/230838461", "name": "[VIS17 Preview] VisSurvey.js - A Web Based Javascript Application for visualisation Evaluation User Studies (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:11:18+00:00", "description": "Poster \n\nAuthors: Hiroaki Natsukawa, Koji Koyamada\n\nAbstract: With the development of noninvasive human brain functional imaging techniques, examining the connectivity of brain functions has become an important task. Convergent cross mapping (CCM) has been employed in the neuroscience field to examine the effective connectivity of brain functions. CCM can detect causality from time series data created from deterministic and nonlinear systems. Because CCM includes complicated processes such as the determination of advance parameters, the confirmation of nonlinearity, and the interpretation of results, which results in a lowering of the usability of CCM, there is a strong need for an effective visual interface. In this paper, we propose a visual analytic system that increases the usability of CCM and contributes to new discoveries in effective connectivity. The usability was evaluated using a domain expert questionnaire. It was confirmed that the usability was improved by comparing the proposed system to the original character user interface from the viewpoint of the results and process comprehensibility. In addition, with the proposed system, new findings in human brain connectivity have been obtained from actual magnetoencephalography data.", "uri": "https://vimeo.com/230838448", "name": "[VIS17 Preview] Visualization and Analysis of Human Effective Connectivity using Convergent Cross Mapping (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:11:08+00:00", "description": "Poster \n\nAuthors: Petra Isenberg, Christoph Kinkeldey, Jean-Daniel Fekete\n\nAbstract: We contribute a visual exploration system for analyzing the behavior of individual entities exchanging Bitcoins. Bitcoin is a cryptocurrency, popular for allowing pseudonymous financial transactions. The Bitcoin blockchain is the public ledger of the Bitcoin system holding data on millions of individual transactions between pseudonymous addresses. These addresses belong to individual entities such as people, services, or enterprises. Understanding how the Bitcoin system is used, however, is difficult because it is unclear which addresses belong to the same entities. Our tool addresses this problem by clustering addresses and displaying transaction detail for individual entities.", "uri": "https://vimeo.com/230838426", "name": "[VIS17 Preview] Exploring Entity Behavior on the Bitcoin Blockchain (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:11:00+00:00", "description": "Poster \n\nAuthors: Salman Mahmood, Klaus Mueller\n\nAbstract: Elo is a popular rating scheme in chess and online games. We propose a modified version of the Elo rating system to compare visualization designs in an efficient and asynchronous manner. Our method allows researchers to evolve the design of a visualization layout and monitor its performance. We study our method for testing small multiple designs. Small multiples is a very popular technique to reduce clutter in visualizations. However, they can be difficult to interpret for people who are not entirely familiar with them. We designed various exploded view methods to make small multiples comprehensible and then compared them using our adaptation of the Elo rating system.", "uri": "https://vimeo.com/230838409", "name": "[VIS17 Preview] Applying multi-player rating schemes to manage user studies of visual analytics designs (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:10:51+00:00", "description": "Poster \n\nAuthors: Jonathan Roberts, Ana Frankenberg-Garcia, Robert Lew, Geraint Rees\n\nAbstract: Text visualisation is gaining popularity. Researchers have investigated methods that display the structure of documents, show how books have evolved over many editions, how words are distributed in text. But few researchers have investigated how visualisation techniques could help someone write better text documents. The goal of the writer is to create texts that are not just grammatically correct, but also idiomatic, and fluent to read. Our focus is to investigate collocations, i.e., words that have become conventional to use together (fast food vs. *quick food, collect data vs. *pick data, strong tea vs. *powerful tea, etc.) While researchers are starting to use visualisation techniques to help users write better computer programs, musicians create music, or artists be more creative, there is less research in writing. In this short paper we not only make a call for action, but we look at related work, and discuss some the main challenges and opportunities for research in this area.", "uri": "https://vimeo.com/230838396", "name": "[VIS17 Preview] Visualisation and graphical techniques to help writers write more idiomatically (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:10:39+00:00", "description": "Poster \n\nAuthors: Kahin Akram Hassan, Jimmy Johansson, Camilla Forsell, Matthew Cooper, Niklas R\u00f6nnberg\n\nAbstract: This work presents the results from a user centered evaluation of visual representations of temporal multivariate data using 2D and 3D parallel coordinates. The objective of the evaluation was to investigate whether 2D or 3D representations increase user performance when the data consists of temporal multivariate data and the visual representation contains interactive user tools. The results show that the 3D parallel coordinates representation outperforms 2D parallel coordinates with regards to both accuracy and response time. This result is of interest to the information visualization community since it shows the usefulness of visual representations of temporal multivariate data.", "uri": "https://vimeo.com/230838376", "name": "[VIS17 Preview] On the Use of Parallel Coordinates for Temporal Multivariate Data (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:10:30+00:00", "description": "Poster \n\nAuthors: Panagiotis Ritsos, James Jackson, Jonathan Roberts\n\nAbstract: The recent popularity of virtual reality (VR), and the emergence of a number of affordable VR interfaces, have prompted researchers and developers to explore new, immersive ways to visualize data. This has resulted in a new research thrust, known as Immersive Analytics (IA). However, in IA little attention has been given to the paradigms of augmented/mixed reality (AR/MR), where computer-generated and physical objects co-exist. In this work, we explore the use of contemporary web-based technologies for the creation of immersive visualizations for handheld AR, combining D3.js with the open standards-based Argon AR framework and A-frame/WebVR. We argue in favor of using emerging standards-based web technologies as they work well with contemporary visualization tools, that are purposefully built for data binding and manipulation.", "uri": "https://vimeo.com/230838347", "name": "[VIS17 Preview] Web-based Immersive Analytics in Handheld Augmented Reality (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:10:20+00:00", "description": "Poster \n\nAuthors: Tanyoung Kim\n\nAbstract: Sports fans can \"favorite\" teams on Yahoo Sports mobile app. Utilizing these data from the app users, we investigate the intersection of sports fans between any two teams among four American professional leagues. To visualize the shared fans across different sports and home cities, we designed two views of visualization: Network view with force-directed graph layout, and Continent view with unique visual encoding. We devised a core functionality that enables viewers to filter teams depending on the numbers of shared fans. This work is a visual analytic tool that helps understanding not only the customers' behavior for business decision but also an important part of American culture.", "uri": "https://vimeo.com/230838319", "name": "[VIS17 Preview] Visualizing Shared Fans between Sports Teams (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:10:10+00:00", "description": "Poster \n\nAuthors: Johannes Waschke, Mario Hlawitschka\n\nAbstract: Full rendering of a high number of trajectories (e.g. streamline tractograms of DTI images, or traffic routes) leads to heavy occlusion and superimposition, which reduces the perception of the main characteristics of the data. Evaluation and exploration can therefore be a challenge for the user. These difficulties of understanding are exacerbated when two datasets, like data from different time points, should be compared. We present a method to facilitate the navigation through trajectory sets and enable comparison of two datasets in one image. We minimize occlusion by bundling the trajectories, resulting in a skeleton-like representation, and allow the user to interactively explore regions of interest in detail.", "uri": "https://vimeo.com/230838296", "name": "[VIS17 Preview] Exploration and Comparison of Trajectory Data (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:09:57+00:00", "description": "Poster \n\nAuthors: Marcel Wunderlich, Tatiana von Landesberger, Volker Knauthe\n\nAbstract: Possible train delays lead to uncertainty while planning train trips because the actual delay can not be known in advance. The visualization of Wunderlich et al. unveils this uncertainty but lacks information about transfer duration, which is required for the traveler to assess the criticality of expected train delays while planning. We extend the visualization accordingly and show various design ideas to incorporate the additional information. A user study demonstrates the suitability of our final design.", "uri": "https://vimeo.com/230838271", "name": "[VIS17 Preview] Design for Transfer Time Criticality in Plans of Uncertain Train Trips (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:09:48+00:00", "description": "Poster \n\nAuthors: Olga Kazakova, Po-shen Lee, Jevin West, Bill Howe\n\nAbstract: Scientific communication depends on visual representations of data, results, analysis, and models. Given the important role of these information objects, we have built a figure-centric search engine called VizioMetrics.org. We have used millions of figures from PubMed Central to better understand effective visual communication and scholarly impact. In this poster we present preliminary results for automatically identifying key figures in scholarly papers. We conducted a large-scale survey asking authors to identify their central figures \u2013 the single visualization that encapsulates key aspects of a paper. If participants were able to identify such figures, they were asked to indicate what the selected figures represent. Our results show that for over 90% of evaluated papers the authors were able to identify a single central figure. In most cases such figures represent results and the most common figure class is the composite, followed by the diagram. We then use this training set to test early-stage algorithms for identifying these graphical abstracts.", "uri": "https://vimeo.com/230838261", "name": "[VIS17 Preview] Viziometrics: Identifying Central Figures in Scientific Papers (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:09:36+00:00", "description": "Poster \n\nAuthors: Raghavendra Sridharamurthy, Adhitya Kamakshidasan, Vijay Natarajan\n\nAbstract: A merge tree captures the topology of sub-level and super-level sets in a scalar field. Estimating the similarity or dissimilarity between merge trees is an important problem with applications to visualization of time-varying and multi-field data. We present a tree edit distance based approach with a general subtree gap model to compare merge trees. The cost model is based on topological persistence. Experimental results on time-varying data show the utility of the \\ method towards a feature-driven analysis of scalar fields.", "uri": "https://vimeo.com/230838228", "name": "[VIS17 Preview] Edit Distances for Comparing Merge Trees (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:09:28+00:00", "description": "Poster \n\nAuthors: Aran Lunzer\n\nAbstract: A dynamic simulation helps its users to explore the behaviour of a modelled domain. Exploration is further enhanced if the users can also see and interact with a history of the simulation results. We present a history interface based on what we call timepoint thumbnails: temporally spaced dynamic views that capture the salient features of a simulation's moment-to-moment state. As an example, we show how this interface can assist in exploring emergent phenomena in simulated vehicle traffic.", "uri": "https://vimeo.com/230838214", "name": "[VIS17 Preview] Exploring Simulation Scenarios with Timepoint Thumbnails (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:09:17+00:00", "description": "Poster \n\nAuthors: P. Samuel Quinan, Lace Padilla, Sarah Creem-Regehr, Miriah Meyer\n\nAbstract: The visualization literature tells us that rainbow color maps are bad, yet domain experts continue to use them. Why? The truth is, we don't know. It turns out that there is a lot we don't know about rainbow color maps. Two of the primary reasons our community argues that rainbow color maps are ineffective can be traced back to the idea that rainbow color maps implicitly discretize the encoded data into hue-based bands; yet there is no research addressing what this discretization looks like or how consistent it is across individuals. This poster discusses an exploratory study designed to test how individuals' perceptual systems discretize widely used spectral schemes and whether this discretization can be modeled by variations in lightness and chroma. We present high-level discussions of the experimental design, our analysis, and the implications of our results.", "uri": "https://vimeo.com/230838204", "name": "[VIS17 Preview] Hue Bands and Human Perception: Revisiting the Rainbow (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:09:07+00:00", "description": "Poster \n\nAuthors: Mariana Shimabukuro, Christopher Collins\n\nAbstract: Long text labels is a known challenge in information visualizations. There are some techniques used in order to solve this problem like setting a very small font size. On the other hand, sometimes the font size is so small that the text can be difficult to read. Wrapping sentences, dropping letters and text truncation are some techniques do deal with this problem. In order to investigate a solution for labeling long words we ran a study on how people create and interpret word abbreviations. Based on the study data we designed a new algorithm to automatically make words as short as they need to fit the text. Examples applications of this algorithm are presented in this paper.", "uri": "https://vimeo.com/230838178", "name": "[VIS17 Preview] Abbreviating Text Labels on Demand (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:09:00+00:00", "description": "Poster \n\nAuthors: Shuai He, Daniel Kerrigan, Ronald Metoyer\n\nAbstract: Food choice is a more confusing task than ever before given the amount of information accessible to the everyday consumer. While the food label includes nutrient and ingredient information, this is only a limited subset of the total information available to consumers. In this project, we explore the design of an interactive visual representation of the total nutrient and ingredient information to support food decision making. Our design integrates (1) representation of nutrients in the form of Daily Intake Percentages, (2) representation of ingredients and sentiment surrounding those ingredients, and (3) small multiples to support comparison.", "uri": "https://vimeo.com/230838162", "name": "[VIS17 Preview] Nutrition Bytes: Visualizing Food Content (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:08:46+00:00", "description": "Poster \n\nAuthors: Tomoaki Tatsukawa, Akira Oyama, Takehisa Kohira, Hiromasa Kemmotsu, Hideo Miyachi\n\nAbstract: This work presents an interactive ScatterPlot Matrix visualization tool (iSPM) developed for interactive visualization and analysis of multidimensional engineering data sets.iSPM is enabled to drawing the scatterplot matrix at high speed using hardware acceleration, sorting variables by quantitative indices, performing linear regression, clustering, and calculating Pareto-ranking for selecting good design candidates.Interactive and scalable visual analysis of multidimensional data becomes possible by using iSPM. The interactivity of analysis is useful for shortening the design process and getting noticed.In this study, we explain the main features of iSPM and show application examples of actual engineering design problems.", "uri": "https://vimeo.com/230838145", "name": "[VIS17 Preview] iSPM - An Interactive Scatterplot Matrix for Visualizing Multidimonsional Engineering Data (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:08:36+00:00", "description": "Poster \n\nAuthors: Aspen Hopkins, Pascal Goffin, Miriah Meyer\n\nAbstract: We present the results of a qualitative study aimed at improving current understanding of the needs and desires of Salt Lake City (SLC) residents in regards to air quality. Currently, government websites provide only coarse data. With the opportunity to get more fine grained data it is important to provide a system that supports the interests of the general public. In addition, we also provide a design mockup of how such a visualization system could look like based on the results of our study.", "uri": "https://vimeo.com/230838122", "name": "[VIS17 Preview] Particulates Matter: Assessing Needs for Air Quality Visualization (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:08:22+00:00", "description": "Poster \n\nAuthors: Florian Heimerl, Michael Gleicher\n\nAbstract: The use of word vector embeddings as the basis for many upstream tasks in text processing has lead to large improvements in accuracy. However, the exact reasons for this success largely remain unclear, as the properties and relations that these embeddings encode are often not well understood. Our goal in this ongoing project is to design effective interactive visualizations that help practitioners and researchers understand and compare such spaces better. The initial steps we have taken is to review relevant literature to identify properties and relations of word vectors that are important for various applications. From these, we derive basic tasks to inform the design of adequate and effective interactive visualizations that help users gain deeper insights into the structure of vector spaces. In addition, we present three initial designs to support these tasks.", "uri": "https://vimeo.com/230838088", "name": "[VIS17 Preview] Visual Exploration of Word Vector Embeddings (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:08:12+00:00", "description": "Poster \n\nAuthors: Bharat Kale, Michael Papka\n\nAbstract: Visualizing large real-world networks, such as social networks and scientific collaboration networks, is challenging not only because they contain large numbers of nodes and links but also due to their multivariate nature. Applications that analyze such datasets tend to focus on problems related to visualizing either multiple attributes on nodes or the topology of the network. Very few applications focus on both. This research explores a new approach to visualizing such multivariate networks using a sunburst chart to encode attributes on the nodes and a combination of a treemap layout and a suitable graph layout to control the topology. We show the results of this approach by creating a collaboration network using a dataset that comprises references to all research papers published by users of the Argonne Leadership Computing Facility in the last three years. The goal of this visualization is to show a holistic view of the scholarly work from a research facility, which in turn helps to identify research groups and the researchers acting as bridges among them.", "uri": "https://vimeo.com/230838076", "name": "[VIS17 Preview] Visualizing the Scholarly Output of a Research Facility (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:08:03+00:00", "description": "Poster \n\nAuthors: Kathrin Ballweg, Margit Pohl, Guenter Wallner, Tatiana von Landesberger\n\nAbstract: A precise and valid measurement of perceived similarity is essential for evaluating comparative visualizations. Our experiment on the perceived similarity of directed acyclic graphs showed that the common Likert scale measurement lacks precision and challenges the measurements' validity due to a mismatch of the Likert scale with the preferences and the mental model of humans. We therefore searched for an alternative measuring instrument. By broadening our search scope we found Tversky's model of perceived pairwise similarity. It provides a valid and well-known model of human similarity perception. So, we transferred it into a measuring instrument. While our proposal still needs to be validated. We think it is capable of solving the Likert scale's issues and, therefore, it will provide substantial benefit for comparative visualization research.", "uri": "https://vimeo.com/230838054", "name": "[VIS17 Preview] A Proposal for Measuring the Perceived Pairwise Similarity Inspired by Tversky's Similarity Model on the...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:07:54+00:00", "description": "Poster \n\nAuthors: Zhe Wang, Paul Rosen, Bei Wang, Carlos Scheidegger\n\nAbstract: Topological data analysis (TDA) algorithms have recently shown promise in making sense of high-dimensional datasets where other techniques fail. However, TDA algorithms are often computationally expensive, especially in an interactive setting, where users might be interested in exploring the topological behavior of different subsets of their dataset. We present work in progress towards TopoCubes, a technique to provide TDA based interactive visual exploration on large datasets. Leveraging the state-of-art of interactive visual exploration techniques and recent work in computational topology, TopoCubes supports interactively calculating persistence homology \u2013 a major tool of TDA \u2013 of user selected subsets of data. We outline how TopoCubes work, and where progress remains to be made.", "uri": "https://vimeo.com/230838031", "name": "[VIS17 Preview] TopoCubes: Interactive Exploration of Persistence Homology of Large Datasets (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:07:46+00:00", "description": "Poster \n\nAuthors: Rebecca Faust, Carlos Scheidegger\n\nAbstract: Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are popular with visualization researchers and experi- enced data analysts, but present serious problems of interpretation. In this paper, we present DimReader, a technique that recovers read- able axes from such techniques. DimReader is based on analyzing infinitesimal perturbations of the dataset with respect to variables of interest. The recovered axes are in direct analogy with positional legends of traditional scatterplots, and show how to solve the com- putational challenges presented by the generalization to non-linear methods. We show how automatic differentiation makes the calcula- tion of such perturbations efficient and can easily be integrated into programs written in modern programming languages.", "uri": "https://vimeo.com/230838014", "name": "[VIS17 Preview] DimReader: Using auto-differentiation to explain non-linear projections (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:07:37+00:00", "description": "Poster \n\nAuthors: Caitlyn McColeman, Mark Blair\n\nAbstract: Multivariate data visualization is as challenging as it is necessary: most real-world data problems involve complex, interrelated variables. This paper explores the efficacy of candlestick plots by recording participants\u2019 forecasting performance after viewing hundreds of such graphs. In addition to the standard candlestick chart, we test an adapted version of the chart, where the data are represented more intuitively (i.e. an up arrow means \u201cthe stock closed higher\u201d) than in the standard chart (i.e. a green box means \u201cthe stock closed higher\u201d). Performance on the adapted candlestick chart is better: participants were more accurate in their forecasting predictions when the data were represented more intuitively. This suggests that our innovation in multivariate financial data visualization helps people make better decisions.", "uri": "https://vimeo.com/230837994", "name": "[VIS17 Preview] Improving financial decision making by updating multivariate data representation in candlestick charts...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:07:28+00:00", "description": "Poster \n\nAuthors: Hannah Kim, Jaegul Choo, Alex Endert, Haesun Park\n\nAbstract: We present a visual analytics system for large-scale document retrieval tasks with high recall where any missing relevant documents can be critical. Our system utilizes a novel user-driven topic modeling called targeted topic modeling, a variant of nonnegative matrix factorization (NMF). Our system visualizes a topic summary in a treemap form and lets users keep relevant topics and incrementally remove uninteresting topics in our treemap view without losing potentially relevant documents.", "uri": "https://vimeo.com/230837976", "name": "[VIS17 Preview] Interactive Visual Retrieval of Large-Scale Documents via Targeted Topic Modeling (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:07:18+00:00", "description": "Poster \n\nAuthors: Charles Perin\n\nAbstract: I present an autobiographical visualization of the past 18 months of my life that shows work and personal activities, steps, floors, emails, SVN commits, and contextual annotations. The spiral layout makes both cyclical and symmetrical patterns emerge. The resulting autobiographical visualization makes it possible to both reminisce and share personal stories.", "uri": "https://vimeo.com/230837961", "name": "[VIS17 Preview] The Symmetry of My Life: An Autobiographical Visualization (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:07:08+00:00", "description": "Poster \n\nAuthors: Qiyu Zhi, Shuai He, Ronald Metoyer\n\nAbstract: Current audio player interfaces generally provide brief information such as title and duration time and support basic control functions including play, pause, rewind, and forward. These features alone are not sufficient for certain user tasks, such as quickly finding a previously-visited location or browsing the main topics covered before listening. We present VisPod, a novel visual audio player interface to visually display main topics and keywords extracted from the uploaded transcript. VisPod supports overviewing the audio content in advance, navigating through topics, and browsing the content of each topic.", "uri": "https://vimeo.com/230837940", "name": "[VIS17 Preview] VisPod: A Visual Audio Player for Content Exploration (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:06:54+00:00", "description": "Poster \n\nAuthors: Alex Bigelow\n\nAbstract: While it is true that fellow tool builders can threaten the success of a design study, there are also advantages to working with individuals that have strong software development skills. \\ We present a review of two information visualization projects using thematic analysis, that provides additional insight and guidance for collaborating with people that have strong software development skills. \\ Additionally, we discuss minor implications for the design of node-link diagrams and adjacency matrices, and report observed differences in engagement with respect to touch tables versus traditional keyboard, mouse, and monitor configurations.", "uri": "https://vimeo.com/230837907", "name": "[VIS17 Preview] Reflections on Working With Fellow Tool Builders (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:06:44+00:00", "description": "Poster \n\nAuthors: Matthew Brehmer, Kyungwon Lee, Ivan Viola, Jinwook Seo, Bongshin Lee\n\nAbstract: Data-driven storytelling is an increasingly popular topic in the visualization research community and a maturing art form in the visualization practitioner community. We describe an open-ended contest dedicated to visual data storytelling, one that took place at PacificVis 2017. We received 15 submissions that varied in terms of topic, visual representation, narrative structure, and presentation medium. A panel of judges recruited from the visualization practitioner community reviewed these submissions, and eight submissions were presented at PacificVis. We reflect on the contest and submissions with the aim of furthering the conversation in the academic community about storytelling with visualization.", "uri": "https://vimeo.com/230837883", "name": "[VIS17 Preview] Demonstrating the Value of Visualization: Highlights from the 2017 PacificVis Visual Data Storytelling...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:06:31+00:00", "description": "Poster \n\nAuthors: Samuel Meyer, Yiyi Chen, Marti Hearst\n\nAbstract: Well-designed visualizations have an important role to play to aid in the public's understanding of algorithms. This work presents a set of design principles for using visualization to explain machine learning algorithms specifically, and demonstrates these principles applied to the operations of the random forest algorithm.", "uri": "https://vimeo.com/230837852", "name": "[VIS17 Preview] Visualizing A Walk Through the Random Forest (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:06:18+00:00", "description": "Poster \n\nAuthors: Tom Horak, Ulrike Kister, Raimund Dachselt\n\nAbstract: In this work, we focus on improving data exploration for the specific multivariate graph application case of value driver trees (VDTs). \\ Based on value drivers and an underlying model, VDTs are used to assess business's performance of companies. \\ Taking into account the specific challenges of VDTs, we present an improved node representation using embedded visualizations as well as interaction concepts for local semantic zooming and simulations or predictions within these trees.", "uri": "https://vimeo.com/230837824", "name": "[VIS17 Preview] Improving Value Driver Trees to Enhance Business Data Analysis (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:06:10+00:00", "description": "Poster \n\nAuthors: Fahim Hasan Khan, Allan Rocha, Usman Alim\n\nAbstract: Modern phase-contrast magnetic resonance imaging (PC-MRI) can acquire both cardiac anatomy and flow function in a single acquisition and deliver high quality volumetric and time-varying (4D) datasets which enable better diagnosis and risk assessment of various cardiovascular diseases. A good way to visualize blood flow from 4D PC-MRI datasets is to use animated pathlines through the anatomical context for representing the trajectories of the blood particles. Artifact correction is one crucial step in the processing pipeline of 4D PC-MRI datasets for representing the cardiac flow using pathlines, which in turn can reduce the overall quality of the useful information in the dataset. In this work, an approach is presented for comparative visualization of 4D PC-MRI datasets before and after artifact correction for qualitative analysis.", "uri": "https://vimeo.com/230837806", "name": "[VIS17 Preview] Comparative Visualizations of Noisy and Filtered Blood Flow from 4D PC-MRI Cardiac Datasets (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:06:00+00:00", "description": "Poster \n\nAuthors: Taylor Mutch, Nikolas Stevenson-Molnar, Judith Bayard Cushing, Chad Zanocco, Mike Bailey, Genevieve Orr, Peter Drake, Denise Lach\n\nAbstract: It is widely accepted that environmental scientists need tools to visualize temporally and spatially complex landscapes on 3D topographies. The interdisciplinary VISTAS (Visualization of Terrestrial and Aquatic Systems) project team (computer, environmental, and social scientists) develops software to rapidly and easily create specialized 3D environmental visualizations without requiring expertise in complex graphics software. VISTAS is currently used to visualize multivariate time series data overlaid on 3D terrain. Current research and development is extending VISTAS to include machine learning and additional analytics to enable scientists to interactively determine not only what is occurring, but why, thus moving from visualization to visual analytics, from qualitative to quantitative analysis.", "uri": "https://vimeo.com/230837784", "name": "[VIS17 Preview] From Visualization to Visual Analytics for Environmental Science (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:05:45+00:00", "description": "Poster \n\nAuthors: Maximillian Chen, Kristin Divis, Laura McNamara, Dan Morrow\n\nAbstract: Longitudinal, multivariate datasets are intrinsic to the study of dynamic, naturalistic behavior. Statistical models provide the ability to identify event patterns in these data under conditions of uncertainty. To make use of statistical models, however, researchers must be able to evaluate how well a model uses available information in a dataset for clustering decisions and for uncertainty estimation. The Gaussian mixture model (GMM) is a prominently used model for clustering multivariate data. However, it has only been recently extended to longitudinal data, and useful visualization tools have yet to be developed in this context. In this paper, we develop novel methods for visualizing the clustering performance and uncertainty of fitting a GMM to multivariate longitudinal data. We demonstrate our methods on eyetracking data and explain the usefulness of uncertainty quantification and visualization with evaluating the performance of clustering models.", "uri": "https://vimeo.com/230837753", "name": "[VIS17 Preview] Visualizing Clustering and Uncertainty Analysis with Multivariate Longitudinal Data (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:05:30+00:00", "description": "Poster \n\nAuthors: Holger Stitz, Samuel Gratzl, Harald Piringer, Marc Streit\n\nAbstract: Storing interaction provenance generates a knowledge base with a large potential for recalling previous results and guiding the user in future analyses. However, search and retrieval of analysis states can become tedious without extensive creation of meta-information by the user. In this work we present an approach for an efficient retrieval of analysis states which are structured as provenance graphs of automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of the requested analysis state. Depending on the use case, this definition may be provided explicitly by the user or inferred from a reference state. We explain the definition by means of a Gapminder-inspired prototype and discuss our implementation for an effective retrieval of previous states.", "uri": "https://vimeo.com/230837725", "name": "[VIS17 Preview] Provenance-Based Visualization Retrieval (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:05:22+00:00", "description": "Poster \n\nAuthors: Katar\u00edna Furmanov\u00e1, Miroslava Jare\u0161ov\u00e1, Bikram Kawan, Holger Stitz, Martin Ennemoser, Samuel Gratzl, Alexander Lex, Marc Streit\n\nAbstract: Tabular data visualizations are easy to understand and powerful at communicating patterns in datasets, especially when paired with interactions such as sorting. In this work we present Taggle, a novel visualization technique for large and complex tables. We consider datasets that are composed of columns of categorical, or numerical data, in addition to homogeneous matrices. The key contribution of Taggle is its ability to aggregate data subsets (rows and columns) on demand based on user-defined grouping rules. Different visual representations for individual cells and aggregated subsets are available. The aggregation strategy is complemented by the ability to sort hierarchically, and by rich data selection and filtering capabilities. We demonstrate the usefulness of Taggle using an AIDS dataset for 160 countries.", "uri": "https://vimeo.com/230837711", "name": "[VIS17 Preview] Taggle: Scaling Table Visualization through Aggregation (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:05:13+00:00", "description": "Poster \n\nAuthors: Ayan Das, Arjun Srinivasan, John Stasko\n\nAbstract: We present CricVis, a web-based system that visualizes cricket matches ball-by-ball. With six coordinated views and brushing-and-linking capabilities, CricVis allows users to gain both, quick overviews and detailed insights about match events, team performances, and individual player patterns in cricket matches.", "uri": "https://vimeo.com/230837689", "name": "[VIS17 Preview] CricVis: Interactive Visual Exploration and Analysis of Cricket Matches (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:05:04+00:00", "description": "Poster \n\nAuthors: He Huang, Yun Wang, Haidong Zhang, Qiufeng YIN, Zhitao Hou, Dongmei Zhang\n\nAbstract: Information Graphics, also known as infographics, have been widely used to effectively convey messages and present insights in data. However, creating expressive data-driven infographics remains a challenge for novice users. In this paper, we present InfoNice, a visualization design tool that facilitates users to easily create infographics. Specifically, InfoNice allows users to customize visual marks by composing and formatting visual elements, such as text, icons, and images. By integrating InfoNice into Microsoft Power BI, we enable users to convert normal-style charts into infographics in a seamless way. Early user feedback demonstrates that users like InfoNice and they are able to create a variety of infographics for common usage scenarios.", "uri": "https://vimeo.com/230837672", "name": "[VIS17 Preview] InfoNice: Easy Customization of Information Graphics (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:04:51+00:00", "description": "Poster \n\nAuthors: Bhavya Ghai, Alok Mishra, Klaus Mueller\n\nAbstract: Dimensionality reduction techniques play a key role in data processing and data visualization. Common dimensionality reduction techniques such as MDS, PCA, etc. do a decent job in projecting high-dimensional data into lower dimensions. But in the case of network data, they simply ignore the relationship between nodes which might result in non-planar graphs with many intersecting edges. In this paper, we have tried to model dimensionality reduction for network data as a multi-objective optimization problem. We have tried to draw graph/network in lower dimensions such that planarity is maximized and stress function is minimized simultaneously. We have used two genetic algorithms namely, NSGA-II and NSGA-III. For them, both objectives are equally important and they optimize them together. These techniques return a set of non-dominated solutions represented by pareto-Optimal front. We observed that genetic algorithms outperformed MDS for some cases. In other cases, genetic algorithms gave solutions with significantly lower number of intersections for slight increase in stress value.", "uri": "https://vimeo.com/230837632", "name": "[VIS17 Preview] Visualization of Multivariate Data with Network Constraints using Multi-Objective Optimization (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:04:43+00:00", "description": "Poster \n\nAuthors: Hrim Mehta, Amira Chalbi, Fanny Chevalier, Christopher Collins\n\nAbstract: Visual storytelling is commonly employed to communicate results of data analyses. Alternatively, automated [1,6] and semi-automated [2] data narratives or \u201ctours\u201d have been proposed as a means to support exploration of massive multidimensional datasets, substituting the more prevalent static overviews to prompt exploration. While these works demonstrate specific instances of data tours, a concrete model to describe the building blocks of such tours is lacking. We present a hierarchical framework, DataTours, to formalize and guide the design of automated and semi-automated tours for data exploration. We also discuss challenges evoked by the framework in the semi-automated and automated authoring of the data tours.", "uri": "https://vimeo.com/230837617", "name": "[VIS17 Preview] DataTours: A Data Narratives Framework (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:04:34+00:00", "description": "Poster \n\nAuthors: Niklas R\u00f6nnberg\n\nAbstract: This poster presents an interactive sonification experiment, designed to evaluate possible benefits of sonification in information visualization. The aim of the present study was to explore the use of composed and deliberately designed musical sounds to enhance perception of color intensity in visual representations. It was hypothesized, that by using musical sounds for sonification perception of color intensity would be improved. In this evaluation, sonification was mapped to color intensity in visual representations, and the participants had to identify and mark the highest color intensity, as well as answer a questionnaire about their experience. Both quantitative and qualitative preliminary results suggest a benefit of sonification, and indicate that sonification is useful in data exploration.", "uri": "https://vimeo.com/230837600", "name": "[VIS17 Preview] Sonification Enhances Perception of Color Intensity (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:04:26+00:00", "description": "Poster \n\nAuthors: Ching-Yao Lin, Kuen-Long Tsai, Hsiu-Ming Chang, Ann-Shyn Chiang\n\nAbstract: We have created a platform to allow researchers to analyze neuron connectivity relationships and visualize results, not only in 3D anatomic structure, but also the 2D graph representation for tens of thousands neurons. It helps neurologists to study the function of neurons in the fruit-fly brains and provides a guide map for possible neural circuitry modification.", "uri": "https://vimeo.com/230837579", "name": "[VIS17 Preview] Combining 2D Graph &amp; 3D Visualization for Neuron Connectivity Analysis (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:04:17+00:00", "description": "Poster \n\nAuthors: James Abello, Fred Hohman, Duen Horng Chau\n\nAbstract: We use an iterative edge decomposition approach, derived from the popular iterative vertex peeling strategy, to globally split each vertex egonet (subgraph induced by a vertex and its neighbors) into a collection of edge-disjoint layers. Each layer is an edge maximal induced subgraph of minimum degree k that determines the layer density. This edge decomposition is derived completely from the overall network topology, and since each vertex can appear in multiple layers, we can associate to each vertex a vector profile that can be used to identify its different \"roles\" across the network. This allows us to explore a network's topology at different levels of granularity, e.g., per layer and across layers. This is only feasible by mapping simultaneously a vertex to a set of 3D coordinates (x, y, and z) where the third coordinate encodes the different layers a vertex belongs to. This is one of the few instances where 3D visualization enhances graph exploration and navigation in an arguably \"natural\" way: a graph now becomes a 3D graph playground where a vertex plays a certain role per layer that is determined by the overall network topology. Our approach helps disentangle hairball looking embeddings produced by conventional 2D graph drawings.", "uri": "https://vimeo.com/230837562", "name": "[VIS17 Preview] 3D Exploration of Graph Layers via Vertex Cloning (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:04:06+00:00", "description": "Poster \n\nAuthors: Sakshi Sanjay Pratap, Alex Endert\n\nAbstract: With the increasing collection of time series data, both related to business and personal use, a substantial amount of research and development efforts are being directed to gain deeper insights from such records. Data mining techniques like similarity search and segmentation are used as tools to enhance the comprehension of this data. While innovative techniques have been examined, there is lesser work done in creating functional tools that use these methods and support seamless visual analysis. We have created PredVis, a mixed-initiative system that uses novel and existing visualization, statistical and machine learning techniques to facilitate enhanced analysis of time series data for forecasting. The system provides tools to interact with model calculated results, visually query variances and integrate other dimensions of the data to improve user comprehension and decision-making. The methods developed in the system are generic and can be employed for analysis in domains including finance, health, marketing and astronomy.", "uri": "https://vimeo.com/230837537", "name": "[VIS17 Preview] PredVis \u2013 Interaction Techniques for Time Series Prediction (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:03:57+00:00", "description": "Poster \n\nAuthors: Fan Zhao, TongHai Jiang, Li Cheng\n\nAbstract: Along with the rapid development of Internet of Things (IoT) technologies, the scale of data collected from data acquisition devices is getting larger. Raw sensing data may contain inaccurate records caused by device failure, software/hardware malfunction, or incorrect operations.It is difficult to detect this type of errors through traditional rule-based data cleaning methods. In this research, we design an interactive visualization system to help human operators to identify and confirm abnormal patterns in data acquisition. With different view models and interaction mechanisms, analysts can explore group and individual data features. Through visualization and examination, analysts can effectively identify possible content-invalid data patterns. We deploy the visualization system in a real-life data acquisition platform. Field data and case study results indicate that our system is able to detect a number of interesting content-invalid patterns.", "uri": "https://vimeo.com/230837519", "name": "[VIS17 Preview] Visualization System for Anomaly Detection for Data Acquisition Systems (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:03:47+00:00", "description": "Poster \n\nAuthors: Ashley Suh, Mustafa Hajij, Bei Wang, Carlos Scheidegger, Paul Rosen\n\nAbstract: Graphs are a common data type for encoding relationships, yet, their abstractness makes them incredibly difficult to analyze. Node-link diagrams are a popular method for drawing graphs, leading to a large body of work in node layouts. Among the most commonly used techniques are layout methods that rely on derived information to position points, which often lack interactive exploration functionalities, and force-directed layouts, which ignore global structure. This project addresses the graph drawing challenge by leveraging Topological Data Analysis as derived information for interactive graph drawing. Our approach uses persistent homology to capture topological features, which allow interactively studying the substructures within a force-directed graph layout. In this work, we briefly describe how to map a graph to a persistent homology problem, discuss how the topological information can be used in a force-directed layout, and examine one dataset.", "uri": "https://vimeo.com/230837496", "name": "[VIS17 Preview] Driving Interactive Graph Exploration Using 0-Dimensional Persistent Homology Features (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:03:38+00:00", "description": "Poster \n\nAuthors: Ya-Hsin Hung, Paul Parsons\n\nAbstract: Engagement is an important aspect of user experience, yet its evaluation has not received much attention in the InfoVis literature. For visualization design and evaluation, it is useful to understand both the characteristics of user engagement and how engagement develops and changes over time. We describe a study that used a mixed-methods approach to evaluate user engagement with two different types of visualizations. Multiple qualitative and quantitative methods \u2013 including think aloud, eye-tracking, questionnaires, and interviews \u2013 were employed. We report initial results involving verbal protocols and eye-tracking data. We reflect on the findings and discuss plans for further data analysis and future research.", "uri": "https://vimeo.com/230837469", "name": "[VIS17 Preview] Evaluating User Engagement in Information Visualization Using Mixed Methods (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T21:03:19+00:00", "description": "Poster \n\nAuthors: Do\u011facan Bilgili, Selim Balcisoy\n\nAbstract: Physical visualizations are promising with certain benefits, that have been shown by several studies, over on-screen visualizations. Yet generating them requires significant manual process and they are cumbersome for non-experts. In this regard, need of physical models shaped around well-defined design rules are a prerequisite. Moreover, digital construction of the designed solid models for manufacturing is the next step to be achieved. However, even for a small set of data, constructing several models becomes a discouraging and highly time consuming task. This problem is addressed in this paper by the introduction of PhysVis, an authoring tool offering two new physical visualization models, named 'Data Tower' and 'Data Circles', which were designed based on specific rules. In order for an \"Overview first, detail on demand\" approach, an augmented reality application was also developed to work with physical models so as to introduce more detailed information such as exact values of data points along with various graphics, if desired. PhysVis is an open-source project and available online (physvis.github.io/physvis/project).", "uri": "https://vimeo.com/230837434", "name": "[VIS17 Preview] PhysVis: A Data Physicalization Pipeline Enhanced With Augmented Reality (Poster)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:52:04+00:00", "description": "Authors: Peter Mindek, David Kouril, Johannes Sorger, Daniel Toloudis, Blair Lyons, Graham Johnson, M. Eduard Gr\u221a\u2202ller, Ivan Viola\n\nAbstract: We propose a system to facilitate biology communication by developing a pipeline to support the instructional visualization of heterogeneous biological data on heterogeneous user-devices. Discoveries and concepts in biology are typically summarized with illustrations assembled manually from the interpretation and application of heterogenous data. The creation of such illustrations is time consuming, which makes it incompatible with frequent updates to the measured data as new discoveries are made. Illustrations are typically non-interactive, and when an illustration is updated, it still has to reach the user. Our system is designed to overcome these three obstacles. It supports the integration of heterogeneous datasets, reflecting the knowledge that is gained from different data sources in biology. After pre-processing the datasets, the system transforms them into visual representations as inspired by scientific illustrations. As opposed to traditional scientific illustration these representations are generated in real-time - they are interactive. The code generating the visualizations can be embedded in various software environments. To demonstrate this, we implemented both a desktop application and a remote-rendering server in which the pipeline is embedded. The remote-rendering server supports multi-threaded rendering and it is able to handle multiple users simultaneously. This scalability to different hardware environments, including multi-GPU setups, makes our system useful for efficient public dissemination of biological discoveries.", "uri": "https://vimeo.com/230835972", "name": "[VIS17 Preview] Visualization Multipipeline for Communicating Biology (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:51:51+00:00", "description": "Authors: Oliver Ruebel, Benjamin Bowen\n\nAbstract: Mass spectrometry imaging (MSI) is a transformative imaging method that supports the untargeted, quantitative measurement of the chemical composition and spatial heterogeneity of complex samples with broad applications in life sciences, bioenergy, and health. While MSI data can be routinely collected, its broad application is currently limited by the lack of easily accessible analysis methods that can process data of the size, volume, diversity, and complexity generated by MSI experiments. The development and application of cutting-edge analytical methods is a core driver in MSI research for new scientific discoveries, medical diagnostics, and commercial-innovation. However, the lack of means to share, apply, and reproduce analyses hinders the broad application, validation, and use of novel MSI analysis methods. To address this central challenge, we introduce the Berkeley Analysis and Storage Toolkit (BASTet), a novel framework for shareable and reproducible data analysis that supports standardized data and analysis interfaces, integrated data storage, data provenance, workflow management, and a broad set of integrated tools. Based on BASTet, we describe the extension of the OpenMSI mass spectrometry imaging science gateway to enable web-based sharing, reuse, analysis, and visualization of data analyses and derived data products. We demonstrate the application of BASTet and OpenMSI in practice to identify and compare characteristic substructures in the mouse brain based on their chemical composition measured via MSI.", "uri": "https://vimeo.com/230835941", "name": "[VIS17 Preview] BASTet: Shareable and reproducible analysis and visualization of mass spectrometry imaging data via OpenMSI...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:50:59+00:00", "description": "Authors: Liz Marai\n\nAbstract: Although visualization design models exist in the literature in the form of higher-level methodological frameworks, these models do not present a clear methodological prescription for the domain characterization step. This work presents a framework and end-to-end model for requirements engineering in problem-driven visualization application design. The framework and model are based on the activity-centered design paradigm, which is an enhancement of human-centered design. The proposed activity-centered approach focuses on user tasks and activities, and allows an explicit link between the requirements engineering process with the abstraction stage\u2014and its evaluation\u2014of existing, higher-level visualization design models. In a departure from existing visualization design models, the resulting model: assigns value to a visualization based on user activities; ranks user tasks before the user data; partitions requirements in activity-related capabilities and nonfunctional characteristics and constraints; and explicitly incorporates the user workflows into the requirements process. A further merit of this model is its explicit integration of functional specifications, a concept this work adapts from the software engineering literature, into the visualization design nested model. A quantitative evaluation using two sets of interdisciplinary projects supports the merits of the activity-centered model. The result is a practical roadmap to the domain characterization step of visualization design for problem-driven data visualization. Following this domain characterization model can help remove a number of pitfalls that have been identified multiple times in the visualization design literature.", "uri": "https://vimeo.com/230835822", "name": "[VIS17 Preview] Activity-Centered Domain Characterization for Problem-Driven Scientific Visualization (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:50:52+00:00", "description": "Authors: Jens Magnus, Stefan Bruckner\n\nAbstract: In recent years, significant progress has been made in developing high-quality interactive methods for realistic volume illumination. However, refraction -- despite being an important aspect of light propagation in participating media -- has so far only received little attention. In this paper, we present a novel approach for refractive volume illumination including caustics capable of interactive frame rates. By interleaving light and viewing ray propagation, our technique avoids memory-intensive storage of illumination information and does not require any precomputation. It is fully dynamic and all parameters such as light position and transfer function can be modified interactively without a performance penalty.", "uri": "https://vimeo.com/230835804", "name": "[VIS17 Preview] Interactive Dynamic Volume Illumination with Refraction and Caustics (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:50:39+00:00", "description": "Authors: Bastian Rieck, Ulderico Fugacci, Jonas Lukasczyk, Heike Leitte\n\nAbstract: Complex networks require effective tools and visualizations for their analysis and comparison. Clique communities have been recognized as a powerful concept for describing cohesive structures in networks. We propose an approach that extends the computation of clique communities by considering persistent homology, a topological paradigm originally introduced to characterize and compare the global structure of shapes. Our persistence-based algorithm is able to detect clique communities and to keep track of their evolution according to different edge weight thresholds. We use this information to define comparison metrics and a new centrality measure, both reflecting the relevance of the clique communities inherent to the network. Moreover, we propose an interactive visualization tool based on nested graphs that is capable of compactly representing the evolving relationships between communities for different thresholds and clique degrees. We demonstrate the effectiveness of our approach on various network types.", "uri": "https://vimeo.com/230835775", "name": "[VIS17 Preview] Clique Community Persistence: A Topological Visual Analysis Approach for Complex Networks (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:50:30+00:00", "description": "Authors: Julian Kreiser, Alexander Hann, Eugen Zizer, Timo Ropinski\n\nAbstract: High-resolution manometry is an imaging modality which enables the categorization of esophageal motility disorders. Spatio-temporal pressure data along the esophagus is acquired using a tubular device and multiple test swallows are performed by the patient. Current approaches visualize these swallows as individual instances, despite the fact that aggregated metrics are relevant in the diagnostic process. Based on the current Chicago Classification, which serves as the gold standard in this area, we introduce a visualization supporting an efficient and correct diagnosis. To reach this goal, we propose a novel decision graph representing the Chicago Classification with workflow optimization in mind. Based on this graph, we are further able to prioritize the different metrics used during diagnosis and can exploit this prioritization in the actual data visualization. Thus, different disorders and their related parameters are directly represented and intuitively influence the appearance of our visualization. Within this paper, we introduce our novel visualization, justify the design decisions, and provide the results of a user study we performed with medical students as well as a domain expert. On top of the presented visualization, we further discuss how to derive a visual signature for individual patients that allows us for the first time to perform an intuitive comparison between subjects, in the form of small multiples.", "uri": "https://vimeo.com/230835755", "name": "[VIS17 Preview] Decision Graph Embedding for High-Resolution Manometry Diagnosis (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:50:20+00:00", "description": "Authors: Haneen Mohammed, Ali Al-Awami, Johanna Beyer, Corrado Cali, Pierre Magistretti, Hanspeter Pfister, Markus Hadwiger\n\nAbstract: This paper presents Abstractocyte, a system for the visual analysis of astrocytes and their relation to neurons, in nanoscale volumes of brain tissue. Astrocytes are glial cells, i.e., non-neuronal cells that support neurons and the nervous system. The study of astrocytes has immense potential for understanding brain function. However, their complex and widely-branching structure requires high-resolution electron microscopy imaging and makes visualization and analysis challenging. Furthermore, the structure and function of astrocytes is very different from neurons, and therefore requires the development of new visualization and analysis tools. With Abstractocyte, biologists can explore the morphology of astrocytes using various visual abstraction levels, while simultaneously analyzing neighboring neurons and their connectivity. We define a novel, conceptual 2D abstraction space for jointly visualizing astrocytes and neurons. Neuroscientists can choose a specific joint visualization as a point in this space. Interactively moving this point allows them to smoothly transition between different abstraction levels in an intuitive manner. In contrast to simply switching between different visualizations, this preserves the visual context and correlations throughout the transition. Users can smoothly navigate from concrete, highly-detailed 3D views to simplified and abstracted 2D views. In addition to investigating astrocytes, neurons, and their relationships, we enable the interactive analysis of the distribution of glycogen, which is of high importance to neuroscientists. We describe the design of Abstractocyte, and present three case studies in which neuroscientists have successfully used our system to assess astrocytic coverage of synapses, glycogen distribution in relation to synapses, and astrocytic-mitochondria coverage.", "uri": "https://vimeo.com/230835732", "name": "[VIS17 Preview] Abstractocyte: A Visual Tool for Exploring Nanoscale Astroglial Cells (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:50:09+00:00", "description": "Authors: Tobias Klein, Ludovic Autin, Barbora Kozlikova, David Goodsell, Arthur Olson, Eduard Groeller, Ivan Viola\n\nAbstract: We present the first approach to integrative structural modeling of the biological mesoscale within an interactive visual environment. These complex models can comprise up to millions of molecules with defined atomic structures, locations, and interactions. Their construction has previously been attempted only within a non-visual and non-interactive environment. Our solution unites the modeling and visualization aspect, enabling interactive construction of atomic resolution mesoscale models of large portions of a cell. We present a novel set of GPU algorithms that build the basis for the rapid construction of complex biological structures. These structures consist of multiple membrane-enclosed compartments including both soluble molecules and fibrous structures. The compartments are defined using volume voxelization of triangulated meshes. For membranes, we present an extension of the Wang Tile concept that populates the bilayer with individual lipids. Soluble molecules are populated within compartments distributed according to a Halton sequence. Fibrous structures, such as RNA or actin filaments, are created by self-avoiding random walks. Resulting overlaps of molecules are resolved by a forced-based system. Our approach opens new possibilities to the world of interactive construction of cellular compartments. We demonstrate its effectiveness by showcasing scenes of different scale and complexity that comprise blood plasma, mycoplasma, and HIV.", "uri": "https://vimeo.com/230835707", "name": "[VIS17 Preview] Instant Construction and Visualization of Crowded Biological Environments (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:49:50+00:00", "description": "Authors: Markus Hadwiger, Ali Al-Awami, Johanna Beyer, Marco Agus, Hanspeter Pfister\n\nAbstract: Recent advances in data acquisition produce volume data of very high resolution and large size, such as terabyte-sized microscopy volumes. These data often contain many fine and intricate structures, which pose huge challenges for volume rendering, and make it particularly important to efficiently skip empty space. This paper addresses two major challenges: (1) The complexity of large volumes containing fine structures often leads to highly fragmented space subdivisions that make empty regions hard to skip efficiently. (2) The classification of space into empty and non-empty regions changes frequently, because the user or the evaluation of an interactive query activate a different set of objects, which makes it unfeasible to pre-compute a well-adapted space subdivision. We describe the novel SparseLeap method for efficient empty space skipping in very large volumes, even around fine structures. The main performance characteristic of SparseLeap is that it moves the major cost of empty space skipping out of the ray-casting stage. We achieve this via a hybrid strategy that balances the computational load between determining empty ray segments in a rasterization (object-order) stage, and sampling non-empty volume data in the ray-casting (image-order) stage. Before ray-casting, we exploit the fast hardware rasterization of GPUs to create a ray segment list for each pixel, which identifies non-empty regions along the ray. The ray-casting stage then leaps over empty space without hierarchy traversal. Ray segment lists are created by rasterizing a set of fine-grained, view-independent bounding boxes. Frame coherence is exploited by re-using the same bounding boxes unless the set of active objects changes. We show that SparseLeap scales better to large, sparse data than standard octree empty space skipping.", "uri": "https://vimeo.com/230835661", "name": "[VIS17 Preview] SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:49:39+00:00", "description": "Authors: Qiaomu Shen, Wei Zeng, Yu Ye, Stefan Mueller Arisona, Simon Schubiger, Remo Burkhard, Huamin Qu\n\nAbstract: Urban forms at human-scale, i.e., urban environments that individuals can sense (e.g., sight, hearing, smell and touch) in daily lives, can provide unprecedented insights for a variety of applications, such as urban planning and environment auditing. Analysis of these urban forms can help planners develop high-quality urban space through evidence-based design. However, such analysis is complex, because of the involvement of spatial, multi-scale (i.e., city, region and street) and multivariate (e.g., greenery and sky ratios) natures of the urban forms. Current analysis methods are either lacking quantitative measurements or limited to a small area. The primary contribution of this work is the design of StreetVizor, an interactive visual analytics system that helps planners leverage their domain knowledge to explore human-scale urban forms based on Google street view images. Our system presents two-stage visual exploration: 1) an AOI Explorer to visually compare spatial distributions and quantitative measurements in two areas-of-interest (AOIs) at city- and region-scale; 2) and a Street Explorer with a novel parallel coordinates plot to explore fine-grained details of the urban forms at street-scale. We integrate visualization techniques with machine learning models to facilitate detection of street view patterns. We illustrate the applicability of our approach with case studies on real-world datasets in four cities (i.e., Hong Kong, Singapore, Greater London and New York City). Interviews with domain experts demonstrate the effectiveness of our system in facilitating various analytical tasks.", "uri": "https://vimeo.com/230835630", "name": "[VIS17 Preview] StreetVizor: Visual Exploration of Human-Scale Urban Forms based on Street Views (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:49:29+00:00", "description": "Authors: Subhashis Hazarika, Ayan Biswas, Han-Wei Shen\n\nAbstract: Distributions are often used to model uncertainty in many scientific datasets. To preserve the correlation among the spatially sampled grid locations in the dataset, various standard multivariate distribution models have been proposed in visualization literature. These models treat each grid location as a univariate random variable which models the uncertainty at that location. Standard multivariate distributions (both parametric and nonparametric) assume that all the univariate marginals are of the same type/family of distribution. But in reality, different grid locations show different statistical behavior which may not be modeled best by the same type of distribution. In this paper, we propose a new multivariate uncertainty modeling strategy to address the needs of uncertainty modeling in scientific datasets. Our proposed method is based on a statistically sound multivariate technique called Copula, which makes it possible to separate the process of estimating the univariate marginals and the process of modeling dependency, unlike the standard multivariate distributions. The modeling flexibility offered by our proposed method makes it possible to design distribution fields which can have different types of distribution (Gaussian, Histogram, KDE etc.) at the grid locations, while maintaining the correlation structure at the same time. Depending on the results of various standard statistical tests, we can choose an optimal distribution representation at each location, resulting in a more cost efficient modeling without significantly sacrificing on the analysis quality. To demonstrate the efficacy of our proposed modeling strategy, we extract and visualize uncertain features like isocontours and vortices in various real world datasets. We also study various modeling criterion to help users in the task of univariate model selection.", "uri": "https://vimeo.com/230835610", "name": "[VIS17 Preview] Uncertainty Visualization Using Copula-Based Analysis in Mixed Distribution Models (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:49:21+00:00", "description": "Authors: Haichao Miao, Elisa De Llano, Johannes Sorger, Yasaman Ahmadi, Tadija Kekic, Tobias Isenberg, Eduard Groeller, Ivan Barisic, Ivan Viola\n\nAbstract: We present an approach to represent DNA nanostructures in varying forms of semantic abstraction, describe ways to smoothly transition between them, and thus create a continuous multiscale visualization and interaction space for applications in DNA nanotechnology. This new way of observing, interacting with, and creating DNA nanostructures enables domain experts to approach their work in any of the semantic abstraction levels, supporting both low-level manipulations and high-level visualization and modifications. Our approach allows them to deal with the increasingly complex DNA objects that they are designing, to improve their features, and to add novel functions in a way that no existing single-scale approach offers today. For this purpose we collaborated with DNA nanotechnology experts to design a set of ten semantic scales. These scales take the DNA's chemical and structural behavior into account and depict it from atoms to the targeted architecture with increasing levels of abstraction. To create coherence between the discrete scales, we seamlessly transition between them in a well-defined manner. We use special encodings to allow experts to estimate the nanoscale object's stability. We also add scale-adaptive interactions that facilitate the intuitive modification of complex structures at multiple scales. We demonstrate the applicability of our approach on an experimental use case. Moreover, feedback from our collaborating domain experts confirmed an increased time efficiency and certainty for analysis and modification tasks on complex DNA structures. Our method thus offers exciting new opportunities with promising applications in medicine and biotechnology.", "uri": "https://vimeo.com/230835588", "name": "[VIS17 Preview] Multiscale Visualization and Scale-Adaptive Modification of DNA Nanostructures (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:49:08+00:00", "description": "Authors: Will Usher, Pavol Klacansky, Frederick Federer, Peer-Timo Bremer, Aaron Knoll, Alessandra Angelucci, Valerio Pascucci\n\nAbstract: Tracing neurons in large-scale microscopy data is crucial to establishing a wiring diagram of the brain, which is needed to understand how neural circuits in the brain process information and generate behavior. Automatic techniques often fail for large and complex datasets, and connectomics researchers may spend weeks or months manually tracing neurons using 2D image stacks. We present a design study of a new virtual reality (VR) system, developed in collaboration with trained neuroanatomists, to trace neurons in microscope scans of the visual cortex of primates. We hypothesize that using consumer-grade VR technology to interact with neurons directly in 3D will help neuroscientists better resolve complex cases and enable them to trace neurons faster and with less physical and mental strain. We discuss both the design process and technical challenges in developing an interactive system to navigate and manipulate terabyte-sized image volumes in VR. Using a number of different datasets, we demonstrate that, compared to widely used commercial software, consumer-grade VR presents a promising alternative for scientists.", "uri": "https://vimeo.com/230835553", "name": "[VIS17 Preview] A Virtual Reality Visualization Tool for Neuron Tracing (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:48:54+00:00", "description": "Authors: Tran Minh Quan, JunYoung Choi, HaeJin Jeong, Won-Ki Jeong\n\nAbstract: In this paper, we propose a novel machine learning-based voxel classification method for highly-accurate volume rendering. Unlike conventional voxel classification methods that incorporate intensity-based features, the proposed method employs dictionary based features learned directly from the input data using hierarchical multi-scale 3D convolutional sparse coding, a novel extension of the state-of-the-art learning-based sparse feature representation method. The proposed approach automatically generates highdimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built on a random forest classifier for accurately classifying voxels from only a handful of selection scribbles made directly on the input data by the user. We apply the probabilistic transfer function to further customize and refine the rendered result. The proposed method is more intuitive to use and more robust to noise in comparison with conventional intensity-based classification methods. We evaluate the proposed method using several synthetic and real-world volume datasets, and demonstrate the methods usability through a user study.", "uri": "https://vimeo.com/230835524", "name": "[VIS17 Preview] An Intelligent System Approach for Probabilistic Volume Rendering using Hierarchical 3D Convolutional Sparse...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:48:44+00:00", "description": "Authors: Jiang Zhang, Hanqi Guo, Fan Hong, Xiaoru Yuan, Tom Peterka\n\nAbstract: We propose a dynamically load-balanced algorithm for parallel particle tracing, which periodically attempts to evenly redistribute particles across processes based on k-d tree decomposition. Each process is assigned with (1) a statically partitioned, axis-aligned data block that partially overlaps with neighboring blocks in other processes and (2) a dynamically determined k-d tree leaf node that bounds the active particles for computation; the bounds of the k-d tree nodes are constrained by the geometries of data blocks. Given a certain degree of overlap between blocks, our method can balance the number of particles as much as possible. Compared with other load-balancing algorithms for parallel particle tracing, the proposed method does not require any preanalysis, does not use any heuristics based on flow features, does not make any assumptions about seed distribution, does not move any data blocks during the run, and does not need any master process for work redistribution. Based on a comprehensive performance study up to 8K processes on a Blue Gene/Q system, the proposed algorithm outperforms baseline approaches in both load balance and scalability on various flow visualization and analysis problems.", "uri": "https://vimeo.com/230835488", "name": "[VIS17 Preview] Dynamic Load Balancing Based on Constrained K-D Tree Decomposition for Parallel Particle Tracing (SciVis...", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:48:36+00:00", "description": "Authors: Ashok Jallepalli, Julia Docampo, Jennifer Ryan, Bob Haimes, Mike Kirby\n\nAbstract: As the finite element method (FEM) and the finite volume method (FVM), both traditional and high-order variants, continue their proliferation into various applied engineering disciplines, it is important that the visualization techniques and corresponding data analysis tools that act on the results produced by these methods faithfully represent the underlying data. To state this in another way: the interpretation of data generated by simulation needs to be consistent with the numerical schemes that underpin the specific solver technology. As the verifiable visualization literature has demonstrated: visual artifacts produced by the introduction of either explicit or implicit data transformations, such as data resampling, can sometimes distort or even obfuscate key scientific features in the data. In this paper, we focus on the handling of elemental continuity, which is often only C0 continuous or piecewise discontinuous, when visualizing primary or derived fields from FEM or FVM simulations. We demonstrate that traditional data handling and visualization of these fields introduce visual errors. In addition, we show how the use of the recently proposed line-SIAC filter provides a way of handling elemental continuity issues in an accuracy-conserving manner with the added benefit of casting the data in a smooth context even if the representation is element discontinuous.", "uri": "https://vimeo.com/230835460", "name": "[VIS17 Preview] On the Treatment of Field Quantities and Elemental Continuity in FEM Solutions (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:48:26+00:00", "description": "Authors: Lawrence Roy, Prashant Kumar, Sanaz Golbabaei, Yue Zhang, Eugene Zhang\n\nAbstract: Branched covering spaces are a mathematical concept which originates from complex analysis and topology and has applications in tensor field topology and geometry remeshing. Given a manifold surface and an N-way rotational symmetry field, a branched covering space is a manifold surface that has an N-to-1 map to the original surface except at the ramification points, which correspond to the singularities in the rotational symmetry field. Understanding the notion and mathematical properties of branched covering spaces is important to researchers in tensor field visualization and geometry processing, and their application areas. In this paper, we provide a framework to interactively design and visualize the branched covering space (BCS) of an input mesh surface and a rotational symmetry field defined on it. In our framework, the user can visualize not only the BCSs but also their construction process. In addition, our system allows the user to design the geometric realization of the BCS using mesh deformation techniques as well as connecting tubes. This enables the user to verify important facts about BCSs such as that they are manifold surfaces around singularities, as well as the Riemann-Hurwitz formula which relates the Euler characteristic of the BCS to that of the original mesh. Our system is evaluated by student researchers in scientific visualization and geometry processing as well as faculty members in mathematics at our university who teach topology. We include their evaluations and feedback in the paper.", "uri": "https://vimeo.com/230835443", "name": "[VIS17 Preview] Interactive Design and Visualization of Branched Covering Spaces (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:48:15+00:00", "description": "Authors: Michael Kern, Tim Hewson, Filip Sadlo, Ruediger Westermann, Marc Rautenhaus\n\nAbstract: Jet-streams, their core lines and their role in atmospheric dynamics have been subject to considerable meteorological research since the first half of the twentieth century. Yet, until today no consistent automated feature detection approach has been proposed to identify jet-stream core lines from 3D wind fields. Such 3D core lines can facilitate meteorological analyses previously not possible. Although jet-stream cores can be manually analyzed by meteorologists in 2D as height ridges in the wind speed field, to the best of our knowledge no automated ridge detection approach has been applied to jet-stream core detection. In this work, we \u2013a team of visualization scientists and meteorologists\u2013 propose a method that exploits directional information in the wind field to extract core lines in a robust and numerically less involved manner than traditional 3D ridge detection. For the first time, we apply the extracted 3D core lines to meteorological analysis, considering real-world case studies and demonstrating our method\u2019s benefits for weather forecasting and meteorological research.", "uri": "https://vimeo.com/230835424", "name": "[VIS17 Preview] Robust detection and visualization of jet-stream core lines in atmospheric flow (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:48:05+00:00", "description": "Authors: Alexander Bock, Harish Doraiswamy, Adam Summers, Claudio Silva\n\nAbstract: We present TopoAngler, a visualization framework that enables an interactive user-guided segmentation of fishes contained in a micro-CT scan. The inherent noise in the CT scan coupled with the often disconnected (and sometimes broken) skeletal structure of fishes makes an automatic segmentation of the volume impractical. To overcome this, our framework combines techniques from computational topology with an interactive visual interface, enabling the human-in-the-loop to effectively extract fishes from the volume. In the first step, the join tree of the input is used to create a hierarchical segmentation of the volume. Through the use of linked views, the visual interface then allows users to interactively explore this hierarchy, and gather parts of individual fishes into a coherent sub-volume, thus reconstructing entire fishes. Our framework was primarily developed for its application to CT scans of fishes, generated as part of the ScanAllFish project, through close collaboration with their lead scientist. However, we expect it to also be applicable in other biological applications where a single data set contains multiple specimen; a common routine that is now widely followed in laboratories to increase throughput of expensive CT scanners.", "uri": "https://vimeo.com/230835409", "name": "[VIS17 Preview] TopoAngler:  Interactive Topology-based Extraction of Fishes (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:47:51+00:00", "description": "Authors: Mohamed Ibrahim, Patrick Wickenhauser, Peter Rautek, Guido Reina, Markus Hadwiger\n\nAbstract: Molecular dynamics (MD) simulations are crucial to investigating important processes in physics and thermodynamics. The simulated atoms are usually visualized as hard spheres with Phong shading, where individual particles and their local density can be perceived well in close-up views. However, for large-scale simulations with 10 million particles or more, the visualization of large fields-of-view usually suffers from strong aliasing artifacts, because the mismatch between data size and output resolution leads to severe under-sampling of the geometry. Excessive super-sampling can alleviate this problem, but is prohibitively expensive. This paper presents a novel visualization method for large-scale particle data that addresses aliasing while enabling interactive high-quality rendering. We introduce the novel concept of screen-space normal distribution functions (S-NDFs) for particle data. S-NDFs represent the distribution of surface normals that map to a given pixel in screen space, which enables high-quality re-lighting without re-rendering particles. In order to facilitate interactive zooming, we cache S-NDFs in a screen-space mipmap (S-MIP). Together, these two concepts enable interactive, scale-consistent re-lighting and shading changes, as well as zooming, without having to re-sample the particle data. We show how our method facilitates the interactive exploration of real-world large-scale MD simulation data in different scenarios.", "uri": "https://vimeo.com/230835372", "name": "[VIS17 Preview] Screen-Space Normal Distribution Function Caching for Consistent Multi-Resolution Rendering of Large...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:47:39+00:00", "description": "Authors: Roxana Bujack, Terece Turton, Francesca Samsel, David Rogers, James Ahrens, Colin Ware\n\nAbstract: A myriad of design rules for what constitutes a \u201cgood\u201d colormap can be found in the literature. Some common rules include order, uniformity, and high discriminative power. However, the meaning of many of these terms is often ambiguous or open to interpretation. At times, different authors may use the same term to describe different concepts or the same rule is described by varying nomenclature. These ambiguities stand in the way of collaborative work, the design of experiments to assess the characteristics of colormaps, and automated colormap generation. In this paper, we review current and historical guidelines for colormap design. We propose a specified taxonomy and provide unambiguous mathematical definitions for the most common design rules.", "uri": "https://vimeo.com/230835348", "name": "[VIS17 Preview] The Good, the Bad, and the Ugly: A Theoretical Framework for the Assessment of Continuous Colormaps (SciVis...", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:47:30+00:00", "description": "(Best Paper Award)\n\nAuthors: Karl Bladin, Emil Axelsson, Erik Broberg, Carter Emmart, Patric Ljung, Alexander Bock, Anders Ynnerman\n\nAbstract: Results of planetary mapping are often shared openly for use in scientific research and mission planning. In its raw format, however, the data is not accessible to non-experts due to the difficulty in grasping the context and the intricate acquisition process. We present work on tailoring and integration of multiple data processing and visualization methods to interactively contextualize geospatial surface data of celestial bodies for use in science communication. As our approach handles dynamic data sources, streamed from online repositories, we are significantly shortening the time between discovery and dissemination of data and results. We describe the image acquisition pipeline, the pre-processing steps to derive a 2.5D terrain, and a chunked level-of-detail, out-of-core rendering approach to enable interactive exploration of global maps and high-resolution digital terrain models. The results are demonstrated for three different celestial bodies. The first case addresses high-resolution map data on the surface of Mars. A second case is showing dynamic processes, such as high-resolution, concurrent weather conditions on Earth that require temporal datasets. As a final example we use data from the New Horizons spacecraft which acquired images during a single flyby of Pluto. We visualize the acquisition process as well as the resulting surface data. Our work has been implemented in the OpenSpace software, which enables interactive presentations in a range of environments such as immersive dome theaters, interactive touch tables, and virtual reality headsets.", "uri": "https://vimeo.com/230835323", "name": "[VIS17 Preview] Globe Browsing: Contextualized Spatio-Temporal Planetary Surface Visualization (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:47:13+00:00", "description": "(Honorable Mention Award)\n\nAuthors: Julien Tierny, Guillaume Favelier, Joshua Levine, Charles Gueunet, Michael Michaux\n\nAbstract: This system paper presents the Topology ToolKit (TTK), a software platform designed for the topological analysis of scalar data in scientific visualization. While topological data analysis has gained in popularity over the last two decades, it has not yet been widely adopted as a standard data analysis tool for end users or developers. TTK aims at addressing this problem by providing a unified, generic, efficient, and robust implementation of key algorithms for the topological analysis of scalar data, including: critical points, integral lines, persistence diagrams, persistence curves, merge trees, contour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots, Jacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due to a tight integration with ParaView. It is also easily accessible to developers through a variety of bindings (Python, VTK/C++) for fast prototyping or through direct, dependency-free, C++, to ease integration into pre-existing complex systems. While developing TTK, we faced several algorithmic and software engineering challenges, which we document in this paper. In particular, we present an algorithm for the construction of a discrete gradient that complies to the critical points extracted in the piecewise-linear setting. This algorithm guarantees a combinatorial consistency across the topological abstractions supported by TTK, and importantly, a unified implementation of topological data simplification for multi-scale exploration and analysis. We also present a cached triangulation data structure, that supports time efficient and generic traversals, which self-adjusts its memory usage on demand for input simplicial meshes and which implicitly emulates a triangulation for regular grids with no memory overhead. Finally, we describe an original software architecture, which guarantees memory efficient and direct accesses to TTK features, while still allowing for researchers powerful and easy bindings and extensions. TTK is open source (BSD license) and its code, online documentation and video tutorials are available on TTK\u2019s website.", "uri": "https://vimeo.com/230835282", "name": "[VIS17 Preview] The Topology ToolKit (SciVis Paper)", "year": "2017", "event": "SCIVIS, PREVIEW"}, {"created_time": "2017-08-23T20:42:06+00:00", "description": "Authors: Federico, P., Heimerl, F., Koch, S., and Miksch, S.\n\nAbstract: The increasingly large number of available writings describing technical and scientific progress, calls for advanced analytic tools for their efficient analysis. This is true for many application scenarios in science and industry and for different types of writings, comprising patents and scientific articles. Despite important differences between patents and scientific articles, both have a variety of common characteristics that lead to similar search and analysis tasks. However, the analysis and visualization of these documents is not a trivial task due to the complexity of the documents as well as the large number of possible relations between their multivariate attributes. In this survey, we review interactive analysis and visualization approaches of patents and scientific articles, ranging from exploration tools to sophisticated mining methods. In a bottom-up approach, we categorize them according to two aspects: (a) data type (text, citations, authors, metadata, and combinations thereof), and (b) task (finding and comparing single entities, seeking elementary relations, finding complex patterns, and in particular temporal patterns, and investigating connections between multiple behaviours). Finally, we identify challenges and research directions in this area that ask for future investigations.", "uri": "https://vimeo.com/230834614", "name": "[VIS17 Preview] A Survey on Visual Approaches for Analyzing Scientific Literature and Patents (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:41:47+00:00", "description": "Authors: Liang Zhou and Daniel Weiskopf\n\nAbstract: We address the problem of visualizing multivariate correlations in parallel coordinates. We focus on multivariate correlation in the form of linear relationships between multiple variables. Traditional parallel coordinates are well prepared to show negative correlations between two attributes by distinct visual patterns. However, it is difficult to recognize positive correlations in parallel coordinates. Furthermore, there is no support to highlight multivariate correlations in parallel coordinates. In this paper, we exploit the indexed point representation of p-flats (planes in multidimensional data) to visualize local multivariate correlations in parallel coordinates. Our method yields clear visual signatures for negative and positive correlations alike, and it supports large datasets. All information is shown in a unified parallel coordinates framework, which leads to easy and familiar user interactions for analysts who have experience with traditional parallel coordinates. The usefulness of our method is demonstrated through examples of typical multidimensional datasets.", "uri": "https://vimeo.com/230834568", "name": "[VIS17 Preview] Indexed-Points Parallel Coordinates Visualization of Multivariate Correlations (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:41:37+00:00", "description": "Authors: Elham Sakhaee, Alireza Entezari\n\nAbstract: With uncertainty present in almost all modalities of data acquisition, reduction, transformation, and representation, there is a growing demand for mathematical analysis of uncertainty propagation in data processing pipelines. In this paper, we present a statistical framework for quantification of uncertainty and its propagation in the main stages of the visualization pipeline. We propose a novel generalization of Irwin-Hall distributions from the statistical viewpoint of splines and box-splines, that enables interpolation of random variables. Moreover, we introduce a probabilistic transfer function classification model that allows for incorporating probability density functions into the volume rendering integral. Our statistical framework allows for incorporating distributions from various sources of uncertainty which makes it suitable in a wide range of visualization applications. We demonstrate effectiveness of our approach in visualization of ensemble data, visualizing large datasets at reduced scale, iso-surface extraction, and visualization of noisy data.", "uri": "https://vimeo.com/230834547", "name": "[VIS17 Preview] A Statistical Direct Volume Rendering Framework for Visualization of Uncertain Data (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:41:29+00:00", "description": "Authors: Eric Alexander, Chih-Ching Chang, Mariana Shimabukuro, Steven Franconeri, Christopher Collins, Michael Gleicher\n\nAbstract: Many visualizations, including word clouds, cartographic labels, and word trees, encode data within the sizes of fonts. While font size can be an intuitive dimension for the viewer, using it as an encoding can introduce factors that may bias the perception of the underlying values. Viewers might conflate the size of a word\u2019s font with a word\u2019s length, the number of letters it contains, or with the larger or smaller heights of particular characters (\u2018o\u2019 vs. \u2018p\u2019 vs. \u2018b\u2019). We present a collection of empirical studies showing that such factors\u2014which are irrelevant to the encoded values\u2014can indeed influence comparative judgements of font size, though less than conventional wisdom might suggest. We highlight the largest potential biases, and describe a strategy to mitigate them.", "uri": "https://vimeo.com/230834531", "name": "[VIS17 Preview] Perceptual Biases in Font Size as a Data Encoding (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:41:20+00:00", "description": "Authors: Bung Wang and Klaus Mueller\n\nAbstract: Analyzing high-dimensional data and finding hidden patterns is a difficult problem and has attracted numerous research efforts. Automated methods can be useful to some extent but bringing the data analyst into the loop via interactive visual tools can help the discovery process tremendously. An inherent problem in this effort is that humans lack the mental capacity to truly understand spaces exceeding three spatial dimensions. To keep within this limitation, we describe a framework that decomposes a high-dimensional data space into a continuum of generalized 3D subspaces. Analysts can then explore these 3D subspaces individually via the familiar trackball interface while using additional facilities to smoothly transition to adjacent subspaces for expanded space comprehension. Since the number of such subspaces suffers from combinatorial explosion, we provide a set of data-driven subspace selection and navigation tools which can guide users to interesting subspaces and views. A subspace trail map allows users to manage the explored subspaces, keep their bearings, and return to interesting subspaces and views. Both trackball and trail map are each embedded into a word cloud of attribute labels which aid in navigation. We demonstrate our system via several use cases in a diverse set of application areas \u2013 cluster analysis and refinement, information discovery, and supervised training of classifiers. We also report on a user study that evaluates the usability of the various interactions our system provides.", "uri": "https://vimeo.com/230834512", "name": "[VIS17 Preview] The Subspace Voyager: Exploring High-Dimensional Data along a Continuum of Salient 3D Subspaces (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:41:11+00:00", "description": "Authors: Sabrina Nusrat, Md. Jawaherul Alam, Stephen Kobourov\n\nAbstract: Cartograms are maps in which areas of geographic regions, such as countries and states, appear in proportion to some variable of interest, such as population or income. Cartograms are popular visualizations for geo-referenced data that have been used for over a century to illustrate patterns and trends in the world around us. Despite the popularity of cartograms, and the large number of cartogram types, there are few studies evaluating the effectiveness of cartograms in conveying information. Based on a recent task taxonomy for cartograms, we evaluate four major types of cartograms: contiguous, non-contiguous, rectangular, and Dorling cartograms. We first evaluate the effectiveness of these cartogram types by quantitative performance analysis (time and error). Second, we collect qualitative data with an attitude study and by analyzing subjective preferences. Third, we compare the quantitative and qualitative results with the results of a metrics-based cartogram evaluation. Fourth, we analyze the results of our study in the context of cartography, geography, visual perception, and demography. Finally, we consider implications for design and possible improvements.", "uri": "https://vimeo.com/230834493", "name": "[VIS17 Preview] Evaluating Cartogram Effectiveness (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:41:02+00:00", "description": "Authors: Grzegorz Karch, Fabian Beck, Moritz Ertl, Christian Meister, Kathrin Schulte, Bernhard Weigand, Thomas Ertl, and Filip Sadlo\n\nAbstract: In single-phase flow visualization, research focuses on the analysis of vector field properties. In two-phase flow, in contrast, analysis of the phase components is typically of major interest. So far, visualization research of two-phase flow concentrated on proper interface reconstruction and the analysis thereof. In this paper, we present a novel visualization technique that enables the investigation of complex two-phase flow phenomena with respect to the physics of breakup and coalescence of inclusions. On the one hand, we adapt dimensionless quantities for a localized analysis of phase instability and breakup, and provide detailed inspection of breakup dynamics with emphasis on oscillation and its interplay with rotational motion. On the other hand, we present a parametric tightly linked space-time visualization approach for an effective interactive representation of the overall dynamics. We demonstrate the utility of our approach using several two-phase CFD datasets.", "uri": "https://vimeo.com/230834468", "name": "[VIS17 Preview] Visual Analysis of Inclusion Dynamics in Two-Phase Flow (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:40:53+00:00", "description": "Authors: Bahador Saket, Arjun Srinivasan, Eric D. Ragan, Alex Endert\n\nAbstract: User interfaces for data visualization often consist of two main components: control panels for user interaction and visual representation. A recent trend in visualization is directly embedding user interaction into the visual representations. For example, instead of using control panels to adjust visualization parameters, users can directly adjust basic graphical encodings (e.g., changing distances between points in a scatterplot) to perform similar parameterizations. However, enabling embedded interactions for data visualization requires a strong understanding of how user interactions influence the ability to accurately control and perceive graphical encodings. In this paper, we study the effectiveness of these graphical encodings when serving as the method for interaction. Our user study includes 12 interactive graphical encodings. We discuss the results in terms of task performance and interaction effectiveness metrics.", "uri": "https://vimeo.com/230834446", "name": "[VIS17 Preview] Evaluating Interactive Graphical Encodings for Data Visualization (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:40:43+00:00", "description": "Authors: Hanfei Lin, Siyuan Gao, David Gotz, Jingrui He, Nan Cao\n\nAbstract: Rare category identification is an important task in many application domains, ranging from network security to financial fraud detection, to personalized medicine. These are all applications which require the discovery and characterization of sets of rare but structurally-similar data entities which are obscured within a larger but structurally different dataset. This paper introduces RCLens, a visual analytics system designed to support user-guided rare category exploration and identification. RCLens adopts a novel active learning-based algorithm to iteratively identify more accurate rare categories in response to user-provided feedback. The algorithm is tightly integrated with an interactive visualization-based interface which supports a novel and effective workflow for rare category identification. This paper (1) defines RCLens' underlying active-learning algorithm; (2) describes the visualization and interaction designs, including a discussion of how the designs support user-guided rare category identification; and (3) presents results from an evaluation demonstrating RCLens' ability to support the rare category identification process.", "uri": "https://vimeo.com/230834420", "name": "[VIS17 Preview] RCLens: Interactive Rare Category Exploration and Identification (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:40:35+00:00", "description": "Authors: M. Adil Yalcin, Niklas Elmqvist, Benjamin B. Bederson\n\nAbstract: General purpose graphical interfaces for data exploration are typically based on manual visualization and interaction specifications. While designing manual specification can be very expressive, it demands high efforts to make effective decisions, therefore reducing exploratory speed. Instead, principled automated designs can increase exploratory speed, decrease learning efforts, help avoid ineffective decisions, and therefore better support data analytics novices. Towards these goals, we present Keshif, a new systematic design for tabular data exploration. To summarize a given dataset, Keshif aggregates records by value within attribute summaries, and visualizes aggregate characteristics using a consistent design based on data types. To reveal data distribution details, Keshif features three complementary linked selections: highlighting, filtering, and comparison. Keshif further increases expressiveness through aggregate metrics, absolute/part-of scale modes, calculated attributes, and saved selections, all working in synchrony. Its automated design approach also simplifies authoring of dashboards composed of summaries and individual records from raw data using fluid interaction. We show examples selected from 160+ datasets from diverse domains. Our study with novices shows that after exploring raw data for 15 minutes, our participants reached close to 30 data insights on average, comparable to other studies with skilled users using more complex tools.", "uri": "https://vimeo.com/230834404", "name": "[VIS17 Preview] Keshif: Rapid and Expressive Tabular Data Exploration for Novices (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:40:26+00:00", "description": "Authors: Harrison, D. G., Efford, N. D., Fisher, Q. J., &amp; Ruddle, R. A.\n\nAbstract: The aim of the PETMiner software is to reduce the time and monetary cost of analysing petrophysical data that is obtained from reservoir sample cores. Analysis of these data requires tacit knowledge to fill \u2018gaps\u2019 so that predictions can be made for incomplete data. Through discussions with 30 industry and academic specialists, we identified three analysis use cases that exemplified the limitations of current petrophysics analysis tools. We used those use cases to develop nine core requirements for PETMiner, which is innovative because of its ability to display detailed images of the samples as data points, directly plot multiple sample properties and derived measures for comparison, and substantially reduce interaction cost. An 11-month evaluation demonstrated benefits across all three use cases by allowing a consultant to: (1) generate more accurate reservoir flow models, (2) discover a previously unknown relationship between one easy-to-measure property and another that is costly, and (3) make a 100-fold reduction in the time required to produce plots for a report.", "uri": "https://vimeo.com/230834380", "name": "[VIS17 Preview] PETMiner \u2013 A Visual Analysis Tool for Petrophysical Properties of Core Sample Data (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:40:19+00:00", "description": "Authors: Pascal Goffin, Jeremy Boy, Wesley Willett, Petra Isenberg\n\nAbstract: We contribute an investigation of the design and function of word-scale graphics and visualizations embedded in text documents. Word-scale graphics include both data-driven representations such as word-scale visualizations and sparklines, and non-data-driven visual marks. Their design, function, and use has so far received little research attention. We present the results of an open ended exploratory study with 9 graphic designers. The study resulted in a rich collection of different types of graphics, data provenance, and relationships between text, graphics, and data. Based on this corpus, we present a systematic overview of word-scale graphic designs, and examine how designers used them. We also discuss the designers\u2019 goals in creating their graphics, and characterize how they used word-scale graphics to visualize data, add emphasis, and create alternative narratives. Building on these examples, we discuss implications for the design of authoring tools for word-scale graphics and visualizations, and explore how new authoring environments could make it easier for designers to integrate them into documents.", "uri": "https://vimeo.com/230834366", "name": "[VIS17 Preview] An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:40:09+00:00", "description": "Authors: Le Liu, Alexander Boone, Ian Ruginski, Lace Padilla, Mary Hegarty, Sarah H. Creem-Regehr, William B. Thompson, Cem Yuksel, Donald H. House\n\nAbstract: Data ensembles are often used to infer statistics to be used for a summary display of an uncertain prediction. In a spatial context, these summary displays have the drawback that when uncertainty is encoded via a spatial spread, display glyph area increases in size with prediction uncertainty. This increase can be easily confounded with an increase in the size, strength or other attribute of the phenomenon being presented. We argue that by directly displaying a carefully chosen subset of a prediction ensemble, so that uncertainty is conveyed implicitly, such misinterpretations can be avoided. Since such a display does not require uncertainty annotation, an information channel remains available for encoding additional information about the prediction. We demonstrate these points in the context of hurricane prediction visualizations, showing how we avoid occlusion of selected ensemble elements while preserving the spatial statistics of the original ensemble, and how an explicit encoding of uncertainty can also be constructed from such a selection. We conclude with the results of a cognitive experiment demonstrating that the approach can be used to construct storm prediction displays that significantly reduce the confounding of uncertainty with storm size, and thus improve viewers\u2019 ability to estimate potential for storm damage.", "uri": "https://vimeo.com/230834346", "name": "[VIS17 Preview] Uncertainty Visualization by Representative Sampling from Prediction Ensembles (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:40:00+00:00", "description": "Authors: Seokyeon Kim, Seongmin Jeong, Insoo Woo, Yun Jang, Ross Maciejewski, David Ebert\n\nAbstract: Geographic visualization research has focused on a variety of techniques to represent and explore spatiotemporal data. The goal of those techniques is to enable users to explore events and interactions over space and time in order to facilitate the discovery of patterns, anomalies and relationships within the data. However, it is difficult to extract and visualize data flow patterns over time for non-directional statistical data without trajectory information. In this work, we develop a novel flow analysis technique to extract, represent, and analyze flow maps of non-directional spatiotemporal data unaccompanied by trajectory information. We estimate a continuous distribution of these events over space and time, and extract flow fields for spatial and temporal changes utilizing a gravity model. Then, we visualize the spatiotemporal patterns in the data by employing flow visualization techniques. The user is presented with temporal trends of geo-referenced discrete events on a map. As such, overall spatiotemporal data flow patterns help users analyze geo-referenced temporal events, such as disease outbreaks, crime patterns, etc. To validate our model, we discard the trajectory information in an origin-destination dataset and apply our technique to the data and compare the derived trajectories and the original. Finally, we present spatiotemporal trend analysis for statistical datasets including twitter data, maritime search and rescue events, and syndromic surveillance.", "uri": "https://vimeo.com/230834322", "name": "[VIS17 Preview] Data Flow Analysis and Visualization for Spatiotemporal Statistical Data without Trajectory Information...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:39:51+00:00", "description": "Authors: Hoa Nguyen and Paul Rosen\n\nAbstract: Parallel coordinates plots (PCPs) are a well-studied technique for exploring multi-attribute datasets. In many situations, users find them a flexible method to analyze and interact with data. Unfortunately, using PCPs becomes challenging as the number of data items grows large or multiple trends within the data mix in the visualization. The resulting overdraw can obscure important features. A number of modifications to PCPs have been proposed, including using color, opacity, smooth curves, frequency, density, and animation to mitigate this problem. However, these modified PCPs tend to have their own limitations in the kinds of relationships they emphasize. We propose a new data scalable design for representing and exploring data relationships in PCPs. The approach exploits the point/line duality property of PCPs and a local linear assumption of data to extract and represent relationship summarizations. This approach simultaneously shows relationships in the data and the consistency of those relationships. Our approach supports various visualization tasks, including mixed linear and nonlinear pattern identification, noise detection, and outlier detection, all in large data. We demonstrate these tasks on multiple synthetic and real-world datasets.", "uri": "https://vimeo.com/230834300", "name": "[VIS17 Preview] DSPCP: A Data Scalable Approach for Identifying Relationships in Parallel Coordinates (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:39:42+00:00", "description": "Authors: Fabio Miranda, Lauro Lins, James T. Klosowski, Claudio T. Silva\n\nAbstract: From economics to sports to entertainment and social media, ranking objects according to some notion of importance is a fundamental tool we humans use all the time to better understand our world. With the ever-increasing amount of user-generated content found online, \u201cwhat\u2019s trending\u201d is now a commonplace phrase that tries to capture the zeitgeist of the world by ranking the most popular microblogging hashtags in a given region and time. However, before we can understand what these rankings tell us about the world, we need to be able to more easily create and explore them, given the significant scale of today's data. In this paper, we describe the computational challenges in building a real-time visual exploratory tool for finding top-ranked objects; build on the recent work involving in-memory and rank-aware data cubes to propose TOPKUBE: a data structure that answers top-k queries up to one order of magnitude faster than the previous state of the art; demonstrate the usefulness of our methods using a set of real-world, publicly available datasets; and provide a new set of benchmarks for other researchers to validate their methods and compare to our own.", "uri": "https://vimeo.com/230834277", "name": "[VIS17 Preview] TopKube: A Rank-Aware Data Cube for Real-Time Exploration of Spatiotemporal Datasets (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:39:34+00:00", "description": "Authors: Matthew Brehmer, Bongshin Lee, Benjamin Bach, Nathalie Henry Riche, Tamara Munzner\n\nAbstract: There are many ways to visualize event sequences as timelines. In a storytelling context where the intent is to convey multiple narrative points, a richer set of timeline designs may be more appropriate than the narrow range that has been used for exploratory data analysis by the research community. Informed by a survey of 263 timelines, we present a design space for storytelling with timelines that balances expressiveness and effectiveness, identifying 14 design choices characterized by three dimensions: representation, scale, and layout. Twenty combinations of these choices are viable timeline designs that can be matched to different narrative points, while smooth animated transitions between narrative points allow for the presentation of a cohesive story, an important aspect of both interactive storytelling and data videos. We further validate this design space by realizing the full set of viable timeline designs and transitions in a proof-of-concept sandbox implementation that we used to produce seven example timeline stories. Ultimately, this work is intended to inform and inspire the design of future tools for storytelling with timelines.", "uri": "https://vimeo.com/230834257", "name": "[VIS17 Preview] Timelines Revisited: A Design Space and Considerations for Expressive Storytelling (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:39:21+00:00", "description": "Authors: Franz Sauer, Jinrong Xie, Kwan-Liu Ma\n\nAbstract: The Eulerian and Lagrangian reference frames each provide a unique perspective when studying and visualizing results from scientific systems. As a result, many large-scale simulations produce data in both formats, and analysis tasks that simultaneously utilize information from both representations are becoming increasingly popular. However, due to their fundamentally different nature, drawing correlations between these data formats is a computationally difficult task, especially in a large-scale setting. In this work, we present a new data representation which combines both reference frames into a joint Eulerian-Lagrangian format. By reorganizing Lagrangian information according to the Eulerian simulation grid into a ``unit cell'' based approach, we can provide an efficient out-of-core means of sampling, querying, and operating with both representations simultaneously. We also extend this design to generate multi-resolution subsets of the full data to suit the viewer's needs and provide a fast flow-aware trajectory construction scheme. We demonstrate the effectiveness of our method using three large-scale real world scientific datasets and provide insight into the types of performance gains that can be achieved.", "uri": "https://vimeo.com/230834211", "name": "[VIS17 Preview] A Combined Eulerian-Lagrangian Data Representation for Large-scale Applications (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:39:13+00:00", "description": "Authors: Gennady Andrienko, Natalia Andrienko, Georg Fuchs, Jo Wood\n\nAbstract: Origin-destination (OD) movement data describe moves or trips between spatial locations by specifying the origins, destinations, start, and end times, but not the routes travelled. For studying the spatio-temporal patterns and trends of mass mobility, individual OD moves of many people are aggregated into flows (collective moves) by time intervals. Time-variant flow data pose two difficult challenges for visualization and analysis. First, flows may connect arbitrary locations (not only neighbors), thus making a graph with numerous edge intersections, which is hard to visualize in a comprehensible way. Even a single spatial situation consisting of flows in one time step is hard to explore. The second challenge is the need to analyze long time series consisting of numerous spatial situations. We present an approach facilitating exploration of long-term flow data by means of spatial and temporal abstraction. It involves a special way of data aggregation, which allows representing spatial situations by diagram maps instead of flow maps, thus reducing the intersections and occlusions pertaining to flow maps. The aggregated data are used for clustering of time intervals by similarity of the spatial situations. Temporal and spatial displays of the clustering results facilitate the discovery of periodic patterns and longer-term trends in the mass mobility behavior.", "uri": "https://vimeo.com/230834183", "name": "[VIS17 Preview] Revealing Patterns and Trends of Mass Mobility through Spatial and Temporal Abstraction of Origin-...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:39:02+00:00", "description": "Authors: Johannes Fuchs, Petra Isenberg, Anastasia Bezerianos, Daniel Keim\n\nAbstract: We systematically reviewed 64 user-study papers on data glyphs to help researchers and practitioners gain an informed understanding of tradeoffs in the glyph design space. The glyphs we consider are individual representations of multi-dimensional data points, often meant to be shown in small-multiple settings. Over the past 60 years many different glyph designs were proposed and many of these designs have been subjected to perceptual or comparative evaluations. Yet, a systematic overview of the types of glyphs and design variations tested, the tasks under which they were analyzed, or even the study goals and results does not yet exist. In this paper we provide such an overview by systematically sampling and tabulating the literature on data glyph studies, listing their designs, questions, data, and tasks. In addition we present a concise overview of the types of glyphs and their design characteristics analyzed by researchers in the past, and a synthesis of the study results. Based on our meta analysis of all results we further contribute a set of design implications and a discussion on open research directions.", "uri": "https://vimeo.com/230834154", "name": "[VIS17 Preview] A Systematic Review of Experimental Studies on Data Glyphs (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:38:53+00:00", "description": "Authors: Petra Isenberg, Florian Heimerl, Steffen Koch, Tobias Isenberg, Panpan Xu, Charles Stolper, Michael Sedlmair, Jian Chen, Torsten M\u00f6ller, John Stasko\n\nAbstract: We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.", "uri": "https://vimeo.com/230834115", "name": "[VIS17 Preview] vispubdata.org: A Metadata Collection about IEEE Visualization (VIS) Publications (TVCG Paper)", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:30:57+00:00", "description": "Authors: J\u00fcrgen Bernard, Marco Hutter, Matthias Zeppelzauer, Dieter Fellner, Michael Sedlmair\n\nAbstract: Labeling data instances is an important task in machine learning and visual analytics. \\ Both fields provide a broad set of labeling strategies, whereby machine learning (and in particular active learning) follows a rather model-centered approach and visual analytics employs rather user-centered approaches (visual-interactive labeling). Both approaches have individual strengths and weaknesses. \\ In this work, we conduct an experiment with three parts to assess and compare the performance of these different labeling strategies. \\ In our study, we (1) identify different visual labeling strategies for user-centered labeling, (2) investigate strengths and weaknesses of labeling strategies for different labeling tasks and task complexities, and (3) shed light on the effect of using different visual encodings to guide the visual-interactive labeling process. \\ We further compare labeling of single versus multiple instances at a time, and quantify the impact on efficiency. We systematically compare the performance of visual interactive labeling with that of active learning. \\ Our main findings are that visual-interactive labeling can outperform active learning, given the condition that dimension reduction separates well the class distributions. Moreover, using dimension reduction in combination with additional visual encodings that expose the internal state of the learning model turns out to improve the performance of visual-interactive labeling.", "uri": "https://vimeo.com/230832907", "name": "[VIS17 Preview] Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:23:29+00:00", "description": "Authors: Enamul Hoque, Vidya Setlur, Melanie Tory, Isaac Dykeman\n\nAbstract: Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools", "uri": "https://vimeo.com/230831885", "name": "[VIS17 Preview] Applying Pragmatics Principles for Interaction with Visual Analytics (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:23:14+00:00", "description": "Authors: Bilal Alsallakh, Amin Jourabloo, Mao Ye, Xiaoming Liu, Liu Ren\n\nAbstract: Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help identifying various quality issues in the training data.", "uri": "https://vimeo.com/230831852", "name": "[VIS17 Preview] Do Convolutional Neural Networks learn Class Hierarchy? (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:17:07+00:00", "description": "Authors: Shunan Guo, Ke Xu, Rongwen Zhao, David Gotz, Hongyuan Zha, Nan Cao\n\nAbstract: Event sequence data such as electronic health records, a person's academic records, or car service records, are ordered series of events which have occurred over a period of time. Analyzing collections of event sequences can reveal common or semantically important sequential patterns. For example, event sequence analysis might reveal frequently used care plans for treating a disease, typical publishing patterns of professors, and the patterns of service that result in a well-maintained car. It is challenging, however, to visually explore large numbers of event sequences, or sequences with large numbers of event types. Existing methods focus on extracting explicitly matching patterns of events using statistical analysis to create stages of event progression over time. However, these methods fail to capture latent clusters of similar but not identical evolutions of event sequences. In this paper, we introduce a novel visualization system named EventThread which clusters event sequences into threads based on tensor analysis and visualizes the latent stage categories and evolution patterns by interactively grouping the threads by similarity into time-specific clusters. We demonstrate the effectiveness of EventThread through usage scenarios in three different application domains and via interviews with an expert user.", "uri": "https://vimeo.com/230831051", "name": "[VIS17 Preview] EventThread: Visual Summarization and Stage Analysis of Event Sequence Data (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:16:58+00:00", "description": "Authors: Josua Krause, Aritra Dasgupta, Jordan Swartz, Yindalon Aphinyanaphongs, Enrico Bertini\n\nAbstract: Human-in-the-loop data analysis applications necessitate greater transparency in machine learning models for experts to understand and trust their decisions. To this end,    \\ we propose a visual analytics workflow to help data scientists and domain experts explore, diagnose, and understand the decisions made by a binary classifier. The approach leverages \"instance-level explanations\", measures of local feature relevance that explain single instances, and uses them to build a set of visual representations that guide the users in their investigation. The workflow is based on three main visual representations and steps: one based on aggregate statistics to see how data distributes across correct / incorrect decisions, one based on explanations to understand which features are used to make these decisions, and one based on raw data, to derive insights on potential root causes for the observed patterns. The workflow is derived from a long-term collaboration with a group of machine learning and healthcare professionals who used our method to make sense of machine learning models they developed. The case study from this collaboration demonstrates that the proposed workflow helps experts derive useful knowledge about the model and the phenomena it describes, thus experts can generate useful hypotheses on how a model can be improved.", "uri": "https://vimeo.com/230831026", "name": "[VIS17 Preview] A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:16:48+00:00", "description": "Authors: Angus Forbes, Andrew Burks, Kristine Lee, Xing Li, Pierre Boutillier, Jean Krivine, Walter Fontana\n\nAbstract: We introduce the Dynamic Influence Network (DIN), a novel visual analytics technique for representing and analyzing rule-based models of protein-protein interaction networks. Rule-based modeling has proved instrumental in developing biological models that are concise, comprehensible, easily extensible, and that mitigate the combinatorial complexity of multi-state and multi-component biological molecules. Our technique visualizes the dynamics of these rules as they evolve over time. Using the data produced by KaSim, an open source stochastic simulator of rule-based models written in the Kappa language, DINs provide a node-link diagram that represents the influence that each rule has on the other rules. That is, rather than representing individual biological components or types, we instead represent the rules about them (as nodes) and the current influence of these rules (as links). Using our interactive DIN-Viz software tool, researchers are able to query this dynamic network to find meaningful patterns about biological processes, and to identify salient aspects of complex rule-based models. To evaluate the effectiveness of our approach, we investigate a simulation of a circadian clock model that illustrates the oscillatory behavior of the KaiC protein phosphorylation cycle.", "uri": "https://vimeo.com/230831007", "name": "[VIS17 Preview] Dynamic Influence Networks for Rule-based Models (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:16:39+00:00", "description": "(Best Paper Honorable Mention)\n\nAuthors: Jian Zhao, Michael Glueck, Petra Isenberg, Fanny Chevalier, Azam Khan\n\nAbstract: During asynchronous collaborative analysis, handoff of partial findings is challenging because externalizations produced by analysts may not adequately communicate their investigative process. To address this challenge, we developed techniques to automatically capture and help encode tacit aspects of the investigative process based on an analyst\u2019s interactions, and streamline explicit authoring of handoff annotations. We designed our techniques to mediate awareness of analysis coverage, support explicit communication of progress and uncertainty with annotation, and implicit communication through playback of investigation histories. To evaluate our techniques, we developed an interactive visual analysis system, KTGraph, that supports an asynchronous investigative document analysis task. We conducted a two-phase user study to characterize a set of handoff strategies and to compare investigative performance with and without our techniques. The results suggest that our techniques promote the use of more effective handoff strategies, help increase an awareness of prior investigative process and insights, as well as improve final investigative outcomes.", "uri": "https://vimeo.com/230830982", "name": "[VIS17 Preview] Supporting Handoff in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:16:31+00:00", "description": "Authors: John Wenskovitch, Ian Crandell, Naren Ramakrishnan, Leanna House, Scotland Leman, Chris North\n\nAbstract: Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics.  Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets.  Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems.  However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence.  A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection.  This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.", "uri": "https://vimeo.com/230830954", "name": "[VIS17 Preview] Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:16:22+00:00", "description": "Authors: Manuel Stein, Halld\u00f3r Janetzko, Andreas Lamprecht, Thorsten Breitkreutz, Philipp Zimmermann, Bastian Goldl\u00fccke, Tobias Schreck, Gennady Andrienko, Michael Grossniklaus, Daniel Keim\n\nAbstract: Analysts in professional team sport regularly perform analysis to gain strategic and tactical insights into player and team behavior. Goals of team sport analysis regularly include identification of weaknesses of opposing teams, or assessing performance and improvement potential of a coached team.  Current analysis workflows are typically based on the analysis of team videos. Also, analysts can rely on techniques from Information Visualization, to depict e.g., player or ball trajectories. However, video analysis is typically a time-consuming process, where the analyst needs to memorize and annotate scenes. In contrast, visualization typically relies on an abstract data model, often using abstract visual mappings, and is not directly linked to the observed movement context anymore. We propose a visual analytics system that tightly integrates team sport video recordings with abstract visualization of underlying trajectory data. We apply appropriate computer vision techniques  to extract trajectory data from video input. Furthermore, we apply advanced trajectory and movement analysis techniques to derive relevant team sport analytic measures for region, event and player analysis in the case of soccer analysis. Our system seamlessly integrates video and visualization modalities, enabling analysts to draw on the advantages of both analysis forms. Several expert studies conducted with team sport analysts indicate the effectiveness of our integrated approach.", "uri": "https://vimeo.com/230830922", "name": "[VIS17 Preview] Bring it to the Pitch: Combining Video and Movement Data to Enhance Team Sport Analysis (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:16:14+00:00", "description": "Authors: Darren Edge, Nathalie Henry Riche, Jonathan Larson, Christopher White\n\nAbstract: As Visual Analytics (VA) research grows and diversifies to encompass new systems, techniques, and use contexts, gaining a holistic view of analytic practices is becoming ever more challenging. However, such a view is essential for researchers and practitioners seeking to develop systems for broad audiences that span multiple domains. In this paper, we interpret VA research through the lens of Activity Theory (AT)\u2014a framework for modelling human activities that has been influential in the field of Human-Computer Interaction. We first provide an overview of Activity Theory, showing its potential for thinking beyond tasks, representations, and interactions to the broader systems of activity in which interactive tools are embedded and used. Next, we describe how Activity Theory can be used as an organizing framework in the construction of activity typologies, building and expanding upon the tradition of abstract task taxonomies in the field of Information Visualization. We then apply the resulting process to create an activity typology for Visual Analytics, synthesizing a wide range of systems and activity concepts from the literature. Finally, we use this typology as the foundation of an activity-centered design process, highlighting both tensions and opportunities in the design space of VA systems.", "uri": "https://vimeo.com/230830892", "name": "[VIS17 Preview] Beyond Tasks: An Activity Typology for Visual Analytics (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:16:05+00:00", "description": "Authors: Alexander Kumpf, Bianca Tost, Marlene Baumgart, Michael Riemer, R\u00fcdiger Westermann, Marc Rautenhaus\n\nAbstract: In meteorology, cluster analysis is frequently used to determine representative trends in ensemble weather predictions in a selected spatio-temporal region, e.g., to reduce a set of ensemble members to simplify and improve their analysis. \\ Identified clusters (i.e., groups of similar members), however, can be very sensitive to small changes of the selected region, so that clustering results can be misleading and bias subsequent analyses. \\ In this article, we --a team of visualization scientists and meteorologists-- deliver visual analytics solutions to analyze the sensitivity of clustering results with respect to changes of a selected region. \\ We propose an interactive visual interface that enables simultaneous visualization of a) the variation in composition of identified clusters (i.e., their robustness), b) the variability in cluster membership for individual ensemble members, and c) the uncertainty in the spatial locations of identified trends. \\ We demonstrate that our solution shows meteorologists how representative a clustering result is, and with respect to which changes in the selected region it becomes unstable. \\ Furthermore, our solution helps to identify those ensemble members which stably belong to a given cluster and can thus be considered similar. \\ In a real-world application case we show how our approach is used to analyze the clustering behavior of different regions in a forecast of ``Tropical Cyclone Karl'', guiding the user towards the cluster robustness information required for subsequent ensemble analysis.", "uri": "https://vimeo.com/230830865", "name": "[VIS17 Preview] Visualizing Confidence in Cluster-based Ensemble Weather Forecast Analyses (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:15:55+00:00", "description": "Authors: Thomas M\u00fchlbacher, Lorenz Linhardt, Torsten M\u00f6ller, Harald Piringer\n\nAbstract: Balancing accuracy gains with other objectives such as interpretability is a key challenge when building decision trees. However, this process is difficult to automate because it involves know-how about the domain as well as the purpose of the model. This paper presents TreePOD, a new approach for sensitivity-aware model selection along trade-offs. TreePOD is based on exploring a large set of candidate trees generated by sampling the parameters of tree construction algorithms. Based on this set, visualizations of quantitative and qualitative tree aspects provide a comprehensive overview of possible tree characteristics. Along trade-offs between two objectives, TreePOD provides efficient selection guidance by focusing on Pareto-optimal tree candidates. TreePOD also conveys the sensitivities of tree characteristics on variations of selected parameters by extending the tree generation process with a full-factorial sampling. We demonstrate how TreePOD supports a variety of tasks involved in decision tree selection and describe its integration in a holistic workflow for building and selecting decision trees. For evaluation, we illustrate a case study for predicting critical power grid states, and we report qualitative feedback from domain experts in the energy sector. This feedback suggests that TreePOD enables users with and without statistical background a confident and efficient identification of suitable decision trees.", "uri": "https://vimeo.com/230830828", "name": "[VIS17 Preview] TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:15:46+00:00", "description": "Authors: Paolo Federico, Markus Wagner, Alexander Rind, Albert Amor-Amor\u00f3s, Silvia Miksch, Wolfgang Aigner\n\nAbstract: Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans\u2019 tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.", "uri": "https://vimeo.com/230830806", "name": "[VIS17 Preview] The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:15:34+00:00", "description": "Authors: Takanori Fujiwara, Preeti Malakar, Khairi Reda, Venkatram Vishwanath, Michael Papka, Kwan-Liu Ma\n\nAbstract: Current and upcoming supercomputers have more than thousands of compute nodes interconnected with high-dimensional networks and complex network topologies for improved performance. Application developers are required to write scalable parallel programs in order to achieve high throughput on these machines. Application performance is largely determined by efficient inter-process communication. A common way to analyze and optimize performance is through profiling the parallel codes to identify communication bottlenecks. However, understanding gigabytes of profile data is not a trivial task. In this paper, we present a visual analytics system for understanding the scalability bottlenecks and improving the communication efficiency of massively parallel applications. Visualization methods used in this system are designed to comprehend large-scale and varied communication patterns on thousands of nodes in complex networks such as the 5D torus and the dragonfly. We also present efficient rerouting and remapping algorithms that can be coupled with our interactive visual analytics design for performance optimization. We demonstrate the utility of our system with several case studies using three benchmark applications on two leading supercomputers. The mapping suggestion from our system led to 38% improvement in hop-bytes for MiniAMR application on 4,096 MPI processes.", "uri": "https://vimeo.com/230830776", "name": "[VIS17 Preview] A Visual Analytics System for Optimizing Communications in Massively Parallel Applications (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:15:25+00:00", "description": "Authors: Xumeng Wang, Jia-Kai Chou, Wei Chen, Huihua Guan, Wenlong Chen, tianyi lao, Kwan-Liu Ma\n\nAbstract: Sharing data for public usage requires sanitization to prevent sensitive information from leaking.  \\ Previous studies have presented methods for creating privacy preserving visualizations, however, few of them provide sufficient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allow users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.", "uri": "https://vimeo.com/230830747", "name": "[VIS17 Preview] A Utility-aware Visual Approach for Anonymizing Multi-attribute Tabular Data (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:15:15+00:00", "description": "Authors: Michael Glueck, Mahdi Pakdaman Naeini, Finale Doshi-Velez, Fanny Chevalier, Azam Khan, Daniel Wigdor, Michael Brudno\n\nAbstract: PhenoLines is a visual analysis tool for the interpretation of disease subtypes, derived from the application of topic models to clinical data. Topic models enable one to mine cross-sectional patient comorbidity data (e.g., electronic health records) and construct disease subtypes---each with its own temporally evolving prevalence and co-occurrence of phenotypes---without requiring aligned longitudinal phenotype data for all patients. However, the dimensionality of topic models makes interpretation challenging, and de facto analyses provide little intuition regarding phenotype relevance or phenotype interrelationships. PhenoLines enables one to compare phenotype prevalence within and across disease subtype topics, thus supporting subtype characterization, a task that involves identifying a proposed subtype's dominant phenotypes, ages of effect, and clinical validity. We contribute a data transformation workflow that employs the Human Phenotype Ontology to hierarchically organize phenotypes and aggregate the evolving probabilities produced by topic models. We introduce a novel measure of phenotype relevance that can be used to simplify the resulting topology. The design of PhenoLines was motivated by formative interviews with machine learning and clinical experts. We describe the co-operative design process, distill high-level tasks, and report on initial evaluations with machine learning experts and a medical domain expert. These results suggest that PhenoLines demonstrates promising approaches to support the characterization and optimization of topic models.", "uri": "https://vimeo.com/230830718", "name": "[VIS17 Preview] PhenoLines: Phenotype Comparison Visualizations for Disease Subtyping via Topic Models (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:15:06+00:00", "description": "Authors: Bum Chul Kwon, Ben Eysenbach, Janu Verma, Kenney Ng, Christopher deFilippi, Walter Stewart, Adam Perer\n\nAbstract: Clustering, the process of grouping together similar items into distinct partitions, is a common type of unsupervised machine learning that can be useful for summarizing and aggregating complex multi-dimensional data. However, data can be clustered in many ways, and there exist a large body of algorithms designed to reveal different patterns. While having access to a wide variety of algorithms is helpful, in practice, it is quite difficult for data scientists to choose and parameterize algorithms to get the clustering results relevant for their dataset and analytical tasks. To alleviate this problem, we built Clustervision, a visual analytics tool that helps ensure data scientists find the right clustering among the large amount of techniques and parameters available. Our system clusters data using a variety of clustering techniques and parameters and then ranks clustering results utilizing five quality scoring metrics. In addition, users can guide the system to produce more relevant results by providing task-relevant constraints on the data. Our visual user interface allows users to find high quality clustering results, explore the clusters using several coordinated visualization techniques, and select the cluster result that best suits their task. We demonstrate this novel approach using a case study with a team of researchers in medical domain and showcase that our system empowers users to choose is an effective representation of their complex data.", "uri": "https://vimeo.com/230830698", "name": "[VIS17 Preview] Clustervision: Visual Supervision of Unsupervised Clustering (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:14:57+00:00", "description": "Authors: Haeyong Chung, Sai Prashanth Dasari, Santhosh Nandhakumar, Christopher Andrews\n\nAbstract: We present CRICTO, a new crowdsourcing visual analytics environment for making sense of and analyzing text data, whereby multiple crowdworkers are able to parallelize the simple information schematization tasks of relating and connecting entities across documents. The diverse links from these schematization tasks are then automatically combined and the system visualizes them based on the semantic types of the linkages. CRICTO also includes several techniques that allow analysts to interactively explore and refine crowd\u2019s results to better support their own sensemaking process. We evaluated CRICTO\u2019s techniques and analysis workflow with deployments of CRICTO using Amazon Mechanical Turk and a user study that assess the effect of crowdsourced schematization in sensemaking tasks. The results of our evaluation show that CRICTO\u2019s crowdsourcing approaches and workflow help analysts explore diverse aspects of datasets, and uncover more accurate hidden stories embedded in the datasets.", "uri": "https://vimeo.com/230830671", "name": "[VIS17 Preview] CRICTO: Supporting Sensemaking through Crowdsourced Information Schematization (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:14:47+00:00", "description": "Authors: Yuanzhe Chen, Panpan Xu, Liu Ren\n\nAbstract: Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis.  Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.", "uri": "https://vimeo.com/230830645", "name": "[VIS17 Preview] Sequence Synopsis: Optimize Visual Summary of Temporal Event Data (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:14:38+00:00", "description": "(Best Paper Honorable Mention)\n\nAuthors: Mennatallah El-Assady, Rita Sevastjanova, Fabian Sperrle, Daniel Keim, Christopher Collins\n\nAbstract: Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.", "uri": "https://vimeo.com/230830619", "name": "[VIS17 Preview] Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:14:30+00:00", "description": "Authors: Emily Wall, Leslie M. Blaha, Lyndsey Franklin, Alex Endert\n\nAbstract: Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models, however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.", "uri": "https://vimeo.com/230830599", "name": "[VIS17 Preview] Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:14:19+00:00", "description": "Authors: Emily Wall, Subhajit Das, Ravish Chawla, Bharath Kalidindi, Eli T. Brown, Alex Endert\n\nAbstract: People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user\u2019s data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user\u2019s subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.", "uri": "https://vimeo.com/230830573", "name": "[VIS17 Preview] Podium: Ranking Data Using Mixed-Initiative Visual Analytics (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:14:09+00:00", "description": "Authors: Deokgun Park, Seungyeon Kim, Jurim Lee, Jaegul Choo, Nicholas Diakopoulos, Niklas Elmqvist\n\nAbstract: Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building such concepts from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of human language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides the user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts using user seed terms, we introduce a bipolar concept model and support for irrelevant words. We validate the interactive lexicon building interface via a user study and expert reviews. The quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.", "uri": "https://vimeo.com/230830546", "name": "[VIS17 Preview] ConceptVector: Text Visual Analytics via  Interactive Lexicon Building using Word Embedding (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:13:59+00:00", "description": "Authors: Jian Zhao, Maoyuan Sun, Francine Chen, Patrick Chiu\n\nAbstract: Discovering and analyzing biclusters, i.e., two sets of related entities with close relationships, is a critical task in many real-world applications, such as exploring entity co-occurrences in intelligence analysis, and studying gene expression in bio-informatics. While the output of biclustering techniques can offer some initial low-level insights, visual approaches are required on top of that due to the algorithmic output complexity.This paper proposes a visualization technique, called BiDots, that allows analysts to interactively explore biclusters over multiple domains. BiDots overcomes several limitations of existing bicluster visualizations by encoding biclusters in a more compact and cluster-driven manner. A set of handy interactions is incorporated to support flexible analysis of biclustering results. More importantly, BiDots addresses the cases of weighted biclusters, which has been underexploited in the literature. The design of BiDots is grounded by a set of analytical tasks derived from previous work. We demonstrate its usefulness and effectiveness for exploring computed biclusters with an investigative document analysis task, in which suspicious people and activities are identified from a text corpus.", "uri": "https://vimeo.com/230830529", "name": "[VIS17 Preview] BiDots: Visual Exploration of Weighted Biclusters (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:13:48+00:00", "description": "Authors: Nan Cao, Chaoguang Lin, Qiuhan Zhu, Yu-Ru Lin, Xian Teng, Xidao Wen\n\nAbstract: The increasing availability of spatiotemporal data continuously collected from various sources provides new opportunities for a timely understanding of the data in their spatial and temporal context. Finding abnormal patterns in such data poses significant challenges. Given that there is often no clear boundary between normal and abnormal patterns, existing solutions are limited in their capacity of identifying anomalies in large, dynamic and heterogeneous data, interpreting anomalies in their multifaceted, spatiotemporal context, and allowing users to provide feedback in the analysis loop. In this work, we introduce a unified visual interactive system and framework, Voila, for interactively detecting anomalies in spatiotemporal data collected from a streaming data source. The system is designed to meet two requirements in real-world applications, i.e., online monitoring and interactivity. We propose a novel tensor-based anomaly analysis algorithm with visualization and interaction design that dynamically produces contextualized, interpretable data summaries and allows for interactively ranking anomalous patterns based on user input. Using the \"smart city\" as an example scenario, we demonstrate the effectiveness of the proposed framework through quantitative evaluation and qualitative case studies.", "uri": "https://vimeo.com/230830500", "name": "[VIS17 Preview] Voila: Visual Anomaly Detection and Monitoring with Streaming Spatiotemporal Data (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:13:37+00:00", "description": "Authors: Jie Liu, Tim Dwyer, Kim Marriott, Jeremy Millar, Annette Haworth\n\nAbstract: The fields of operations research and computer science have long sought to find automatic solver techniques that can find high-quality solutions to difficult real-world optimisation problems. The traditional workflow is to exactly model the problem and then enter this model into a general-purpose \u201cblack-box\u201d solver. In practice, however, many problems cannot be solved completely automatically, but require a \u201chuman-in-the-loop\u201d to iteratively refine the model and give hints to the solver. In this paper, we explore the parallels between this interactive optimisation workflow and the visual analytics sense-making loop. We assert that interactive optimisation is essentially a visual analytics task and propose a problem-solving loop analogous to the sense-making loop. We explore these ideas through an in-depth analysis of a use-case in prostate brachytherapy, an application where interactive optimisation may be able to provide significant assistance to practitioners in creating prostate cancer treatment plans customised to each patient\u2019s tumour characteristics. However, current brachytherapy treatment planning is usually a careful, mostly manual process involving multiple professionals. We developed a prototype interactive optimisation tool for brachytherapy that goes beyond current practice in supporting focal therapy - targeting tumour cells directly rather than simply seeking coverage of the whole prostate gland. We conducted semi-structured interviews, in two stages, with seven radiation oncology professionals in order to establish whether they would prefer to use interactive optimisation for treatment planning and whether such a tool could improve their trust in the novel focal therapy approach and in machine generated solutions to the problem.", "uri": "https://vimeo.com/230830474", "name": "[VIS17 Preview] Understanding the Relationship between Interactive Optimisation and Visual Analytics in the Context of...", "year": "2017", "event": "PREVIEW"}, {"created_time": "2017-08-23T20:13:26+00:00", "description": "Authors: Shixia Liu, Jiannan Xiao, Junlin Liu, Xiting Wang, Jing Wu, Jun Zhu\n\nAbstract: Tree boosting, which combines weak learners (typically decision trees) to generate a strong learner, is a highly effective and widely used machine learning method. However, the development of a high performance tree boosting model is a time-consuming process that requires numerous trial-and-error experiments. To tackle this issue, we have developed a visual diagnosis tool, BOOSTVis, to help experts quickly analyze and diagnose the training process of tree boosting. In particular, we have designed a temporal confusion matrix visualization, and combined it with a t-SNE projection and a tree visualization. These visualization components work together to provide a comprehensive overview of a tree boosting model, and enable an effective diagnosis of an unsatisfactory training process. Two case studies that were conducted on the Otto Group Product Classification Challenge dataset demonstrate that BOOSTVis can provide informative feedback and guidance to improve understanding and diagnosis of tree boosting algorithms. \\", "uri": "https://vimeo.com/230830447", "name": "[VIS17 Preview] Visual Diagnosis of Tree Boosting Methods (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:13:16+00:00", "description": "Authors: Gennady Andrienko, Natalia Andrienko, Georg Fuchs, Jose Manuel Cordero Garcia\n\nAbstract: Clustering of trajectories of moving objects by similarity is an important technique in movement analysis. Existing distance \\ functions assess the similarity between trajectories based on properties of the trajectory points or segments. The properties may \\ include the spatial positions, times, and thematic attributes. There may be a need to focus the analysis on certain parts of trajectories, \\ i.e., points and segments that have particular properties. According to the analysis focus, the analyst may need to cluster trajectories \\ by similarity of their relevant parts only. Throughout the analysis process, the focus may change, and different parts of trajectories may \\ become relevant. We propose an analytical workflow in which interactive filtering tools are used to attach relevance flags to elements of \\ trajectories, clustering is done using a distance function that ignores irrelevant elements, and the resulting clusters are summarized for \\ further analysis. We demonstrate how this workflow can be useful for different analysis tasks in three case studies with real data from \\ the domain of air traffic. We propose a suite of generic techniques and visualization guidelines to support movement data analysis by \\ means of relevance-aware trajectory clustering.", "uri": "https://vimeo.com/230830419", "name": "[VIS17 Preview] Clustering Trajectories by Relevant Parts for Air Traffic Analysis (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:13:03+00:00", "description": "Authors: Nicola Pezzotti, Thomas H\u00f6llt, Jan van Gemert, Boudewijn P, F, Lelieveldt, Elmar Eisemann, Anna Vilanova\n\nAbstract: Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional \\ classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of \\ handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as \\ the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic \\ design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform \\ due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports \\ the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a \\ stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as \\ superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system \\ through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.", "uri": "https://vimeo.com/230830385", "name": "[VIS17 Preview] DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:12:55+00:00", "description": "Authors: Jiazhi Xia, Fenjin Ye, Wei Chen, Yusi Wang, Weifeng Chen, Yuxin Ma, Anthony K, H, Tung\n\nAbstract: Many approaches for analyzing high-dimensional dataset assume that dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to testify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph based representation of latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to x axis and y axis using 1D MDS respectively. Unlike traditional dimensionality reduction methods which preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS (in x axis) and the variation of LTS in structures (in the combination of x axis and y axis). We design and implement a suite of visual toolkits for navigating and reasoning intrinsic structures of a high-dimensional data. Case studies verify the effectiveness of our approach.", "uri": "https://vimeo.com/230830366", "name": "[VIS17 Preview] LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:12:46+00:00", "description": "Authors: Siwei Fu, Hao Dong, Weiwei Cui, Jian Zhao, Huamin Qu\n\nAbstract: Whether and how does family tree structure differ by ancestral traits over generations? This is a fundamental question regarding the structural heterogeneity of family trees for the multi-generational transmission research. However, due to the lack of efficient tools to handle the complex family tree structure spanning over many generations, most previous multi-generational research only focuses on parent-child and three-generational scenarios. Through an iterative design study with social scientists and historians, we develop TreeEvo that assists users to generate and test empirical hypotheses for multi-generational research. TreeEvo summarizes and organizes family trees by structural features in a dynamic manner based on a traditional Sankey diagram. A pixel-based technique is further proposed to compactly encode trees with complex structures in each Sankey Node. Detailed information of trees is accessible through a space-efficient visualization with semantic zooming. Moreover, TreeEvo embeds Multinomial Logit Model (MLM) to examine statistical associations between tree structure and ancestral traits. We demonstrate the effectiveness and usefulness of TreeEvo through an in-depth case-study with domain experts using a real-world dataset (containing 54,128 family trees of 126,196 individuals).", "uri": "https://vimeo.com/230830337", "name": "[VIS17 Preview] How Do Ancestral Traits Shape Family Trees over Generations? (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:12:38+00:00", "description": "Authors: Yen-Ting Kuan, Yu-Shuen Wang, Jung-Hong Chuang\n\nAbstract: We present a visualization system for users to examine real-time strategy games, which have become very popular globally in recent years. Unlike previous systems that focus on showing statistics and build order, our system can depict the most important part -- battles in the games. Specifically, we visualize detailed movements of armies belonging to respective nations on a map and enable users to examine battles from a global view to a local view. In the global view, battles are depicted by curved arrows revealing how the armies enter and escape from the battlefield. By observing the arrows and the height map, users can make sense of offensive and defensive strategies easily. In the local view, units of each type are rendered on the map, and their movements are represented by animation. We also render an attack line between a pair of units if one of them can attack the other to help users analyze the advantages and disadvantages of a particular formation. Accordingly, users can utilize our system to discover statistics, build order, and battles, and learn strategies from games played by professionals.", "uri": "https://vimeo.com/230830320", "name": "[VIS17 Preview] Visualizing Real-Time Strategy Games: The Example of StarCraft II (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:12:29+00:00", "description": "Authors: Isaac Cho, Ryan Wesslen, Alireza Karduni, Sashank Santhanam, Samira Shaikh, Wenwen Dou\n\nAbstract: Anchoring effect is the tendency to focus too heavily on one piece of information when making decisions. In this paper, we present a novel, systematic study and resulting analyses that investigate the effects of anchoring effect on human decision-making using visual analytic systems. Visual analytics interfaces typically contain multiple views that present various aspects of information such as spatial, temporal, and categorical. These views are designed to present complex, heterogeneous data in accessible forms that aid decision-making. However, human decision-making is often hindered by the use of heuristics, or cognitive biases, such as anchoring effect. Anchoring effect can be triggered by the order in which information is presented or the magnitude of information presented. Through carefully designed laboratory experiments, we present evidence of anchoring effect in analysis with visual analytics interfaces when users are primed by representation of different pieces of information. We also describe detailed analyses of users' interaction logs which reveal the impact of anchoring bias on the visual representation preferred and paths of analysis. We discuss implications for future research and possibly detect and alleviate anchoring bias.", "uri": "https://vimeo.com/230830292", "name": "[VIS17 Preview] The Anchoring Effect in Decision-Making with Visual Analytics (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:12:20+00:00", "description": "Authors: Isaac Cho, Ryan Wesslen, Svitlana Volkova, Bill Ribarsky, Wenwen Dou\n\nAbstract: Social media data bear valuable insights regarding events that occur around the world. Events are inherently temporal and spatial. Existing visual text analysis systems have focused on detecting and analyzing past and ongoing events. Few have leveraged social media information to look for events that may occur in the future. In this paper, we present an interactive visual analytic system, CrystalBall, that automatically identifies and ranks future events from Twitter streams. CrystalBall integrates new methods to discover events with interactive visualizations that permit sensemaking of the identified future events. Our computational methods integrate seven different measures to identify and characterize future events, leveraging information regarding time, location, social networks, and the informativeness of the messages. A visual interface is tightly coupled with the computational methods to present a concise summary of the possible future events. A novel connection graph and glyphs are designed to visualize the characteristics of the future events. To demonstrate the efficacy of CrystalBall in identifying future events and supporting interactive analysis, we present multiple case studies on analyzing events derived from Twitter data.", "uri": "https://vimeo.com/230830262", "name": "[VIS17 Preview] CrystalBall: A Visual Analytic System for Future Event Discovery and Analysis from Social Media Data (VAST...", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:12:10+00:00", "description": "Authors: Andrea Julca, Niklas Elmqvist\n\nAbstract: Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a ``visualization gap'' during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.", "uri": "https://vimeo.com/230830233", "name": "[VIS17 Preview] The Interactive Visualization Gap in Initial Exploratory Analysis (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:12:00+00:00", "description": "Authors: Siming Chen, Shuai Chen, Lijing Lin, Xiaoru Yuan, Jie Liang, Xiaolong (Luke) Zhang\n\nAbstract: Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.", "uri": "https://vimeo.com/230830207", "name": "[VIS17 Preview] E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:11:52+00:00", "description": "Authors: Mengchen Liu, Jiaxin Shi, Kelei Cao, Jun Zhu, Shixia Liu\n\nAbstract: Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their \\ training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine \\ learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.", "uri": "https://vimeo.com/230830184", "name": "[VIS17 Preview] Analyzing the Training Processes of Deep Generative Models (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:11:42+00:00", "description": "Authors: Robert Pienta, Fred Hohman, Alex Endert, Acar Tamersoy, Kevin Roundy, Chris Gates, Shamkant Navathe, Duen Horng Chau\n\nAbstract: Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts\u2019 sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results, and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR's ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.", "uri": "https://vimeo.com/230830163", "name": "[VIS17 Preview] VIGOR: Interactive Visual Exploration of Graph Query Results (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:11:32+00:00", "description": "(Best Paper Award)\n\nAuthors: Kanit Wongsuphasawat, Daniel Smilkov, James Wexler, Jimbo Wilson, Dandelion Man\u00e9, Doug Fritz, Dilip Krishnan, Fernanda B, Viegas, Martin Wattenberg\n\nAbstract: We present a design study of the TensorFlow Graph Visualizer, a component in the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enables standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion.  Finally, we detect and highlight repeated structures to convey the modular composition in the model.  To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback.  Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.", "uri": "https://vimeo.com/230830126", "name": "[VIS17 Preview] Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:11:23+00:00", "description": "Authors: Arjun Srinivasan, Hyunwoo Park, Alex Endert, Rahul Basole\n\nAbstract: Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.", "uri": "https://vimeo.com/230830104", "name": "[VIS17 Preview] Graphiti: Interactive Specification of Attribute-based Edges for Network Modeling and Visualization (VAST...", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:11:13+00:00", "description": "Authors: Dustin Arendt, Megan Pirrung\n\nAbstract: Storylines are adept at communicating complex change by encoding time on the x-axis and using the proximity of lines in the y direction to represent interaction between entities. The original definition of a storyline visualization requires data defined in terms of explicit interaction groups. Relaxing this definition allows storyline visualization to be applied more generally, but this creates questions about how the y-coordinate should encode interactions when an this is tied to a particular place or state. To answer this question, we conducted a design study where we considered two layout algorithm design alternatives within a geo-temporal analysis tool written to solve part of the VAST Challenge 2014. We measured the performance of users at overview and detail oriented tasks between two storyline layout algorithms. To the best of our knowledge, this paper is the first work to question the design principles for storyline visualization, and what we found surprised us. For overview tasks with the alternative layout, which has a consistent encoding for the y-coordinate, users performed significantly better (p&lt;.05) than the storyline layout based on existing design constraints and aesthetic criteria. Our empirical findings were also supported by first-hand accounts taken from interviews with multiple expert analysts, who suggested that the inconsistent meaning of the y-axis was misleading. These findings led us to design a new storyline layout algorithm that is a ``best of both'' where the y-axis has a consistent meaning but aesthetic criteria (e.g., line crossings) are considered.", "uri": "https://vimeo.com/230830077", "name": "[VIS17 Preview] The \"y\" of it Matters; Even for Storyline Visualization (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:11:03+00:00", "description": "Authors: Yao Ming, Shaozu CAO, Ruixiang Zhang, Zhen LI, Yuanzhe Chen, Yangqiu Song, Huamin Qu\n\nAbstract: Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain individual hidden states based on its expected response to input texts. We then co-cluster hidden states and words based on the expected response and further visualize co-clustering results as memory cells and word clouds to provide more structured knowledge on RNNs\u2019 hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN\u2019 s hidden state at the sentence level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.", "uri": "https://vimeo.com/230830054", "name": "[VIS17 Preview] Understanding Hidden Memories of Recurrent Neural Networks (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:10:54+00:00", "description": "Authors: Dominik Sacha, Matthias Kraus, J\u00fcrgen Bernard, Michael Behrisch, Tobias Schreck, Yuki Asano, Daniel Keim\n\nAbstract: Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.", "uri": "https://vimeo.com/230830031", "name": "[VIS17 Preview] SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:10:44+00:00", "description": "Authors: Roger Leite, Theresia Gschwandtner, Silvia Miksch, Simone Kriglstein, Margit Pohl, Erich Gstrein, Johannes Kuntner\n\nAbstract: Financial institutions are interested in ensuring security and quality for their customers. Banks, for instance, need to identify and stop harmful transactions in a timely manner. In order to detect fraudulent operations, data mining techniques and customer profile analysis are commonly used. However, these approaches are not supported by Visual Analytics techniques yet. Visual Analytics techniques have potential to considerably enhance the knowledge discovery process and increase the detection and prediction accuracy of financial fraud detection systems. Thus, we propose EVA, a Visual Analytics approach for supporting fraud investigation, fine-tuning fraud detection algorithms, and thus, reducing false positive alarms.", "uri": "https://vimeo.com/230830001", "name": "[VIS17 Preview] EVA: Visual Analytics to Identify Fraudulent Events (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:10:34+00:00", "description": "Authors: Stefan J\u00e4nicke, David Wrisley\n\nAbstract: Textual criticism consists of the identification and analysis of variant readings among different versions of a text. Being a relatively simple task for modern languages, the collation of medieval text traditions ranges from the complex to the virtually impossible depending on the degree of instability of textual transmission. We present a visual analytics environment that supports computationally aligning such complex textual differences typical of orally inflected medieval poetry. For the purpose of analyzing alignment, we provide interactive visualizations for different text hierarchy levels, specifically, a meso reading view to support investigating repetition and variance at the line level across text segments. In addition to outlining important aspects of our interdisciplinary collaboration, we emphasize the utility of the proposed system by various usage scenarios in medieval French literature.", "uri": "https://vimeo.com/230829975", "name": "[VIS17 Preview] Interactive Visual Alignment of Medieval Text Versions (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:10:25+00:00", "description": "Authors: Dominik J\u00e4ckle, Michael Hund, Michael Behrisch, Daniel A, Keim, Tobias Schreck\n\nAbstract: Subspace analysis methods have gained interest for identifying patterns in subspaces of high-dimensional data. Existing techniques allow to visualize and compare patterns in subspaces. However, many subspace analysis methods produce an abundant amount of patterns, which often remain redundant and are difficult to relate. Creating effective layouts for comparison of subspace patterns remains challenging. We introduce Pattern Trails, a novel approach for visually ordering and comparing subspace patterns. Central to our approach is the notion of pattern transitions as an interpretable structure imposed to order and compare patterns between subspaces. The basic idea is to visualize projections of subspaces side-by-side, and indicate changes between adjacent patterns in the subspaces by a linked representation, hence introducing pattern transitions. Our contributions comprise a systematization for how pairs of subspace patterns can be compared, and how changes can be interpreted in terms of pattern transitions. We also contribute a technique for visual subspace analysis based on a data-driven similarity measure between subspace representations. This measure is useful to order the patterns, and interactively group subspaces to reduce redundancy. We demonstrate the usefulness of our approach by application to several use cases, indicating that data can be meaningfully ordered and interpreted in terms of pattern transitions. \\", "uri": "https://vimeo.com/230829949", "name": "[VIS17 Preview] Pattern Trails: Visual Analysis of Pattern Transitions in Subspaces (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:10:16+00:00", "description": "Authors: Xun Zhao, Yanhong Wu, Weiwei Cui, Xinnan Du, Yuan Chen, Yong Wang, Dik-Lun Lee, Huamin Qu\n\nAbstract: Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e., the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.", "uri": "https://vimeo.com/230829927", "name": "[VIS17 Preview] SkyLens: Visual Analysis of Skyline on Multi-dimensional Data (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:10:07+00:00", "description": "Authors: Minsuk Kahng, Pierre Andrews, Aditya Kalro, Duen Horng Chau\n\nAbstract: While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance- and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.", "uri": "https://vimeo.com/230829900", "name": "[VIS17 Preview] ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:09:55+00:00", "description": "Authors: Jun Wang, Klaus Mueller\n\nAbstract: Deriving the exact casual model that governs the relations between variables in a multidimensional dataset is difficult in practice. It is because causal inference algorithms by themselves typically cannot encode an adequate amount of domain knowledge to break all ties. To that end, visual analytic approaches are considered a feasible alternative to fully automated methods. However, when applying this visual causality analysis in real-world scenarios many practical issues need to be solved. This paper focuses on these practical aspects of visual causality analysis. The most imperative of these aspects is posed by Simpson\u2019 Paradox. It implies the existence of multiple causal models differing in both structure and parameter depending on how the data is subdivided. We propose a comprehensive interface that engages human experts in identifying these subdivisions and allowing them to establish the corresponding causal models via a rich set of interactive facilities. Other features of our interface include: (1) a new causal network visualization that emphasizes the flows of causal dependencies, (2) a model scoring mechanism with visual hints for interactive model refinement, and (3) flexible approaches for handling heterogeneous \\ data. We demonstrate our framework with several real-world datasets from a diverse set of domains.", "uri": "https://vimeo.com/230829883", "name": "[VIS17 Preview] Visual Causality Analysis Made Practical (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:09:44+00:00", "description": "Authors: Andrea Unger, Nadine Dr\u00e4ger, Mike Sips, Dirk Lehmann\n\nAbstract: This design study focuses on the analysis of a time sequence of categorical sequences. Such data is relevant for the geoscientific research field of landscape and climate development. It results from microscopic analysis of lake sediment cores. The goal is to gain hypotheses about landscape evolution and climate conditions in the past. To this end, geoscientists identify which categorical sequences are similar in the sense that they indicate similar conditions. Categorical sequences are similar if they have similar meaning (semantic similarity) and appear in similar time periods (temporal similarity). \\ For data sets with many different categorical sequences, the task to identify similar sequences becomes a challenge. Our contribution is a tailored visual analysis concept that effectively supports the analytical process. Our visual interface comprises coupled visualizations of semantics and temporal context for the exploration and assessment of the similarity of categorical sequences. Integrated automatic methods reduce the analytical effort substantially. They (1) extract unique sequences in the data and (2) rank sequences by a similarity measure during the search for similar sequences. We evaluated our concept by demonstrations of our prototype to a larger audience and hands-on analysis sessions for two different lakes. According to geoscientists, our approach fills an important methodological gap in the application domain.", "uri": "https://vimeo.com/230829855", "name": "[VIS17 Preview] Understanding a sequence of sequences: Visual exploration of categorical states in lake sediment cores (VAST...", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:09:34+00:00", "description": "Authors: Nan-Chen Chen, Been Kim\n\nAbstract: ping sophisticated artificial intelligence (AI) systems requires AI researchers to experiment with different designs and analyze results from evaluations (we refer this task as evaluation analysis). In this paper, we tackle the challenges of evaluation analysis in the domain of question-answering (QA) systems. Through in-depth studies with QA researchers, we identify tasks and goals of evaluation analysis and derive a set of design rationales, based on which we propose a novel approach termed prismatic analysis. Prismatic analysis examines data through multiple ways of categorization (referred as angles). Categories in each angle are measured by aggregate metrics to enable diverse comparison scenarios.  \\  \\ To facilitate prismatic analysis of QA evaluations, we design and implement the Question Space Anglyzer (QSAnglyzer), a visual analytics (VA) tool. In QSAnglyzer, the high-dimensional space formed by questions is divided into categories based on several angles (e.g., topic and question type). Each category is aggregated by accuracy, the number of questions, and accuracy variance across evaluations. QSAnglyzer visualizes these angles so that QA researchers can examine and compare evaluations from various aspects both individually and collectively. Furthermore, QA researchers filter questions based on any angle by clicking to construct complex queries. We validate QSAnglyzer through controlled experiments and by expert reviews. The results indicate that when using QSAnglyzer, users perform analysis tasks faster (p &lt; 0.01) and more accurately (p &lt; 0.05), and are quick to gain new insight. We discuss how prismatic analysis and QSAnglyzer scaffold evaluation analysis, and provide directions for future research.", "uri": "https://vimeo.com/230829824", "name": "[VIS17 Preview] QSAnglyzer: Visual Analytics for Prismatic Analysis of Question Answering System Evaluations (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T20:09:25+00:00", "description": "Authors: Leland Wilkinson\n\nAbstract: Visualizing outliers in massive datasets requires statistical pre-processing in order to reduce the scale of the problem to a size amenable to rendering systems like D3, Plotly or analytic systems like R or SAS. This paper presents a new algorithm, called hdoutliers, for detecting multidimensional outliers. It is unique for a) dealing with a mixture of categorical and continuous variables, b) dealing with big-p (many columns of data), c) dealing with big-n (many rows of data), d) dealing with outliers that mask other outliers, and e) dealing consistently with unidimensional and multidimensional datasets. Unlike ad hoc methods found in many machine learning papers, hdoutliers is based on a distributional model that allows outliers to be tagged with a probability. This critical feature reduces the likelihood of false discoveries.", "uri": "https://vimeo.com/230829804", "name": "[VIS17 Preview] Visualizing Big Data Outliers through Distributed Aggregation (VAST Paper)", "year": "2017", "event": "VAST, PREVIEW"}, {"created_time": "2017-08-23T19:56:28+00:00", "description": "VIS Arts Program\n\nAuthors: \n\nAbstract:", "uri": "https://vimeo.com/230827929", "name": "[VIS17 Preview] VIS Art Program (VisAP Paper)", "year": "2017", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2017-08-23T19:56:15+00:00", "description": "VIS Arts Program\n\nAuthors: Yoon Chung Han, Praful Surve, Subin Kim, Josh Cuellar\n\nAbstract: This paper introduces our project Causes and Effects, which visualizes California sea lion unusual mortality events (UME) to create a new layer of understanding of the situation as an important environmental issue. It examines the causes of and impacts on sea lion UME by controlling multivariate factors that impact sea lions\u2019 health and stranding. Previous visualizations for sea lion mortality only captured temporal data and the relationship between causes and effects using simple graphs. However, sea lion UME results from multiple causes and it requires multivariate visualization to establish clear solutions for future results. The resulting images of our visualization are not only visually appealing but also allow users to explore how environmental factors impact the lives and situations of sea lions. The visualization using connected circles addresses important issues related the nature\u2019s never-ending cycles and infinite continuations in many aspects.", "uri": "https://vimeo.com/230827900", "name": "[VIS17 Preview] Visualizing Causes and Effects of California Sea Lion Unusual Mortality Event (UME) (VisAP Paper)", "year": "2017", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2017-08-23T19:56:02+00:00", "description": "VIS Arts Program\n\nAuthors: Manuela Garreton, Karina Hyland, Denis Parra\n\nAbstract: Recent studies in cognitive neuroscience have discovered a complex neural network that activates when not performing a task. Every moment when the mind wanders and an inner conversation takes place, a series of brain regions work together to achieve some very important mental processes. These regions conform the default mode network (DMN), and its study has become critical for understanding how consciousness operates. With the intention to introduce this novel scientific finding to a non-expert audience and motivated by related works that combine science and art we designed and implemented Historias por Default: an interactive and immersive experience on the web. Using features of web interactive documentaries, we present the main characteristics and relevance of the DMN in a language that can be understood by almost anyone. This website was published and available online for a month, where we collected navigation data (n=98) and analyzed the interaction between users and the interface. After this study, we were able to detect patterns on the interactions and intersect the findings with how the audience perceives and understands the presented subject. This work will allow us to redesign a second phase of Historias por Default and also will serve as a starting point for new projects that present complex scientific research to non-expert public, by using interactive web experiences.", "uri": "https://vimeo.com/230827867", "name": "[VIS17 Preview] Understanding People's interaction with Neural Sci-Art (VisAP Paper)", "year": "2017", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2017-08-23T19:55:50+00:00", "description": "VIS Arts Program\n\nAuthors: Clarissa Ribeiro, Mick Lorusso\n\nAbstract: Conceived as a peaceful and playful exploration of the interstellar space, this augmented reality and sound installation invites the audience to access the experiential dimension of space technologies and how the huge amount of data derived from space exploration can be accessed, appropriated, integrated into the artist\u2019s poetics and experienced by the audience. The audience is invited to walk through a softly illuminated room where a few transparent cables come from the ceiling having small augmented reality markers in its extremities. Holding an ipad mini while exploring the space, one will find him/herself immersed in a soundscape populated with 3D models derived from actual micro scale images. The 3D models were generated via parametric design strategies from NASA Stardust Discovery-class mission's database images of aerogel samples which have captured cosmic dust particles. The soundscape, or the soundtrack for navigating this Augmented Reality interstellar space, is a composition using a combination of sounds derived from images of identified stardust particles in nano scale from the same NASA mission. The installation is a tribute to H\u00e9lio Oiticica\u2019s radical series of red and yellow \u2018Spatial Reliefs\u2019 (1960).", "uri": "https://vimeo.com/230827836", "name": "[VIS17 Preview] Spatial Reliefs: Cross-Scale Space-Scapes (VisAP Paper)", "year": "2017", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2017-08-23T19:55:39+00:00", "description": "VIS Arts Program\n\nAuthors: Jennifer Weiler, Kat Fowler\n\nAbstract: There has been growing interest in using artistic approaches to communicate scientific data. Most of this work has also taken advantage of new technology by incorporating digital and interactive media to convey complex or abstract concepts. In our work, we use these tools, data analysis and 3D printing, to explore means of representing genetic information using traditional art materials. From variations in DNA, we generate 3D models and used them as a basis for creating both a bronze sculpture and a low cost, tactilely-interactive piece. Looking forward, we are interested in understanding how viewers interpret data displays differently based on the type of materials used to construct the visualizations.", "uri": "https://vimeo.com/230827801", "name": "[VIS17 Preview] 3D Visualization of Genetic Networks Using Diverse Art Materials (VisAP Paper)", "year": "2017", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2017-08-23T19:55:17+00:00", "description": "VIS Arts Program\n\nAuthors: Ben Rydal Shapiro, Francis A. Pearman\n\nAbstract: This paper adapts and uses a dynamic visualization environment I have developed and call the Interaction Geography Slicer (IGS) to visualize data about New York City\u2019s Stop &amp; Frisk Program. Findings and discussion focus on how this tool provides new ways to view, interact with and query large-scale data sets over space and through time to support analysis of and public discussion about New York City\u2019s Stop &amp; Frisk Program.", "uri": "https://vimeo.com/230827740", "name": "[VIS17 Preview] Using the Interaction Geography Slicer to Visualize New York City Stop &amp; Frisk (VisAP Paper)", "year": "2017", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2017-08-23T19:00:14+00:00", "description": "Authors: Cagatay Turkay, Erdem Kaya, Selim Balcisoy, and Helwig Hauser", "uri": "https://vimeo.com/230819252", "name": "VAST 2016: Designing Progressive and Interactive Analytics Processes for High-Dimensional Data Analysis", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-23T18:47:51+00:00", "description": "Authors: R. Jordan Crouser, Lyndsey Franklin, Alex Endert, and Kris Cook", "uri": "https://vimeo.com/230817334", "name": "VAST 2016: Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-23T18:44:34+00:00", "description": "Authors: Davide Ceneda, Theresia Gschwandtner, Thorsten May, Silvia Miksch, Hans-J\u00f6rg Schulz, Marc Streit, and Christian Tominski", "uri": "https://vimeo.com/230816831", "name": "VAST 2016: Characterizing Guidance in Visual Analytics", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-23T18:05:15+00:00", "description": "Authors: Cristian Felix, Anshul Vikram Pandey, and Enrico Bertini", "uri": "https://vimeo.com/230810847", "name": "VAST 2016: TextTile: An Interactive Visualization Tool for Seamless Exploratory Analysis of Structured Data and Unstructured Tex", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-23T17:58:02+00:00", "description": "Authors: Minjeong Kim, Kyeongpil Kang, Deokgun Park, Jaegul Choo, and Niklas Elmqvist", "uri": "https://vimeo.com/230809852", "name": "VAST 2016: TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale Document Collections", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-23T17:52:52+00:00", "description": "Authors: Qiaomu Shen, Tongshuang Wu, Haiyan Yang, Yanhong Wu, Huamin Qu, and Weiwei Cui", "uri": "https://vimeo.com/230809077", "name": "VAST 2016: NameClarifier: A Visual Analytics System for Author Name Disambiguation", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-23T17:44:14+00:00", "description": "VIS Arts Program\n\nAuthors: Ozge Samanci, Adam Snyder\n\nAbstract: This paper explores the interactive art installation, Fiber Optic Ocean, portraying the consequences of technology\u2019s invasion of oceans. Three life-size shark skeletons are trapped in an ocean made of fiber optic threads. This 20-foot-by-20-foot installation combines fiber optics with sculptural elements and data-driven light and sound design. Data visualization and sonification emerge from the use of two data sets: human data (tweets per second) and shark data (speed of live sharks tagged with GPS). This paper examines the affordances and constraints of design with fiber optic threads. Using fiber optics in this media context expands the expressive opportunities for artists and creates a platform for representation of data.", "uri": "https://vimeo.com/230807894", "name": "[VIS17 Preview] Fiber Optic Ocean: Merging Media for Data Representation (VisAP Paper)", "year": "2017", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2017-08-23T17:43:07+00:00", "description": "VIS Arts Program\n\nAuthors: Pedro Cruz\n\nAbstract: Wage inequality in Portugal has been systematically around 27%. This paper describes a set of animated cartograms that show this inequality for several years. The visualization was designed for an exhibition setting, and instead of presenting the final cartograms, it shows the cartogram formation for each year as a way of reiterating the message. The model used is a modification of Dorling cartograms that visually appears as a set of contiguous amalgams of total wages earned by subregion. This aspect confers an organic tone to the artifact, as if liquids of different densities were being poured into the canvas without mixing, and depicting a tension-based behavior where amalgams of men and women push among themselves to compete for the same space.", "uri": "https://vimeo.com/230807737", "name": "[VIS17 Preview] Adapted Dorling cartogram on wage inequality in Portugal (VisAP Paper)", "year": "2017", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2017-08-22T22:03:59+00:00", "description": "Authors: Ji Hwan Park, Saad Nadeem, Seyedkoosha Mirhosseini, Arie Kaufman", "uri": "https://vimeo.com/230677597", "name": "VAST 2016: C2A: Crowd Consensus Analytics for Virtual Colonoscopy", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T21:59:02+00:00", "description": "Authors: Michael Glueck, Alina Gvozdik, Fanny Chevalier, Azam Khan, Michael Brudno, and Daniel Wigdor", "uri": "https://vimeo.com/230677050", "name": "VAST 2016: PhenoStacks: Cross-Sectional Cohort Phenotype Comparison Visualizations", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T21:51:31+00:00", "description": "Authors: Xinsong Yang, Lei Shi, Madelaine Daianu, Hanghang Tong, Qingsong Liu, and Paul Thompson", "uri": "https://vimeo.com/230676204", "name": "VAST 2016: Blockwise Human Brain Network Visual Comparison Using NodeTrix Representation", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T21:43:46+00:00", "description": "Authors: Ievgeniia Gutenko, Konstantin Dmitriev, Arie E. Kaufman, and Matthew A. Barish", "uri": "https://vimeo.com/230675191", "name": "VAST 2016: AnaFe: Visual Analytics of Image-derived Temporal Features \u2013 Focusing on the Spleen", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T19:37:53+00:00", "description": "Authors: Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, and Shixia Liu", "uri": "https://vimeo.com/230657814", "name": "VAST 2016: Towards Better Analysis of Deep Convolutional Neural Networks", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T19:30:22+00:00", "description": "Authors: Junpeng Wang, Xiaotong Liu, Han-Wei Shen, and Guang Lin", "uri": "https://vimeo.com/230656777", "name": "VAST 2016: Multi-Resolution Climate Ensemble Parameter Analysis with Nested Parallel Coordinates Plots", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T19:05:54+00:00", "description": "Authors: Gary K. L. Tam, Vivek Kothari, and Min Chen", "uri": "https://vimeo.com/230653323", "name": "VAST 2016: An Analysis of Machine- and Human-Analytics in Classification", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T19:00:41+00:00", "description": "Authors: Donghao Ren, Saleema Amershi, Bongshin Lee, Jina Suh, and Jason D. Williams", "uri": "https://vimeo.com/230652564", "name": "VAST 2016: Squares: Supporting Interactive Performance Analysis for Multiclass Classifiers", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T18:56:26+00:00", "description": "Authors: Sriram Karthik Badam, Fereshteh Amini, Niklas Elmqvist, Pourang Irani", "uri": "https://vimeo.com/230651943", "name": "VAST 2016: Supporting Visual Exploration for Multiple Users in Large Display Environments", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T18:51:17+00:00", "description": "Authors: Cong Xie, Wen Zhong, and Klaus Mueller", "uri": "https://vimeo.com/230651186", "name": "VAST 2016: A Visual Analytics Approach for Categorical Joint Distribution Reconstruction from Marginal Projections", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T18:45:05+00:00", "description": "Authors: Filip Dabek and Jesus J Caban", "uri": "https://vimeo.com/230650327", "name": "VAST 2016: A Grammar-based Approach for Modeling User Interactions and Generating Suggestions During the Data Exploration Proces", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T18:39:13+00:00", "description": "Authors: Michael Behrisch, Benjamin Bach, Michael Hund, Michael Delz, Laura von R\u00fcden, Jean-Daniel Fekete, and Tobias Schreck", "uri": "https://vimeo.com/230649535", "name": "VAST 2016: Magnostics: Image-based Search of Interesting Matrix Views for Guided Network Exploration", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T18:11:23+00:00", "description": "Authors: Ali Sarvghad, Melanie Tory, and Narges Mahyar", "uri": "https://vimeo.com/230645579", "name": "VAST 2016: Visualizing Dimension Coverage to Support Exploratory Analysis", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T17:39:46+00:00", "description": "Authors: Shamal AL-Dohuki, Farah Kamw, Ye Zhao, Chao Ma, Yingyu Wu, Jing Yang, Xinyue Ye, Fei Wang, Xin Li, and Wei Chen", "uri": "https://vimeo.com/230640931", "name": "VAST 2016: SemanticTraj: A New Approach to Interacting with Massive Taxi Trajectories", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-22T17:27:18+00:00", "description": "Authors: Dongyu Liu, Di Weng, Yuhong Li, Jie Bao, Yu Zheng, Huamin Qu, and Yingcai Wu", "uri": "https://vimeo.com/230639208", "name": "VAST 2016: SmartAdP: Visual Analytics of Large-scale Taxi Trajectories for Selecting Billboard Locations", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T21:07:00+00:00", "description": "Authors: Yuanzhe Chen, Qing Chen, Mingqian Zhao, Sebastien Boyer, Kalyan Veeramachaneni, Huamin Qu", "uri": "https://vimeo.com/230212191", "name": "VAST 2016: DropoutSeer: Visualizing Learning Patterns in Massive Open Online Courses for Dropout Reasoning and Prediction", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T21:01:04+00:00", "description": "Authors: Quan Li, Peng Xu, Yeuk Yin Chan, Yun Wang, Zhipeng Wang, Huamin Qu, and Xiaojuan Ma", "uri": "https://vimeo.com/230211483", "name": "VAST 2016: [TVCG] A Visual Analytics Approach for Understanding Reasons behind Snowballing and Comeback in MOBA Games", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T20:55:27+00:00", "description": "Authors: Siwei Fu, Jian Zhao, Weiwei Cui, and Huamin Qu", "uri": "https://vimeo.com/230210778", "name": "VAST 2016: Visual Analysis of MOOC Forums with iForum", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T20:51:01+00:00", "description": "Authors: Johannes Weissenb\u00f6ck, Artem Amirkhanov, Eduard Gr\u00f6ller, Johann Kastner, Christoph Heinzl", "uri": "https://vimeo.com/230210178", "name": "VAST 2016: PorosityAnalyzer: Visual Analysis and Evaluation of Segmentation Pipelines to Determine the Porosity in Fiber-Reinfor", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T20:45:07+00:00", "description": "Authors: Phong Nguyen, Kai Xu, Andy Bardill, Betul Salman, Kate Herd, William Wong", "uri": "https://vimeo.com/230209413", "name": "VAST 2016: SenseMap: Supporting Browser-based Online Sensemaking through Analytic Provenance", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T20:35:31+00:00", "description": "Authors: Sarah Goodwin, Christopher Mears, Tim Dwyer, Maria Garcia de la Banda, Guido Tack, and Mark Wallace", "uri": "https://vimeo.com/230208245", "name": "VAST 2016: What do Constraint Programming Users Want to See? Exploring the role of Visualisation in Profiling of Models and Sear", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T20:21:51+00:00", "description": "Authors: Aritra Dasgupta, Joon-Yong Lee, Ryan Wilson, Robert A. Lafrance, Nick Cramer, Kristin Cook, and Samuel Payne", "uri": "https://vimeo.com/230206654", "name": "VAST 2016: Familiarity Vs Trust: A Comparative Study of Domain Scientists\u2019 Trust in Visual Analytics and Conventional Analysis M", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T20:10:35+00:00", "description": "Authors: Jian Zhao, Michael Glueck, Simon Breslav, Fanny Chevalier, and Azam Khan", "uri": "https://vimeo.com/230205309", "name": "VAST 2016: [TVCG] Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data based on User-Authored Annotations", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T19:59:40+00:00", "description": "Authors: Jing Xia, Wei Chen, Yumeng Hou, Wanqi Hu, Xinxin Huang, David Ebert", "uri": "https://vimeo.com/230203961", "name": "VAST 2016: DimScanner: A Relation-based Visual Exploration Approach Towards Data Dimension Inspection", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T19:53:38+00:00", "description": "Authors: Bowen Yu and Cl\u00e1udio T. Silva", "uri": "https://vimeo.com/230203248", "name": "VAST 2016: VisFlow - Web-based Visualization Framework for Tabular Data with a Subset Flow Model", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T19:40:38+00:00", "description": "Authors: Po-Ming Law, Wenchao Wu, Yixian Zheng, and Huamin Qu", "uri": "https://vimeo.com/230201637", "name": "VAST 2016: VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T19:34:43+00:00", "description": "Authors: Bum Chul Kwon, Hannah Kim, Emily Wall, Jaegul Choo, Haesun Park, and Alex Endert", "uri": "https://vimeo.com/230200904", "name": "VAST 2016: AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T00:30:06+00:00", "description": "Authors: Xiaotong Liu, Anbang Xu, Liang Gou, Haibin Liu, Rama Akkiraju, Han-Wei Shen", "uri": "https://vimeo.com/230087434", "name": "VAST 2016: SocialBrands: Visual Analysis of Public Perceptions of Brands on Social Media", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T00:24:01+00:00", "description": "Authors: Fan Du, Catherine Plaisant, Neil Spring, Ben Shneiderman", "uri": "https://vimeo.com/230086946", "name": "VAST 2016: EventAction: Visual Analytics for Temporal Event Sequence Recommendation", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T00:19:42+00:00", "description": "Authors: Xiting Wang, Shixia Liu, Yang Chen, Tai-Quan Peng, Jing Su, Jing Yang, Baining Guo", "uri": "https://vimeo.com/230086609", "name": "VAST 2016: How Ideas Flow across Multiple Social Groups", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-18T00:04:10+00:00", "description": "Authors: Siming Chen, Shuai Chen, Zhenhuang Wang, Jie Liang, Xiaoru Yuan, Nan Cao, Yadong Wu", "uri": "https://vimeo.com/230085356", "name": "VAST 2016: D-Map: Visual Analysis of Ego-centric Information Diffusion Patterns in Social Media", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-17T23:50:58+00:00", "description": "Authors: Tanja Blascheck, Fabian Beck, Sebastian Baltes, Thomas Ertl, Daniel Weiskopf", "uri": "https://vimeo.com/230084249", "name": "VAST 2016: Visual Analysis and Coding of Data-Rich User Behavior", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-17T23:40:01+00:00", "description": "Authors: Michael Correll, Michael Gleicher", "uri": "https://vimeo.com/230083201", "name": "VAST 2016: The Semantics of Sketch: A Visual Query System for Time Series Data", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-17T23:33:17+00:00", "description": "Authors: Prithiviraj Muthumanickam, Katerina Vrotsou, Matthew Cooper, Jimmy Johansson", "uri": "https://vimeo.com/230082525", "name": "VAST 2016: Shape Grammar Extraction for Efficient Query-by-Sketch Pattern Matching in Long Time Series", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-17T22:43:18+00:00", "description": "Authors: Panpan Xu, Honghui Mei, Liu Ren, and Wei Chen", "uri": "https://vimeo.com/230078065", "name": "VAST 2016: ViDX: Visual Diagnostics of Assembly Line Performance in Smart Factories", "year": "2017", "event": "VAST"}, {"created_time": "2017-08-15T21:27:57+00:00", "description": "Authors: Tatiana von Landesberger, Dennis Basgier, Meike Becker", "uri": "https://vimeo.com/229769317", "name": "SciVis 2016: [TVCG INVITED] Comparative Local Quality Assessment of 3D Medical Image Segmentations with Focus on Statistical...", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-15T21:18:13+00:00", "description": "Authors: Monique Meuschke, Samuel Voss, Oliver Beuing, Bernhard Preim, and Kai Lawonn", "uri": "https://vimeo.com/229768236", "name": "SciVis 2016: [TVCG] Combined Visualization of Vessel Deformation and Hemodynamics in Cerebral Aneurysms", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-15T21:08:04+00:00", "description": "Authors: Saad Nadeem, Joseph Marino, Xianfeng Gu, and Arie Kaufman", "uri": "https://vimeo.com/229767048", "name": "SciVis 2016: [TVCG] Corresponding Supine and Prone Colon Visualization Using Eigenfunction Analysis and Fold Modeling", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-15T20:52:47+00:00", "description": "Authors: Noeska Smit, Kai Lawonn, Annelot Kraima, Marco DeRuiter, Hessam Sokooti, Stefan Bruckner, Elmar Eisemann, and Anna Vilanova", "uri": "https://vimeo.com/229765188", "name": "SciVis 2016: [TVCG] PelVis: Atlas-based Surgical Planning for Oncological Pelvic Surgery", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-14T22:43:57+00:00", "description": "Authors: Johannes Zagermann, Ulrike Pfeil, and Harald Reiterer", "uri": "https://vimeo.com/229637826", "name": "BELIV 2016: Measuring Cognitive Load using Eye Tracking Technology in Visual Computing", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-14T22:42:22+00:00", "description": "Authors: Prithiviraj Muthumanickam, Camilla Forsell, Katerina Vrotsou, Jimmy Johansson, and Matthew Cooper", "uri": "https://vimeo.com/229637669", "name": "BELIV 2016: Supporting Exploration of Eye Tracking Data: Identifying Changing Behavior Over Long Durations", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-14T22:41:06+00:00", "description": "Authors: Paolo Federico, Albert Amor-Amoros, and Silvia Miksch", "uri": "https://vimeo.com/229637552", "name": "BELIV 2016: A Nested Workflow Model for Visual Analytics Design and Validation", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-14T22:39:29+00:00", "description": "Authors: Eva Mayr, G\u00fcnther Schreder, Michael Smuc, and Florian Windhager", "uri": "https://vimeo.com/229637392", "name": "BELIV 2016: Looking at the Representations in our Mind: Measuring Mental Models of Information Visualizations", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-14T22:37:11+00:00", "description": "Authors: Adil Yalcin, Niklas Elmqvist, and Ben Bederson", "uri": "https://vimeo.com/229637162", "name": "BELIV 2016: Cognitive Stages in Visual Data Exploration", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-14T22:35:05+00:00", "description": "Authors: Omar ElTayeby and Wenwen Dou", "uri": "https://vimeo.com/229636976", "name": "BELIV 2016: A Survey on Interaction Log Analysis for Evaluating Exploratory Visualizations", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-14T22:32:22+00:00", "description": "Authors: Laura McNamara, Travis Bauer, Laura Matzen, and Michael Haass", "uri": "https://vimeo.com/229636689", "name": "BELIV 2016: Information Theoretic Measures for Visual Analytics: The Silver Ticket?", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-14T22:29:25+00:00", "description": "Authors: Zening Qu and Jessica Hullman", "uri": "https://vimeo.com/229636412", "name": "BELIV 2016: Evaluating Visualization Sets: Trade-offs Between Local Effectiveness and Global Consistency", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-14T22:27:46+00:00", "description": "Authors: Heli V\u00e4\u00e4t\u00e4j\u00e4, Jari Varsaluoma, Tomi Heimonen, Katariina Tiitinen, Jaakko Hakulinen, Markku Turunen, and Harri Nieminen", "uri": "https://vimeo.com/229636257", "name": "BELIV 2016: Information Visualization Heuristics in Practical Expert Evaluation", "year": "2017", "event": "BELIV"}, {"created_time": "2017-08-11T22:37:34+00:00", "description": "Authors: Kui Wu, Aaron Knoll, Benjamin J Isaac, Hamish Carr, and Valerio Pascucci", "uri": "https://vimeo.com/229344348", "name": "SciVis 2016: [TVCG] Direct Multifield Volume Ray Casting of Fiber Surfaces", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-11T22:25:00+00:00", "description": "Authors: I Wald, GP Johnson, J Amstutz, C Brownlee, A Knoll, J Jeffers, J G\u00fcnther, and P Navratil", "uri": "https://vimeo.com/229343112", "name": "SciVis 2016: [TVCG] OSPRay \u2013 A CPU Ray Tracing Framework for Scientific Visualization", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-11T21:42:31+00:00", "description": "Authors: Jens Schneider and Peter Rautek", "uri": "https://vimeo.com/229338755", "name": "SciVis 2016: [TVCG] A Versatile and Efficient GPU Data Structure for Spatial Indexing", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-11T21:20:29+00:00", "description": "Authors: Daniel J\u00f6nsson and Anders Ynnerman", "uri": "https://vimeo.com/229336418", "name": "SciVis 2016: [TVCG] Correlated Photon Mapping for Interactive Global Illumination of Time-Varying Volumetric Data", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-11T21:06:52+00:00", "description": "Authors: Mona Hosseinkhani Loorak, Charles Perin, Christopher Collins, and Sheelagh Carpendale", "uri": "https://vimeo.com/229334898", "name": "InfoVis 2016: [TVCG] Exploring the Possibilities of Embedding Heterogeneous Data Attributes in Familiar Visualizations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-11T20:49:12+00:00", "description": "Authors: Michail Schwab, Hendrik Strobelt, James Tompkin, Colin Fredericks, Connor Huff, Dana Higgins, Anton Strezhnev, Mayya Komisarchik, Gary King, and Hanspeter Pfister", "uri": "https://vimeo.com/229332804", "name": "InfoVis 2016: [TVCG] booc.io: An Education System with Hierarchical Concept Maps and Dynamic Non-linear Learning Plans", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-11T20:35:19+00:00", "description": "Authors: Shiqing He and Eytan Adar", "uri": "https://vimeo.com/229331056", "name": "InfoVis 2016: [TVCG] VIZITCARDS: A Card-Based Toolkit for Infovis Design Education", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-11T20:19:11+00:00", "description": "Authors: Sukwon Lee, Sung-Hee Kim, and Bum Chul Kwon", "uri": "https://vimeo.com/229329002", "name": "InfoVis 2016: [TVCG] VLAT: Development of a Visualization Literacy Assessment Test", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-09T05:06:07+00:00", "description": "Authors: Bilal Alsallakh and Liu Ren", "uri": "https://vimeo.com/228921816", "name": "InfoVis 2016: [TVCG] PowerSet: A Comprehensive Visualization of Set Intersections", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-09T04:48:44+00:00", "description": "Authors: Mi Feng, Cheng Deng, Evan M. Peck, and Lane Harrison", "uri": "https://vimeo.com/228920728", "name": "InfoVis 2016: [TVCG] HindSight: Encouraging Exploration through Direct Encoding of Personal Interaction History", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-09T04:33:00+00:00", "description": "Authors: Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer", "uri": "https://vimeo.com/228919603", "name": "InfoVis 2016: [TVCG] Vega-Lite: A Grammar of Interactive Graphics", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-09T04:14:43+00:00", "description": "Authors: Bahador Saket, Hannah Kim, Eli T. Brown, and Alex Endert", "uri": "https://vimeo.com/228918233", "name": "InfoVis 2016: [TVCG] Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-09T03:59:27+00:00", "description": "Pierre Dragicevic, Ronald Rensink, Jessica Hullman, Matthew Kay, Steve Haroz (Organizer)\n\nPlease visit http://steveharoz.com/publications/vis2016-panel/ for a writeup and copy of the slides.", "uri": "https://vimeo.com/228917292", "name": "VIS 2016: [Panel] How can we improve empirical research on understanding visual information?", "year": "2017", "event": "PANEL"}, {"created_time": "2017-08-09T02:51:49+00:00", "description": "Mike Kirby, David Laidlaw, Klaus Mueller, Han-Wei Shen, Anders Ynnerman, Bob Laramee (Organizer)", "uri": "https://vimeo.com/228912336", "name": "VIS 2016: [Panel] On the Death of Scientific Visualization", "year": "2017", "event": "PANEL"}, {"created_time": "2017-08-08T23:00:36+00:00", "description": "Authors: Joseph Cottam, Andrew Lumsdaine", "uri": "https://vimeo.com/228892755", "name": "VDS 2016: Data Shading: Building Data Models from Visualizations", "year": "2017", "event": "VDS"}, {"created_time": "2017-08-08T22:43:05+00:00", "description": "Speaker: Patrick Lucey", "uri": "https://vimeo.com/228891166", "name": "VDS 2016: [Keynote] Interactive Sports Analytics: Going Beyond Spreadsheets", "year": "2017", "event": "KEYNOTE"}, {"created_time": "2017-08-08T21:33:25+00:00", "description": "Authors: Chad Steed, Ryan Dehoff, William Halsey, Sean Yoder, Vincent Paquit, Sarah Powers", "uri": "https://vimeo.com/228883397", "name": "VDS 2016: Advancing Additive Manufacturing Through Visual Data Science", "year": "2017", "event": "VDS"}, {"created_time": "2017-08-08T21:09:25+00:00", "description": "Authors: Josua Krause, Aritra Dasgupta, Enrico Bertini", "uri": "https://vimeo.com/228880369", "name": "VDS 2016: Explanatory Visual Analytics for Enhancing Human Interpretability of Machine Learning Models", "year": "2017", "event": "VDS"}, {"created_time": "2017-08-08T20:22:23+00:00", "description": "Keynote Speaker: Sarah Williams", "uri": "https://vimeo.com/228873817", "name": "VDS 2016: [Keynote] Big Data For A Public Good", "year": "2017", "event": "KEYNOTE"}, {"created_time": "2017-08-04T19:05:56+00:00", "description": "Authors: Matthew Berger, Katherine McDonough, and Lee M. Seversky", "uri": "https://vimeo.com/228413442", "name": "InfoVis 2016: [TVCG] cite2vec: Citation-Driven Document Exploration via Word Embeddings", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-04T18:47:04+00:00", "description": "Authors: Zhe Wang, Nivan Ferreira, Youhao Wei, Aarthy Sankari Bhaskar, and Carlos Scheidegger", "uri": "https://vimeo.com/228411100", "name": "InfoVis 2016: [TVCG] Gaussian Cubes: Real-Time Modeling for Visual Exploration of Large Multidimensional Datasets", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-04T18:31:50+00:00", "description": "Authors: C\u00edcero A. L. Pahins, Sean A. Stephens, Carlos Scheidegger, and Jo\u00e3o L. D. Comba", "uri": "https://vimeo.com/228409205", "name": "InfoVis 2016: [TVCG] Hashedcubes: Simple, Low Memory, Real-Time Visual Exploration of Big Data", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-04T07:19:44+00:00", "description": "Authors: Holger Stitz, Samuel Gratz, Wolfang Aigner, Marc Streit", "uri": "https://vimeo.com/228335709", "name": "InfoVis 2016: [TVCG INVITED] ThermalPlot: Visualizing Multi-Attribute Time-Series Data Using a Thermal Metaphor", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-04T07:07:12+00:00", "description": "Authors: Michael Correll and Jeffrey Heer", "uri": "https://vimeo.com/228334237", "name": "InfoVis 2016: [TVCG] Surprise! Bayesian Weighting for De-Biasing Thematic Maps", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-04T06:46:50+00:00", "description": "Authors: Clemens Arbesser, Florian Spechtenhauser, Thomas M\u00fchlbacher, and Harald Piringer", "uri": "https://vimeo.com/228332331", "name": "InfoVis 2016: [TVCG] Visplause: Visual Data Quality Assessment of Many Time Series Using Plausibility Checks", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-04T06:31:13+00:00", "description": "Authors: Ayan Biswas, Guang Lin, Xiaotong Liu, and Han-Wei Shen", "uri": "https://vimeo.com/228330946", "name": "SciVis 2016: [TVCG] Visualization of Time-Varying Weather Ensembles Across Multiple Resolutions", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-04T06:10:28+00:00", "description": "Authors: Florian Ferstl, Mathias Kanzler, Marc Rautenhaus, and R\u00fcdiger Westermann", "uri": "https://vimeo.com/228329172", "name": "SciVis 2016: [TVCG] Time-hierarchical Clustering and Visualization of Weather Forecast Ensembles", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-04T04:24:25+00:00", "description": "Authors: Allan Rocha, Usman Alim, Julio Daniel Silva, and Mario Costa Sousa", "uri": "https://vimeo.com/228320731", "name": "SciVis 2016: [TVCG] Decal-maps: Real-Time Layering of Decals on Surfaces for Multivariate Visualization", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-04T01:15:15+00:00", "description": "Authors: Arthur van Goethem, Frank Staals, Maarten L\u00f6ffler, Jason Dykes, and Bettina Speckmann", "uri": "https://vimeo.com/228306427", "name": "InfoVis 2016: [TVCG] Multi-Granular Trend Detection for Time-Series Analysis", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-08-02T01:00:24+00:00", "description": "Authors: Tobias G\u00fcnther and Holger Theisel", "uri": "https://vimeo.com/227994273", "name": "SciVis 2016: [TVCG] Backward Finite-Time Lyapunov Exponents in Inertial Flows", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-02T00:43:05+00:00", "description": "Authors: Julien Tierny and Hamish Carr", "uri": "https://vimeo.com/227992719", "name": "SciVis 2016: [TVCG] Jacobi Fiber Surfaces for Bivariate Reeb Space Computation", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-02T00:20:13+00:00", "description": "Authors: Antoni Sagrist\u00e0 Sell\u00e9s, Stefan Jordan, Andreas Just, F\u00e1bio Dias, Gustavo Nonato, and Filip Sadlo", "uri": "https://vimeo.com/227990783", "name": "SciVis 2016: [TVCG] Topological Analysis of Inertial Dynamics", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-02T00:01:58+00:00", "description": "Authors: Soumya Dutta, Chun-Ming Chen, Gregory Heinlein, Han-Wei Shen, and Jen-Ping Chen", "uri": "https://vimeo.com/227989016", "name": "SciVis 2016: [TVCG] In Situ Distribution Guided Analysis and Visualization of Transonic Jet Engine Simulations", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-01T23:41:50+00:00", "description": "Authors: Kai Lawonn, Erik Trostmann, Bernhard Preim, and Klaus Hildebrandt", "uri": "https://vimeo.com/227987139", "name": "SciVis 2016: [TVCG] Visualization and Extraction of Carvings for Heritage Conservation", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-01T23:28:02+00:00", "description": "Authors: Fabio Miranda, Harish Doraiswamy, Marcos Lage, Kai Zhao, Bruno Gon\u00e7alves, Luc Wilson, Mondrian Hsieh, and Cl\u00e1udio T. Silva", "uri": "https://vimeo.com/227985827", "name": "SciVis 2016: [TVCG] Urban Pulse: Capturing the Rhythm of Cities", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-01T23:13:00+00:00", "description": "Authors: Andreas J. Lind and Stefan Bruckner", "uri": "https://vimeo.com/227984379", "name": "SciVis 2016: [TVCG] Comparing Cross-Sections and 3D Renderings for Surface Matching Tasks using Physical Ground Truths", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-01T22:55:42+00:00", "description": "Authors: Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Jian Chen, and Torsten M\u00f6ller", "uri": "https://vimeo.com/227982779", "name": "SciVis 2016: [TVCG] Visualization as Seen Through its Research Paper Keywords", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-01T22:41:48+00:00", "description": "Authors: Pedro Hermosilla, Jorge Estrada, Victor Guallar, Timo Ropinski, \u00c0lvar Vinacua, and Pere-Pau V\u00e1zquez", "uri": "https://vimeo.com/227981403", "name": "SciVis 2016: [TVCG] Physics-based Visual Characterization of Molecular Interaction Forces", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-01T22:24:04+00:00", "description": "Authors: Jiaxi Hu, Hajar Hamidian, Zichun Zhong, and Jing Hua", "uri": "https://vimeo.com/227979572", "name": "SciVis 2016: [TVCG] Visualizing Shape Deformations with Variation of Geometric Spectrum", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-01T22:04:07+00:00", "description": "Authors: Chris Bryan, Gregory Guterman, Kwan-Liu Ma, Harris Lewin, Denis Larkin, Jaebum Kim, Jian Ma, and Marta Farr\u00e9", "uri": "https://vimeo.com/227977455", "name": "SciVis 2016: [TVCG] Synteny Explorer: An Interactive Visualization Application for Teaching Genome Evolution", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-08-01T21:46:07+00:00", "description": "Authors: Michael Krone, Florian Frie\u00df, Katrin Scharnowski, Guido Reina, Silvia Fademrecht, Tobias Kulschewski, J\u00fcrgen Pleiss, and Thomas Ertl", "uri": "https://vimeo.com/227975219", "name": "SciVis 2016: [TVCG] Molecular Surface Maps", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-07-31T00:00:51+00:00", "description": "Authors: Trevor Hogan, Uta Hinrichs, Eva Hornecker", "uri": "https://vimeo.com/227663100", "name": "InfoVis 2016: [TVCG Invited] The Elicitation Interview Technique: Capturing People\u2019s Experiences of Data Representations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T23:46:16+00:00", "description": "Authors: Lace Padilla, P. Samuel Quinan, Miriah Meyer, and Sarah H. Creem-Regehr", "uri": "https://vimeo.com/227662017", "name": "InfoVis 2016: [TVCG] Evaluating the Impact of Binning 2D Scalar Fields", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T23:30:06+00:00", "description": "Authors: Rudolf Netzel, Marcel Hlawatsch, Michael Burch, Sanjeev Balakrishnan, Hansj\u00f6rg Schmauder, and Daniel Weiskopf", "uri": "https://vimeo.com/227660796", "name": "InfoVis 2016: [TVCG] An Evaluation of Visual Search Support in Maps", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T23:10:53+00:00", "description": "Authors: Yalong Yang, Tim Dwyer, Sarah Goodwin, and Kim Marriott", "uri": "https://vimeo.com/227659290", "name": "InfoVis 2016: [TVCG] Many-to-Many Geographically-Embedded Flow Visualisation: An Evaluation", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T22:56:34+00:00", "description": "Authors: Yanhong Wu, Nan Cao, Daniel Archambault, Qiaomu Shen, Huamin Qu, and Weiwei Cui", "uri": "https://vimeo.com/227658156", "name": "InfoVis 2016: [TVCG] Evaluation of Graph Sampling: A Visualization Perspective", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T03:29:53+00:00", "description": "Authors: Quirijn W. Bouts, Tim Dwyer, Jason Dykes, Bettina Speckmann, Sarah Goodwin, Nathalie Henry Riche, Sheelagh Carpendale, Ariel Liebman", "uri": "https://vimeo.com/227582655", "name": "InfoVis 2016: [TVCG Invited] Visual Encoding of Dissimilarity Data via Topology-Preserving Map Deformation", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T03:14:47+00:00", "description": "Authors: Roger Beecham, Jason Dykes, Wouter Meulemans, Aidan Slingsby, Cagatay Turkay, and Jo Wood", "uri": "https://vimeo.com/227581989", "name": "InfoVis 2016: [TVCG] Map LineUps: effects of spatial structure on graphical inference", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T03:00:41+00:00", "description": "Authors: Wouter Meulemans, Jason Dykes, Aidan Slingsby, Cagatay Turkay, and Jo Wood", "uri": "https://vimeo.com/227581358", "name": "InfoVis 2016: [TVCG] Small Multiples with Gaps", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T02:45:59+00:00", "description": "Authors: Yifan Zhang and Ross Maciejewski", "uri": "https://vimeo.com/227580719", "name": "InfoVis 2016: [TVCG] Quantifying the Visual Impact of Classification Boundaries in Choropleth Maps", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T02:14:18+00:00", "description": "Authors: Matthew van der Zwan, Valeriu Codreanu, and Alexandru Telea", "uri": "https://vimeo.com/227579269", "name": "InfoVis 2016: [TVCG Invited] CUBu: Universal real-time bundling for large graphs", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T01:57:36+00:00", "description": "Authors: Benjamin Bach, Nathalie Henry Riche, Christophe Hurter, Kim Marriott, and Tim Dwyer", "uri": "https://vimeo.com/227578439", "name": "InfoVis 2016: [TVCG] Towards Unambiguous Edge Bundling: Investigating Confluent Drawings for Network Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-30T01:43:45+00:00", "description": "Authors: Christoph Schulz, Arlind Nocaj, Jochen Goertler, Oliver Deussen, Ulrik Brandes, and Daniel Weiskopf", "uri": "https://vimeo.com/227577765", "name": "InfoVis 2016: [TVCG] Probabilistic Graph Layout for Uncertain Network Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-27T22:35:52+00:00", "description": "Authors: Anzu Hakone, Lane Harrison, Alvitta Ottley, Nathan Winters, Caitlin Gutheil, Paul K. J. Han, and Remco Chang", "uri": "https://vimeo.com/227338101", "name": "InfoVis 2016: [TVCG] PROACT: Iterative Design of a Patient-Centered Visualization for Effective...", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-27T21:11:47+00:00", "description": "Authors: Rafael Veras and Christopher Collins", "uri": "https://vimeo.com/227326170", "name": "InfoVis 2016: [TVCG] Optimizing Hierarchical Visualizations with the Minimum Description Length Principle", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-27T20:58:49+00:00", "description": "Authors: Mengdie Hu, Krist Wongsuphasawat, and John Stasko", "uri": "https://vimeo.com/227324185", "name": "InfoVis 2016: [TVCG] Visualizing Social Media Content with SentenTree", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-27T20:44:22+00:00", "description": "Authors: Stephan Pajer, Marc Streit, Thomas Torsney-Weir, Florian Spechtenhauser, Torsten M\u00f6ller, and Harald Piringer", "uri": "https://vimeo.com/227322012", "name": "InfoVis 2016: [TVCG] WeightLifter: Visual Weight Space Exploration for Multi-Criteria Decision Making", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-27T20:18:06+00:00", "description": "Authors: Kasper Dinkla, Hendrik Strobelt, Bryan Genest, Stephan Reiling, Mark Borowsky, and Hanspeter Pfister", "uri": "https://vimeo.com/227317580", "name": "InfoVis 2016: [TVCG] Screenit: Visual Analysis of Cellular Screens", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-21T19:29:07+00:00", "description": "Authors: Evanthia Dimara, Anastasia Bezerianos, and Pierre Dragicevic", "uri": "https://vimeo.com/226491954", "name": "InfoVis 2016: [TVCG] The Attraction Effect in Information Visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-21T19:03:32+00:00", "description": "Authors: Wesley Willett, Yvonne Jansen, and Pierre Dragicevic", "uri": "https://vimeo.com/226488668", "name": "InfoVis 2016: [TVCG] Embedded Data Representations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-21T18:49:16+00:00", "description": "Authors: Maxime Cordeil, Tim Dwyer, Karsten Klein, Bireswar Laha, Kim Marriott, and Bruce H. Thomas", "uri": "https://vimeo.com/226486952", "name": "InfoVis 2016: [TVCG] Immersive Collaborative Analysis of Network Connectivity: CAVE-style or Head-Mounted Display?", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-07-20T18:44:11+00:00", "description": "Authors: Faisal Taher, Yvonne Jansen, Jonathan Woodruff, John Hardy, Kasper Hornb\u00e6k, and Jason Alexander", "uri": "https://vimeo.com/226346376", "name": "InfoVis 2016: [TVCG] Investigating the Use of a Dynamic Physical Bar Chart for Data Exploration and Presentation", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-01-31T02:02:07+00:00", "description": "Authors: Alex Bigelow, Steven Drucker, Danyel Fisher, and Miriah Meyer", "uri": "https://vimeo.com/201779979", "name": "InfoVis 2016: [TVCG] Iterating Between Tools to Create and Edit Visualizations", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-01-31T00:58:08+00:00", "description": "Authors: H. Fang, S. Walton, E. Delahaye, J. Harris, D. A. Storchak, and M. Chen", "uri": "https://vimeo.com/201772939", "name": "SciVis 2016: [TVCG] Categorical Colormap Optimization with Visualization Case Studies", "year": "2017", "event": "SCIVIS"}, {"created_time": "2017-01-31T00:06:16+00:00", "description": "Authors: Nam Wook Kim, Eston Schweickart, Zhicheng Liu, Mira Dontcheva, Wilmot Li, Jovan Popovic, and Hanspeter Pfister", "uri": "https://vimeo.com/201767031", "name": "InfoVis 2016: [TVCG] Data-Driven Guides: Supporting Expressive Design for Information Graphics", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-01-30T23:48:16+00:00", "description": "Authors: Fereshteh Amini, Nathalie Henry Riche, Bongshin Lee, Andres Monroy-Hernandez, and Pourang Irani", "uri": "https://vimeo.com/201764833", "name": "InfoVis 2016: [TVCG] Authoring Data-Driven Videos with DataClips", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-01-30T23:26:49+00:00", "description": "Authors: Chris Bryan, Kwan-Liu Ma, and Jonathan Woodring", "uri": "https://vimeo.com/201761945", "name": "InfoVis 2016: [TVCG] Temporal Summary Images: An Approach to Narrative Visualization via Interactive Annotation Generation...", "year": "2017", "event": "INFOVIS"}, {"created_time": "2017-01-30T23:03:41+00:00", "description": "Authors: Connor C. Gramazio, David H. Laidlaw, and Karen B. Schloss", "uri": "https://vimeo.com/201758901", "name": "InfoVis 2016: [TVCG] Colorgorical: Creating discriminable and preferable color palettes for information visualization", "year": "2017", "event": "INFOVIS"}, {"created_time": "2016-11-11T22:33:25+00:00", "description": "Moderator: Gunther Weber\n\nPanelists: Sheelagh Carpendale, David Ebert, Brian Fisher, Hans Hagen, Ben Shneiderman, Anders Ynnerman\n\nThis panel will start a discussion in the community about what goals an application paper ought to have, what its main contributions to the state of art of visualization should be, and how it ought to be evaluated by reviewers. How do we as a community generate clear evaluation criteria for this type of paper?", "uri": "https://vimeo.com/191225652", "name": "VIS 2016: Panel: Application Papers: What are they and how should they be evaluated?", "year": "2016", "event": "PANEL"}, {"created_time": "2016-09-20T08:29:59+00:00", "description": "VIS Tutorial \n\nOrganizers: Tamara Munzner\n\nAbstract: This introductory tutorial will provide a broad foundation for thinking systematically about visualization sys- tems, built around the idea that becoming familiar with analyzing existing systems is a good springboard for designing new ones. The major data types of concern in visual analytics, information visualization, and scientific visualization will all be covered: tables, networks, and sampled spatial data. This tutorial is focused on data and task abstractions, and the design choices for visual encoding and interaction; it will not cover algorithms. No background in computer science or visualization is assumed.\n\nSunday, Oct. 23, 2:00\u20135:15 \u2013 Holiday 6", "uri": "https://vimeo.com/183453277", "name": "[VIS16 Preview] Visualization Analysis and Design", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-20T07:57:12+00:00", "description": "VIS Posters \n\nOrganizers / Panelists: Tobias Schreck, Jing Yang, Tim Dwyer, Petra Isenberg, Gunther H. Weber, Xiaoru Yuan\n\nAbstract: The IEEE VIS 2016 Poster Program offers a timely venue to present and discuss new visualization research through a forum that encourages graphical presentation, demonstration and active engagement with IEEE VIS participants.", "uri": "https://vimeo.com/183449819", "name": "[VIS16 Preview] VIS Poster Session", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-20T07:01:15+00:00", "description": "VIS Panel \n\nOrganizers / Panelists: Georges Grinstein, Robert Baker, Betsy Beaumon, Karl Groves, Mike Paciello, Joss Stubblefield\n\nAbstract: Individuals with accessibility issues are not able to access much of our work in visualization and we, as a community, have not paid a great deal of attention to that community. This panel will address the misconceptions, the law, and the steps that need to be taken to resolve this.", "uri": "https://vimeo.com/183444781", "name": "[VIS16 Preview] How Data Visualization and Regulation Meet on the Modern Web", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-20T07:01:02+00:00", "description": "VISAP Panel \n\nOrganizers / Panelists: Angus Forbes, Dietmar Offenhuber, Jessica Hullman, Heather Dewey-Hagborg, Marian Dork\n\nAbstract: How could a critical approach to visualization promote disclosure, plurality, contingency, and empowerment? What opportunities are there for incorporating human-centered inquiry into visualization research? Does articulating value, bias, and ideology have a place in scientific discourse? The Critical Visualization panel will introduce the diverse work of the four panelists and provide a forum for discussing critical approaches to visualization.", "uri": "https://vimeo.com/183444771", "name": "[VIS16 Preview] Critical Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-20T07:00:51+00:00", "description": "VIS Panel \n\nOrganizers / Panelists: Robert S. Laramee\n\nAbstract: While InfoVis and VAST have been expanding for the last decade, SciVis seems to be, in general, contracting. This apparent contraction coincides roughly with Bill Lorenson\u2019s famous paper on the Death of Visualization. This panel discusses what appears to be a trend of the SciVis track of the conference contracting. This panel addresses some very challenging, core, fundamental questions.", "uri": "https://vimeo.com/183444755", "name": "[VIS16 Preview] On the Death of Scientific Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-20T07:00:39+00:00", "description": "VIS Panel \n\nOrganizers / Panelists: Gunther Weber, Sheelagh Carpendale, David Ebert, Brian Fisher, Hans Hagen, Ben Shneiderman, Anders Ynnermann\n\nAbstract: This panel will start a discussion in the community about what goals an application paper ought to have, what its main contributions to the state of art of visualization should be, and how it ought to be evaluated by reviewers. How do we as a community generate clear evaluation criteria for this type of paper?", "uri": "https://vimeo.com/183444742", "name": "[VIS16 Preview] Application Papers: What are they and how should they be evaluated?", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-20T06:59:47+00:00", "description": "VIS Panel \n\nOrganizers / Panelists: Min Chen, Georges Grinstein, Chris Johnson, Jessie Kennedy, Tamara Munzner, Melanie Tory\n\nAbstract: This panel focuses on the question \u201cHow can we build a theoretic foundation for visualization collectively as a community?\u201d The panellists will envision the pathways in four different aspects of a theoretic foundation, namely (i) taxonomies and ontologies, (ii) principles and guidelines, (iii) conceptual models and theoretic frameworks, and (iv) quantitative laws and theoretic systems.", "uri": "https://vimeo.com/183444681", "name": "[VIS16 Preview] Pathways for Theoretical Advances in Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-20T06:58:13+00:00", "description": "VISAP \n\nAuthors: Angus Forbes, Fanny Chevalier\n\nAbstract: Artists, graphic designers, and visualization researchers share common goals: to make things visible which are normally difficult to see, and to enable reasoning about information that we might otherwise remain ignorant of. A conventional explanation of the differences between creative practice and visualization research is that artistic exploration raises new questions while visualization research aims to help domain experts answer existing questions. However, these categorizations may be oversimplified. Media artists create opportunities for reflecting on cultural issues, but also highlight how we absorb technology and explore how the exposure to tremendous amounts of data affects our daily lives. Visualization researchers raise new questions by pushing the boundaries of what we can perceive when introducing new interactions with and representations of complex data.\\n\\nThe theme for VISAP\u201916, the IEEE VIS 2016 Arts Program, is Metamorphoses. We encourage artists, designers, and researchers to think about transformation as a fundamental component of the pipeline from raw data to meaning. In Ovid\u2019s epic poem\u2014 our thematic inspiration\u2014 transformations are often capricious occurrences, arising from the whim or wrath of a Greek god, yet nonetheless serve to explain the mysteries of the natural world and social order. In the realm of science, the movement from observation to hypothesis to measurement is, ideally, quantifiable and reproducible. Yet visualization researchers are also explorers, investigating new types of visual encodings and interaction techniques on ever new technologies in order to augment our understanding of the modern world. Similarly, artists often emphasize the metamorphosis of subjective experience through conceptual lenses to create artifacts that transforms our understanding of culture and technology. For this year\u2019s Call for Entries we ask artists, designers, and researchers to question and chronicle their process of meaning making. We are especially interested in projects and papers that explore the relationships between visualization research and arts and/or design practice, and that present or discuss creative visual techniques that emphasize transformative aspects of scientific or cultural exploration.\\n\\nSelected artworks and design submissions will be installed in a gallery setting (the location is still to be determined). The exhibition is open to the general public as well as to VIS attendees. Other submissions will be chosen to present for a shorter time during the IEEE VIS conference. All accepted submissions (whether full installations or not) will be featured in the VISAP\u201916 Exhibition Catalog.", "uri": "https://vimeo.com/183444545", "name": "[VIS16 Preview] VIS Arts Program", "year": "2016", "event": "ARTS PROGRAM, PREVIEW"}, {"created_time": "2016-09-20T06:08:56+00:00", "description": "VISAP Paper \n\nAuthors: Romain Vuillemot, Samuel Huron\n\nAbstract: Glitches \u2013 unexpected coding errors \u2013 are often discarded as they are considered as design failures. This is surprising as they relentlessly flourish on the web, due the interest their aesthetic triggers. In this paper, we report a preliminary work that aims at understanding glitches in the context of information visualization. We first conducted an empirical study by collecting glitches examples on social media that we grouped by visual and semantic similarities. Glitches descriptions by their creators allowed us to grasp the interest behind sharing such a failure. However, understanding the (unattended) cause of glitches remains usually difficult as it also remains unknown for their creators; nonetheless we reverse engineered some of them in a synthetic manner. We discuss glitches expressive possibilities, which can be considered as a generative design method for visualization, and suggest including them in design studies as a way to better document design processes.", "uri": "https://vimeo.com/183440694", "name": "[VIS16 Preview] Glitches as a Generative Design Process", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T18:31:58+00:00", "description": "VISAP Paper \n\nAuthors: Salvador Orara\n\nAbstract: This paper introduces the Affection Research Lab (ARL), a project that leverages pre-existing animistic tendencies to create a new layer of understanding and meaning of smartphones. Utilizing Animistic Design Principles, [10] the ARL introduces the development of new modes of device affection through the concept of the post-mythical object; within the context of a landscape saturated with task-oriented utilitarian digital objects. ARL solicits device-affection by listening to their intrinsic nature of electromagnetism as a source of raw data, and transforms this raw data through the lenses of noise and sound. ARL provokes a paradigm shift in the development of digital objects and seeks to deduce meaning out of the overwhelming and seemingly meaninglessness of noise. Furthermore, this paper discusses the state of the human condition as a result of our cultural developments [1] and expresses the strides one must take in order to create deeper connections with our digital objects and the mythical possibilities we can have with them. The paper will discuss two core projects that define the ARL: The Signal Archive and the Affection Stations, breaking down their different conceptual approaches, and the animistic results from participant feedback; providing evidence of pre-existing tendencies and enabling those tendencies to transform into a new level of perception. The ARL seeks to enable new stories and myths to be created with digital objects. In doing so we advocate for the approach of mythical-centered design where the focus of designers and engineers is in the creation of rich and meaningful experiences with digital objects which allow us to reflect, collaborate, and participate; within an ever increasing context of ubiquitous data and the digital objects which mediate our understanding and experiences within it.", "uri": "https://vimeo.com/183357799", "name": "[VIS16 Preview] Altering our Perception of Smartphones through Noise: Introducing the Affection Research Lab", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T18:19:39+00:00", "description": "VISAP Paper \n\nAuthors: Till Nagel, Christopher Pietsch, Marian D\u00f6rk\n\nAbstract: In this paper we examine the concept of staged analysis through a case study on visualizing urban mobility exhibited in a public gallery space. Recently, many cities introduced bike-sharing in order to promote cycling among locals and visitors. We explore how citizens can be guided from evocative impressions of bicycling flows to comparative analysis of three bike-sharing systems. The main aim for visualizations in exhibition contexts is to encourage a shift from temporary interest to deeper insight into a complex phenomenon. To pursue this ambition we introduce cf. city flows, a comparative visualization environment of urban bike mobility designed to help citizens casually analyze three bike-sharing systems in the context of a public exhibition space. Multiple large screens show the space of flows in bike-sharing for three selected world cities: Berlin, London, and New York. Bike journeys are represented in three geospatial visualizations designed to be progressively more analytical, from animated trails to small-multiple glyphs. In this paper, we describe our design concept and process, the exhibition setup, and discuss some of the insights visitors gained while interacting with the visualizations.", "uri": "https://vimeo.com/183355855", "name": "[VIS16 Preview] Staged Analysis: From Evocative to Comparative Visualizations of Urban Mobility", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T17:27:30+00:00", "description": "VISAP Paper \n\nAuthors: Raphael Reimann, Benedikt Gro\u00df, Philipp Schmitt\n\nAbstract: The proverb \u2018all roads lead to Rome\u2019 is, by a closer look, a very interesting suggestive mobility statement. The goal of the \u2018Roads to Rome\u2019 was to find an automated way to visualize this saying. During the process of finding the right methods and approaches the authors encountered several inspiring further threads of ideas. The authors created maps using algorithms for routing from multiple starts to a single destination and also multiple destinations. The researchers also used the developed methodology on a small scale to visualize mobility network diagrams of selected cities. The resulting images are not only visually intriguing, but also allow conclusions about how road infrastructure reflects regional, political and geographical situations.", "uri": "https://vimeo.com/183347949", "name": "[VIS16 Preview] All Roads to Rome: Visualizing Mobility at Scale", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T17:11:45+00:00", "description": "VISAP Paper \n\nAuthors: Weili Shi\n\nAbstract: Shan Shui in the World presents shanshui (\u5c71\u6c34, landscape) paintings of selected places in the world (Manhattan, New York in this first production) generated by a computational process based on geography-related information. This project revisits the ideas implicit in Chinese literati paintings of shan shui: the relationship between urban life and people\u2019s yearning for the nature, and between social responsibility and spiritual purity. For an audience living in an urban area, a traditional shanshui painting provides them with spiritual support through the depiction of the natural scene of elsewhere. With generative technology, however, Shan Shui in the World has the ability to represent any place in the world\u2014including the city where the audience is\u2014in the form of a shanshui painting based on geography-related information of the place. The notion that shan shui can exist right here not only underscores the contrast between the artificial world and nature, but also reminds the audience of an alternative approach to spiritual strength: instead of resorting to the shan shui of elsewhere, we may be able to obtain inner peace from the \u201cshan shui\u201d of our present location by looking inward.", "uri": "https://vimeo.com/183345476", "name": "[VIS16 Preview] Shan Shui in the World: A Generative Approach to Traditional Chinese Landscape Painting", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T16:08:50+00:00", "description": "VISAP Paper \n\nAuthors: Yoon Chung Han, Shankar Tiwari\n\nAbstract: We present our approach for visualizing and sonifying multivariate data describing California\u2019s drought using physical data sculptures and projection-mapping images with user interactions. We provide an interaction tool to depict the causes and impact of the drought and promote awareness of water consumption. It offers an opportunity to experience a metamorphosis of water and its impact on the drought. Prototypes of an interactive multimodal data visualization and sonification depict the past, present, and future of the drought by altering water morphology (water metamorphosis) which occurs as a result of climate changes. Thus, this multimodal data representation not only provides an aesthetically meaningful visualization but also encourages good environmental stewardship using the hybrid practices of art and design. This data representation also leads to a new media interactive interface utilizing audio synthesis, visualization, and real-time interaction. In this paper, we describe the design process and illustrate how this interface was developed based on environmental issues and can also be applied to other interactive media artwork to visually reveal informative patterns.", "uri": "https://vimeo.com/183335621", "name": "[VIS16 Preview] California Drought Impact: Multimodal Data Representation to Predict the Water Cycle", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T15:57:33+00:00", "description": "VISAP Paper \n\nAuthors: Tim McGraw\n\nAbstract: Neuronal connectivity graphs can give valuable insight into the structure of the brain and patterns of functional activity. In this paper we describe a connectivity visualization technique using contrasting styles to emphasize the differences between pairs of graphs. Our proposed technique enables interactive exploration of connectivity graphs at the various levels of the hierarchical structure of the brain - atlas regions, brain lobes and cerebral hemispheres. We demonstrate our methods on data obtained from matched pairs of subjects in a Parkinson's disease (PD) study. Control group data is displayed in a smooth, organic style with continuous transitions between views at different levels of the brain hierarchy using edge bundling and a new node bundling technique. Data from the PD group is displayed with glitch and noise effects inspired by PD symptoms, highlighting the differences between each subject and their matched pair from the control group. We conclude by describing avenues for further evaluation of this preliminary work.", "uri": "https://vimeo.com/183333376", "name": "[VIS16 Preview] Glitch style visualization of disrupted neuronal connectivity in Parkinson\u2019s disease", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:59:57+00:00", "description": "TVCG Paper \n\nOrganizers: Danielle Albers Szafir, Alper Sarikaya, Michael Gleicher\n\nAbstract: Color is a common channel for displaying data in surface visualization, but is affected by the shadows and shading used to convey surface depth and shape. Understanding encoded data in the context of surface structure is critical for effective analysis in a variety of domains, such as in molecular biology. In the physical world, lightness constancy allows people to accurately perceive shadowed colors; however, its effectiveness in complex synthetic environments such as surface visualizations is not well understood. We report a series of crowdsourced and laboratory studies that confirm the existence of lightness constancy effects for molecular surface visualizations using ambient occlusion. We provide empirical evidence of how common visualization design decisions can impact viewers' abilities to accurately identify encoded surface colors. These findings suggest that lightness constancy aids in understanding color encodings in surface visualization and reveal a correlation between visualization techniques that improve color interpretation in shadow and those that enhance perceptions of surface depth. These results collectively suggest that understanding constancy in practice can inform effective visualization design.\n\nThursday, Oct. 27, 10:30\u201312:10 \u2013 Key 1+2+5", "uri": "https://vimeo.com/183313706", "name": "[VIS16 Preview] Lightness Constancy in Surface Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:48:47+00:00", "description": "TVCG Paper \n\nOrganizers: Shixia Liu, Jialun Yin, Xiting Wang, Weiwei Cui, Kelei Cao, Jian Pei\n\nAbstract: We present an online visual analytics approach to helping users explore and understand hierarchical topic evolution in high-volume text streams. The key idea behind this approach is to identify representative topics in incoming documents and align them with the existing representative topics that they immediately follow (in time). To this end, we learn a set of streaming tree cuts from topic trees based on user-selected focus nodes. A dynamic Bayesian network model has been developed to derive the tree cuts in the incoming topic trees to balance the fitness of each tree cut and the smoothness between adjacent tree cuts. By connecting the corresponding topics at different times, we are able to provide an overview of the evolving hierarchical topics. A sedimentation-based visualization has been designed to enable the interactive analysis of streaming text data from global patterns to local details. We evaluated our method on real-world datasets and the results are generally favorable\n\nWednesday, Oct. 26, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/183311895", "name": "[VIS16 Preview] Online Visual Analytics of Text Streams", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:48:32+00:00", "description": "TVCG Paper \n\nOrganizers: Hans-J\u00f6rg Schulz, Marco Angelini, Giuseppe Santucci, and Heidrun Schumann\n\nAbstract: With today's technical possibilities, a stable visualization scenario can no longer be assumed as a matter of course, as underlying data and targeted display setup are much more in flux than in traditional scenarios. Incremental visualization approaches are a means to address this challenge, as they permit the user to interact with, steer, and change the visualization at intermediate time points and not just after it has been completed. In this paper, we put forward a model for incremental visualizations that is based on the established Data State Reference Model, but extends it in ways to also represent partitioned data and visualization operators to facilitate intermediate visualization updates. In combination, partitioned data and operators can be used independently and in combination to strike tailored compromises between output quality, shown data quantity, and responsiveness\u2014i.e., frame rates. We showcase the new expressive power of this model by discussing the opportunities and challenges of incremental visualization in general and its usage in a real world scenario in particular.\n\nThursday, Oct. 27, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311868", "name": "[VIS16 Preview] An Enhanced Visualization Process Model for Incremental Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:48:20+00:00", "description": "TVCG Paper \n\nOrganizers: Corinna Vehlow, Fabian Beck, and Daniel Weiskopf\n\nAbstract: Graphs are used to model relations between objects, where these objects can be grouped hierarchically based on their connectivity. In many applications, the relations change over time and so does the hierarchical group structure. We developed a visualization technique that supports the analysis of the topology and the hierarchical group structure of a dynamic graph and the tracking of changes over time. Each graph of a sequence is visualized by an adjacency matrix, where the hierarchical group structure is encoded within the matrix using indentation and nested contours, complemented by icicle plots attached to the matrices. The density within and between subgroups of the hierarchy is represented within the matrices using a gray scale. To visualize changes, transitions and dissimilarities between the hierarchically structured graphs are shown using a flow metaphor and color coding. The design of our visualization technique allows us to show more than one hierarchical group structure of the same graph by stacking the sequences, where hierarchy comparison is supported not only within but also between sequences. To improve the readability, we minimize the number of crossing curves within and between sequences based on a sorting algorithm that sweeps through the sequences of hierarchies.\n\nWednesday, Oct. 26, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311844", "name": "[VIS16 Preview] Visualizing Dynamic Hierarchies in Graph Sequences", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:48:08+00:00", "description": "TVCG Paper \n\nOrganizers: Quirijn W. Bouts, Tim Dwyer, Jason Dykes, Bettina Speckmann, Sarah Goodwin, Nathalie Henry Riche, Sheelagh Carpendale, Ariel Liebman\n\nAbstract: We present an efficient technique for topology-preserving map deformation and apply it to the visualization of dissimilarity data in a geographic context. Map deformation techniques such as value-by-area cartograms are well studied. However, using deformation to highlight (dis)similarity between locations on a map in terms of their underlying data attributes is novel. We also identify an alternative way to represent dissimilarities on a map through the use of visual overlays. These overlays are complementary to deformation techniques and enable us to assess the quality of the deformation as well as to explore the design space of blending the two methods. Finally, we demonstrate how these techniques can be useful in several-quite different-applied contexts: travel-time visualization, social demographics research and understanding energy flowing in a wide-area power-grid.\n\nTuesday, Oct. 25, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311824", "name": "[VIS16 Preview] Visual Encoding of Dissimilarity Data via Topology-Preserving Map Deformation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:47:53+00:00", "description": "TVCG Paper \n\nOrganizers: Nicola Pezzotti, Boudewijn P.F. Lelieveldt, Laurens van der Maaten, Thomas H\u00f6llt, Elmar Eisemann, Anna Vilanova\n\nAbstract: Progressive Visual Analytics aims at improving the interactivity in existing analytics techniques by means of visualization as well as interaction with intermediate results. One key method for data analysis is dimensionality reduction, for example, to produce 2D embeddings that can be visualized and analyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a well-suited technique for the visualization of several high-dimensional data. tSNE can create meaningful intermediate results but suffers from a slow initialization that constrains its application in Progressive Visual Analytics. We introduce a controllable tSNE approximation (A-tSNE), which trades off speed and accuracy, to enable interactive data exploration. We offer real-time visualization techniques, including a density-based solution and a Magic Lens to inspect the degree of approximation. With this feedback, the user can decide on local refinements and steer the approximation level during the analysis. We demonstrate our technique with several datasets, in a real-world research scenario and for the real-time analysis of high-dimensional streams to illustrate its effectiveness for interactive data analysis.\n\nWednesday, Oct. 26, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/183311789", "name": "[VIS16 Preview] Approximated and User Steerable tSNE for Progressive Visual Analytics", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:47:38+00:00", "description": "TVCG Paper \n\nOrganizers: Sayeed Safayet Alam, Radu Jianu\n\nAbstract: Eye-tracking data is currently analyzed in the image space that gaze-coordinates were recorded in, generally with the help of overlays such as heatmaps or scanpaths, or with the help of manually defined areas of interest (AOI). Such analyses, which focus predominantly on where on the screen users are looking, require significant manual input and are not feasible for studies involving many subjects, long sessions, and heavily interactive visual stimuli. Alternatively, we show that it is feasible to collect and analyze eye-tracking information in data space. Specifically, the visual layout of visualizations with open source code that can be instrumented is known at rendering time, and thus can be used to relate gaze-coordinates to visualization and data objects that users view, in real time. We demonstrate the effectiveness of this approach by showing that data collected using this methodology from nine users working with an interactive visualization, was well aligned with the tasks that those users were asked to solve, and similar to annotation data produced by five human coders. Moreover, we introduce an algorithm that, given our instrumented visualization, could translate gaze-coordinates into viewed objects with greater accuracy than simply binning gazes into dynamically defined AOIs. Finally, we discuss the challenges, opportunities, and benefits of analyzing eye-tracking in visualization and data space.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 6", "uri": "https://vimeo.com/183311760", "name": "[VIS16 Preview] Analyzing Eye-Tracking Information in Visualization and Data Space: from Where on the Screen to What on the...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:47:08+00:00", "description": "TVCG Paper \n\nOrganizers: Holger Stitz, Samuel Gratz, Wolfang Aigner, Marc Streit\n\nAbstract: Multi-attribute time-series data plays a vital role in many different domains, such as economics, sensor networks, and biology. An important task when making sense of such data is to provide users with an overview to identify items that show an interesting development over time, including both absolute and relative changes in multiple attributes simultaneously. However, this is not well supported by existing visualization techniques. To address this issue, we present ThermalPlot, a visualization technique that summarizes combinations of multiple attributes over time using an item's position, the most salient visual variable. More precisely, the x-position in the ThermalPlot is based on a user-defined degree-of-interest (DoI) function that combines multiple attributes over time. The y-position is determined by the relative change in the DoI value (DDoI) within a user-specified time window. Animating this mapping via a moving time window gives rise to circular movements of items over time-as in thermal systems. To help the user to identify important items that match user-defined temporal patterns and to increase the technique's scalability, we adapt the level of detail of the items' representation based on the DoI value. Furthermore, we present an interactive exploration environment for multi-attribute time-series data that ties together a carefully chosen set of visualizations, designed to support analysts in interacting with the ThermalPlot technique. We demonstrate the effectiveness of our technique by means of two usage scenarios that address the visual analysis of economic development data and of stock market data.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311655", "name": "[VIS16 Preview] ThermalPlot: Visualizing Multi-Attribute Time-Series Data Using a Thermal Metaphor", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:46:54+00:00", "description": "TVCG Paper \n\nOrganizers: Thomas Ortner, Johannes Sorger, Harald Steinlechner, Gerd Hesina, Harald Piringer, Eduard Gr\u00f6ller\n\nAbstract: 3D visibility analysis plays a key role in urban planning for assessing the visual impact of proposed buildings on the cityscape. A call for proposals typically yields around 30 candidate buildings that need to be evaluated with respect to selected viewpoints. Current visibility analysis methods are very time-consuming and limited to a small number of viewpoints. Further, analysts neither have measures to evaluate candidates quantitatively, nor to compare them efficiently. The primary contribution of this work is the design study of Vis-A-Ware, a visualization system to qualitatively and quantitatively evaluate, rank, and compare visibility data of candidate buildings with respect to a large number of viewpoints. Vis-A-Ware features a 3D spatial view of an urban scene and non-spatial views of data derived from visibility evaluations, which are tightly integrated by linked interaction. To enable a quantitative evaluation we developed four metrics in accordance with experts from urban planning. We illustrate the applicability of Vis-A-Ware on the basis of a use case scenario and present results from informal feedback sessions with domain experts from urban planning and development. This feedback suggests that Vis-A-Ware is a valuable tool for visibility analysis allowing analysts to answer complex questions more efficiently and objectively.\n\nTuesday, Oct. 25, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/183311609", "name": "[VIS16 Preview] Vis-A-Ware: Integrating Spatial and Non-Spatial Visualization for Visibility-Aware Urban Planning", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:46:33+00:00", "description": "TVCG Paper \n\nOrganizers: Erick Gomez-Nieto, Wallace Casaca, Danilo Motta, Ivar Hartmann, Gabriel Taubin, Luis Gustavo Nonato.\n\nAbstract: Existing algorithms for building layouts from geometric primitives are typically designed to cope with requirements such as orthogonal alignment, overlap removal, optimal area usage, hierarchical organization, among others. However, most techniques are able to tackle just a few of those requirements simultaneously, impairing their use and flexibility. In this work we propose a novel methodology for building layouts from geometric primitives that concurrently addresses a wider range of requirements. Relying on multidimensional projection and mixed integer optimization, our approach arranges geometric objects in the visual space so as to generate well structured layouts that preserve the semantic relation among objects while still making an efficient use of display area. Moreover, scalability is handled through a hierarchical representation scheme combined with navigation tools. A comprehensive set of quantitative comparisons against existing geometry-based layouts and applications on text, image, and video data set visualization prove the effectiveness of our approach.\n\nThursday, Oct. 27, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311526", "name": "[VIS16 Preview] Dealing with Multiple Requirements in Geometric Arrangements", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:46:20+00:00", "description": "TVCG Paper \n\nOrganizers: Matthew van der Zwan, Valeriu Codreanu, Alexandru Telea\n\nAbstract: Visualizing very large graphs by edge bundling is a promising method, yet subject to several challenges: speed, clutter, level-of-detail, and parameter control. We present CUBu, a framework that addresses the above problems in an integrated way. Fully GPU-based, CUBu bundles graphs of up to a million edges at interactive framerates, being over 50 times faster than comparable state-of-the-art methods, and has a simple and intuitive control of bundling parameters. CUBu extends and unifies existing bundling techniques, offering ways to control bundle shapes, separate bundles by edge direction, and shade bundles to create a level-of-detail visualization that shows both the graph core structure and its details. We demonstrate CUBu on several large graphs extracted from real-life application domains.\n\nWednesday, Oct. 26, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311464", "name": "[VIS16 Preview] CUBu: Universal real-time bundling for large graphs", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:46:05+00:00", "description": "TVCG Paper \n\nOrganizers: Pavol Klacansky, Julien Tierny, Hamish Carr, Zhao Geng\n\nAbstract: Isosurfaces are fundamental geometrical objects for the analysis and visualization of volumetric scalar fields. Recent work has generalized them to bivariate volumetric fields with fiber surfaces, the pre-image of polygons in range space. However, the existing algorithm for their computation is approximate, and is limited to closed polygons. Moreover, its runtime performance does not allow instantaneous updates of the fiber surfaces upon user edits of the polygons. Overall, these limitations prevent a reliable and interactive exploration of the space of fiber surfaces. This paper introduces the first algorithm for the exact computation of fiber surfaces in tetrahedral meshes. It assumes no restriction on the topology of the input polygon, handles degenerate cases and better captures sharp features induced by polygon bends. The algorithm also allows visualization of individual fibers on the output surface, better illustrating their relationship with data features in range space. To enable truly interactive exploration sessions, we further improve the runtime performance of this algorithm. In particular, we show that it is trivially parallelizable and that it scales nearly linearly with the number of cores. Further, we study acceleration data-structures both in geometrical domain and range space and we show how to generalize interval trees used in isosurface extraction to fiber surface extraction. Experiments demonstrate the superiority of our algorithm over previous work, both in terms of accuracy and running time, with up to two orders of magnitude speedups. This improvement enables interactive edits of range polygons with instantaneous updates of the fiber surface for exploration purpose. A VTK-based reference implementation is provided as additional material to reproduce our results.\n\nTuesday, Oct. 25, 4:15\u20135:55 \u2013 Key 1+2+5", "uri": "https://vimeo.com/183311420", "name": "[VIS16 Preview] Fast and Exact Fiber Surfaces for Tetrahedral Meshes", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:45:53+00:00", "description": "TVCG Paper \n\nOrganizers: Alexey Fofonov, Vladimir Molchanov, Lars Linsen\n\nAbstract: Multi-run simulations are widely used to investigate how simulated processes evolve depending on varying initial conditions. Frequently, such simulations model the change of spatial phenomena over time. Isocontours have proven to be effective for the visual representation and analysis of 2D and 3D spatial scalar fields. We propose a novel visualization approach for multi-run simulation data based on isocontours. By introducing a distance function for isocontours, we generate a distance matrix used for a multidimensional scaling projection. Multiple simulation runs are represented by polylines in the projected view displaying change over time. We propose a fast calculation of isocontour differences based on a quasi-Monte Carlo approach. For interactive visual analysis, we support filtering and selection mechanisms on the multi-run plot and on linked views to physical space visualizations. Our approach can be effectively used for the visual representation of ensembles, for pattern and outlier detection, for the investigation of the influence of simulation parameters, and for a detailed analysis of the features detected. The proposed method is applicable to data of any spatial dimensionality and any spatial representation (gridded or unstructured). We validate our approach by performing a user study on synthetic data and applying it to different types of multi-run spatio-temporal simulation data.\n\nWednesday, Oct. 26, 8:30\u201310:10 \u2013 Key 1+2+5", "uri": "https://vimeo.com/183311390", "name": "[VIS16 Preview] Visual Analysis of Multi-run Spatio-temporal Simulations Using Isocontour Similarity for Projected Views", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:45:39+00:00", "description": "TVCG Paper \n\nOrganizers: Tatiana von Landesberger, Dennis Basgier, Meike Becker\n\nAbstract: The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.\n\nFriday, Oct. 28, 8:30\u201310:10 \u2013 Key 1+2+5", "uri": "https://vimeo.com/183311352", "name": "[VIS16 Preview] Comparative Local Quality Assessment of 3D Medical Image Segmentations with Focus on Statistical Shape...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:45:26+00:00", "description": "TVCG Paper \n\nOrganizers: Henan Zhao, Garnett W. Bryant, Wesley Griffin, Judith E. Terrill, Jian Chen\n\nAbstract: We designed and evaluated SplitVectors, a new vector field display approach to help scientists perform new discrimination tasks on large-magnitude-range scientific data shown in three-dimensional (3D) visualization environments. SplitVectors uses scientific notation to display vector magnitude, thus improving legibility. We present an empirical study comparing the SplitVectors approach with three other approaches - direct linear representation, logarithmic, and text display commonly used in scientific visualizations. Twenty participants performed three domain analysis tasks: reading numerical values (a discrimination task), finding the ratio between values (a discrimination task), and finding the larger of two vectors (a pattern detection task). Participants used both mono and stereo conditions. Our results suggest the following: (1) SplitVectors improve accuracy by about 10 times compared to linear mapping and by 4 times to logarithmic in discrimination tasks; (2) SplitVectors have no significant differences from the textual display approach, but reduce cluttering in the scene; (3) SplitVectors and textual display are less sensitive to data scale than linear and logarithmic approaches; (4) using logarithmic can be problematic as participants\u2019 confidence was as high as directly reading from the textual display, but their accuracy was poor; and (5) Stereoscopy improved performance, especially in more challenging discrimination tasks.\n\nThursday, Oct. 27, 4:15\u20135:55 \u2013 Key 1+2+5", "uri": "https://vimeo.com/183311329", "name": "[VIS16 Preview] Validation of SplitVectors Encoding for Quantitative Visualization of Large-Magnitude-Range Vector Fields", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:45:14+00:00", "description": "TVCG Paper \n\nOrganizers: Guodao Sun, Ronghua Liang, Huamin Qu, Yingcai Wu\n\nAbstract: Analysis and exploration of spatio-temporal data such as traffic flow and vehicle trajectories have become important in urban planning and management. In this paper, we present a novel visualization technique called route-zooming that can embed spatio-temporal information into a map seamlessly for occlusion-free visualization of both spatial and temporal data. The proposed technique can broaden a selected route in a map by deforming the overall road network. We formulate the problem of route-zooming as a nonlinear least squares optimization problem by defining an energy function that ensures the route is broadened successfully on demand while the distortion caused to the road network is minimized. The spatio-temporal information can then be embedded into the route to reveal both spatial and temporal patterns without occluding the spatial context information. The route-zooming technique is applied in two instantiations including an interactive metro map for city tourism and illustrative maps to highlight information on the broadened roads to prove its applicability. We demonstrate the usability of our spatio-temporal visualization approach with case studies on real traffic flow data. We also study various design choices in our method, including the encoding of the time direction and choices of temporal display, and conduct a comprehensive user study to validate our embedded visualization design.\n\nTuesday, Oct. 25, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/183311298", "name": "[VIS16 Preview] Embedding Spatio-temporal Information into Maps by Route-Zooming", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:45:00+00:00", "description": "TVCG Paper \n\nOrganizers: Steve Haroz, Robert Kosara, and Steven L. Franconeri\n\nAbstract: The connected scatterplot visualizes two related time series in a scatterplot and connects the points with a line in tem- poral sequence. News media are increasingly using this technique to present data under the intuition that it is understandable and engaging. To explore these intuitions, we (1) describe how paired time series relationships appear in a connected scatterplot, (2) qual- itatively evaluate how well people understand trends depicted in this format, (3) quantitatively measure the types and frequency of misinterpretations, and (4) empirically evaluate whether viewers will preferentially view graphs in this format over the more traditional format. The results suggest that low-complexity connected scatterplots can be understood with little explanation, and that viewers are biased towards inspecting connected scatterplots over the more traditional format. We also describe misinterpretations of connected scatterplots and propose further research into mitigating these mistakes for viewers unfamiliar with the technique.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311275", "name": "[VIS16 Preview] The Connected Scatterplot for Presenting Paired Time Series", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:44:49+00:00", "description": "TVCG Paper \n\nOrganizers: Oh-Hyun Kwon, Chris Muelder, Kyungwon Lee, Kwan-Liu Ma\n\nAbstract: Information visualization has traditionally limited itself to 2D representations, primarily due to the prevalence of 2D displays and report formats. However, there has been a recent surge in popularity of consumer grade 3D displays and immersive head-mounted displays (HMDs). The ubiquity of such displays enables the possibility of immersive, stereoscopic visualization environments. While techniques that utilize such immersive environments have been explored extensively for spatial and scientific visualizations, contrastingly very little has been explored for information visualization. In this paper, we present our considerations of layout, rendering, and interaction methods for visualizing graphs in an immersive environment. We conducted a user study to evaluate our techniques compared to traditional 2D graph visualization. The results show that participants answered significantly faster with a fewer number of interactions using our techniques, especially for more difficult tasks. While the overall correctness rates are not significantly different, we found that participants gave significantly more correct answers using our techniques for larger graphs.\n\nTuesday, Oct. 25, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311249", "name": "[VIS16 Preview] A Study of Layout, Rendering, and Interaction Methods for Immersive Graph Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:44:21+00:00", "description": "TVCG Paper \n\nOrganizers: Min Chen, Amos Golan\n\nAbstract: In this paper, we present an abstract model of visualization and inference processes, and describe an informationtheoretic measure for optimizing such processes. In order to obtain such an abstraction, we first examined six classes of workflows in data analysis and visualization, and identified four levels of typical visualization components, namely disseminative, observational, analytical and model-developmental visualization. We noticed a common phenomenon at different levels of visualization, that is, the transformation of data spaces (referred to as alphabets) usually corresponds to the reduction of maximal entropy along a workflow. Based on this observation, we establish an information-theoretic measure of cost-benefit ratio that may be used as a cost function for optimizing a data visualization process. To demonstrate the validity of this measure, we examined a number of successful visualization processes in the literature, and showed that the information-theoretic measure can mathematically explain the advantages of such processes over possible alternatives.\n\nWednesday, Oct. 26, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/183311183", "name": "[VIS16 Preview] What May Visualization Processes Optimize?", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:44:08+00:00", "description": "TVCG Paper \n\nOrganizers: Jonathan Palacios, Harry Yeh, Wenping Wang, Yue Zhang, Robert S. Laramee, Ritesh Sharma, Thomas Schultz, Eugene Zhang\n\nAbstract: Three-dimensional symmetric tensor fields have a wide range of applications in solid and fluid mechanics. Recent advances in the (topological) analysis of 3D symmetric tensor fields focus on degenerate tensors which form curves. In this paper, we introduce a number of feature surfaces, such as neutral surfaces and traceless surfaces, into tensor field analysis, based on the notion of eigenvalue manifold. Neutral surfaces are the boundary between linear tensors and planar tensors, and the traceless surfaces are the boundary between tensors of positive traces and those of negative traces. Degenerate curves, neutral surfaces, and traceless surfaces together form a partition of the eigenvalue manifold, which provides a more complete tensor field analysis than degenerate curves alone. We also extract and visualize the isosurfaces of tensor modes, tensor isotropy, and tensor magnitude, which we have found useful for domain applications in fluid and solid mechanics. Extracting neutral and traceless surfaces using the Marching Tetrahedra method can lead to the loss of geometric and topological details, which can lead to false physical interpretation. To robustly extract neutral surfaces and traceless surfaces, we develop a polynomial description of them which enables us to borrow techniques from algebraic surface extraction, a topic well-researched by the computer-aided design (CAD) community as well as the algebraic geometry community. In addition, we adapt the surface extraction technique, called A-patches, to improve the speed of finding degenerate curves. Finally, we apply our analysis to data from solid and fluid mechanics as well as scalar field analysis.\n\nThursday, Oct. 27, 4:15\u20135:55 \u2013 Key 1+2+5", "uri": "https://vimeo.com/183311146", "name": "[VIS16 Preview] Feature Surfaces in Symmetric Tensor Fields Based on Eigenvalue Manifold", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:43:54+00:00", "description": "TVCG Paper \n\nOrganizers: Trevor Hogan, Uta Hinrichs, Eva Hornecker\n\nAbstract: Information visualization has become a popular tool to facilitate sense-making, discovery and communication in a large range of professional and casual contexts. However, evaluating visualizations is still a challenge. In particular, we lack techniques to help understand how visualizations are experienced by people. In this paper we discuss the potential of the Elicitation Interview technique to be applied in the context of visualization. The Elicitation Interview is a method for gathering detailed and precise accounts of human experience. We argue that it can be applied to help understand how people experience and interpret visualizations as part of exploration and data analysis processes. We describe the key characteristics of this interview technique and present a study we conducted to exemplify how it can be applied to evaluate data representations. Our study illustrates the types of insights this technique can bring to the fore, for example, evidence for deep interpretation of visual representations and the formation of interpretations and stories beyond the represented data. We discuss general visualization evaluation scenarios where the Elicitation Interview technique may be beneficial and specify what needs to be considered when applying this technique in a visualization context specifically.\n\nWednesday, Oct. 26, 10:30-12:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/183311099", "name": "[VIS16 Preview] The Elicitation Interview Technique: Capturing People\u00b9s Experiences of Data Representations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-19T13:43:38+00:00", "description": "TVCG Paper \n\nOrganizers: Fan Du, Ben Shneiderman, Catherine Plaisant, Sana Malik, Adam Perer\n\nAbstract: The growing volume and variety of data presents both opportunities and challenges for visual analytics. Addressing these challenges is needed for big data to provide valuable insights and novel solutions for business, security, social media, and healthcare. In the case of temporal event sequence analytics it is the number of events in the data and variety of temporal sequence patterns that challenges users of visual analytic tools. This paper describes 15 strategies for sharpening analytic focus that analysts can use to reduce the data volume and pattern variety. Four groups of strategies are proposed: (1) extraction strategies, (2) temporal folding, (3) pattern simplification strategies, and (4) iterative strategies. For each strategy, we provide examples of the use and impact of this strategy on volume and/or variety. Examples are selected from 20 case studies gathered from either our own work, the literature, or based on email interviews with individuals who conducted the analyses and developers who observed analysts using the tools. Finally, we discuss how these strategies might be combined and report on the feedback from 10 senior event sequence analysts.\n\nThursday, Oct. 27, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/183311073", "name": "[VIS16 Preview] Coping with Volume and Variety in Temporal Event Sequences: Strategies for Sharpening Analytic Focus", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:55:47+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Henan Zhao, Jian Chen\n\nAbstract: We present results of an empirical study comparing integral and separable dimensions for encoding large-magnitude-range vector fields in quantum physics simulation results. Vector magnitude is represented using scientific notation for digits plus exponential terms to improve legibility. To encode the exponential and digit terms, five encoding approaches (lengthy -lengthy, lengthx -lengthy, color-lengthy, texture-lengthy, and color/lengthx -lengthy) are compared in an interactive environment on four tasks: reading magnitude (MAG), ratio estimation (RATIO), magnitude comparison (COMP), and searching for extreme value (MAX). Our results suggested that color-lengthy was the most efficient and effective for the MAG and RATIO tasks. Color/lengthx -lengthy, color-lengthy, and texture-lengthy led to more accurate answers for the MAX tasks.", "uri": "https://vimeo.com/182987261", "name": "[VIS16 Preview] Empirical Guidance on Integral and Separable Marker Substrate for Large-Magnitude-Range Vector Field...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:55:35+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Jian Chen\n\nAbstract: Evaluating complex three-dimensional structures in scientific visualization domains is challenging partly because of the lack of common terminology for visualization techniques. I present a semiotics approach, inspired and extended upon Bertin's semiotics theory, to characterizing diffusion tensor magnetic resonance brain imaging. I demonstrate the usefulness of this approach by mapping selected tensor field visualization papers to this taxonomy.", "uri": "https://vimeo.com/182987239", "name": "[VIS16 Preview] A Semiotics Approach to Characterize Diffusion Tensor Imaging Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:55:22+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Christine Nothelfer, Michael Gleicher, Steven Franconeri\n\nAbstract: Multiclass data visualizations allow viewers to compare one dataset to another. The visual marks that represent these datasets, or classes, are visually distinguished from one another by easily perceived visual feature differences, such as color or shape. A designer of a graph or map might encode one class of marks as either red, or circular, and another class as either blue, or triangular. One common technique is to combine these cues in a redundant fashion, encoding one class as red and circular, and the other as blue and triangular, under the assumption that a larger difference (via multiple differing features) should help. Recent work [6] has empirically demonstrated strengthened grouping and improved accuracy in segmentation of redundantly coded objects. Does this redundancy benefit generalize to more realistic displays, and to other measures such as segmentation speed? We demonstrate in an experiment that redundant coding can lead to a small improvement in speed of visual differentiation in a simulated dataset in a crowded display.", "uri": "https://vimeo.com/182987201", "name": "[VIS16 Preview] Redundant Coding Can Speed Up Segmentation in Multiclass Displays", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:55:09+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Evan Barba, Yifang Wei, Janet Mann, Lisa Singh\n\nAbstract: This work investigates how exploration can enhance traditional scientific methodologies. Specifically, we look at the scientific reasoning process of student researchers involved with the Shark Bay Dolphin Research Project. We use the Invenio-Workflow system as a visual data exploration tool to see if it can augment traditional research methodology to enhance the scientific process. We find that seeing and interacting with dolphin social networks recreated in the software appears to offer benefits to traditional scientific inquiry.", "uri": "https://vimeo.com/182987182", "name": "[VIS16 Preview] Integrating Visual Exploration into Traditional Scientific Research Methodology", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:54:52+00:00", "description": "VIS Poster (Best InfoVis Poster)\n\nOrganizers / Panelists: Abhisekh Patra, Lyn Bartram, Maureen Stone\n\nAbstract: The communication of affect, a feeling or emotion, is central to creating engaging visual experiences. We report research into how different colour properties (lightness, chroma and hue) contribute to different affective impressions in information visualization applications. Our results provide initial evidence that colour properties can be manipulated to achieve affective expressiveness in information visualization.", "uri": "https://vimeo.com/182987151", "name": "[VIS16 Preview] Affective Colour Palettes in Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:54:26+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: S\u00f8ren Knudsen, Mikkel R\u00f8nne Jakobsen\n\nAbstract: We introduce Schedulater: A tool that visualizes production data and predictions to help plant operators schedule their tasks. The tool visualizes streaming data from the production system and a predictive model based on first principles. We follow a design study approach, collaborating with engineers and operators. We describe the tool, key tasks of operators, design goals, and discuss challenges in integrating predictions in the context of streaming data.", "uri": "https://vimeo.com/182987116", "name": "[VIS16 Preview] Schedulater: Supporting Plant Operators in Scheduling Tasks by Visualizing Streaming Process Data and Model...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:50:26+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Vanessa Pe\u00f1a-Araya, Jorge Bahamonde, Barbara Poblete, Benjamin Bustos\n\nAbstract: Several techniques can be used to display and analyze geographical evolution of data over time, including single views, multi views or animations. However, displaying geographical multivariate data can still be a challenge. To address this issue, we designed Cartoglyphs; a visual representation of the world in the shape of simple glyphs. With Cartoglyphs, the world is reduced to small icons which make comparison easier when several frames are displayed at the same time. In this paper we present our preliminary versions of Cartoglyphs using Dorling cartograms. We describe their application in a case study in which we followed the geographical distribution of tweets commenting a particular news event over several days. In addition, by using them we identified political interactions among countries within this real-world event. We observed that Cartoglyphs allows us to observe change of more than one geographical variable over time. In the future, we will explore variations of this initial design and formally test the effectiveness of this type of visualization.", "uri": "https://vimeo.com/182986765", "name": "[VIS16 Preview] Cartoglyphs: Reducing the World to a Glyph for Quick Exploration and Comparison of Spatio-Temporal Change", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:50:14+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Rishu Vaid, David Koop\n\nAbstract: Vessel trajectories in coastal waters, while still constrained by shipping lanes, can be more diverse than those where traffic moves along defined paths like roads, paths, or tracks. Thus, displaying these trajectories as line segments can lead to visual clutter where it becomes difficult to estimate density or locate patterns. Previous work has shown density to be a meaningful way to visualize vessel traffic, and we investigate the utility of hexagonal binning in order to locate patterns. Instead of binning all self-reported locations, we group all points by voyage and characterize density by the number of voyages passing through a region. While the bins help characterize the use of a particular area, they do not provide any detail about direction. We propose an interactive method where hovering over one of six triangles of a hexagon displays the trajectories in that direction that pass through the regions.", "uri": "https://vimeo.com/182986740", "name": "[VIS16 Preview] Maritime Trajectory Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:49:56+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: John R. Hott, Worthy N. Martin, Kathleen Flake\n\nAbstract: We adapt chord and flow visualization techniques to display complex evolving familial and lineage structures. We modified flow diagrams into left-to-right \"lineage flows,\" depicting the family units as nodes. Each unit is then connected with others based on the participants; each edge representing a participant. Since these family units evolve, as parents are married and children are born and adopted, we link each node to a temporal chord diagram adapted with a \"time-slider\" to portray the dynamic intra-familial relationships. We apply our approach to the marital structures of Mormon society in mid-1800s Nauvoo, IL. In this application, our visualizations present family units that include multiple spouses and several definitions of inter-spousal and parent-child relationships.", "uri": "https://vimeo.com/182986711", "name": "[VIS16 Preview] Visualizing Dynamics of Complex Familial Structures", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:49:45+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Julia Hocket, Shweta Bansal, Han-Hsi Liu, Lisa Singh\n\nAbstract: We present a visual analytic dashboard that allows for exploratory and comparative spatial analysis of two data sets on influenza incidence and influenza vaccination policy. The dashboard allows researchers and policy makers to intuitively analyze flu incidence and policy at multiple spacial and temporal scales, enabling intuitive analysis of their relationship across scales.", "uri": "https://vimeo.com/182986698", "name": "[VIS16 Preview] Analyzing influenza incidence and policy data at different temporal and spatial scales", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:49:27+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Shigeo Takahashi, Hsiang-Yun Wu, Masatoshi Arikawa, Sheung-Hung Poon\n\nAbstract: Consistent displacement and selection of map elements in accordance with the map scale has recently been technically important in digital cartographic generalization. This is primary due to the recent demand for informative mapping systems especially in use of smartphones and tablets. However, such sophisticated map editing has usually been conducted manually by expert cartographers, and thus usually results in a time-consuming and error-prone process. In this study, we formulate the displacement and selection process, especially in the context of cartographic genralization, as a constrained optimization problem. We first identify underlying spatial relationships among map elements such as points and lines on each map scale as constraints, and optimize the cost function that penalizes excessive displacement and elimination of the map elements. Several examples are also provided to demonstrate that the proposed approach automatically maintains consistent mapping regardless of the change in the map scale.", "uri": "https://vimeo.com/182986683", "name": "[VIS16 Preview] Optimized Displacement and Selection in Scale-Aware Map Editing", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:49:16+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Shenghui Cheng, Wei Xu, Wen Zhong, Klaus Mueller\n\nAbstract: A wide variety of color schemes have been devised for mapping scalar data to color. Some use the data value to index a color scale. Others assign colors to different, usually blended disjoint materials, to handle areas where materials overlap. A number of methods can map low-dimensional data to color, however, these methods do not scale to higher dimensional data. Likewise, schemes that take a more artistic approach through color mixing and the like also face limits when it comes to the number of variables they can encode. We address the challenge of mapping multivariate data to color and avoid these limitations at the same time. It is a data driven method, which first gauges the similarity of the attributes and then arranges them according to the periphery of a convex 2D color space, such as HSL. The color of a multivariate data sample is then obtained via generalized barycentric coordinate (GBC) interpolation.", "uri": "https://vimeo.com/182986664", "name": "[VIS16 Preview] A Data-Driven Approach for Mapping Multivariate Data to Color", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:49:03+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Bishal Chamling, Adam M. Terwilliger, Jonathan P. Leidig, Greg Wolffe\n\nAbstract: Human migration patterns are critical to understanding the social, economic, and cultural forces specific to a country. In practical terms, accurate and dynamic models of human migration and daily behavior can assist in the design and implementation of local and national development policies. Anonymized mobile phone records are being used to extend traditional data resources such as census surveys in order to provide new, rich data sources that can be mined to create visualizations of mobility models. In this project, mobile phone data records and custom visualization tools were utilized to study migration patterns along several different dimensions. The combination of fine- and coarse-grained analyses allow for the discovery and investigation of trends and patterns in geographic mobility, and the visualizations allow researchers to better understand these phenomena.", "uri": "https://vimeo.com/182986638", "name": "[VIS16 Preview] Multi-Granularity Visualizations of Geographic Mobility", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:48:51+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Jillian Aurisano, Abhinav Kumar, Alberto Gonzalez, Jason Leigh, Barbara DiEugenio, Andrew Johnson\n\nAbstract: InfoVis novices' have been found to struggle with visual data exploration. A 'conversational interface' which would take natural language inputs to visualization generation and modification, while maintaining a history of the requests, visualizations and findings of the user, has the potential to ameliorate many of these challenges. We present Articulate2, initial work toward a conversational interface to visual data exploration.", "uri": "https://vimeo.com/182986622", "name": "[VIS16 Preview] Articulate2: Toward a Conversational Interface for Visual Data Exploration", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:48:36+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Jos\u00e9 Matute, Lars Linsen\n\nAbstract: Cohort studies allow us to explore the combined effects of life-style, occupation, social, environmental, psychosocial, and genetic factors on disease development. Investigating etiology of a disease depends on the combination of tacit medical knowledge and multivariate analysis on a wide array of collected data. We propose a visual analysis method to explore mixed categorical, ordinal, and numerical attributes in a single view. A frequency-based representation for categorical and ordinal attributes is presented and a proposed initial categorical con\u00ef\u00ac\u0081guration is selected based on maximizing separability of data points. The proposed method allows for exploring mixed multivariate datasets for classi\u00ef\u00ac\u0081cation in a human expertise inclusive manner. We used the proposed approach to explore determinants with respect to a chosen classi\u00ef\u00ac\u0081cation.", "uri": "https://vimeo.com/182986593", "name": "[VIS16 Preview] Visual Analysis of Mixed Numerical and Categorical Data in Cohort Studies", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:48:15+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Nathan Hodas, Alex Endert\n\nAbstract: Interactive visual analytic systems enable users to discover insights from complex data. Users can express and test hypotheses via user interaction, leveraging their domain expertise and prior knowledge to guide and steer the analytic models in the system. For example, semantic interaction techniques enable systems to learn from the user's interactions and steer the underlying analytic models based on the user's analytical reasoning. However, an open challenge is how to not only steer models based on the dimensions or features of the data, but how to add dimensions or attributes to the data based on the domain expertise of the user. In this paper, we present a technique for inferring and appending dimensions onto the dataset based on the prior expertise of the user expressed via user interactions. Our technique enables users to directly manipulate a spatial organization of data, from which both the dimensions of the data are weighted, and also dimensions created to represent the prior knowledge the user brings to the system. We describe this technique and demonstrate its utility via a use case.", "uri": "https://vimeo.com/182986557", "name": "[VIS16 Preview] Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:48:00+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Seth Borges, David Koop\n\nAbstract: There has been significant work on interactive systems for analyzing hierarchical data using treemap visualizations, and much of it has focused on advanced filtering and querying. Less work has been done on developing more intuitive interactions that allow users to more fluidly browse the data shown by treemaps. We present two techniques that leverage direct manipulation to help users compare nodes and investigate hierarchy relationships. First, to compare two nodes, a user can drag one node on top of the other, and the dragged node will transform to march the aspect ratio of the node below, facilitating direct visual comparison. This allows efficient investigation of relationships for a number of target regions. The second technique, inspired by relief shearing, aids users in understanding hierarchical relationships in a treemap. As a user selects and drags a region, the hierarchy of that region shears to better show the depth and groupings of nodes. We have conducted a preliminary user study to evaluate these ideas, and its analysis suggests possibilities for future research.", "uri": "https://vimeo.com/182986535", "name": "[VIS16 Preview] Fluid Treemap Interactions", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:47:46+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Vasundhara Dehiya, Klaus Mueller\n\nAbstract: Due to the controversy regarding use of personal email on non-government servers by Hillary Clinton during her time as Secretary of State, her email data was made public. Yet, reading through them is impractical. In this poster, we provide a visual analysis of the content of these emails. Based on the 7945 emails available, we identify the relation between the textual content of the emails with world policies. We unravel how the content of these emails are reflective of US emotion and behaviour with other countries around the world along with their relative importance. We also unravel correlations in the data to predict some features of content in redacted emails based on available data. \\", "uri": "https://vimeo.com/182986518", "name": "[VIS16 Preview] Analyzing Hillary Clinton's Emails", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:47:28+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Rafael Henkin, Aidan Slingsby, Jason Dykes\n\nAbstract: Designing visualizations for exploration of temporal data requires several choices based on aspects of time and visual representation. Previous taxonomies have described existing visualizations based on these aspects without relating the visual representations. We propose to characterize existing visualization techniques based on both semantic aspects of time and visual representations. Our design space helps to identify how these different visual representations relate and give the possibility to combine attributes of representation from different techniques. We compare two examples of visualizations from the literature based on our taxonomy.", "uri": "https://vimeo.com/182986490", "name": "[VIS16 Preview] Characterizing Visual Exploration Techniques for Temporal Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:47:05+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Peter Kerpedjiev, Kasper Dinkla, Hendrik Strobelt, Hanspeter Pfister, Peter Park, Nils Gehlenborg\n\nAbstract: By collating and comparing evidence researchers can not only support existing assertions but formulate and explore new hypotheses. In the case of large genomic datasets, aggregation and abstraction are necessary to pare down the raw data to a tractable size for web-based display. In this submission, we tile both one and two-dimensional genomic data to support high-level overviews and zooming and place them on a common coordinate frame to facilitate collation and comparison. Researchers using the resulting application can compare data produced using differing experimental protocols at varying resolutions to look for patterns that support or reject existing hypotheses or to simply explore and discover correlations. A demo is available at http://hms-dbmi.github. io/higlass/.", "uri": "https://vimeo.com/182986440", "name": "[VIS16 Preview] Multiscale Display of 1D, 2D, and 3D Genomic Information", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:46:48+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Lee Ebinger, Avery Sandborn\n\nAbstract: The purpose of the geospatial analysis illustrated in the poster is to identify patterns of crop migration and change in the North Central region of the United States for corn, soybeans, and spring wheat, from 2006 to 2015. The resulting geospatial analysis and accompanying cartographic products illustrate the crop footprint over the past ten years has continuously extended in the northwest and west directions. The graphics also demonstrate geographic information system (GIS) and cartographic techniques used to visualize migration and change in the raster crop-specific data for different perspectives and purposes including: 1) emphasize early or recent years along a continuum of years, 2) compare data separately across years, 3) summarize multiple years of data, and 4) graph the percent and characteristic of the change.", "uri": "https://vimeo.com/182986415", "name": "[VIS16 Preview] GIS and Cartographic Techniques Using Multi-Temporal Raster Datasets to Illustrate Crop Migration and Change", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:46:34+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Mithileysh Sathiyanarayanan, Cagatay Turkay\n\nAbstract: Electronic discovery (E-discovery) is a legal process for investigating various events in the corporate world, for the purpose of producing/obtaining evidence, one such example is an email communication (eg. Enron case). Investigating emails collected over a period of time, manually, is a strenuous process and the tools currently available on the market are based on simple keyword search and legal firms charge companies based on the volume of information produced by the search, which is then manually reviewed intensely. This results in significant costs for the company or in a number of cases settlement because they can\u00e2\u20ac\u2122t afford the costs of E-discovery. So, there is a great need to determine, visualise and understand whether email subsets are normal or abnormal, pertinent or privileged, relevant (interesting) or immaterial in a quick time. In order to determine relevant subsets for a legal case and to gain invaluable insight in a quick time from the email communications, we propose a multi-modal and multi-level approach which will generate auto- mated visual representations using a manual keyword search facility that will extract the most relevant information from the email data and aids in comparing two subsets of information. In this paper, we discuss the literature review carried out, initial design process, prototypes developed and the workshops conducted. As a future work, we aim to develop a full-fledged E-discovery tool that could be implemented by the organisations to investigate email communications.", "uri": "https://vimeo.com/182986391", "name": "[VIS16 Preview] Determining and Visualising E-mail Subsets to Support E-discovery", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:46:21+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Jing Wang, Yufang Ho, Zhijie Xu, Dan McIntyre, Jane Lugea\n\nAbstract: Forensic statements are lengthy and contain large amounts of complex information. Consequently, it is often difficult for readers of such reports to identify connections between disparate pieces of evidence and to properly and objectively assess their value to the case in question. Readers have no alternative but to rely on intuition and experience to make sense of the complex arguments and propositions arising from forensic evidence. This research investigates the opportunities in the convergence of linguistic approaches to extracting and reconstructing the cognitive structure, i.e. ``Text-Worlds'', in a statement, and the computerized operational settings for enabling effective and hopefully more accurate interpretation of forensic discourse through visualization. This will be of benefit to a wide range of stakeholders, including investigating officers, prosecuting and defence counsels, judges and jurors.", "uri": "https://vimeo.com/182986380", "name": "[VIS16 Preview] A Visualization Method for Understanding Forensic Statements", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:46:05+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Fangyan Zhang, Song Zhang, Andrew Mercer\n\nAbstract: Ensemble data are becoming increasingly more common in scientific research. Consequently, ensemble visualization has become an important means for analyzing and inferences of the simulation results. One challenge in ensemble visualization is the overwhelming amount of details that disguise important large-scale features. This poster approaches the problem by applying the scale space methods to ensemble data visualization. Features extracted from data and contours like extrema points, and maximal/minimal curvature points are constructed and shown in scale space. This helps remove small features at a high scale in visualization and facilitates the comparison between ensemble members and the identification of differences at varying scales, making it easier to focus on features of a certain size and the key differences among separate ensemble members. We applied our approach to an ensemble numerical weather data, and conducted a qualitative evaluation of the approach. The results show that our method is useful in a research environment, in particular, for feature tracking and identification of large-scale phenomena within ensemble data sets.", "uri": "https://vimeo.com/182986347", "name": "[VIS16 Preview] Visualizing Ensemble Data in Scale Space", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:45:50+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Nelson Silva, Lin Shao, Tobias Schreck, Eva Eggeling, Dieter Fellner\n\nAbstract: We present a new open-source prototype framework to explore and visualize eye-tracking experiments data. Firstly, standard eye-trackers are used to record raw eye gaze data-points on user experiments. Secondly, the analyst can configure gaze analysis parameters, such as, the definition of areas of interest, multiple thresholds or the labeling of special areas, and we upload the data to a search server. Thirdly, a faceted web interface for exploring and visualizing the users\u00e2\u20ac\u2122 eye gaze on a large number of areas of interest is available. Our framework integrates several common visualizations and it also includes new combined representations like an eye analysis overview and a clustered matrix that shows the attention time strength between multiple areas of interest. The framework can be readily used for the exploration of eye tracking experiments data. We make available the source code of our prototype framework for eye-tracking data analysis.", "uri": "https://vimeo.com/182986330", "name": "[VIS16 Preview] Sense.me - Open Source Framework for the Exploration and Visualization of Eye Tracking Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:45:40+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Holger Stitz, Samuel Gratzl, Harald Rogner, Marc Streit\n\nAbstract: The workload of a cloud infrastructure is continuously changing. Administrators need to find the balance between quality of service and cost efficiency. Proactively avoiding performance bottlenecks is a challenging endeavor due to the number and heterogeneity of the network components, the relationships among them, and the associated attributes. While pattern detection and simulation methods can be used to predict the performance of the network, efficient tools for exploring and evaluating those predictions are missing. In this poster we present first results of a design study for visually evaluating cloud infrastructure performance predictions. We combine established visualization techniques for exploring large item collections with custom techniques for investigating predicted time-series data, allowing the administrator to effectively monitor, evaluate, and optimize cloud infrastructure.", "uri": "https://vimeo.com/182986309", "name": "[VIS16 Preview] Visual Evaluation of Cloud Infrastructure Performance Predictions", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:45:27+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Fritz Lekschas, Nils Gehlenborg\n\nAbstract: The number of data sets in biomedical repositories has grown rapidly over the past decade, providing scientists with tremendous opportunities to re-use data. In order to effectively exploit existing data, it is crucial to understand the content of repositories and to discover data relevant to a question of interest. These are challenging tasks, as most repositories currently only support finding data sets through text-based search of metadata and in some cases also through metadata-based browsing. In order to address these challenges, we have developed SATORI\u00e2\u20ac\u201dan ontology-guided visual exploration system\u00e2\u20ac\u201dthat combines a powerful meta-data search with a treemap and a node-link diagram that visualize the repository structure, provide context to retrieved data sets, and serve as an interface to drive semantic querying and browsing of the repository. We have integrated an open-source, web-based implementation of SATORI in the data repository of the Refinery Platform for biomedical data analysis and visualization (http://satori.refinery-platform.org).", "uri": "https://vimeo.com/182986293", "name": "[VIS16 Preview] SATORI: A System for Ontology-Guided Visual Exploration of Biomedical Data Repositories", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:45:14+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Li Liu, Deborah Silver, Karen Bemis\n\nAbstract: Understanding the relationship between jobs and majors is of interest today in the higher education landscape but is not always straightforward. Students struggle to understand such connections. Many professors cannot provide clear links between what is being taught and opportunities that are available.\u00c2 In this paper, we propose a novel visual exploration approach called JobViz that allows students (and professors) to explore job posting data in a more holistic and education-centric way. The JobViz plots the relationships between majors and jobs, displays keywords of the majors or jobs selected by users, and computes some simple statistics based on the search choices. It provides an interactive view of the data within a focus+context framework. The focus+context framework is important to allow comparisons between different majors and employment groups. We worked with Rutgers Career Service and evaluated our visualization of the job database by running focus groups for undergraduate students and career counselors using our JobViz.", "uri": "https://vimeo.com/182986281", "name": "[VIS16 Preview] JobViz: Interactive Visualization of Majors &amp; Jobs", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:44:59+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Gordan Ristovski, Nicole Garbers, Horst K. Hahn, Tobias Preusser, Lars Linsen\n\nAbstract: Radiofrequency (RF) ablation has become a widely used standard procedure for liver tumor treatments. Avoiding under-ablation means lowering the chances for a tumor recurrence. At the same time, avoiding over-ablation means avoiding destruction of healthy tissue cells. Medical experts use biomedical simulations as an essential part of the treatment plan to assess the ablation area, and therefore, to lower the risk of under-ablation. The many parameters and assumptions that come into play in the RF ablation simulation as well as the imaging techniques lead to uncertainty that, if not conveyed properly to the medical experts, can hinder optimal treatments in decision making. We present an approach to capture the uncertainty associated with every image voxel through stochastic simulations and aggregate the uncertainties captured by different parameter configurations. We convey the uncertainties to the medical experts in a three-tier decision using traffic light maps.", "uri": "https://vimeo.com/182986252", "name": "[VIS16 Preview] Capturing and Visualizing Uncertainty in Liver Ablation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:44:47+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Ji Lan, Xiao Xie, Junhua Lu, Tai-Quan Peng, Wei Chen, Yingcai Wu\n\nAbstract: With the rapid development of massively multiplayer online role-playing games (MMORPGs), a huge amount of fine grained data about players\u00e2\u20ac\u2122 in-game activities has been recorded by MMORPGs operators. The data resents great opportunities to study the dynamic interplay between players\u00e2\u20ac\u2122 multiplex behaviors and investigate the roles of various social structures underlying such interplay. However, modeling and visualizing these behavioral data has remained a challenge. This study proposes a new visual analytics system called BeXplorer to integrate the computational model and interactive visualizations, explore the dynamic interplay between players\u00e2\u20ac\u2122 consumption and communication behavior, and examine how this interplay is bounded by social structures in which players are embedded.", "uri": "https://vimeo.com/182986223", "name": "[VIS16 Preview] BeXplorer: Visual Analytics of Multiplex Behaviors in MMORPGs", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:44:27+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Karen Bemis, Li Liu, Deborah Silver, Dujuan Kang, Enrique Curchitser\n\nAbstract: Visualization of Gulf Stream eddies produced in a regional ocean model simulation has improved understanding of eddy kinetics and structure. In this work, we extract and track eddies over three successive years (2005, 2006, and 2007) of daily frames. The tracking metadata permits the generation of a series of long-lived (&gt;two weeks) eddy evolution paths which reflect the life- cycle of individual eddies. Important geometric and physical properties of the eddies lead to three types of visualizations. First, a regional three-dimensional eddy distribution map shows eddy occurrence over time in the context of the ocean basin. Second, illustrative visualization of eddy evolution paths demonstrates the kinetics of eddies. Third, a vorticity isosurface, color-coded for ocean temperature and salinity, illustrates internal structure and vertical transport. We provide GUI-based tools to permit scientists to easily browse back and forth exploring eddy characteristics at different levels. This is the first time that the three-dimensional evolution of eddies has been observed directly from Regional Ocean Modeling System (ROMS) data.", "uri": "https://vimeo.com/182986192", "name": "[VIS16 Preview] Case Study on Visualizing Gulf Stream Eddies from ROMS", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:44:15+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Juliette Pardue, Mridul Sen, Christos Tsolakis, Reid Rankin, Ayush Khandelwal, Michele Weigle\n\nAbstract: In this paper, we present our tool, WorldVis, a generalized tool for visualizing datasets of quantitative attributes per country over time via a choropleth map. The datasets for an individual country is visualized using small-multiples line graphs. We extend the functionality of the choropleth map to encode the attributes based on continent average. We restricted the number of saturation bins for the choropleth map to make the visualization more salient. A histogram was added to show how many countries had data for a particular year. Summary data is computed for each dataset for each year, so at a glance, the user can see statistical information including the country with the minimum and maximum value. Our focus is on providing a generalized visualization tool that is suitable for common task abstractions.", "uri": "https://vimeo.com/182986179", "name": "[VIS16 Preview] WorldVis:\\nA Visualization Tool for World Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:44:03+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Maya Okawa, Aki Hayashi, Kim Hideaki, Takuya Nishimura, Hiroyuki Toda\n\nAbstract: In this paper, we present a visual analytical framework that aggregates the many complex trajectories in the spatiotemporal domain; and visualizes the aggregated trajectories with temporal changes in them. The spatiotemporal aggregation is realized by incorporating the temporal aspect of trajectories into the existing partition-and- group framework. Furthermore, we developed 3D interactive visualization tool which allows users to effectively explore the ag- gregated results.", "uri": "https://vimeo.com/182986159", "name": "[VIS16 Preview] Visualization of Crowd movements at Large-scale Events", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:43:49+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Wei Chen, Minfeng Zhu, Feiran Wu, Zhaosong Huang, Wanqi Hu, Tao Wang, Lingfei Zhao, Xumeng Wang, Fan Zhang, Ross Maciejewski\n\nAbstract: Modeling human mobility is a critical task in fields such as urban planning, ecology, and epidemiology. Given the current use of mobile phones, there is an abundance of data that can be used to create models of high reliability. Existing techniques can reveal the macro-patterns of crowd movement, or analyze the trajectory of an individual object; however, they typically focus on geographical characteristics. This paper presents a data-driven visual exploration approach that characterizes and studies crowd mobility over multiple granularities. The key to our approach is an adaptive data representation, the adaptive mobility transition graph, that is globally generated from citywide human mobility data by defining the temporal trends of crowd mobility and the interleaved transitions between different mobility patterns. We describe the design, creation and manipulation of the adaptive mobility transition graph.", "uri": "https://vimeo.com/182986135", "name": "[VIS16 Preview] Adaptive Mobility Transition Graph: A Visual Exploration Approach for Citywide Crowd Mobility", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:43:38+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Tomomi Takashina, Yuji Kokumai\n\nAbstract: One of the purposes of visualization is to support the intellectual activities of humans. Some intellectual activities require the rapid iterations of decisions and actions. If we use visualization to support such activities, visualization should be applied in real time and in complex event loops. However, most of conventional visualization toolkits lacks designs to satisfy such requirements. Therefore, we propose HistoryMan as a generic light-weight and loose-coupling visualization toolkit especially for interaction history with a device. Visual interaction history is important because it supports trial-and-errors, that is, one of the popular methodologies for problem solving and it is potentially useful in a wide variety of applications. HistoryMan manages interaction history data and calculates the layout of visual interaction history based on a specification given in the initialization phase of toolkit. HistoryMan layouts interaction history data in meaningful ways in order to accord with the mental model of users. We applied HistoryMan to microscope control application and demonstrated its effectiveness. Though visual interaction history has been available only for limited kinds of applications so far, HistoryMan enables generic applications provide visual interaction history function. \\", "uri": "https://vimeo.com/182986120", "name": "[VIS16 Preview] HistoryMan: a Generic History Visualization Framework in the Loop", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:43:13+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Minsuk Choi, Jaeseong Yoo, Ashley S. Beavers, Scott Langevin, Chris Bethune, Sean McIntyre, Drake Barry, Jaegul Choo, Park Haesun\n\nAbstract: We present a visual analytics system that supports the geospatiotemporal analysis of social media data based on a large-scale distributed topic modeling technique. Through the analysis of social media data in a given time and region, we can identify critical events in real time. However, it takes significant time to perform such analyses against a large amount of social media data. As a way to handle this issue, we developed an efficient tile-based topic modeling approach, which divides textual data into multiple subsets with respect to different regions and time frames at different zoom levels and applies topic modeling to each subset.", "uri": "https://vimeo.com/182986077", "name": "[VIS16 Preview] Tile-Based Spatio-Temporal Visual Analytics via Topic Modeling on Social Media", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:43:00+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Fangzhou Guo, Yingcai Wu, Chenyang Ji, Bingzhang Dai, Tianye Zhang, Huihua Guan, Wei Chen, Tim Dwyer\n\nAbstract: The theory of structural holes suggests that a person who is a member of a social network benefits by connecting disconnected people. It is practically useful and theoretically significant to detect such structural hole spanners and track their changing status in dynamic social networks. Researchers have proposed various models to detect structural hole spanners. However, different models have their own strengths and weaknesses. Detecting and tracking structural hole spanners in a dynamic network remains challenging. In this study, we propose the fusion of multiple, widely used models through a novel multi-scale visualization of a sequence of ensembles of rankings (i.e., temporal ensemble rankings). An ensemble consists of multiple rankings at a time stamp and each ranking is computed by a model to rank the nodes according to their likelihood of being a spanner. Thus, we transform the problem of finding spanners into a new problem of visualizing temporal ensemble rankings. Our visualization, called SpannerFinder, consists of three major views: a cluster view, a rank view, and a detail view with two novel glyph designs.", "uri": "https://vimeo.com/182986061", "name": "[VIS16 Preview] SpannerFinder: Interactive Visualization of Temporal Ensemble Rankings to Explore Structural Holes in...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:42:46+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Fangzhou Guo, Junhua Lu, Feiran Wu, Tianye Zhang, Wei Chen, Lei Shi, Huaming Qu\n\nAbstract: Discovering the correlations among variables of multiple sensor data is challenging because the correlation time-series are long-lasting, multi-faceted, and information-sparse. In this paper, we propose a novel visual representation, called Time-correlation Partitioning (TCP) tree that compactly characterizes correlations of multiple variables and their evolutions. A TCP tree is generated by partitioning information-theoretic correlation time-series into pieces with respect to the variable hierarchy and temporal variations, and reorganizing the pieces into a hierarchical nested structure. The visual exploration of a TCP tree provides a sparse data traversal of the correlation variations, and a situation-aware analysis of correlations among variables.", "uri": "https://vimeo.com/182986044", "name": "[VIS16 Preview] Information-Theoretic Visual Exploration of Multivariate Sensor Time-series with A Time-Correlation...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:42:34+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Hsiang-Yun Wu, Shigeo Takahashi, Masatoshi Arikawa, Sheung-Hung Poon\n\nAbstract: Nowadays, digital map service accommodates a large amount of spatial data and thus facilitates users to dynamically navigate the map contents across multiple scales on small mobile devices. In this context, consistently placing map labels through user intervention is quite significant but still technically challenging especially when labels are associated with the multiple layers inherent in the map contents. In this paper, we introduce a genetic based approach for optimizing placement of annotation labels having different ranges of map scales, by maximizing their visibility across the entire scale while avoiding unwanted mutual overlaps and sudden popping effects. This is accomplished by grouping the annotation labels according to their importance first and then composing composite chromosomes each of which is reordered for optimizing the target objective function. We also present the effectiveness of the proposed approach together with the design of our prototype system for better interactive understanding of the map contents.", "uri": "https://vimeo.com/182986033", "name": "[VIS16 Preview] Consistent placement of labels with different scale ranges", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:42:17+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Hyoji Ha, Hyunwoo Han, Sungyun Bae, Jihye Lee, Sunjoo Bang, Sangjoon Son, Changhyung Hong, Hyunjung Shin, Kyungwon Lee\n\nAbstract: This study aims to develop a visualization system based on automated similarity analysis in order to propose a method to help facilitate the diagnosis of dementia patients. This cluster visualization consists of a node-link diagram, which involves a clustering of dementia patients as they represent each node, and parallel coordinate visualization, which suggests the test results of patients in the form of line graphs. We configured this visualization after analysing the variables used when examining the patients with dementia and the actual result records obtained from CREDOS (Clinical Research Center for Dementia of South Korea). This system provides a real-time response depending on which variables are selected in the parallel coordinate, and automatically analyses similarity for the node-link diagram while changing the clustering structure and the location of the node according to its dynamic similarity. We thus discovered that it allows users to easily analyse the test progress based upon certain variables, by combining the selected variables via parallel coordinate and node-link diagram analysis. This work is expected to satisfy the growing need for a new system in which the clinician can subdivide and regroup the patients as needed, which eventually helps diagnose various aspects of dementia patients.", "uri": "https://vimeo.com/182986004", "name": "[VIS16 Preview] A Visualization System for Clustering Dementia Patients based on Automated Similarity Analysis", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:42:01+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Tan Tang, Guodao Sun, Tai-Quan Peng, Ronghua Liang, Hong Zhou, Yingcai Wu\n\nAbstract: Rapid advancement of social media tremendously facilitates and acceler- ates the information diffusion among users around the world. How can we quantify the interaction between users from different geolocations in the diffusion process? How will the spatial patterns of information dif- fusion change over time? To address these questions, a dynamic social gravity model (SGM) is proposed to quantify the time-varying spatial interaction behavior among social media users in information diffusion. SocialFlow, a visual analytic system, is also developed to support spatial and temporal investigative tasks. It provides a temporal visualization to allow users to quickly identify the overall temporal diffusion patterns. When a meaningful temporal patterns is identified, SocialFlow utilizes a new occlusion-free spatial visualization, which integrates a node-link diagram into a circular cartogram for further analysis.", "uri": "https://vimeo.com/182985967", "name": "[VIS16 Preview] SocialFlow: Visual Analysis of Spatio-temporal Diffusion of Information on Social Media", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:41:47+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Hua Guo, David Laidlaw\n\nAbstract: We present an empirical study of a visual analysis tool for developing research proposals. We consider the process as a sensemaking task and grounded the design of the tool in sensemaking theories. Our tool, ThoughtFlow, is designed to structure and visualize literature collections using topic models to bridge the information gap between two sensemaking phases -- framing and elaboration -- in research proposal writing. To help users transition smoothly between the two phases, we designed two types of embedded visualizations -- sparkline visualizations of citation information and paper thumbnails showing the ``age'' of different parts of the proposal. We report findings from two preliminary user studies. The results suggest that explorations afforded by topic models match well with later stages of the proposal development process when coherent topics have emerged, but keyword-based search seem to be more efficient during the early stages when users are still relying heavily on individual keywords to gather background knowledge. We also found a positive relationship between reduced number of transitions between the two sensemaking phases and increased proposal quality.", "uri": "https://vimeo.com/182985943", "name": "[VIS16 Preview] Supporting Sensemaking Transitions in Research Proposal Writing through Topic Modeling and Embedded...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:41:35+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Robert Pienta, Alex Endert, Shamkant Navathe, Duen Horng Chau\n\nAbstract: Large data-rich network datasets are ubiquitous. Increasing research and development in graph databases offers analysts the ability to query large networks for interesting patterns. Graph querying allows analysts to quickly find matches for desired subgraphs from a large network (e.g., a clique of suspiciously communicating machines on a company intranet). Many of these systems do not support the exploration of results and leave the analyst with ineffective visualizations like tables and lists. We present our work in progress, VIGOR, an interactive visual system for summarizing and investigating large numbers of graph querying results.", "uri": "https://vimeo.com/182985929", "name": "[VIS16 Preview] Making Sense of Graph Query Results: Interactive Summarization and Exploration", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:41:20+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Shenghui Cheng, Pengcheng Cui, Klaus Mueller\n\nAbstract: Embedding high-dimensional data into a 2D canvas is a popular strategy for their visualization. It allows users to appreciate similarity relationships among the data points by their spatial organization on the 2D display. In this work we consider the case where the collection of data points also serves as a scalar field for one of the dataset\u00e2\u20ac\u2122s attributes, essentially forming samples of the attribute\u00e2\u20ac\u2122s continuous function in this embedded space. We study methods by which this continuous function can be estimated from the discrete samples, making certain assumptions on the function\u00e2\u20ac\u2122s smoothness in high-dimensional space. Our method allows users to create distance fields, iso-contours, topographic maps, and even extrapolations to embed the possibly odd-shaped point assembly into a filled rectangular region.", "uri": "https://vimeo.com/182985899", "name": "[VIS16 Preview] Extending Scatterplots to Scalar Fields", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:41:05+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Yusuke Ishikawa, Issei Fujishiro\n\nAbstract: Individual plays and their correlations in a specific kind of ball game can be encoded and visualized with relative ease. Team plays, in contrast, are much more difficult to visualize because of the tremendously increased complexity that stems from the combination of individual plays with many organizational options. In this study, we take various features inherent to rugby into account to propose a novel pixel-oriented visualization method that helps spectators immediately grasp the transition of tactile situations in a match. We also deploy a set of indices to quantitatively analyze the strategic advantage that a team gains in the match. We demonstrate the effectiveness of our visual analysis by applying it to a real match: South Africa vs. Japan in the Rugby World Cup 2015.", "uri": "https://vimeo.com/182985863", "name": "[VIS16 Preview] Visual Analysis of Rugby Matches: Pixel-oriented Visualization and Evaluation Indices", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:40:50+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Vincent Raveneau, Julien Blanchard, Yannick Pri\u00e9\n\nAbstract: Due to the ever increasing use of data analysis tools, their performance has become an important scientific question, notably regarding the tools' ability to assist the analyst using them. We present our first proposals toward building a data analysis tool allowing an analyst to interact with his data mining algorithm, while working with interaction traces. First, we present an architecture for such a system, based on the ideas of progressive analytics. We then present the challenges that will need to be faced in future work.", "uri": "https://vimeo.com/182985840", "name": "[VIS16 Preview] Pattern-based progressive analytics on interaction traces", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:40:39+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Min Lu, Jie Liang, Zongru Li, Siming Chen, Xiaoru Yuan\n\nAbstract: Interactions are essential in effective visualization and visual analysis. However, many visualizations available online lack sufficient support of interactions. We introduce Filter+, a visual technique which can be easily laid over existing web-based visualizations and provides flexible interactions with underlying visual objects.", "uri": "https://vimeo.com/182985816", "name": "[VIS16 Preview] Filter+: Interaction Argument for Web-based Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:40:06+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Shaked Kaufmann, Peter Bak, Noam Tractinsky\n\nAbstract: TransUccess is an interactive visualization tool aiming to allow residents, transit operators and decision-makers to easily find, analyze and validate patterns of accessibility and social equity in public transit systems. The tool comprises of interactive choropleth map and parallel coordinates chart that assist users in investigating accessibility levels throughout different city zones, so called administrative zones, while making demographic properties of the population, such as mean household income, assessable. The results of our investigations show a complex interplay of demographics and accessibility. Our approach and tool is thought to be applicable to other domains, such as public health.", "uri": "https://vimeo.com/182985763", "name": "[VIS16 Preview] TransUccess: Investigating Social Equity in Accessing Public Transportation through Visual Analytics", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:39:50+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Richard Pusch, Charles Perin, Sheelagh Carpendale\n\nAbstract: We present SDCurve.js, a JavaScript library for creating interactive subdivision curves. Although subdivision curves are well-suited for use in interactive visualizations, the InfoVis community has used them sparingly. SDCurve.js is a lightweight, open-source, D3-compatible library that implements several common curve schemes, such as interpolating curves and B-Splines of any degree, in a subdivision framework that allows fast and easy curve interaction. We hope InfoVis researchers and designers will take advantage of it and contribute to its expansion.", "uri": "https://vimeo.com/182985737", "name": "[VIS16 Preview] SDCurve.js: A JavaScript Library for Interactive Subdivision Curves", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:39:36+00:00", "description": "VIS Poster (Best InfoVis Poster Honorable Mention)\n\nOrganizers / Panelists: Eric Alexander, Chih-Ching Chang, Mariana Shimabukuro, Steven Franconeri, Christopher Collins, Michael Gleicher\n\nAbstract: From word clouds to cartographic labels to word trees, many visualizations encode data within the sizes of fonts. While font size can be an intuitive dimension for the viewer, it may also bias the perception of the underlying values. Viewers might conflate the size of a word's font with a word's width, with the number of letters it contains, or with the larger or smaller heights of particular characters ('o' vs. 'p' vs. 'b'). In an ongoing set of experiments, we have found that such factors \u2013 which are irrelevant to the encoded values \u2013 can indeed influence comparative judgements of font size. For this poster, we present one such experiment showing the biasing effect of word length.", "uri": "https://vimeo.com/182985718", "name": "[VIS16 Preview] The Biasing Effect of Word Length in Font Size Encodings", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:39:24+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Chufan Lai, Ying Zhao, Xiaoru Yuan\n\nAbstract: Dimension reduced projection approximates the original distribution in a low-dimensional space. It makes a good overview, but can- not meet the needs of local analyses. On one hand, distortions largely harm the perception of local relationships. On the other hand, a sole projection can't satisfy local analyses with ever-changing targets. To address this problem, we propose an interactive exploration method, to help users customize linear projections for a better local analysis. Specifically, we allow users to define their point of interest (POI) data. Then regarding different analytic tasks, we alter the projection to enhance different features of the POI. Furthermore, we provide means to help users discover, analyse, modify and compare multiple POIs. We evaluate our method via case study with a real-world dataset.", "uri": "https://vimeo.com/182985700", "name": "[VIS16 Preview] Exploring High Dimensional Data Through Locally Enhanced Projections", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:39:11+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Donald Johnson, TJ Jankun-Kelly\n\nAbstract: Analysis of multiple overlapping data scenes is a challenging problem with tension between clearly identifying and exploring significant overlaps \\\\&amp; conflicts. Two areas where this problem occurs is when dealing with ensemble data from physical event simulation and when viewing multiple flood scenes that occur in an area of interest. In order to allow easier analysis of scenes with multiple overlapping data layers, we introduce a visualization system designed to aid in the analysis of such scenes. It allows the user to both see where different data sets agree, and categorize areas of disagreement based on participating surfaces in each area. The results are stable with regard to render order and GPU acceleration via OpenCL allows interaction with data large datasets. This interactivity is further enhanced by data streaming which allows datasets too large to be loaded directly onto the GPU to be processed. After demonstrating our approach on a diverse set of ensemble datasets, we provide feedback from expert users.", "uri": "https://vimeo.com/182985677", "name": "[VIS16 Preview] GPU-Assisted Visual Analysis and Categorization of Ensemble Conflict", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:38:55+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Alper Sarikaya, Michael Gleicher\n\nAbstract: Scatterplots are among the most common methods for exploring and presenting data, covering a wide range of tasks and designs. The variety of scatterplot designs has created a proliferation of potential design decisions to consider when constructing a scatterplot. However, there remain many unexamined assumptions in respect to the trade-offs between these decisions. In this short summary, we begin the process of synthesizing recent work to build descriptive knowledge of how design decisions affect the analysis tasks viewers perform with scatterplots. Through deriving twelve abstracted scatterplot tasks, we can start to tease apart the different affordances of design decisions, and begin to formulate a basis for prescriptive scatterplot design.", "uri": "https://vimeo.com/182985641", "name": "[VIS16 Preview] Tasks to Tease Apart Scatterplot Design Decisions", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:38:44+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Dylan Kobayashi, Simon Su, Luis Bravo, Jason Leigh, Dale Shires\n\nAbstract: We present a scientific visualization application running on top of the SAGE2 framework that enables high resolution data visualization. ParaViewWeb was implemented as a SAGE2 application to take advantage of the abstraction provided by SAGE2 that allows utilization of multiple display as one large multi-user workspace. The ParaSAGE implementation was tested on a 24 tiled display in a 6x4 curved configuration with a total of 49.76 megapixel available for display. For visualization computing hardware, a visualization cluster with a head node and three visualization nodes with 8 outputs each are being used to drive the tiled display system. A simulation dataset with 9 million data points were tested successfully with ParaSAGE. ParaSAGE application provides a viable high-resolution data visualization alternative to the desktop-based visualization to the scientific community.", "uri": "https://vimeo.com/182985628", "name": "[VIS16 Preview] ParaSAGE: Scalable Web-based Scientific Visualization for Ultra Resolution Display Environment", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:38:12+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Davide Ceneda, Wolfgang Aigner, Markus B\u00f6gl, Theresia Gschwandtner, Silvia Miksch\n\nAbstract: The analysis of industrial processes allows quality assessment and production monitoring. Usually these operations are carried out exploiting time-series data. In this work, we analyze a concrete design study of space efficient and time-aggregating visualizations for the analysis of high-frequency time-series. We derive recommendations to enhance the design process and demonstrate their applicability to our case study.", "uri": "https://vimeo.com/182985581", "name": "[VIS16 Preview] Guiding the Visualization of Time-Oriented Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:38:01+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Ruimin Gao, Mingran Li, Xinghe Hu, Yingjie Victor Chen\n\nAbstract: Normal hierarchical-structure visualization focuses on individual datasets rather than relationships between them. Since it is difficult to cover hierarchies and multi-dimensional data with static visualization, we are presenting a set of interactions needed to amplify expressing data with hierarchical visualization. With interaction like collapse, expansion and selection of elements from different levels, the relationships and properties will change accordingly.", "uri": "https://vimeo.com/182985566", "name": "[VIS16 Preview] A Hierarchical Interaction Design for Multi-dimensional Flow Datasets", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:37:43+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Kahin Akram Hassan, Niklas R\u00f6nnberg, Camilla Forsell, Jimmy Johansson\n\nAbstract: This work presents the results from an evaluation of stereoscopic versus monoscopic 3D parallel coordinates. The objective of the evaluation was to investigate if stereopsis increases user performance. The results show that stereoscopy has no effect at all on user performance compared to monoscopy. This result is important when it comes to the potential use of stereopsis within the information visualization community.", "uri": "https://vimeo.com/182985525", "name": "[VIS16 Preview] On the Performance of Stereoscopic Versus Monoscopic 3D Parallel Coordinates", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:37:28+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Niklas R\u00f6nnberg, Gustav Hallstr\u00f6m, Tobias Erlandsson, Jimmy Johansson\n\nAbstract: This poster presents an experiment designed to evaluate the possible benefits of sonification in information visualization. It is hypothesized, that by using musical sounds for sonification when visualizing complex data, interpretation and comprehension of the visual representation could be increased. In this evaluation of sonification in parallel coordinates and scatter plots, participants had to identify and mark different density areas in the representations. Both quantitative and qualitative results suggest a benefit of sonification. These results indicate that sonification might be useful for data exploration, and give rise to new research questions and challenges.", "uri": "https://vimeo.com/182985506", "name": "[VIS16 Preview] Sonification Support for Information Visualization Dense Data Displays", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:37:15+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Mickael Sereno, Mehdi Ammi, Tobias Isenberg, Lonni Besan\u00e7on\n\nAbstract: We present the design and evaluation of an interface that combines tactile and tangible paradigms for 3D visualization. While studies have demonstrated that both tactile and tangible input can be efficient for a subset of 3D manipulation tasks, we reflect here on the possibility to combine the two complementary input types. Based on a field study and follow-up interviews, we present a conceptual framework of the use of these different interaction modalities for visualization both separately and combined---focusing on free exploration as well as precise control. We present a prototypical application of a subset of these combined mappings for fluid dynamics data visualization using a portable, position-aware device which offers both tactile input and tangible sensing. We evaluate our approach with domain experts and report on their qualitative feedback.", "uri": "https://vimeo.com/182985489", "name": "[VIS16 Preview] Tangible Brush: Performing 3D Selection with Portable and Position-aware Devices", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:36:52+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Alexander Koch, Tim De Meyer, Jana Jeschke, Wim Van Criekinge\n\nAbstract: In recent years, increasing amounts of genomic and clinical cancer data have become publicly available through large-scale collaborative projects such as The Cancer Genome Atlas (TCGA). However, as long as these datasets are difficult to access and interpret, they are essentially useless for a major part of the cancer research community and their scientific potential will not be fully realized. To address this issue, we developed MEXPRESS, an easy-to-use and straightforward web tool for the integration and visualization of the expression, DNA methylation and clinical TCGA data on a single-gene level (http://mexpress.be). In comparison to existing tools, MEXPRESS allows researchers to quickly visualize and interpret the different TCGA datasets and their relationships for a single gene. Since the publication of MEXPRESS on August 26, 2015, it has received over 4,700 unique visitors worldwide and user feedback has been positive. Based on this feedback we have already been making small changes and are currently working on a larger update.", "uri": "https://vimeo.com/182985443", "name": "[VIS16 Preview] Visualizing Cancer Genomics Data with MEXPRESS", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:36:40+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Johannes Liem, Jo Wood, Greg Slabaugh\n\nAbstract: Storytelling techniques - whether wittingly or not - are widely used in visual communication. But they are still an under-researched topic and it is not yet clear how, why and when mechanisms behind storytelling may or may not work when used in visualization. In this contribution we focus on the aspect of authorial saliency, the degree of storytelling techniques applied to visualizations, in this case especially to flow visualizations. We examine to what extent the degree of authorial saliency influences a user\u00e2\u20ac\u2122s ability to apply acquired understanding later on by conducting a longitudinal and between-subjects online experiment. We are hoping to reveal indications about the efficacy of storytelling mechanisms applied to the visualization of flow and movement data. The experiment will be conducted in July 2016, results will be included in the poster.", "uri": "https://vimeo.com/182985423", "name": "[VIS16 Preview] Flowstory: Storytelling for Improving Memorability and Comprehension", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:36:18+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Soon Tang, Jonathan Leidig\n\nAbstract: Visual analytics web applications may be used to support local board of a public works\u00e2\u20ac\u2122 sustainable energy efforts. The objective of the project was to play a pivotal role in one board\u00e2\u20ac\u2122s efforts in surveilling and educating consumers on energy consumption and reduction progress. It utilizes visualization techniques to allow customers to analyze electric and gas energy consumptions at a variety of resolutions. End consumers and ward managers compare energy consumptions around their city geospatially as well as against their own historical baselines. The visualization suite allows a variety of users to discover patterns, note outliers, target high-impact consumers, gamify individual energy reduction through resident education, and support public behavior modification strategies.", "uri": "https://vimeo.com/182985393", "name": "[VIS16 Preview] Promoting and Gamifying Energy Sustainability through Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:36:02+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Quan Nguyen, Seokhee Hong, Peter Eades\n\nAbstract: Drawings in two-and-a-half dimensions are effective for large graphs, especially large clustered graphs. Edge bundling is popular for reducing visual clusters to show high-level structures in large dense networks. Previous bundling work mainly focuses on two and three dimensional drawings of graphs. In this poster, we present a study of edge bundling in 2.5D. We introduce plane compatibility to guide the bundling of intra-plane and inter-plane edges in 2.5 dimensional visualizations. Our results show that the new method is effective to highlight the important skeletal structures in large clustered graphs. \\", "uri": "https://vimeo.com/182985370", "name": "[VIS16 Preview] 2.5D Edge Bundling", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:35:48+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Dan Zhang, Darius Coelho, Klaus Mueller\n\nAbstract: There are occasions when data displays have certain information items which should only be visible to a subset of the display\u00e2\u20ac\u2122s viewers. This could be classified information, private information, or simply information only of interest to specific viewers. We explore the use of Google Glass as a means to augment the display\u00e2\u20ac\u2122s visual content with these types of private data, superimposing them seamlessly. The display itself could be a desktop display or a large display wall. We constructed a prototype for the former and discuss three case studies.", "uri": "https://vimeo.com/182985350", "name": "[VIS16 Preview] Google Glass for Personalized Augmentations of Data Visualizations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:35:36+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Peter Butcher, Jonathan Roberts, Panagiotis Ritsos\n\nAbstract: We present our initial investigation of a low-cost, web-based virtual reality platform for immersive analytics, using a Google Cardboard, with a view of extending to other similar platforms such as Samsung\u00e2\u20ac\u2122s Gear VR. Our prototype uses standards-based emerging frameworks, such as WebVR and explores some the challenges faced by developers in building effective and informative immersive 3D visualizations, particularly those that attempt to resemble recent physical visualizations built in the community.", "uri": "https://vimeo.com/182985336", "name": "[VIS16 Preview] Immersive Analytics with WebVR and Google Cardboard", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:35:25+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Zhutian Chen, Guodao Sun, Nan Cao, Huamin Qu, Yingcai Wu\n\nAbstract: Analyzing Twitter streams is important in many applications, such as crisis management. However, the considerable diversity, increasing volume, and high dynamics of social streams of large events present a great challenge for exploring social streams with limited computing resources. We propose StreamExplorer to facilitate visual analysis, tracking, and comparison of a real-time social stream at different levels. We conduct a case study to demonstrate the effectiveness and usefulness of StreamExplorer.", "uri": "https://vimeo.com/182985312", "name": "[VIS16 Preview] StreamExplorer: A Multi-Stage System for Visually Exploring Events in Social Stream", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:35:12+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Gennady Andrienko, Natalia Andrienko, Guido Budzjak, Tatiana von Landesberger, Hendrik Weber\n\nAbstract: From a set of trajectories of the players and the ball in a football (soccer) game, we computationally estimate, for each time frame, the pressure of the defending players upon the ball and the opponents. The extracted pressure relationships are visualized in detailed and summarized forms. Interactive filtering enables exploration of the pressure relationships in selected game episodes or in game situations satisfying specific query conditions.", "uri": "https://vimeo.com/182985295", "name": "[VIS16 Preview] Exploring Pressure in Football", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:35:01+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Ulrike Kister, Konstantin Klamka, Raimund Dachselt\n\nAbstract: Due to the deployment of novel interaction techniques, additional challenges for logging purposes in information visualizations arise. In this position paper, we discuss specific challenges regarding four different example setups illustrated with projects of our own. In each setup, various aspects need to be considered to enable, e.g., a meaningful logging of (multiple) input streams or the replaying of logs. We do not aim to provide a technical solution for logging interaction in the various setups, but rather want to share our insights and experiences from a set of projects that apply novel interaction techniques and multi-display setups to visualizations.", "uri": "https://vimeo.com/182985263", "name": "[VIS16 Preview] Supporting Graph Exploration Tasks on Display Walls Using Spatially-Aware Mobile Devices", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:34:41+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Tomasz Opach, Jan Ketil R\u00f8d\n\nAbstract: First we briefly discuss existing participatory mechanisms for visual interfaces; data collection techniques and types of collected knowledge are taken into account. As available solutions are unsuitable for our tool supporting climate change adaptation, we propose two novel mechanisms: an interactive Chapati diagram for collecting explicit knowledge and the spatiotemporal storytelling for the tacit one. Both solutions are shortly introduced and suggestions for the further research are given.", "uri": "https://vimeo.com/182985236", "name": "[VIS16 Preview] Two Novel Participatory Solutions for Visual Interfaces Supporting Decision Making Processes", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:34:28+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Ulrich Engelke, Eser Kandogan\n\nAbstract: Optimizing and automating visual analytics processes is strongly dependent on a holistic specification of the design space and a language that facilitates describing all elements and interactions therein. In this paper, our goal is to begin formalizing said specification to inform the design of a visual analytics algebra. Such an algebra will bring together the visual analytics community via a common language, much like relational algebra did in the data management community. We focus here on the high level concepts of the algebra and present an illustrative example.", "uri": "https://vimeo.com/182985213", "name": "[VIS16 Preview] Towards an Algebra for the Visual Analytics Design Process", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:34:17+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Allen Korenevsky, Zoe Wood\n\nAbstract: Interactive data visualizations must respond fluidly to user input to be effective, or so we assume. In fact it is unknown exactly how fast a visualization must run to present every facet within a dataset. An engineering team with limited resources is left with intuition and estimates to determine if their application performs sufficiently well. In this project we explore how latency affects users' comprehension of data visualizations, specifically 3D geospatial visualizations with large data sets. A climate visualization showing temperatures spanning from the 19th to the 21st century, along with associated multiple choice questions was developed for a user study. Metrics like user eye movements, time per question, and test score were recorded. Unbeknownst to the participants, the latency was toggled between questions, subjugating frame rendering times to intervals between 33 1/3 and 200 milliseconds. Analysis of eye movements and question completion time and accuracy show that latency did not impact how users explore the visualization or comprehend the data presented. However, user fixation times on overlaid 2D visualization tools are impacted by latency. This finding validates how resilient users are in navigating and understanding virtual 3D environments - a conclusion supported by previous studies about video game latency.", "uri": "https://vimeo.com/182985192", "name": "[VIS16 Preview] The Effects of Latency on 3D Interactive Data Visualizations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:34:00+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Albert Amor-Amor\u00f3s, Paolo Federico, Silvia Miksch\n\nAbstract: Many real-world problems appearing in diverse application domains involve large multivariate interrelated data. For this reason, graph-based data models have gained popularity in recent years. Graph traversal is a powerful computational paradigm addressing the challenges of graph data management; yet, its complexity and specificity might hinder its use for interactive data exploration by non-expert users in absence of appropriate interfaces. We have designed and implemented a system for visually-supported graph traversal, featuring (1) a graphical block metaphor for traversal formulation and execution, and (2) data probes providing relevant visual feedback about the results. The proposed approach aims at enhancing the usability of graph querying and retrieval techniques, in order to assist users with gaining and interpreting insights during exploratory analysis.", "uri": "https://vimeo.com/182985175", "name": "[VIS16 Preview] Visually-supported graph traversals for exploratory analysis", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:33:48+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Thomas Plank, Markus Helfert, Peter Hofer\n\nAbstract: Interactive visualizations are supposed to support the sense-making process of vast amounts of data by gaining new insights and therefore enhancing decision-making outcome. However, to attain this objective it is necessary for the decision-maker to visually encode interactive visualizations and to properly use them for various analytical tasks. In this poster the authors analyze current literature and highlight the differences and intersections between various research disciplines such as Sense-Making, Cognition, Human-Computer- Interaction, Education, Information Visualization. Additionally, the authors will identify current focuses of research as well as gaps to direct further research efforts. The keyword search resulted in 55 publications published in 29 peer-reviewed journals and led to ten assigned research clusters, namely Visual Encoding, Impact of Interaction, User-Centered, Knowledge Building, Design Principles, Task-Centered, Storytelling, Guidance, Digital Educational Governance, and Evaluation. Among all research disciplines and clusters a strong focus on the comprehension of the visualized information and not on the comprehension of the interactive visualization itself can be identified.", "uri": "https://vimeo.com/182985154", "name": "[VIS16 Preview] Supporting the Comprehension of Interactive Visualizations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:33:32+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Christopher Paul Kappe, Michael B\u00f6ttinger, Heike Leitte\n\nAbstract: Scalar fields are omnipresent in scientific research and adequate visualizations are well known. Sets of multiple scalar fields that share a common context, as e.g. in ensemble data, however, demand more sophisticated analysis and visualization tools. In this work, we survey visualization techniques for sets of scalar fields, and distance functions for their comparison. We contribute to the field by proposing a new diagram that can summarize one or more scalar field in a single figure allowing for a good overview of the data.", "uri": "https://vimeo.com/182985129", "name": "[VIS16 Preview] Investigation of Scalar Field Metrics and Respective Visualization Techniques", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:33:20+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Samuel Clarke, Nathan Dass, Duen Horng Chau\n\nAbstract: We are investigating new ways for people to explore time-evolving graph data represented as the recently-introduced Matrix Cube, through natural gestures in a 3D environment. In the cube, one axis is time, and the other two axes comprise an adjacency matrix of a graph. We want to understand how recent advances in virtual reality technologies may help the user more naturally explore the dimensionality and richness of this 3D visualization, enabling them to more effectively gain insights into relationships and anomalies in the data. We use a Leap Motion controller to capture the user\u00e2\u20ac\u2122s hand gestures that manipulate the cube into its many possible projections. We prototyped this synergy of data visualization and virtual reality on a time-evolving graph of annual trading volumes among major countries from 1995 through 2010. Please see the accompanying video for a demo of our prototype.", "uri": "https://vimeo.com/182985123", "name": "[VIS16 Preview] NaturalMotion: Exploring Gesture Controls for Visualizing Time-Evolving Graphs", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:33:05+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Yining Nie, Siming Chen, Xiaoru Yuan, Zhimin Huang, Ka Wai Chan\n\nAbstract: Narrative visualizations are powerful tools for storytellors, like journalists, to convey an intended story. For this purpose, the prime task of visualization is to communicate the content with the audience effectively. Here, we present the design of China\u00e2\u20ac\u2122s Property Market Visual Report, a web-based visualization project that adopts 3D visualization, matrix and annular histogram to illustrate the fluctuation of China\u00e2\u20ac\u2122s property market in a decade. The intuitive interactive visualization and generated reports enhance the story telling process. This poster describes our inspirations and expounds how to combine multiple visualization methods together in a particular narrative visualization project.", "uri": "https://vimeo.com/182985103", "name": "[VIS16 Preview] China's Property Market Visual Report: An Interactive Web-based Narrative Visualization for Data Journalism", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:32:51+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Nadia Boukhelifa, Marc-Emmanuel Perrin, Samuel Huron, James Eagan\n\nAbstract: Uncertainty plays an important and complex role in data analysis and affects many domains. To understand how domain experts analyse data under uncertainty and the tasks they engage in, we conducted a qualitative user study with 12 participants from a variety of domains. We collected data from audio and video recordings of think-aloud demo sessions and semi-structured interviews. We found that analysts sometimes ignore known uncertainties in their data, but only when these are not relevant to their tasks. More often however, they deploy various coping strategies, aiming to understand, minimise or exploit the uncertainty. Within these coping strategies, we identified five high level tasks that appear to be common amongst all of our participants. We believe our findings and further analysis of this data will yield concrete design guidelines for uncertainty-aware visual analytics. \\", "uri": "https://vimeo.com/182985077", "name": "[VIS16 Preview] Eliciting Strategies and Tasks in Uncertainty-Aware Data Analytics", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:32:35+00:00", "description": "VIS Poster (Best InfoVis Poster Honorable Mention)\n\nOrganizers / Panelists: Ricardo Langner, Tom Horak, Raimund Dachselt\n\nAbstract: We present a concept for tangible visualization views using co-located, spatially-aware mobile devices. The proposed concept takes advantage of ad-hoc device combinations and spatial arrangements, allowing users to interact with multiple coordinated visualization views distributed across mobile displays. In this work, we describe the basics of this concept and illustrate the potential of our approach by describing and implementing use cases of various visualization techniques.", "uri": "https://vimeo.com/182985048", "name": "[VIS16 Preview] Towards Combining Mobile Devices for Visual Data Exploration", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:32:16+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: John R Thompson, John Stasko\n\nAbstract: Our ongoing work aims to better understand the relationship between visual marks and data encodings that comprise innovative data visualizations. By innovative, we refer to data visualizations that in part extend or deviate from conventional visualization techniques. Authors of these types of visualizations engage and inform their audience by incorporating novel data-bound compositions. In our work, we deconstruct a set of these visualizations into their elemental data-driven marks by employing a semi-structured coding process. This process serves as the initial step for defining a visual grammar that can act as the building blocks for creative data visualizations. The primary purpose of this model is to define the set of graphics, data-bindings, and visual styles that researchers may then incorporate into a data visualization authoring tool. Here, we present our analysis approach to deconstruct the set of visualizations and how we cataloged the coded data for dissemination among the visualization community. We then highlight interesting trends observed within the collection of visualizations analyzed thus far.", "uri": "https://vimeo.com/182985030", "name": "[VIS16 Preview] Understanding Data-Driven Visual Encodings through Deconstruction", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:31:54+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Kui Wu, Duong Hoang, Alexander Lex\n\nAbstract: Traditional literature search engines work well for researchers who look for specific information, e.g., based on authors or keywords. Those tools however are of limited value when someone wants to explore data, e.g., to get a quick overview of the most important papers in a field, or of how trends in research problems and techniques to solve them evolve over the years. Exploring the literature is especially valuable to those unfamiliar with the field who want to get an overview. To address the exploration of such corpora, we developed an interactive tool to visualize connections between papers of a field that were published over multiple years. Our tool lets users quickly identify important papers and authors, read paper abstracts, follow citation relationships between papers and collaboration relationships between authors, as well as explore research topics of interest and see how their popularity changes over time. We believe that by helping researchers quickly acquire a high level understanding of both the foundations and the state-of-the-art in the field of interest, this tool can accelerate their research significantly. We demonstrate our software using the papers published in SIGGRAPH\u00e2\u20ac\u201dthe most important conference in computer graphics.", "uri": "https://vimeo.com/182984971", "name": "[VIS16 Preview] Visualizing Publication Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:31:41+00:00", "description": "VIS Poster (Best InfoVis Poster Honorable Mention)\n\nOrganizers / Panelists: Bon Adriel Aseniero, Charles Perin, Marjan Eggermont, Sheelagh Carpendale\n\nAbstract: We present Fireflies, an exploration of bio-inspired visualization using animal swarming behaviours and plant phyllotaxis. We applied Fireflies to a Canadian attitudinal survey on HIV/AIDS. In-formation about the survey participants is encoded within different visual elements of fireflies; most importantly, the motion of the fireflies depicts comfort levels concerning HIV/AIDS. The bio-inspired visual properties of Fireflies meld into an expressive representation of a sensitive dataset.", "uri": "https://vimeo.com/182984954", "name": "[VIS16 Preview] Fireflies: Biomimicry-Inspired InfoVis for Exploring Public Opinion about an Infectious Disease", "year": "2016", "event": "INFOVIS, PREVIEW"}, {"created_time": "2016-09-16T11:31:27+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Jonathan Roberts, James Jackson, Christopher Headleand, Panagiotis Ritsos\n\nAbstract: Visualizations have been used to explain algorithms to learners, in order to help them understand complex processes. These `explanatory visualizations' can help learners understand computer algorithms and data-structures. But most are created by an educator and merely watched by the learner. In this paper, we explain how we get learners to plan and develop their own explanatory visualizations of algorithms. By actively developing their own visualizations learners gain a deeper insight of the algorithms that they are explaining. These depictions can also help other learners understand the algorithm.", "uri": "https://vimeo.com/182984932", "name": "[VIS16 Preview] Creating Explanatory Visualizations of Algorithms for Active Learning", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:31:14+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Arjun Srinivasan, John Stasko\n\nAbstract: Developing natural language interfaces for visualization systems is a challenging task and requires system developers to spend time and effort on implementing Natural Language Processing (NLP) components necessary to convert natural language queries into visualizations. Especially for developers without a background in NLP, this learning curve can be even more challenging and time consuming. We are developing the Natural Language Driven Data Visualization (NL4DV) toolkit that provides high-level functions developers can use to create natural language-driven data visualization systems.", "uri": "https://vimeo.com/182984909", "name": "[VIS16 Preview] NL4DV: Toolkit for Natural Language Driven Data Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:31:04+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Jun Tao, Chaoli Wang, Nitesh Chawla, Lei Shi\n\nAbstract: We introduce semantic flow graph (SFG), a novel graph representation and interaction framework that enables users to explore the relationships among key objects (i.e., streamlines, critical points, and spatial regions) of a 3D flow field. The objects and their relationships are organized as a heterogeneous network. We assign each object a set of attributes, based on which a semantic abstraction of the heterogeneous network is generated. We design a suite of operations to explore the underlying flow fields based on this graph representation and abstraction mechanism. Three linked views are developed to display SFG, its node split criteria and history, and the objects in the spatial volume.", "uri": "https://vimeo.com/182984894", "name": "[VIS16 Preview] Semantic Flow Graph: A Framework to Explore 3D Flow Fields", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:30:52+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Lisa Rogers\n\nAbstract: Despite atmospheric science and weather forecasting being a relevant data to many fields and corporations, there has not been a lot of innovation incorporating the modern breakthroughs and methods of interactive visualization to the meteorological field. There is a plethora of free atmospheric information valuable on the internet that is being under-utilized. Having a more approachable way to present and explore the information in a browser based application could benefit individuals from a number of fields and make use of the avalanche of data available. Due to the complexity and scope of information from Soundings, VASE, like its predecessor the Skew-T chart, is still slanted towards individuals with meteorological training. VASE is an attempt to utilize d3.js and visual variables to redesign the traditional Skew-T chart to be more intuitive and facilitate the comparison of multiple Sounding readings. Even for these experts, Skew-T charts are not particularly effective or efficient for many important forecasting tasks, as the presented format is difficult for comparing two readings side by side to view patterns. VASE attempts to present this information in a way that trained users can innately spot patterns to compare or contrast readings.", "uri": "https://vimeo.com/182984878", "name": "[VIS16 Preview] VASE (Visualized Atmosphere Sounding Exploration)", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:30:41+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Krist Wongsuphasawat\n\nAbstract: Tile Grid Map has been adopted for creating Cholopleth map. However, multiple publishers had already developed different map layouts. To compare these layouts, I define a set of quality metrics that can be used to evaluate and compare them. A case study comparing six map layouts was also included.", "uri": "https://vimeo.com/182984858", "name": "[VIS16 Preview] Quality Metrics for Tile Grid Maps", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:30:28+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Tran Minh Quan, JunYoung Choi, Won-Ki Jeong\n\nAbstract: In this paper, we propose a novel probabilistic voxel classification method for high-quality volume rendering. Our approach automatically generates high-dimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built upon a multi-layer shallow feed-forward neural network for accurate voxel classification from only a handful of selection scribbles made directly on the input data by the user. We demonstrate the proposed system's performance on several synthetic and widely used volume datasets.", "uri": "https://vimeo.com/182984838", "name": "[VIS16 Preview] Probabilistic Volume Rendering using Data-driven High-dimensional Features", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:30:10+00:00", "description": "VIS Poster \n\nOrganizers / Panelists: Jaemin Jo, Wonjae Kim, Seunghoon Yoo, Bohyoung Kim, Jinwook Seo\n\nAbstract: The advance in distributed computing technologies opens up new possibilities of data exploration even for datasets with a few billion entries. In this paper, we present SwiftTuna, an interactive system that brings in modern cluster computing technologies (i.e., in-memory computing) to InfoVis, allowing rapid and incremental exploration of large-scale multidimensional data without building precomputed data structures (e.g., data cubes). Our performance evaluation demonstrates that SwiftTuna enables data exploration of a real-world dataset with four billion records while preserving the latency between incremental responses within a few seconds.", "uri": "https://vimeo.com/182984801", "name": "[VIS16 Preview] SwiftTuna: Incrementally Exploring Large-scale Multidimensional Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:29:58+00:00", "description": "VAST Paper \n\nOrganizers: Cristian Felix, Anshul Vikram Pandey, Enrico Bertini\n\nAbstract: We describe TextTile, a data visualization tool for investigation of datasets and questions that require seamless and flexible analysis of structured data and unstructured text. TextTile is based on real-world data analysis problems gathered through our interaction with a number of domain experts and provides a general purpose solution to such problems. The system integrates a set of operations that can interchangeably be applied to the structured as well as to unstructured text part of the data to generate useful data summaries. Such summaries are then organized in visual tiles in a grid layout to allow their analysis and comparison. We validate TextTile with task analysis, use cases and a user study showing the system can be easily learned and proficiently used to carry out nontrivial tasks.\n\nWednesday, Oct. 26, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984787", "name": "[VIS16 Preview] TextTile: An Interactive Visualization Tool for Seamless Exploratory Analysis of Structured Data and...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:29:44+00:00", "description": "VAST Paper \n\nOrganizers: Filip Dabek, Jesus Caban\n\nAbstract: Despite the recent popularity of visual analytics focusing on big data, little is known about how to support users that use visualization techniques to explore multi-dimensional datasets and accomplish specific tasks. Our lack of models that can assist end-users during the data exploration process has made it challenging to learn from the user\u00e2\u20ac\u2122s interactive and analytical process.The ability to model how a user interacts with a specific visualization technique and what difficulties they face are paramount in supporting individuals with discovering new patterns within their complex datasets. This paper introduces the notion of visualization systems understanding and modeling user interactions with the intent of guiding a user through a task thereby enhancing visual data exploration. The challenges faced and the necessary future steps to take are discussed; and to provide a working example,a grammar-based model is presented that can learn from user interactions, determine the common patterns among a number of subjects using a K-Reversible algorithm, build a set of rules, and apply those rules in the form of suggestions to new users with the goal of guiding them along their visual analytic process. A formal evaluation study with 300 subjects was performed showing that our grammar-based model is effective at capturing the interactive process followed by users and that further research in this area has the potential to positively impact how users interact with a visualization system.\n\nTuesday, Oct. 25, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984771", "name": "[VIS16 Preview] A Grammar-based Approach for Modeling User Interactions and Generating Suggestions During the Data...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:29:33+00:00", "description": "VAST Paper \n\nOrganizers: Michael Behrisch, Benjamin Bach, Michael Hund, Michael Delz, Laura von R\u00fcden, Jean-Daniel Fekete, Tobias Schreck\n\nAbstract: In this work we address the problem of retrieving potentially interesting matrix views to support the exploration of networks. We introduce Matrix Diagnostics (or MAGNOSTICS), following in spirit related approaches for rating and ranking other visualization techniques, such as Scagnostics for scatter plots. Our approach ranks matrix views according to the appearance of specific visual patterns, such as blocks and lines, indicating the existence of topological motifs in the data, such as clusters, bi-graphs, or central nodes. MAGNOSTICS can be used to analyze, query, or search for visually similar matrices in large collections, or to assess the quality of matrix reordering algorithms. While many feature descriptors for image analyzes exist, there is no evidence how they perform for detecting patterns in matrices. In order to make an informed choice of feature descriptors for matrix diagnostics, we evaluate 30 feature descriptors \u00e2\u20ac\u201d27 existing ones and three new descriptors that we designed specifically for MAGNOSTICS\u00e2\u20ac\u201d with respect to four criteria: pattern response, pattern variability, pattern sensibility, and pattern discrimination. We conclude with an informed set of six descriptors as most appropriate for MAGNOSTICS and demonstrate their application in two scenarios; exploring a large collection of matrices and analyzing temporal networks.\n\nTuesday, Oct. 25, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984756", "name": "[VIS16 Preview] Magnostics: Image-based Search of Interesting Matrix Views for Guided Network Exploration", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:29:22+00:00", "description": "VAST Paper (Best Paper Honorable Mention)\n\nOrganizers: Panpan Xu, Honghui Mei, Liu Ren, Wei Chen\n\nAbstract: Visual analytics plays a key role in the era of connected industry (or industry 4.0, industrial internet) as modern machines and assembly lines generate large amounts of data and effective visual exploration techniques are needed for troubleshooting, process optimization, and decision making. However, developing effective visual analytics solutions for this application domain is a challenging task due to the sheer volume and the complexity of the data collected in the manufacturing processes. We report the design and implementation of a comprehensive visual analytics system, ViDX. It supports both real-time tracking of assembly line performance and historical data exploration to identify inefficiencies, locate anomalies, and form hypotheses about their causes and effects. The system is designed based on a set of requirements gathered through discussions with the managers and operators from manufacturing sites. It features interlinked views displaying data at different levels of detail. In particular, we apply and extend the Marey's graph by introducing a time-aware outlier-preserving visual aggregation technique to support effective troubleshooting in manufacturing processes. We also introduce two novel interaction techniques, namely the quantiles brush and samples brush, for the users to interactively steer the outlier detection algorithms. We evaluate the system with example use cases and an in-depth user interview, both conducted together with the managers and operators from manufacturing plants. The result demonstrates its effectiveness and reports a successful pilot application of visual analytics for manufacturing in smart factories.\n\nFriday, Oct. 28, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984735", "name": "[VIS16 Preview] ViDX: Visual Diagnostics of Assembly Line Performance in Smart Factories", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:28:57+00:00", "description": "VAST Paper \n\nOrganizers: Xiaotong Liu, Anbang Xu, Liang Gou, Haibin Liu, Rama Akkiraju, Han-Wei Shen\n\nAbstract: Public perceptions of a brand is critical to its performance. While social media has demonstrated a huge potential to shape public perceptions of brands, existing tools are not intuitive and explanatory for domain users to use as they fail to provide a comprehensive analysis framework for perceptions of brands. In this paper, we present SocialBrands, a novel visual analysis tool for brand managers to understand public perceptions of brands on social media. SocialBrands leverages brand personality framework in marketing literature and social computing approaches to automatically compute the personality of brands from three driving factors (user imagery, employee imagery, and official announcement) on social media, and construct an evidence network explaining the association between brand personality and driving factors. These computational results are then integrated with new interactive visualizations to help brand managers understand the personality traits and the associated social media factors. We demonstrate the usefulness and effectiveness of SocialBrands through a series of user studies with brand managers in an enterprise context. Design lessons are also derived from our studies.\n\nThursday, Oct. 27, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984695", "name": "[VIS16 Preview] SocialBrands: Visual Analysis of Public Perceptions of Brands on Social Media", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:28:38+00:00", "description": "VAST Paper \n\nOrganizers: Xinsong Yang, Lei Shi, Madelaine Daianu, Hanghang Tong, Qingsong Liu, Paul Thompson\n\nAbstract: Visually comparing human brain networks from multiple population groups serves as an important task in the field of brain connectomics. The commonly used brain network representation, consisting of nodes and edges, may not be able to reveal the most compelling network differences when the reconstructed networks are dense and homogeneous. In this paper, we leveraged the block information on the Region Of Interest (ROI) based brain networks and studied the problem of blockwise brain network visual comparison. An integrated visual analytics framework was proposed. In the first stage, a two-level ROI block hierarchy was detected by optimizing the anatomical structure and the predictive comparison performance simultaneously. In the second stage, the NodeTrix representation was adopted and customized to visualize the brain network with block information. We conducted controlled user experiments and case studies to evaluate our proposed solution. Results indicated that our visual analytics method outperformed the commonly used node-link graph and adjacency matrix design in the blockwise network comparison tasks. We have shown compelling findings from two real-world brain network data sets, which are consistent with the prior connectomics studies.\n\nWednesday, Oct. 26, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984661", "name": "[VIS16 Preview] Blockwise Human Brain Network Visual Comparison Using NodeTrix Representation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:28:26+00:00", "description": "VAST Paper \n\nOrganizers: Dominik Sacha, Leishi Zhang, Michael Sedlmair, John A. Lee, Jaakko Peltonen, Daniel Weiskopf, Stephen C. North, Daniel A. Keim\n\nAbstract: Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be useful in exploratory data analysis, they need to be adapted to human needs and domain-specific problems, ideally, interactively, and on-the-fly. Many visual analytics systems have already demonstrated the benefits of tightly integrating DR with interactive visualizations. Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant features, or choosing among several DR algorithms. We investigate specific implementations of visual analysis systems integrating DR, and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a \u00e2\u0080\u009chuman in the loop\u00e2\u0080\u009d process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and classify several systems previously described in the literature, and to derive future research opportunities.\n\nThursday, Oct. 27, 10:30\u201312:10 \u2013 Holday 4+5", "uri": "https://vimeo.com/182984646", "name": "[VIS16 Preview] Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:28:14+00:00", "description": "VAST Paper \n\nOrganizers: Po-Ming Law, Wenchao Wu, Yixian Zheng, Huamin Qu\n\nAbstract: Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.\n\nThursday, Oct. 27, 10:30\u201312:10 \u2013 Holday 4+5", "uri": "https://vimeo.com/182984633", "name": "[VIS16 Preview] VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:28:01+00:00", "description": "VAST Paper \n\nOrganizers: Phong Nguyen, Kai Xu, Andy Bardill, Betul Salman, Kate Herd, William Wong\n\nAbstract: Sensemaking is described as the process in which people collect, organize and create representations of information, all centered around some problem they need to understand. People often get lost when solving complicated tasks using big datasets over long periods of exploration and analysis. They may forget what they have done, are unaware of where they are in the context of the overall task, and are unsure where to continue. In this paper, we introduce a tool - SenseMap - to address these issues in the context of browser-based online sensemaking. We conducted a semi-structured interview with nine participants to explore their behaviors in online sensemaking with existing browser functionality. A simplified sensemaking model based on Pirolli and Card's model is derived to better represent the behaviors we found: users iteratively collect information sources relevant to the task, curate them in a way that makes sense, and finally communicate their findings to others. A series of design workshops was followed to derive requirements, discuss designs, implement and test the prototype in an agile setting. SenseMap automatically captures provenance of user sensemaking actions and provides multi-linked views to visualize the collected information and enable users to curate and communicate their findings. To explore how SenseMap is used, we conducted a user study in a naturalistic work setting with five participants completing the same sensemaking task related to their daily work activities. All participants found the visual representation and interaction of the tool intuitive to use. Three of them engaged with the tool and produced successful outcomes. It helped them to organize information sources, to quickly find and navigate to the sources they wanted, and to effectively communicate their findings. We also present our insights and lessons learned in the evaluation.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984615", "name": "[VIS16 Preview] SenseMap: Supporting Browser-based Online Sensemaking through Analytic Provenance", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:27:50+00:00", "description": "VAST Paper \n\nOrganizers: Aritra Dasgupta, Joon-Yong Lee, Ryan Wilson, Robert A. Lafrance, Nick Cramer, Kristin Cook, Samuel Payne\n\nAbstract: Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and ma- chines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts\u00e2\u20ac\u2122 trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984598", "name": "[VIS16 Preview] Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:27:39+00:00", "description": "VAST Paper \n\nOrganizers: Johannes Weissenb\u00f6ck, Artem Amirkhanov, Eduard Gr\u00f6ller, Johann Kastner, Christoph Heinzl\n\nAbstract: In this paper we present PorosityAnalyzer, a novel tool for detailed evaluation and visual analysis of pore segmentation pipelines to determine the porosity in fiber-reinforced polymers (FRPs). The presented tool consists of two modules: the computation module and the analysis module. The computation module enables a convenient setup and execution of distributed off-line-computations on industrial 3D X-ray computed tomography datasets. It allows the user to assemble individual segmentation pipelines in the form of single pipeline steps, and to specify the parameter ranges as well as the sampling of the parameter-space of each pipeline segment. The result of a single segmentation run consists of the input parameters, the calculated 3D binary-segmentation mask, the resulting porosity value, and other derived results (e.g., segmentation pipeline runtime). The analysis module presents the data at different levels of detail by drill-down filtering in order to determine accurate and robust segmentation pipelines. Overview visualizations allow to initially compare and evaluate the segmentation pipelines. With a scatter plot matrix (SPLOM), the segmentation pipelines are examined in more detail based on their input and output parameters. Individual segmentation-pipeline runs are selected in the SPLOM and visually examined and compared in 2D slice views and 3D renderings by using aggregated segmentation masks and statistical contour renderings. PorosityAnalyzer has been thoroughly evaluated with the help of twelve domain experts. Two case studies demonstrate the applicability of our proposed concepts and visualization techniques, and show that our tool helps domain experts to gain new insights and improve their workflow efficiency.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984586", "name": "[VIS16 Preview] PorosityAnalyzer: Visual Analysis and Evaluation of Segmentation Pipelines to Determine the Porosity in...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:27:26+00:00", "description": "VAST Paper \n\nOrganizers: Tanja Blascheck, Fabian Beck, Sebastian Baltes, Thomas Ertl, Daniel Weiskopf\n\nAbstract: Investigating user behavior involves abstracting low-level events to higher-level concepts. This requires an analyst to study individual user activities, assign codes which categorize behavior, and develop a consistent classification scheme. To better support this reasoning process of an analyst, we suggest a novel visual analytics approach which integrates rich user data including transcripts, videos, eye movement data, and interaction logs. Word-sized visualizations embedded into a tabular representation provide a space-efficient and detailed overview of user activities. An analyst assigns codes, grouped into code categories, as part of an interactive process. Filtering and searching helps to select specific activities and focus an analysis. A comparison visualization summarizes results of coding and reveals relationships between codes. Editing features support efficient assignment, refinement, and aggregation of codes. We demonstrate the practical applicability and usefulness of our approach in a case study and describe expert feedback.\n\nFriday, Oct. 28, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984570", "name": "[VIS16 Preview] Visual Analysis and Coding of Data-Rich User Behavior", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:27:14+00:00", "description": "VAST Paper \n\nOrganizers: Zhicheng Liu, Yang Wang, Mira Dontcheva, Matthew Hoffman, Seth Walker, Alan Wilson\n\nAbstract: Modern web clickstream data consists of long, high-dimensional sequences of multivariate events, making it difficult to analyze. Following the overarching principle that the visual interface should provide information about the dataset at multiple levels of granularity and allow users to easily navigate across these levels, we identify four levels of granularity in clickstream analysis: patterns, segments, sequences and events. We present an analytic pipeline consisting of three stages: pattern mining, pattern pruning and coordinated exploration between patterns and sequences. Based on this approach, we discuss properties of maximal sequential patterns, propose methods to reduce the number of patterns and describe design considerations for visualizing the extracted sequential patterns and the corresponding raw sequences. We demonstrate the viability of our approach through an analysis scenario and discuss the strengths and limitations of the methods based on user feedback.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 6", "uri": "https://vimeo.com/182984557", "name": "[VIS16 Preview] Patterns and Sequences: Interactive Exploration of Clickstreams to Understand Common Visitor Paths", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:27:01+00:00", "description": "VAST Paper \n\nOrganizers: Fan Du, Catherine Plaisant, Neil Spring, Ben Shneiderman\n\nAbstract: Recommender systems are being widely used to assist people in making decisions, for example, recommending films to watch or books to buy. Despite its ubiquity, the problem of presenting the recommendations of temporal event sequences has not been studied. We propose EventAction, which to our knowledge, is the first attempt at a prescriptive analytics interface designed to present and explain recommendations of temporal event sequences. EventAction provides a visual analytics approach to (1) identify similar records, (2) explore potential outcomes, (3) review recommended temporal event sequences that might help achieve the users' goals, and (4) interactively assist users as they define a personalized action plan associated with a probability of success. Following the design study framework, we designed and deployed EventAction in the context of student advising and reported on the evaluation with a student review manager and three graduate students.\n\nThursday, Oct. 27, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984546", "name": "[VIS16 Preview] EventAction: Visual Analytics for Temporal Event Sequence Recommendation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:26:49+00:00", "description": "VAST Paper \n\nOrganizers: Bum Chul Kwon, Hannah Kim, Emily Wall, Jaegul Choo, Haesun Park, Alex Endert\n\nAbstract: Visual analytics techniques help users explore high-dimensional data. However, it is often challenging for users to express their domain knowledge in order to steer the underlying data model, especially when they have little attribute-level knowledge. Furthermore, users' complex, high-level domain knowledge, compared to low-level attributes, posits even greater challenges. To overcome these challenges, we introduce a technique to interpret a user's drawings with an interactive, nonlinear axis mapping approach called AxiSketcher. This technique enables users to impose their domain knowledge on a visualization by allowing interaction with data entries rather than with data attributes. The proposed interaction is performed through directly sketching lines over the visualization. Using this technique, users can draw lines over selected data points and the system forms the axes that represent a nonlinear, weighted combination of multidimensional attributes. In this paper, we describe our techniques in three areas: 1) the design space of sketching methods for eliciting users' nonlinear domain knowledge; 2) the underlying model that translates users' input, extracts patterns behind the selected data points, and results in nonlinear axes reflecting users complex intent; and 3) the interactive visualization for viewing, assessing, and reconstructing the newly formed, nonlinear axes.\n\nThursday, Oct. 27, 10:30\u201312:10 \u2013 Holday 4+5", "uri": "https://vimeo.com/182984506", "name": "[VIS16 Preview] AxiSketcher: Interactive Nonlinear Axis Mapping of Visualizations through User Drawings", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:26:36+00:00", "description": "VAST Paper \n\nOrganizers: Hyunjoo Song, Jeongjin Lee, Tae Jung Kim, Kyoung Ho Lee, Bohyoung Kim, Jinwook Seo\n\nAbstract: We present an interactive visual analytics framework, GazeDx (abbr. of GazeDiagnosis), for the comparative analysis of gaze data from multiple readers examining volumetric images while integrating important contextual information with the gaze data. Gaze pattern comparison is essential to understanding how radiologists examine medical images, and to identifying factors influencing the examination. Most prior work depended upon comparisons with manually juxtaposed static images of gaze tracking results. Comparative gaze analysis with volumetric images is more challenging due to the additional cognitive load on 3D perception. A recent study proposed a visualization design based on direct volume rendering (DVR) for visualizing gaze patterns in volumetric images; however, effective and comprehensive gaze pattern comparison is still challenging due to a lack of interactive visualization tools for comparative gaze analysis. We take the challenge with GazeDx while integrating crucial contextual information such as pupil size and windowing into the analysis process for more in-depth and ecologically valid findings. Among the interactive visualization components in GazeDx, a context-embedded interactive scatterplot is especially designed to help users examine abstract gaze data in diverse contexts by embedding medical imaging representations well known to radiologists in it. We present the results from two case studies with two experienced radiologists, where they compared the gaze patterns of 14 radiologists reading two patient's volumetric CT images.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 6", "uri": "https://vimeo.com/182984488", "name": "[VIS16 Preview] GazeDx: Interactive Visual Analytics Framework for Comparative Gaze Analysis with Volumetric Medical Images", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:26:22+00:00", "description": "VAST Paper \n\nOrganizers: Jian Zhao, Michael Glueck, Simon Breslav, Fanny Chevalier, Azam Khan\n\nAbstract: User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst\u00e2\u20ac\u2122s manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984461", "name": "[VIS16 Preview] Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data based on User-Authored Annotations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:26:00+00:00", "description": "VAST Paper (Best Paper Honorable Mention)\n\nOrganizers: Cong Xie, Wen Zhong, Klaus Mueller\n\nAbstract: Oftentimes multivariate data are not available as sets of equally multivariate tuples, but only as sets of projections into subspaces spanned by subsets of these attributes. For example, one may find data with five attributes stored in six tables of two attributes each, instead of a single table of five attributes. This prohibits the visualization of these data with standard high-dimensional methods, such as parallel coordinates or MDS, and there is hence the need to reconstruct the full multivariate (joint) distribution from these marginal ones. Most of the existing methods designed for this purpose use an iterative procedure to estimate the joint distribution. With insufficient marginal distributions and domain knowledge, they lead to results whose joint errors can be large. Moreover, enforcing smoothness for regularizations in the joint space is not applicable if the attributes are not numerical but categorical. We propose a visual analytics approach that integrates both anecdotal data and human experts to iteratively narrow down a large set of plausible solutions. The solution space is populated using a Monte Carlo procedure which uniformly samples the solution space. A level-of-detail high dimensional visualization system helps the user understand the patterns and the uncertainties. Constraints that narrow the solution space can then be added by the user interactively during the iterative exploration, and eventually a subset of solutions with narrow uncertainty intervals emerges.\n\nTuesday, Oct. 25, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984431", "name": "[VIS16 Preview] A Visual Analytics Approach for Categorical Joint Distribution Reconstruction from Marginal Projections", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:25:47+00:00", "description": "VAST Paper \n\nOrganizers: Cagatay Turkay, Erdem Kaya, Selim Balcisoy, Helwig Hauser\n\nAbstract: In interactive data analysis processes, the dialogue between the human and the computer is the enabling mechanism that can lead to actionable observations about the phenomena being investigated. It is of paramount importance that this dialogue is not interrupted by slow computational mechanisms that do not consider any known temporal human-computer interaction characteristics that prioritize the perceptual and cognitive capabilities of the users. In cases where the analysis involves an integrated computational method, for instance to reduce the dimensionality of the data or to perform clustering, such non-optimal processes are often likely. To remedy this, progressive computations, where results are iteratively improved, are getting increasing interest in visual analytics. In this paper, we present techniques and design considerations to incorporate progressive methods within interactive analysis processes that involve high-dimensional data. We define methodologies to facilitate processes that adhere to the perceptual characteristics of users and describe how online algorithms can be incorporated within these. A set of design recommendations and according methods to support analysts in accomplishing high-dimensional data analysis tasks are then presented. Our arguments and decisions here are informed by observations gathered over a series of analysis sessions with analysts from finance. We document observations and recommendations from this study and present evidence on how our approach contribute to the efficiency and productivity of interactive visual analysis sessions involving high-dimensional data.\n\nWednesday, Oct. 26, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984414", "name": "[VIS16 Preview] Designing Progressive and Interactive Analytics Processes for High-Dimensional Data Analysis", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:25:33+00:00", "description": "VAST Paper \n\nOrganizers: David McColgin, Paul Hoover, Mark Igra\n\nAbstract: The DataSpace for HIV vaccine studies is a discovery tool available on the web to hundreds of investigators. We designed it to help them better understand activity in the field and explore new ideas latent in completed research. The DataSpace harmonizes immunoassay results and study metadata so that a broader research community can pursue more flexible discovery than the typical centrally planned analyses. Insights from human-centered design and beta evaluation suggest strong potential for visual analytics that may also apply to other efforts in open science. The contribution of this paper is to elucidate key domain challenges and demonstrate an application that addresses them. We made several changes to familiar visualizations to support key tasks such as identifying and filtering to a cohort of interest, making meaningful comparisons of time series data from multiple studies that have different plans, and preserving analytic context when making data transformations and comparisons that would normally exclude some data.\n\nWednesday, Oct. 26, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984381", "name": "[VIS16 Preview] The DataSpace for HIV Vaccine Studies", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:25:16+00:00", "description": "VAST Paper \n\nOrganizers: Shamal AL-Dohuki, Farah Kamw, Ye Zhao, Jing Yang, Chao Ma, Yingyu Wu, Xinyue Ye, Fei Wang, Xin Li, Wei Chen\n\nAbstract: Massive taxi trajectory data is exploited for knowledge discovery in transportation and urban planning. Existing tools typically require users to select and brush geospatial regions on a map when retrieving and exploring taxi trajectories and passenger trips. To answer seemingly simple questions such as ``What were the taxi trips starting from Main Street and ending at Wall Street in the morning?'' or ``Where are the taxis arriving at the Art Museum at noon typically coming from?'', tedious and time consuming interactions are usually needed since the numeric GPS points of trajectories are not directly linked to the keywords such as ``Main Street'', ``Wall Street'', and ``Art Museum''. In this paper, we present SemanticTraj, a new method for managing and visualizing taxi trajectory data in an intuitive, semantic rich, and efficient means. With SemanticTraj, domain and public users can find answers to the aforementioned questions easily through direct queries based on the terms. They can also interactively explore the retrieved data in visualizations enhanced by semantic information of the trajectories and trips. In particular, taxi trajectories are converted into taxi documents through a textualization transformation process. This process maps GPS points into a series of street/POI names and pick-up/drop-off locations. It also converts vehicle speeds into user-defined descriptive terms. Then, a corpus of taxi documents is formed and indexed to enable flexible semantic queries over a text search engine. Semantic labels and meta-summaries of the results are integrated with a set of visualizations in a SemanticTraj prototype, which helps users study taxi trajectories quickly and easily. A set of usage scenarios are presented to show the usability of the system. We also collected feedback from domain experts and conducted a preliminary user study to evaluate the visual system.\n\nTuesday, Oct. 25, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984349", "name": "[VIS16 Preview] SemanticTraj: A New Approach to Interacting with Massive Taxi Trajectories", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:25:02+00:00", "description": "VAST Paper \n\nOrganizers: Michael Glueck, Alina Gvozdik, Fanny Chevalier, Azam Khan, Michael Brudno, Daniel Wigdor\n\nAbstract: Cross-sectional phenotype studies are used by genetics researchers to better understand how phenotypes vary across patients with genetic diseases, both within and between cohorts. Analyses within cohorts identify patterns between phenotypes and patients (e.g., co-occurrence) and isolate special cases (e.g., potential outliers). Comparing the variation of phenotypes between two cohorts can help distinguish how different factors affect disease manifestation (e.g., causal genes, age of onset, etc.). PhenoStacks is a novel visual analytics tool that supports the exploration of phenotype variation within and between cross-sectional patient cohorts. By leveraging the semantic hierarchy of the Human Phenotype Ontology, phenotypes are presented in context, can be grouped and clustered, and are summarized via overviews to support the exploration of phenotype distributions. The design of PhenoStacks was motivated by formative interviews with genetics researchers: we distil high-level tasks, present an algorithm for simplifying ontology topologies for visualization, and report the results of a deployment evaluation with four expert genetics researchers. The results suggest that PhenoStacks can help identify phenotype patterns, investigate data quality issues, and inform data collection design.\n\nWednesday, Oct. 26, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984326", "name": "[VIS16 Preview] PhenoStacks: Cross-Sectional Cohort Phenotype Comparison Visualizations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:24:49+00:00", "description": "VAST Paper \n\nOrganizers: Xiting Wang, Shixia Liu, Yang Chen, Tai-Quan Peng, Jing Su, Jing Yang, Baining Guo\n\nAbstract: Tracking how correlated ideas flow within and across multiple social groups facilitates the understanding of the transfer of information, opinions, and thoughts on social media. In this paper, we present IdeaFlow, a visual analytics system for analyzing the lead-lag changes within and across pre-defined social groups regarding a specific set of correlated ideas, each of which is described by a set of words. To model idea flows accurately, we develop a random-walk-based correlation model and integrate it with Bayesian conditional cointegration and a tensor-based technique. To convey complex lead-lag relationships over time, IdeaFlow combines the strengths of a bubble tree, a flow map, and a timeline. In particular, we develop a Voronoi-treemap-based bubble tree to help users get an overview of a set of ideas quickly. A correlated-clustering-based layout algorithm is used to simultaneously generate multiple flow maps with less ambiguity. We also introduce a focus+context timeline to explore huge amounts of temporal data at different levels of time granularity. Quantitative evaluation and case studies demonstrate the accuracy and effectiveness of IdeaFlow.\n\nThursday, Oct. 27, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984305", "name": "[VIS16 Preview] How Ideas Flow across Multiple Social Groups", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:24:34+00:00", "description": "VAST Paper \n\nOrganizers: Yuanzhe Chen, Qing Chen, Mingqian Zhao, Sebastien Boyer, Kalyan Veeramachaneni, Huamin Qu\n\nAbstract: Aiming at massive participation and open access education, Massive Open Online Courses (MOOCs) have attracted millions of learners over the past few years. However, the high dropout rate of learners is considered to be one of the most crucial factors that may hinder the development of MOOCs. To tackle this problem, statistical models have been developed to predict dropout behavior based on learner activity logs. Although predictive models can foresee the dropout behavior, it is still difficult for users to understand the reasons behind the predicted results and further design interventions to prevent dropout. In addition, with a better understanding of dropout, researchers in the area of predictive modeling in turn can improve the models. In this paper, we introduce DropoutSeer, a visual analytics system which not only helps instructors and education experts understand the reasons for dropout, but also allows researchers to identify crucial features which can further improve the performance of the models. Both the heterogeneous data extracted from three different kinds of learner activity logs (i.e., clickstream, forum posts and assignment records) and the predicted results are visualized in the proposed system. Case studies and expert interviews have been conducted to demonstrate the usefulness and effectiveness of DropoutSeer.\n\nThursday, Oct. 27, 4:15\u20135:55 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984273", "name": "[VIS16 Preview] DropoutSeer: Visualizing Learning Patterns in Massive Open Online Courses for Dropout Reasoning and...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:24:20+00:00", "description": "VAST Paper \n\nOrganizers: Minjeong Kim, Kyeongpil Kang, Deokgun Park, Jaegul Choo, Niklas Elmqvist\n\nAbstract: Topic modeling, which reveals underlying topics of a document corpus, has been actively adopted in visual analytics for large-scale document collections. However, due to its significant processing time and its non-interactive nature, topic modeling has so far not been tightly integrated into a visual analytics workflow. Instead, most such systems are limited to utilizing a fixed, initial set of topics. Motivated by this gap in the literature, we propose a novel interaction technique called TopicLens that allows a user to dynamically explore data through a lens interface where topic modeling and the corresponding 2D embedding are efficiently computed on the fly. To support this interaction in real-time while maintaining view consistency, we propose a novel efficient topic modeling method as well a semi-supervised 2D embedding algorithm. Our work is based on improving state-of-the-art methods such as nonnegative matrix factorization and t-distributed stochastic neighbor embedding, respectively. Furthermore, we have built a web-based visual analytics system implementing TopicLens. We use this implementation to measure the performance and visualization quality of our proposed methods. We also provide scenarios showcasing the capability of TopicLens using several real-world datasets.\n\nWednesday, Oct. 26, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984247", "name": "[VIS16 Preview] TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale Document Collections", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:24:06+00:00", "description": "VAST Paper \n\nOrganizers: Siming Chen, Shuai Chen, Zhenhuang Wang, Jie Liang, Xiaoru Yuan, Nan Cao, Yadong Wu\n\nAbstract: Popular social media platforms could rapidly propagate vital information over social networks among a significant number of people. In this work we present D-Map (Diffusion Map), a novel visualization method to support exploration and analysis of social behaviors during such information diffusion and propagation on typical social media through a map metaphor. In D-Map, users who participated in reposting (i.e., resending a message initially posted by others) one central user's posts (i.e., a series of original tweets) are collected and mapped to a hexagonal grid based on their behavior similarities and in chronological order of the repostings. With additional interaction and linking, D-Map is capable of providing visual portraits of the influential users and describing their social behaviors. A comprehensive visual analysis system is developed to support interactive exploration with D-Map. We evaluate our work with real world social media data and find interesting patterns among users. Key players, important information diffusion paths, and interactions among social communities can be identified.\n\nThursday, Oct. 27, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984228", "name": "[VIS16 Preview] D-Map: Visual Analysis of Ego-centric Information Diffusion Patterns in Social Media", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:23:51+00:00", "description": "VAST Paper \n\nOrganizers: Siwei Fu, Jian Zhao, Weiwei Cui, Huamin Qu\n\nAbstract: Discussion forums of Massive Open Online Courses (MOOC) provide great opportunities for students to interact with instructional staff as well as other students. Exploration of MOOC forum data can offer valuable insights for these staff to enhance the course and prepare the next release. However, it is challenging due to the large, complicated, and heterogeneous nature of relevant datasets, which contain multiple dynamically interacting objects such as users, posts, and threads, each one including multiple attributes. In this paper, we present a design study for developing an interactive visual analytics system, called iForum, that allows for effectively discovering and understanding temporal patterns in MOOC forums. The design study was conducted with three domain experts in an iterative manner over one year, including a MOOC instructor and two official teaching assistants. iForum offers a set of novel visualization designs for presenting the three interleaving aspects of MOOC forums (i.e., posts, users, and threads) at three different scales. To demonstrate the effectiveness and usefulness of iForum, we describe a case study involving field experts, in which they use iForum to investigate real MOOC forum data for a course on JAVA programming.\n\nThursday, Oct. 27, 4:15\u20135:55 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984188", "name": "[VIS16 Preview] Visual Analysis of MOOC Forums with iForum", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:23:38+00:00", "description": "VAST Paper \n\nOrganizers: Sriram Karthik Badam, Fereshteh Amini, Niklas Elmqvist, Pourang Irani\n\nAbstract: We present a design space exploration of interaction techniques for supporting multiple collaborators exploring data on a shared large display. Our proposed solution is based on users controlling individual lenses using both explicit gestures as well as proxemics: the spatial relations between people and physical artifacts such as their distance, orientation, and movement. We discuss different design considerations for implicit and explicit interactions through the lens, and evaluate the user experience to find a balance between implicit and explicit interaction styles. Our findings indicate that users favor implicit interaction through proxemics for navigation and collaboration, but prefer using explicit mid-air gestures to perform actions that are perceived to be direct, such as terminating a lens composition. Based on these results, we propose a hybrid technique utilizing both proxemics and mid-air gestures, along with examples applying this technique to other datasets. Finally, we performed a usability evaluation of the hybrid technique and observed user performance improvements in the presence of both implicit and explicit interaction styles.\n\nTuesday, Oct. 25, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984168", "name": "[VIS16 Preview] Supporting Visual Exploration for Multiple Users in Large Display Environments", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:23:22+00:00", "description": "VAST Paper \n\nOrganizers: Ji Hwan Park, Saad Nadeem, Seyedkoosha Mirhosseini, Arie Kaufman\n\nAbstract: We present a medical crowdsourcing visual analytics platform called C2A to visualize, classify and filter crowdsourced clinical data. More specifically, C2A is used to build consensus on a clinical diagnosis by visualizing crowd responses and filtering out anomalous activity. Crowdsourcing medical applications have recently shown promise where the non-expert users (the crowd) were able to achieve accuracy similar to the medical experts. This has the potential to reduce interpretation/reading time and possibly improve accuracy by building a consensus on the findings beforehand and letting the medical experts make the final diagnosis. In this paper, we focus on a virtual colonoscopy (VC) application with the clinical technicians as our target users, and the radiologists acting as consultants and classifying segments as benign or malignant. In particular, C2A is used to analyze and explore crowd responses on video segments, created from fly-throughs in the virtual colon. C2A provides several interactive visualization components to build crowd consensus on video segments, to detect anomalies in the crowd data and in the VC video segments, and finally, to improve the non-expert user's work quality and performance by A/B testing for the optimal crowdsourcing platform and application-specific parameters. Case studies and domain experts feedback demonstrate the effectiveness of our framework in improving workers' output quality, the potential to reduce the radiologists' interpretation time, and hence, the potential to improve the traditional clinical workflow by marking the majority of the video segments as benign based on the crowd consensus.\n\nWednesday, Oct. 26, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984152", "name": "[VIS16 Preview] C2A: Crowd Consensus Analytics for Virtual Colonoscopy", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:23:06+00:00", "description": "VAST Paper \n\nOrganizers: Mengchen Liu, Jiaxin Shi, Zhen Li, Chongxuan Li, Jun Zhu, Shixia Liu\n\nAbstract: Deep convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks such as image classification. However, the development of high-quality deep models typically relies on a substantial amount of trial-and-error, as there is still no clear understanding of when and why a deep model works. In this paper, we present a visual analytics approach for better understanding, diagnosing, and refining deep CNNs. We formulate a deep CNN as a directed acyclic graph. Based on this formulation, a hybrid visualization is developed to disclose the multiple facets of each neuron and the interactions between them. In particular, we introduce a hierarchical rectangle packing algorithm and a matrix reordering algorithm to show the derived features of a neuron cluster. We also propose a biclustering-based edge bundling method to reduce visual clutter caused by a large number of connections between neurons. We evaluated our method on a set of CNNs and the results are generally favorable.\n\nTuesday, Oct. 25, 4:15\u20135:55 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182984132", "name": "[VIS16 Preview] Towards Better Analysis of Deep Convolutional Neural Networks", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:03:49+00:00", "description": "VAST Paper \n\nOrganizers: Michael Correll, Michael Gleicher\n\nAbstract: Sketching allows analysts to specify complex and free-form patterns of interest. Visual query systems can make use of sketches to locate these patterns of interest in large datasets. However, sketching is ambiguous: the same drawing could represent a multitude of potential queries. In this work, we investigate these ambiguities as they apply to visual query systems for time series data. We define a class of \"invariants\" - the properties of a time series that the analyst wishes to ignore when performing a sketch-based query. We present the results of a crowd-sourced study, showing that these invariants are key components of how people rate the strength of match between sketch and target. We adapt a number of algorithms for time series matching to support invariants in sketches. Lastly, we present a web-deployed prototype sketch-based visual query system that relies on these invariants. We apply the prototype to example datasets from finance, the digital humanities, and political science.\n\nFriday, Oct. 28, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982391", "name": "[VIS16 Preview] The Semantics of Sketch: A Visual Query System for Time Series Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:03:38+00:00", "description": "VAST Paper \n\nOrganizers: Sarah Goodwin, Christopher Mears, Tim Dwyer, Maria Garcia de la Banda, Guido Tack, Mark Wallace\n\nAbstract: Constraint programming allows difficult combinatorial problems to be modelled declaratively and solved automatically. Advances in solver technologies over recent years have allowed the successful use of constraint programming in many application areas. However, when a particular solver\u00e2\u20ac\u2122s search for a solution takes too long, the complexity of the constraint program execution hinders the programmer's ability to profile that search and understand how it relates to their model. Therefore, effective tools to support such profiling and allow users of constraint programming technologies to refine their model or experiment with different search parameters are essential. This paper details the first user-centred design process for visual profiling tools in this domain. We report on: our insights and opportunities identified through an on-line questionnaire and a creativity workshop with domain experts carried out to elicit requirements for analytical and visual profiling techniques; our designs and functional prototypes realising such techniques; and case studies demonstrating how these techniques shed light on the behaviour of the solvers in practice.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982354", "name": "[VIS16 Preview] What do Constraint Programming Users Want to See? Exploring the role of Visualisation in Profiling of Models...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:03:26+00:00", "description": "VAST Paper \n\nOrganizers: Bowen Yu, Claudio T Silva\n\nAbstract: Data flow systems allow the user to design a flow diagram that specifies the relations between system components which process, filter or visually present the data. Visualization systems may benefit from user-defined data flows as an analysis typically consists of rendering multiple plots on demand and performing different types of interactive queries across coordinated views. In this paper, we propose VisFlow, a web-based visualization framework for tabular data that employs a specific type of data flow model called the subset flow model. VisFlow focuses on interactive queries within the data flow, overcoming the limitation of interactivity from past computational data flow systems. In particular, VisFlow applies embedded visualizations and supports interactive selections, brushing and linking within a visualization-oriented data flow. The model requires all data transmitted by the flow to be a data item subset (i.e. groups of table rows) of some original input table, so that rendering properties can be assigned to the subset unambiguously for tracking and comparison. VisFlow features the analysis flexibility of a flow diagram, and at the same time reduces the diagram complexity and improves usability. We demonstrate the capability of VisFlow on two case studies with domain experts on real-world datasets showing that VisFlow is capable of accomplishing a considerable set of visualization and analysis tasks. The VisFlow system is available as open source on GitHub.\n\nThursday, Oct. 27, 10:30\u201312:10 \u2013 Holday 4+5", "uri": "https://vimeo.com/182982323", "name": "[VIS16 Preview] VisFlow - Web-based Visualization Framework for Tabular Data with a Subset Flow Model", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:03:14+00:00", "description": "VAST Paper \n\nOrganizers: Paulo E. Rauber, Samuel G. Fadel, Alexandre X. Falc\u00e3o, Alexandru C. Telea\n\nAbstract: In machine learning, pattern classification assigns high-dimensional vectors (observations) to classes based on generalization from examples. Artificial neural networks currently achieve state-of-the-art results in this task. Although such networks are typically used as black-boxes, they are also widely believed to learn (high-dimensional) higher-level representations of the original observations. In this paper, we propose using dimensionality reduction for two tasks: visualizing the relationships between learned representations of observations, and visualizing the relationships between artificial neurons. Through experiments conducted in three traditional image classification benchmark datasets, we show how visualization can provide highly valuable feedback for network designers. For instance, our discoveries in one of these datasets (SVHN) include the presence of interpretable clusters of learned representations, and the partitioning of artificial neurons into groups with apparently related discriminative roles.\n\nTuesday, Oct. 25, 4:15\u20135:55 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982313", "name": "[VIS16 Preview] Visualizing the Hidden Activity of Artificial Neural Networks", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:03:02+00:00", "description": "VAST Paper \n\nOrganizers: Qiaomu Shen, Tongshuang Wu, Haiyan Yang, Yanhong Wu, Huamin Qu, Weiwei Cui\n\nAbstract: In this paper, we present a novel visual analytics system called NameClarifier to interactively disambiguate author names in publications by keeping humans in the loop. Specifically, NameClarifier quantifies and visualizes the similarities between ambiguous names and those that have been confirmed in digital libraries. The similarities are calculated using three key factors, namely, co-authorships, publication venues, and temporal information. Our system estimates all possible allocations, and then provides visual cues to users to help them validate every ambiguous case. By looping users in the disambiguation process, our system can achieve more reliable results than general data mining models for highly ambiguous cases. In addition, once an ambiguous case is resolved, the result is instantly added back to our system and serves as additional cues for all the remaining unidentified names. In this way, we open up the black box in traditional disambiguation processes, and help intuitively and comprehensively explain why the corresponding classifications should hold. We conducted two use cases and an expert review to demonstrate the effectiveness of NameClarifier.\n\nWednesday, Oct. 26, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982294", "name": "[VIS16 Preview] NameClarifier: A Visual Analytics System for Author Name Disambiguation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:02:49+00:00", "description": "VAST Paper \n\nOrganizers: Prithiviraj Muthumanickam, Katerina Vrotsou, Matthew Cooper, Jimmy Johansson\n\nAbstract: Long time-series, involving thousands or even millions of time steps, are common in many application domains but remain very difficult to explore interactively. Often the analytical task in such data is to identify specific patterns, but this is a very complex and computationally difficult problem and so focusing the search in order to only identify interesting patterns is a common solution. We propose an efficient method for exploring user-sketched patterns, incorporating the domain expert's knowledge, in time series data through a shape grammar based approach. The shape grammar is extracted from the time series by considering the data as a combination of basic elementary shapes positioned across different amplitudes. We represent these basic shapes using a ratio value, perform binning on ratio values and apply a symbolic approximation. Our proposed method for pattern matching is amplitude-, scale- and translation-invariant and, since the pattern search and pattern constraint relaxation happen at the symbolic level, is very efficient permitting its use in a real-time/online system. We demonstrate the effectiveness of our method in a case study on stock market data although it is applicable to any numeric time series data.\n\nFriday, Oct. 28, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982277", "name": "[VIS16 Preview] Shape Grammar Extraction for Efficient Query-by-Sketch Pattern Matching in Long Time Series", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:02:38+00:00", "description": "VAST Paper \n\nOrganizers: Dongyu Liu, Di Weng, Yuhong Li, Yingcai Wu, Jie Bao, Yu Zheng, Huamin Qu\n\nAbstract: The problem of formulating solutions immediately and comparing them rapidly for billboard placements has plagued advertising planners for a long time, owing to the lack of efficient tools for in-depth analyses to make informed decisions. In this study, we attempt to employ visual analytics that combines the state-of-the-art mining and visualization techniques to tackle this problem using large-scale GPS trajectory data. In particular, we present SmartAdP, an interactive visual analytics system that deals with the two major challenges including finding good solutions in a huge solution space and comparing the solutions in a visual and intuitive manner. An interactive framework that integrates a novel visualization-driven data mining model enables advertising planners to effectively and efficiently formulate good candidate solutions. In addition, we propose a set of coupled visualizations: a solution view with metaphor-based glyphs to visualize the correlation between different solutions; a location view to display billboard locations in a compact manner; and a ranking view to present multi-typed rankings of the solutions. This system has been demonstrated using case studies with a real-world dataset and domain-expert interviews. Our approach can be adapted for other location selection problems such as selecting locations of retail stores or restaurants using trajectory data.\n\nTuesday, Oct. 25, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982257", "name": "[VIS16 Preview] SmartAdP: Visual Analytics of Large-scale Taxi Trajectories for Selecting Billboard Locations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:02:25+00:00", "description": "VAST Paper (Best Paper Award)\n\nOrganizers: Gary K.L. Tam, Vivek Kothari, Min Chen\n\nAbstract: In this work, we present a study that traces the technical and cognitive processes in two visual analytics applications to a common theoretic model of soft knowledge that may be added into a visual analytics process for constructing a decision-tree model. Both case studies involved the development of classification models based on the \"bag of features\" approach. Both compared a visual analytics approach using parallel coordinates with a machine learning approach using information theory. Both found that the visual analytics approach had some advantages over the machine learning approach, especially when sparse datasets were used as the ground truth. We examine various possible factors that may have contributed to such advantages, and collect empirical evidence for supporting the observation and reasoning of these factors. We propose an information-theoretic model as a common theoretic basis to explain the phenomena exhibited in these two case studies. Together we provide interconnected empirical and theoretical evidence to support the usefulness of visual analytics.\n\nTuesday, Oct. 25, 4:15\u20135:55 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982236", "name": "[VIS16 Preview] An Analysis of Machine- and Human-Analytics in Classification", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:02:09+00:00", "description": "VAST Paper \n\nOrganizers: Ievgeniia Gutenko, Konstantin Dmitriev, Arie Kaufman, Matthew Barish\n\nAbstract: We present a novel visualization framework, AnaFe, targeted at observing changes in the spleen over time through multiple image-derived features. Accurate monitoring of progressive changes is crucial for diseases that result in enlargement of the organ. Our system is comprised of multiple linked views combining visualization of temporal 3D organ data, related measurements, and features. Thus it enables the observation of progression and allows for simultaneous comparison within and between the subjects. AnaFe offers insights into the overall distribution of robustly extracted and reproducible quantitative imaging features and their changes within the population, and also enables detailed analysis of individual cases. It performs similarity comparison of temporal series of one subject to all other series in both sick and healthy groups. We demonstrate our system through two use case scenarios on a population of 189 spleen datasets from 68 subjects with various conditions observed over time.\n\nWednesday, Oct. 26, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982192", "name": "[VIS16 Preview] AnaFe: Visual Analytics of Image-derived Temporal Features \u00e2\u20ac\u201c Focusing on the Spleen", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:01:50+00:00", "description": "VAST Paper \n\nOrganizers: Florian Heimerl, Markus John, Qi Han, Steffen Koch, Thomas Ertl\n\nAbstract: The creation of interactive visualization to analyze text documents has gained an impressive momentum in recent years. This is not surprising in the light of massive and still increasing amounts of available digitized texts. Websites, social media, news wire, and digital libraries are just few examples of the diverse text sources whose visual analysis and exploration offers new opportunities to effectively mine and manage the information and knowledge hidden within them. A popular visualization method for large text collections is to represent each document by a glyph in 2D space. These landscapes can be the result of optimizing pairwise distances in 2D to represent document similarities, or they are provided directly as meta data, such as geo-locations. For well-defined information needs, suitable interaction methods are available for these spatializations. However, free exploration and navigation on a level of abstraction between a labeled document spatialization and reading single documents is largely unsupported. As a result, vital foraging steps for task-tailored actions, such as selecting subgroups of documents for detailed inspection, or subsequent sense-making steps are hampered. To fill in this gap, we propose DocuCompass, a focus+context approach based on the lens metaphor. It comprises multiple methods to characterize local groups of documents, and to efficiently guide exploration based on users' requirements. DocuCompass thus allows for effective interactive exploration of document landscapes without disrupting the mental map of users by changing the layout itself. We discuss the suitability of multiple navigation and characterization methods for different spatializations and texts. Finally, we provide insights generated through user feedback and discuss the effectiveness of our approach.\n\nWednesday, Oct. 26, 8:30\u201310:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982152", "name": "[VIS16 Preview] DocuCompass: Effective Exploration of Document Landscapes", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:01:35+00:00", "description": "VAST Paper \n\nOrganizers: R. Jordan Crouser, Lyndsey Franklin, Alex Endert, Kristin Cook\n\nAbstract: Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.\n\nWednesday, Oct. 26, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982125", "name": "[VIS16 Preview] Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:01:18+00:00", "description": "VAST Paper \n\nOrganizers: Quan Li, Peng Xu, Yeuk Yin Chan, Yun Wang, Zhipeng Wang, Huamin Qu, Xiaojuan Ma\n\nAbstract: To design a successful Multiplayer Online Battle Arena (MOBA) game, the ratio of snowballing and comeback occurrences to all matches played must be maintained at a certain level to ensure its fairness and engagement. Although it is easy to identify these two types of occurrences, game developers often find it difficult to determine their causes and triggers with so many game design choices and game parameters involved. In addition, the huge amounts of MOBA game data are often heterogeneous, multi-dimensional and highly dynamic in terms of space and time, which poses special challenges for analysts. In this paper, we present a visual analytics system to help game designers find key events and game parameters resulting in snowballing or comeback occurrences in MOBA game data. We follow a user-centered design process developing the system with game analysts and testing with real data of a trial version MOBA game from a commercial video game company. We apply novel visualization techniques in conjunction with well-established ones to depict the evolution of players\u00e2\u20ac\u2122 positions, status and the occurrences of events. Our system can reveal players\u00e2\u20ac\u2122 strategies and performance throughout a single match and suggest patterns, e.g., specific player\u00e2\u20ac\u2122 actions and game events, that have led to the final occurrences. We further demonstrate a workflow of leveraging human analyzed patterns to improve the scalability and generality of match data analysis. Finally, we validate the usability of our system by proving the identified patterns are representative in snowballing or comeback matches in a one-month-long MOBA tournament dataset.\n\nThursday, Oct. 27, 4:15\u20135:55 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982101", "name": "[VIS16 Preview] A Visual Analytics Approach for Understanding Reasons behind Snowballing and Comeback in MOBA Games", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:01:01+00:00", "description": "VAST Paper \n\nOrganizers: Davide Ceneda, Theresia Gschwandtner, Thorsten May, Silvia Miksch, Hans-J\u00f6rg Schulz, Marc Streit, Christian Tominski\n\nAbstract: Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed. Unfortunately, there is a natural correlation between the complexity of the data and the complexity of the tools to study them. An adverse effect of complicated tools is that analytical goals are more difficult to reach. Therefore, it makes sense to consider methods that guide or assist users in the visual analysis process. Several such methods already exist in the literature, yet we are lacking a general model that facilitates in-depth reasoning about guidance. We establish such a model by extending van Wijk's model of visualization with the fundamental components of guidance. Guidance is defined as a process that gradually narrows the gap that hinders effective continuation of the data analysis. We describe diverse inputs based on which guidance can be generated and discuss different degrees of guidance and means to incorporate guidance into VA tools. We use existing guidance approaches from the literature to illustrate the various aspects of our model. As a conclusion, we identify research challenges and suggest directions for future studies. With our work we take a necessary step to pave the way to a systematic development of guidance techniques that effectively support users in the context of VA.\n\nWednesday, Oct. 26, 10:30\u201312:10 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982078", "name": "[VIS16 Preview] Characterizing Guidance in Visual Analytics", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:00:51+00:00", "description": "VAST Paper \n\nOrganizers: Jing Xia, Wei Chen, Yumeng Hou, Wanqi Hu, Xinxin Huang, David Ebert\n\nAbstract: Exploring multi-dimensional datasets can be cumbersome if data analysts have little knowledge about the data. Various dimension relation inspection tools and dimension exploration tools have been proposed for efficient data examining and understanding. However, the needed workload varies largely with respect to data complexity and user expertise, which can only be reduced with rich background knowledge over the data. In this paper we address the workload challenge with the a data structuring and exploring scheme that affords dimension relation detection and that serves as the background knowledge for further investigation. We contribute a novel data structuring scheme that leverages an information-theoretic view structuring algorithm to uncover information-aware relations among different data views, and thereby discloses redundancy and other relation patterns among dimensions. The integrated system, DimScanner, empowers analysts with rich user controls and assisted widgets to interactively detect the relations of multi-dimensional data.\n\nThursday, Oct. 27, 10:30\u201312:10 \u2013 Holday 4+5", "uri": "https://vimeo.com/182982061", "name": "[VIS16 Preview] DimScanner: A Relation-based Visual Exploration Approach Towards Data Dimension Inspection", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:00:38+00:00", "description": "VAST Paper \n\nOrganizers: Junpeng Wang, Xiaotong Liu, Han-Wei Shen, Guang Lin\n\nAbstract: Due to the uncertain nature of weather prediction, climate simulations are usually performed multiple times with different spatial resolutions. The outputs of simulations are multi-resolution spatial temporal ensembles. Each simulation run uses a unique set of values for multiple convective parameters. Distinct parameter settings from different simulation runs in different resolutions constitute a multi-resolution high-dimensional parameter space. Understanding the correlation between the different convective parameters, and establishing a connection between the parameter settings and the ensemble outputs are crucial to domain scientists. The multi-resolution high-dimensional parameter space, however, presents a unique challenge to the existing correlation visualization techniques. We present Nested Parallel Coordinates Plot (NPCP), a new type of parallel coordinates plots that enables visualization of intra-resolution and inter-resolution parameter correlations. With flexible user control, NPCP integrates superimposition, juxtaposition and explicit encodings in a single view for comparative data visualization and analysis. We develop an integrated visual analytics system to help domain scientists understand the connection between multi-resolution convective parameters and the large spatial temporal ensembles. Our system presents intricate climate ensembles with a comprehensive overview and on-demand geographic details. We demonstrate NPCP, along with the climate ensemble visualization system, based on real-world use-cases from our collaborators in computational and predictive science.\n\nTuesday, Oct. 25, 4:15\u20135:55 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982037", "name": "[VIS16 Preview] Multi-Resolution Climate Ensemble Parameter Analysis with Nested Parallel Coordinates Plots", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:00:24+00:00", "description": "VAST Paper \n\nOrganizers: Donghao Ren, Saleema Amershi, Bongshin Lee, Jina Suh, Jason D. Williams\n\nAbstract: Performance analysis is critical in applied machine learning because it influences the models practitioners produce. Current performance analysis tools suffer from issues including obscuring important characteristics of model behavior and dissociating performance from data. In this work, we present Squares, a performance visualization for multiclass classification problems. Squares supports estimating common performance metrics while displaying instance-level distribution information necessary for helping practitioners prioritize efforts and access data. Our controlled study shows that practitioners can assess performance significantly faster and more accurately with Squares than a confusion matrix, a common performance analysis tool in machine learning.\n\nTuesday, Oct. 25, 4:15\u20135:55 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182982014", "name": "[VIS16 Preview] Squares: Supporting Interactive Performance Analysis for Multiclass Classifiers", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T11:00:11+00:00", "description": "VAST Paper \n\nOrganizers: Ali Sarvghad Batn Moghaddam, Melanie Tory, Narges Mahyar\n\nAbstract: Data analysis involves constantly formulating and testing new hypotheses and questions about data. When dealing with a new dataset, especially one with many dimensions, it can be cumbersome for the analyst to clearly remember which aspects of the data have been investigated (i.e., visually examined for patterns, trends, outliers etc.) and which combinations have not. Yet this information is critical to help the analyst formulate new questions that they have not already answered. We observe that for tabular data, questions are typically comprised of varying combinations of data dimensions (e.g., what are the trends of Sales and Profit for different Regions?). We propose representing analysis history from the angle of dimension coverage (i.e., which data dimensions have been investigated and in which combinations). We use scented widgets [30] to incorporate dimension coverage of the analysts\u00e2\u20ac\u2122 past work into interaction widgets of a visualization tool. We demonstrate how this approach can assist analysts with the question formation process. Our approach extends the concept of scented widgets to reveal aspects of one\u00e2\u20ac\u2122s own analysis history, and offers a different perspective on one\u00e2\u20ac\u2122s past work than typical visualization history tools. Results of our empirical study showed that participants with access to embedded dimension coverage information relied on this information when formulating questions, asked more questions about the data, generated more top-level findings, and showed greater breadth of their analysis without sacrificing depth.\n\nTuesday, Oct. 25, 2:00\u20133:40 \u2013 Holiday 4+5", "uri": "https://vimeo.com/182981996", "name": "[VIS16 Preview] Visualizing Dimension Coverage to Support Exploratory Analysis", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:59:49+00:00", "description": "VAST Paper \n\nOrganizers: Kuno Kurzhals, Marcel Hlawatsch, Christof Seeger, Daniel Weiskopf\n\nAbstract: The analysis of eye tracking data often requires the annotation of areas of interest (AOIs) to derive semantic interpretations of human viewing behavior during experiments. This annotation is typically the most time-consuming step of the analysis process. Especially for data from wearable eye tracking glasses, every independently recorded video has to be annotated individually and corresponding AOIs between videos have to be identified. We provide a novel visual analytics approach to ease this annotation process by image-based, automatic clustering of eye tracking data integrated in an interactive labeling and analysis system. The annotation and analysis are tightly coupled by multiple linked views that allow for a direct interpretation of the labeled data in the context of the recorded video stimuli. The components of our analytics environment were developed with a user-centered design approach in close cooperation with an eye tracking expert. We demonstrate our approach with eye tracking data from a real experiment and compare it to an analysis of the data by manual annotation of dynamic AOIs. Furthermore, we conducted an expert user study with 6 external eye tracking researchers to collect feedback and identify analysis strategies they used while working with our application.\n\nThursday, Oct. 27, 2:00\u20133:40 \u2013 Holiday 6", "uri": "https://vimeo.com/182981967", "name": "[VIS16 Preview] Visual Analytics for Mobile Eye Tracking", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:59:36+00:00", "description": "InfoVis Paper \n\nAuthors: Arthur van Goethem, Frank Staals, Maarten L\u00f6ffler, Jason Dykes, Bettina Speckmann\n\nAbstract: Time series (such as stock prices) and ensembles (such as model runs for weather forecasts) are two important types of one-dimensional time-varying data. Such data is readily available in large quantities but visual analysis of the raw data quickly becomes infeasible, even for moderately sized data sets. Trend detection is an effective way to simplify time-varying data and to summarize salient information for visual display and interactive analysis. We propose a geometric model for trend-detection in one-dimensional time-varying data, inspired by topological grouping structures for moving objects in two- or higher-dimensional space. Our model gives provable guarantees on the trends detected and uses three natural parameters: granularity, support-size, and duration. These parameters can be changed on-demand. Our system also supports a variety of selection brushes and a time-sweep to facilitate refined searches and interactive visualization of (sub-)trends. We explore different visual styles and interactions through which trends, their persistence, and evolution can be explored.\n\nSession Time Series \u2013 Thursday, Oct. 27, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182981951", "name": "[VIS16 Preview] Multi-Granular Trend Detection for Time-Series Analysis", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:59:24+00:00", "description": "InfoVis Paper \n\nAuthors: Shiqing He, Eytan Adar\n\nAbstract: Shifts in information visualization practice are forcing a reconsideration of how infovis is taught. Traditional curricula that focused on conveying research-derived knowledge are slowly integrating design thinking as a key learning objective. In part, this is motivated by the realization that infovis is a wicked design problem, requiring a different kind of design work. In this paper we describe, VizItCards, a card-driven workshop developed for our graduate infovis class. The workshop is intended to provide practice with good design techniques and to simultaneously reinforce key concepts. VizItCards relies on principles of collaborative-learning and research on parallel design to generate positive collaborations and high-quality designs. From our experience of simulating a realistic design scenario in a classroom setting, we find that our students were able to meet key learning objectives and their design performance improved during the class. We describe variants of the workshop, discussing which techniques we think match to which learning goals.\n\nSession Visualization Education \u2013 Friday, Oct. 28, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182981931", "name": "[VIS16 Preview] VizItCards: A Card-Based Toolkit for Infovis Design Education", "year": "2016", "event": "INFOVIS, PREVIEW"}, {"created_time": "2016-09-16T10:59:12+00:00", "description": "InfoVis Paper \n\nAuthors: Anzu Hakone, Lane Harrison, Alvitta Ottley, Nathan Winters, Caitlin Gutheil, Paul K. J. Han, Remco Chang\n\nAbstract: Prostate cancer is the most common cancer among men in the US, and yet most cases represent localized cancer for which the optimal treatment is unclear. Accumulating evidence suggests that the available treatment options, including surgery and conservative treatment, result in a similar prognosis for most men with localized prostate cancer. However, approximately 90% of patients choose surgery over conservative treatment, despite the risk of severe side effects like erectile dysfunction and incontinence. Recent medical research suggests that a key reason is the lack of patient-centered tools that can effectively communicate personalized risk information and enable them to make better health decisions. In this paper, we report the iterative design process and results of developing the PROgnosis Assessment for Conservative Treatment (PROACT) tool, a personalized health risk communication tool for localized prostate cancer patients. PROACT utilizes two published clinical prediction models to communicate the patients' personalized risk estimates and compare treatment options. In collaboration with the Maine Medical Center, we conducted two rounds of evaluations with prostate cancer survivors and urologists to identify the design elements and narrative structure that effectively facilitate patient comprehension under emotional distress. Our results indicate that visualization can be an effective means to communicate complex risk information to patients with low numeracy and visual literacy. However, the visualizations need to be carefully chosen to balance readability with ease of comprehension. In addition, due to patients' charged emotional state, an intuitive narrative structure that considers the patients' information need is critical to aid the patients' comprehension of their risk information.\n\nSession Applications \u2013 Wednesday, Oct. 26, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182981910", "name": "[VIS16 Preview] PROACT: Iterative Design of a Patient-Centered Visualization for Effective Prostate Cancer Health Risk...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:58:59+00:00", "description": "InfoVis Paper \n\nAuthors: Lace Padilla, P. Samuel Quinan, Miriah Meyer, Sarah H. Creem-Regehr\n\nAbstract: The expressiveness principle for visualization design asserts that a visualization should encode all of the available data, and only the available data, implying that continuous data types should be visualized with a continuous encoding channel. And yet, in many domains binning continuous data is not only pervasive, but it is accepted as standard practice. Prior work provides no clear guidance for when encoding continuous data continuously is preferable to employing binning techniques or how this choice affects data interpretation and decision making. In this paper, we present a study aimed at better understanding the conditions in which the expressiveness principle can or should be violated for visualizing continuous data. We provided participants with visualizations employing either continuous or binned greyscale encodings of geospatial elevation data and compared participants\u201a\u00c4\u00f4 ability to complete a wide variety of tasks. For various tasks, the results indicate significant differences in decision making, confidence in responses, and task completion time between continuous and binned encodings of the data. In general, participants with continuous encodings were faster to complete many of the tasks, but never outperformed those with binned encodings, while performance accuracy with binned encodings was superior to continuous encodings in some tasks. These findings suggest that strict adherence to the expressiveness principle is not always advisable. We discuss both the implications and limitations of our results and outline various avenues for potential work needed to further improve guidelines for using continuous versus binned encodings for continuous data types.\n\nSession Evaluation \u2013 Wednesday, Oct. 26, 10:30-12:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182981896", "name": "[VIS16 Preview] Evaluating the Impact of Binning 2D Scalar Fields", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:58:44+00:00", "description": "InfoVis Paper (Best Paper Honorable Mention)\n\nAuthors: Roger Beecham, Jason Dykes, Wouter Meulemans, Aidan Slingsby, Cagatay Turkay, Jo Wood\n\nAbstract: Fundamental to the effective use of visualization as an analytic and descriptive tool is the assurance that presenting data visually provides the capability of making inferences from what we see. This paper explores two related approaches to quantifying the confidence we may have in making visual inferences from mapped geospatial data. We adapt Wickham et al.'s 'Visual Line-up' method as a direct analogy with Null Hypothesis Significance Testing (NHST) and propose a new approach for generating more credible spatial null hypotheses. Rather than using as a spatial null hypothesis the unrealistic assumption of complete spatial randomness, we propose spatially autocorrelated simulations as alternative nulls. We conduct a set of crowdsourced experiments (n = 361) to determine the just noticeable difference (JND) between pairs of choropleth maps of geographic units controlling for spatial autocorrelation (Moran's I statistic) and geometric configuration (variance in spatial unit area). Results indicate that people's abilities to perceive differences in spatial autocorrelation vary with baseline autocorrelation structure and the geometric configuration of geographic units. These results allow us, for the first time, to construct a visual equivalent of statistical power for geospatial data. Our JND results add to those provided in recent years by Klippel et al. (2011), Harrison et al. (2014) and Kay &amp; Heer (2015) for correlation visualization. Importantly, they provide an empirical basis for an improved construction of visual line-ups for maps and the development of theory to inform geospatial tests of graphical inference.\n\nSession Geovisualization \u2013 Tuesday, Oct. 25, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182981882", "name": "[VIS16 Preview] Map LineUps: effects of spatial structure on graphical inference", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:58:29+00:00", "description": "InfoVis Paper \n\nAuthors: Wouter Meulemans, Jason Dykes, Aidan Slingsby, Cagatay Turkay, Jo Wood\n\nAbstract: Small multiples enable comparison by providing different views of a single data set in a dense and aligned manner. A common frame defines each view, which varies based upon values of a conditioning variable. An increasingly popular use of this technique is to project two-dimensional locations into a gridded space (e.g. grid maps), using the underlying distribution both as the conditioning variable and to determine the grid layout. Using whitespace in this layout has the potential to carry information, especially in a geographic context. Yet, the effects of doing so on the spatial properties of the original units are not understood. We explore the design space offered by such small multiples with gaps. We do so by constructing a comprehensive suite of metrics that capture properties of the layout used to arrange the small multiples for comparison (e.g. compactness and alignment) and the preservation of the original data (e.g. distance, topology and shape). We study these metrics in geographic data sets with varying properties and numbers of gaps. We use simulated annealing to optimize for each metric and measure the effects on the others. To explore these effects systematically, we take a new approach, developing a system to visualize this design space using a set of interactive matrices. We find that adding small amounts of whitespace to small multiple arrays improves some of the characteristics of 2D layouts, such as shape, distance and direction. This comes at the cost of other metrics, such as the retention of topology. Effects vary according to the input maps, with degree of variation in size of input regions found to be a factor. Optima exist for particular metrics in many cases, but at different amounts of whitespace for different maps. We suggest multiple metrics be used in optimized layouts, finding topology to be a primary factor in existing manually-crafted solutions, followed by a trade-off between shape and displacement. But the rich range of possible optimized layouts leads us to challenge single-solution thinking; we suggest to consider alternative optimized layouts for small multiples with gaps. Key to our work is the systematic, quantified and visual approach to exploring design spaces when facing a trade-off between many competing criteria---an approach likely to be of value to the analysis of other design spaces.\n\nSession Geovisualization \u2013 Tuesday, Oct. 25, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182981859", "name": "[VIS16 Preview] Small Multiples with Gaps", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:57:16+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Chris Bryan, Gregory Guterman, Kwan-Liu Ma, Harris Lewin, Denis Larkin, Jaebum Kim, Jian Ma, Marta Farr\u00e9\n\nAbstract: Rapid advances in biology demand new tools for more active research dissemination and engaged teaching. This paper presents Synteny Explorer, an interactive visualization application designed to let college students explore genome evolution of mammalian species. The tool visualizes synteny blocks: segments of homologous DNA shared between various extant species that can be traced back or reconstructed in extinct, ancestral species. We take a karyogram-based approach to create an interactive synteny visualization, leading to a more appealing and engaging design for undergraduate-level genome evolution education. For validation, we conduct three user studies: two focused studies on color and animation design choices and a larger study that performs overall system usability testing while comparing our karyogram-based designs with two more common genome mapping representations in an educational context. While existing views communicate the same information, study participants found the interactive, karyogram-based views much easier and likable to use. We additionally discuss feedback from biology and genomics faculty, who judge Synteny Explorer's fitness for use in classrooms.", "uri": "https://vimeo.com/182981754", "name": "[VIS16 Preview] Synteny Explorer: An Interactive Visualization Application for Teaching Genome Evolution", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:57:06+00:00", "description": "SciVis Paper (Best Paper Honorable Mention)\n\nOrganizers / Panelists: Daniel J\u00f6nsson, Anders Ynnerman\n\nAbstract: We present a method for interactive global illumination of both static and time-varying volumetric data based on reduction of the overhead associated with re-computation of photon maps. Our method uses the identification of photon traces invariant to changes of visual parameters such as the transfer function (TF), or data changes between time-steps in a 4D volume. This lets us operate on a variant subset of the entire photon distribution. The amount of computation required in the two stages of the photon mapping process, namely tracing and gathering, can thus be reduced to the subset that are affected by a data or visual parameter change. We rely on two different types of information from the original data to identify the regions that have changed. A low resolution uniform grid containing the minimum and maximum data values of the original data is derived for each time step. Similarly, for two consecutive time-steps, a low resolution grid containing the difference between the overlapping data is used. We show that this compact metadata can be combined with the transfer function to identify the regions that have changed. Each photon traverses the low-resolution grid to identify if it can be directly transferred to the next photon distribution state or if it needs to be recomputed. An efficient representation of the photon distribution is presented leading to an order of magnitude improved performance of the raycasting step. The utility of the method is demonstrated in several examples that show visual fidelity, as well as performance. The examples show that visual quality can be retained when the fraction of retraced photons is as low as 40%-50%.", "uri": "https://vimeo.com/182981744", "name": "[VIS16 Preview] Correlated Photon Mapping for Interactive Global Illumination of Time-Varying Volumetric Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:56:55+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Hui Fang, Simon Walton, Emily Delahaye, James Harris, Dmitry Storchak, Min Chen\n\nAbstract: Mapping a set of categorical values to different colors is an elementary technique in data visualization. Users of visualization software routinely rely on the default colormaps provided by a system, or colormaps suggested by software such as ColorBrewer. In practice, users often have to select a set of colors in a semantically meaningful way (e.g., based on conventions, color metaphors, and logological associations), and consequently would like to ensure their perceptual differentiation is optimized. In this paper, we present an algorithmic approach for maximizing the perceptual distances among a set of given colors. We address two technical problems in optimization, i.e., (i) the phenomena of local maxima that halt the optimization too soon, and (ii) the arbitrary reassignment of colors that leads to the loss of the original semantic association. We paid particular attention to different types of constraints that users may wish to impose during the optimization process. To demonstrate the effectiveness of this work, we tested this technique in two case studies. To reach out to a wider range of users, we also developed a web application called Colourmap Hospital.", "uri": "https://vimeo.com/182981726", "name": "[VIS16 Preview] Categorical Colormap Optimization with Visualization Case Studies", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T10:56:40+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Sergej Stoppel, Stefan Bruckner\n\nAbstract: Interaction is an indispensable aspect of data visualization. The presentation of volumetric data, in particular, often significantly benefits from interactive manipulation of parameters such as transfer functions, rendering styles, or clipping planes. However, when we want to create hardcopies of such visualizations, this essential aspect is lost. In this paper, we present a novel approach for creating hardcopies of volume visualizations which preserves a certain degree of interactivity. We present a method for automatically generating Volvelles, printable tangible wheel charts that can be manipulated to explore different parameter settings. Our interactive system allows the flexible mapping of arbitrary visualization parameters and supports advanced features such as linked views. The resulting designs can be easily reproduced using a standard printer and assembled within a few minutes.", "uri": "https://vimeo.com/182981686", "name": "[VIS16 Preview] VolVolvelle: Printable Interactive Volume Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:41:05+00:00", "description": "SciVis Paper \n\nAuthors: Antoni Sagrist\u00e0, Stefan Jordan, Andreas Just, Fabio Dias, Gustavo Nonato, Filip Sadlo\n\nAbstract: Traditional vector field visualization has a close focus on velocity, and is typically constrained to the dynamics of massless particles. In this paper, we present a novel approach to the analysis of the force-induced dynamics of inertial particles. These forces can arise from acceleration fields such as gravitation, but also be dependent on the particle dynamics itself, as in the case of magnetism. Compared to massless particles, the velocity of an inertial particle is not determined solely by its position and time in a vector field. In contrast, its initial velocity can be arbitrary and impacts the dynamics over its entire lifetime. This leads to a four-dimensional problem for 2D setups, and a six-dimensional problem for the 3D case. Our approach avoids this increase in dimensionality and tackles the visualization by an integrated topological analysis approach. We demonstrate the utility of our approach using a synthetic time-dependent acceleration field, a system of magnetic dipoles, and N-body systems both in 2D and 3D.\n\nSession Topology-based Techniques \u2013 Tuesday, Oct. 25, 4:15\u20135:55 \u2013 Key 1+2+5", "uri": "https://vimeo.com/182974674", "name": "[VIS16 Preview] Topological Analysis of Inertial Dynamics", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:34:56+00:00", "description": "SciVis Paper \n\nAuthors: Michael Krone, Florian Frie\u00df, Katrin Scharnowski, Guido Reina, Silvia Fademrecht, Tobias Kulschewski, J\u00fcrgen Pleiss, Thomas Ertl\n\nAbstract: We present Molecular Surface Maps, a novel, view-independent, and concise representation for molecular surfaces. It transfers the well-known world map metaphor to molecular visualization. Our application maps the complex molecular surface to a simple 2D representation through a spherical intermediate, the Molecular Surface Globe. The Molecular Surface Map concisely shows arbitrary attributes of the original molecular surface, such as biochemical properties or geometrical features. This results in an intuitive overview, which allows researchers to assess all molecular surface attributes at a glance. Our representation can be used as a visual summarization of a molecule's interface with its environment. In particular, Molecular Surface Maps simplify the analysis and comparison of different data sets or points in time. Furthermore, the map representation can be used in a Space-time Cube to analyze time-dependent data from molecular simulations without the need for animation. We show the feasibility of Molecular Surface Maps for different typical analysis tasks of biomolecular data.\n\nSession Biological, Molecular and Shape Visualization \u2013 Tuesday, Oct. 25, 10:30\u201312:10 \u2013 Key 1+2+5", "uri": "https://vimeo.com/182974149", "name": "[VIS16 Preview] Molecular Surface Maps", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:31:50+00:00", "description": "SciVis Paper \n\nAuthors: Noeska Smit, Kai Lawonn, Annelot Kraima, Marco DeRuiter, Hessam Sokooti, Stefan Bruckner, Elmar Eisemann, Anna Vilanova\n\nAbstract: Due to the intricate relationship between the pelvic organs and vital structures, such as vessels and nerves, pelvic anatomy is often considered to be complex to comprehend. In oncological pelvic surgery, a trade-off has to be made between complete tumor resection and preserving function by preventing damage to the nerves. Damage to the autonomic nerves causes undesirable post-operative side-effects such as fecal and urinal incontinence, as well as sexual dysfunction in up to 80 percent of the cases. Since these autonomic nerves are not visible in pre-operative MRI scans or during surgery, avoiding nerve damage during such a surgical procedure becomes challenging. \\In this work, we present visualization methods to represent context, target, and risk structures for surgical planning. We employ distance-based and occlusion management techniques in an atlas-based surgical planning tool for oncological pelvic surgery. Patient-specific pre-operative MRI scans are registered to an atlas model that includes nerve information. Through several interactive linked views, the spatial relationships and distances between the organs, tumor and risk zones are visualized to improve understanding, while avoiding occlusion. In this way, the surgeon can examine surgically relevant structures and plan the procedure before going into the operating theater, thus raising awareness of the autonomic nerve zone regions and potentially reducing post-operative complications. Furthermore, we present the results of a domain expert evaluation with surgical oncologists that demonstrates the advantages of our approach.\n\nSession Biomedical Visualization \u2013 Friday, Oct. 28, 8:30\u201310:10 \u2013 Key 1+2+5", "uri": "https://vimeo.com/182973853", "name": "[VIS16 Preview] PelVis: Atlas-based Surgical Planning for Oncological Pelvic Surgery", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:01:46+00:00", "description": "InfoVis Paper \n\nAuthors: Matthew Berger, Katherine McDonough, Lee Seversky\n\nAbstract: Effectively exploring and browsing document collections is a fundamental problem in visualization. Traditionally, document visualization is based on a data model that represents each document as the set of its comprised words, effectively characterizing what the document is. In this paper we take an alternative perspective: motivated by the manner in which users search documents in the research process, we aim to visualize documents via their \\\\emph{usage}, or how documents tend to be used.We present a new visualization scheme -- cite2vec -- that allows the user to dynamically explore and browse documents via how other documents use them, information that we capture through \\\\emph{citation contexts} in a document collection. Starting from a usage-oriented term-document 2D projection, the user can dynamically steer document projections by prescribing semantic concepts, both in the form of phrase/document compositions and phrase:document analogies, enabling the exploration and comparison of documents by their use. The user interactions are enabled by a joint representation of words and documents in a common high-dimensional embedding space where user-specified concepts correspond to linear operations of word and document vectors in this space. Our case studies, centered around a large document corpus of computer vision research papers, highlight the potential for usage-based document visualization.\n\nSession Scalable Algorithms \u2013 Thursday, Oct. 27, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971186", "name": "[VIS16 Preview] cite2vec: Citation-Driven Document Exploration via Word Embeddings", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:01:30+00:00", "description": "InfoVis Paper \n\nAuthors: Rudolf Netzel, Marcel Hlawatsch, Michael Burch, Hansj\u00f6rg Schmauder, Sanjeev Balakrishnan, Daniel Weiskopf\n\nAbstract: Visual search can be time-consuming, especially if the scene contains a large number of possibly relevant objects. An instance of this problem is present when using geographic or schematic maps with many different elements representing cities, streets, sights, and the like. Unless the map is well-known to the reader, the full map or at least large parts of it must be scanned to find the elements of interest. In this paper, we present a controlled eye-tracking study (30 participants) to compare four representative variants of map annotation with labels: within-image annotations, grid reference annotation, directional annotation, and miniature annotation. Within-image annotation places labels directly within the map without any further search support. Grid reference annotation corresponds to the traditional approach, known from common atlas legends. Directional annotation utilizes a label in combination with an arrow pointing in the direction of the label within the map. Miniature annotation shows a miniature grid to guide the reader to the area of the map in which the label is located. The study results show that within-image annotation is outperformed by all other annotation approaches. Best task completion times are achieved with miniature annotation. The analysis of eye-movement data reveals that participants applied significantly different visual task solution strategies for the different visual annotations.\n\nSession Evaluation \u2013 Wednesday, Oct. 26, 10:30-12:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971162", "name": "[VIS16 Preview] An Evaluation of Visual Search Support in Maps", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:01:14+00:00", "description": "InfoVis Paper (Best Paper Honorable Mention)\n\nAuthors: Evanthia Dimara, Anastasia Bezerianos, Pierre Dragicevic\n\nAbstract: The attraction effect is a well-studied cognitive bias in decision making research, where one\u201a\u00c4\u00f4s choice between two alternatives is influenced by the presence of an irrelevant (dominated) third alternative. We examine whether this cognitive bias, so far only tested with three alternatives and simple presentation formats such as numerical tables, text and pictures, also appears in visualizations. Since visualizations can be used to support decision making \u201a\u00c4\u00ee e.g., when choosing a house to buy or an employee to hire \u201a\u00c4\u00ee a systematic bias could have important implications. In a first crowdsource experiment, we indeed partially replicated the attraction effect with three alternatives presented as a numerical table, and observed similar effects when they were presented as a scatterplot. In a second experiment, we investigated if the effect extends to larger sets of alternatives, where the number of alternatives is too large for numerical tables to be practical. Our findings indicate that the bias persists for larger sets of alternatives presented as scatterplots. We discuss implications for future research on how to further study and possibly alleviate the attraction effect.\n\nSession Immersive Analytics \u2013 Tuesday, Oct. 25, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971122", "name": "[VIS16 Preview] The Attraction Effect in Information Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:01:04+00:00", "description": "InfoVis Paper \n\nAuthors: Faisal Taher, Yvonne Jansen, Jonathan Woodruff, John Hardy, Kasper Hornb\u00e6k, Jason Alexander\n\nAbstract: Physical data representations, or data physicalizations, are a promising new medium to represent and communicate data. Previous work mostly studied passive physicalizations which require humans to perform all interactions manually. Dynamic shape-changing displays address this limitation and facilitate data exploration tasks such as sorting, navigating in data sets which exceed the fixed size of a given physical display, or preparing \u201a\u00c4\u00faviews\u201a\u00c4\u00f9 to communicate insights about data. However, it is currently unclear how people approach and interact with such data representations. We ran an exploratory study to investigate how non-experts made use of a dynamic physical bar chart for an open-ended data exploration and presentation task. We asked 16 participants to explore a data set on European values and to prepare a short presentation of their insights using a physical display. We analyze: (1) users\u201a\u00c4\u00f4 body movements to understand how they approach and react to the physicalization, (2) their hand-gestures to understand how they interact with physical data, (3) system interactions to understand which subsets of the data they explored and which features they used in the process, and (4) strategies used to explore the data and present observations. We discuss the implications of our findings for the use of dynamic data physicalizations and avenues for future work.\n\nSession Immersive Analytics \u2013 Tuesday, Oct. 25, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971107", "name": "[VIS16 Preview] Investigating the Use of a Dynamic Physical Bar Chart for Data Exploration and Presentation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:00:45+00:00", "description": "InfoVis Paper \n\nAuthors: Chris Bryan, Kwan-Liu Ma, Jonathan Woodring\n\nAbstract: Visualization is a powerful technique for analysis and communication of complex, multidimensional, and time-varying data. However, it can be difficult to manually synthesize a coherent narrative in a chart or graph due to the quantity of visualized attributes, a variety of salient features, and the awareness required to interpret points of interest (POIs). We present Temporal Summary Images (TSIs) as an approach for both exploring this data and creating stories from it. As a visualization, a TSI is composed of three common components: (1) a temporal layout, (2) comic strip-style data snapshots, and (3) textual annotations. To augment user analysis and exploration, we have developed a number of interactive techniques that recommend relevant data features and design choices, including an automatic annotations workflow. As the analysis and visual design processes converge, the resultant image becomes appropriate for data storytelling. For validation, we use a prototype implementation for TSIs to conduct two case studies with large-scale, scientific simulation datasets.\n\nSession Storytelling / Presentation \u2013 Thursday, Oct. 27, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971084", "name": "[VIS16 Preview] Temporal Summary Images: An Approach to Narrative Visualization via Interactive Annotation Generation and...", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:00:35+00:00", "description": "InfoVis Paper \n\nAuthors: Mengdie Hu, Krist Wongsuphasawat, John Stasko\n\nAbstract: We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.\n\nSession Applications \u2013 Wednesday, Oct. 26, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971053", "name": "[VIS16 Preview] Visualizing Social Media Content with SentenTree", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:00:22+00:00", "description": "InfoVis Paper \n\nAuthors: Benjamin Bach, Nathalie Henry Riche, Christophe Hurter, Kim Marriott, Tim Dwyer\n\nAbstract: In this paper, we investigate Confluent Drawings (CD), a technique for bundling edges in node-link diagrams based on network connectivity. Edge-bundling techniques are designed to reduce edge clutter in node-link diagrams by coalescing lines into common paths or bundles. Unfortunately, traditional bundling techniques introduce ambiguity since edges are only bundled by spatial proximity, rather than network connectivity; following an edge from its source to its target can lead to the perception of incorrect connectivity if edges are not clearly separated within the bundles. Contrary, CDs bundle edges based on common sources or targets. Thus, a smooth path along a confluent bundle indicates precise connectivity. While CDs have been described in theory, practical investigation and application to real-world networks (i.e., networks beyond those with certain planarity restrictions) is currently lacking. Here, we provide the first algorithm for constructing CDs from arbitrary directed and undirected networks and present a simple layout method, embedded in a sand box environment providing techniques for interactive exploration. We then investigate patterns and artifacts in CDs, which we compare to other common edge-bundling techniques. Finally, we present the first user study that compares edge-compression techniques, including CD, power graphs, metro-style, and common edge bundling. We found that users without particular expertise in visualization or network analysis are able to read small CDs without difficulty. Compared to existing bundling techniques, CDs are more likely to allow people to correctly perceive connectivity. \\\n\nSession Graphs \u2013 Wednesday, Oct. 26, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971043", "name": "[VIS16 Preview] Towards Unambiguous Edge Bundling: Investigating Confluent Drawings for Network Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T09:00:04+00:00", "description": "InfoVis Paper \n\nAuthors: Fereshteh Amini, Nathalie Henry Riche, Bongshin Lee, Andr\u00e9s Monroy-Hern\u00e1ndez, Pourang Irani\n\nAbstract: Data videos, or short data-driven motion graphics, are an increasingly popular medium for storytelling. However, creating data videos is difficult as it involves pulling together a unique combination of skills. We introduce DataClips, an authoring tool aimed at lowering the barriers to crafting data videos. DataClips allows non-experts to assemble data-driven \u201a\u00c4\u00faclips\u201a\u00c4\u00f9 together to form longer sequences. We constructed the library of data clips by analyzing the composition of over 70 data videos produced by reputable sources such as The New York Times and The Guardian. We demonstrate that DataClips can reproduce over 90% of our data videos corpus. We also report on a qualitative study comparing the authoring process and outcome achieved by (1) non-experts using DataClips, and (2) experts using Adobe Illustrator and After Effects to create data-driven clips. Results indicated that non-experts are able to learn and use DataClips with a short training period. In the span of one hour, they were able to produce more videos than experts using a professional editing tool, and their clips were rated similarly by an independent audience.\n\nSession Storytelling / Presentation \u2013 Thursday, Oct. 27, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971025", "name": "[VIS16 Preview] Authoring Data-Driven Videos with DataClips", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:59:52+00:00", "description": "InfoVis Paper \n\nAuthors: Michael Correll, Jeffrey Heer\n\nAbstract: Thematic maps are commonly used for visualizing the density of events in spatial data. However, these maps can mislead by giving visual prominence to known base rates (such as population densities) or to artifacts of sample size and normalization (such as outliers arising from smaller, and thus more variable, samples). In this work, we adapt Bayesian surprise to generate maps that counter these biases. Bayesian surprise, which has shown promise for modeling human visual attention, weights information with respect to how it updates beliefs over a space of models. We introduce Surprise Maps, a visualization technique that weights event data relative to a set of spatio-temporal models. Unexpected events (those that induce large changes in belief over the model space) are visualized more prominently than those that follow expected patterns. Using both synthetic and real-world datasets, we demonstrate how Surprise Maps overcome some limitations of traditional event maps.\n\nSession Time Series \u2013 Thursday, Oct. 27, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971017", "name": "[VIS16 Preview] Surprise! Bayesian Weighting of Spatio-Temporal Events", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:59:40+00:00", "description": "InfoVis Paper \n\nAuthors: Wesley Willett, Yvonne Jansen, Pierre Dragicevic\n\nAbstract: \u201a\u00c4\u00eeWe introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents \u201a\u00c4\u00ec the real-world entities and spaces to which data corresponds \u201a\u00c4\u00ec and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.\n\nSession Immersive Analytics \u2013 Tuesday, Oct. 25, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182971005", "name": "[VIS16 Preview] Embedded Data Representations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:59:16+00:00", "description": "InfoVis Paper \n\nAuthors: Nam Wook Kim, Eston Schweickart, Zhicheng Liu, Mira Dontcheva, Wilmot Li, Jovan Popovic, Hanspeter Pfister\n\nAbstract: In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.\n\nSession Storytelling / Presentation \u2013 Thursday, Oct. 27, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970973", "name": "[VIS16 Preview] Data-Driven Guides: Supporting Expressive Design for Information Graphics", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:59:03+00:00", "description": "InfoVis Paper \n\nAuthors: Zhe Wang, Aarthy Sankari Bhaskar, Youhao Wei, Nivan Ferreira, Carlos Scheidegger\n\nAbstract: Recently proposed techniques have finally made it possible for analysts to interactively explore very large datasets in real time. However powerful, the class of analyses these systems enable is somewhat limited: specifically, one can only quickly obtain plots such as histograms and heatmaps. In this paper, we contribute Gaussian Cubes, which significantly improves on state-of-the-art systems by providing interactive modeling capabilities, which include but are not limited to linear least squares and principal components analysis (PCA). The fundamental insight in Gaussian Cubes is that instead of precomputing counts of many data subsets (as state-of-the-art systems do), Gaussian Cubes precomputes the best multivariate Gaussian for the respective data subsets. As an example, Gaussian Cubes can fit hundreds of models over millions of data points in well under a second, enabling novel types of visual exploration of such large datasets. We present three case studies that highlight the visualization and analysis capabilities in Gaussian Cubes, using earthquake safety simulations, astronomical catalogs, and transportation statistics. The dataset sizes range around one hundred million elements and 5 to 10 dimensions. We present extensive performance results, a discussion of the limitations in Gaussian Cubes, and future research directions.\n\nSession Scalable Algorithms \u2013 Thursday, Oct. 27, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970958", "name": "[VIS16 Preview] Gaussian Cubes: Real-Time Data Modeling for Interactive Exploration of Large Multidimensional Datasets", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:58:50+00:00", "description": "InfoVis Paper \n\nAuthors: C\u00edcero A. L. Pahins, Sean A. Stephens, Carlos Scheidegger, Jo\u00e3o L. D. Comba\n\nAbstract: We propose Hashedcubes, a data structure that enables real-time visual exploration of large datasets that improves the state of the art by virtue of its low memory requirements, low query latencies, and implementation simplicity. In some instances, Hashedcubes notably requires two orders of magnitude less space than recent data cube visualization proposals. In this paper, we describe the algorithms to build and query Hashedcubes, and how it can drive well-known interactive visualizations such as binned scatterplots, linked histograms and heatmaps. We report memory usage, build time and query latencies for a variety of synthetic and real-world datasets, and find that although sometimes Hashedcubes offers slightly slower querying times to the state of the art, the typical query is answered fast enough to easily sustain a interaction. In datasets with hundreds of millions of elements, only about 2% of the queries take longer than 40ms. Finally, we discuss the limitations of data structure, potential spacetime tradeoffs, and future research directions.\n\nSession Scalable Algorithms \u2013 Thursday, Oct. 27, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970940", "name": "[VIS16 Preview] Hashedcubes: Simple, Low Memory, Real-Time Visual Exploration of Big Data", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:58:29+00:00", "description": "InfoVis Paper \n\nAuthors: Michail Schwab, Hendrik Strobelt, James Tompkin, Colin Fredericks, Connor Huff, Dana Higgins, Anton Strezhnev, Mayya Komisarchik, Gary King, Hanspeter Pfister\n\nAbstract: Information hierarchies are difficult to express when real-world space or time constraints force traversing the hierarchy in linear presentations, such as in educational books and classroom courses. We present booc.io, which allows linear and non-linear presentation and navigation of educational concepts and material. To support a breadth of material for each concept, booc.io is Web based, which allows adding material such as lecture slides, book chapters, course videos, and LTIs. A visual interface assists the creation of the needed hierarchical structures. The goals of our system were formed in expert interviews, and we explain how our design meets these goals. We adapt a real-world course into booc.io, and perform introductory qualitative evaluation with students.\n\nSession Visualization Education \u2013 Friday, Oct. 28, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970903", "name": "[VIS16 Preview] booc.io: An Education System with Hierarchical Concept Maps and Dynamic Non-linear Learning Plans", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:58:13+00:00", "description": "InfoVis Paper \n\nAuthors: Stephan Pajer, Marc Streit, Thomas Torsney-Weir, Florian Spechtenhauser, Torsten Moeller, Harald Piringer\n\nAbstract: A common strategy in Multi-Criteria Decision Making (MCDM) is to rank alternative solutions by weighted summary scores. Weights, however, are often abstract to the decision maker and can only be set by vague intuition. While previous work supports a point-wise exploration of weight spaces, we argue that MCDM can benefit from a regional and global visual analysis of weight spaces. Our main contribution is WeightLifter, a novel interactive visualization technique for weight-based MCDM that facilitates the exploration of weight spaces with up to ten criteria. %without an inherent upper limit regarding their dimensionality. Our technique enables users to better understand the sensitivity of a decision to changes of weights, to efficiently localize weight regions where a given solution ranks high, and to filter out solutions which do not rank high enough for any plausible combination of weights. We provide a comprehensive requirement analysis for weight-based MCDM and describe an interactive workflow that meets these requirements. For evaluation, we describe a usage scenario of WeightLifter in automotive engineering and report qualitative feedback from users of a deployed version as well as preliminary feedback from decision makers in multiple domains. This feedback confirms that WeightLifter increases both the efficiency of weight-based MCDM and the awareness of uncertainty in the ultimate decisions.\n\nSession Applications \u2013 Wednesday, Oct. 26, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970872", "name": "[VIS16 Preview] WeightLifter: Visual Weight Space Exploration for Multi-Criteria Decision Making", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:58:02+00:00", "description": "InfoVis Paper \n\nAuthors: Rafael Veras, Christopher Collins\n\nAbstract: In this paper we examine how the Minimum Description Length (MDL) principle can be used to efficiently select aggregated views of hierarchical datasets that feature a good balance between clutter and information. We present MDL formulae for generating uneven tree cuts tailored to treemap and sunburst diagrams, taking into account the available display space and information content of the data. We present the results of a proof-of-concept implementation. In addition, we demonstrate how such tree cuts can be used to enhance drill-down interaction in hierarchical visualizations by implementing our approach in an existing visualization tool. Validation is done with the feature congestion measure of clutter in views of a subset of the current DMOZ web directory, which contains nearly half million categories. The results show that MDL views achieve near constant clutter level across display resolutions. We also present the results of a crowdsourced user study where participants were asked to find targets in views of DMOZ generated by our approach and a set of baseline aggregation methods. The results suggest that, in some conditions, participants are able to locate targets (in particular, outliers) faster using the proposed approach.\n\nSession Applications \u2013 Wednesday, Oct. 26, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970860", "name": "[VIS16 Preview] Optimizing Hierarchical Visualizations with the Minimum Description Length Principle", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:57:40+00:00", "description": "InfoVis Paper \n\nAuthors: Clemens Arbesser, Florian Spechtenhauser, Thomas M\u00fchlbacher, Harald Piringer\n\nAbstract: Trends like decentralized energy production lead to an exploding number of time series from sensors and other sources that need to be assessed regarding their data quality (DQ). While the identification of DQ problems for such routinely collected data is typically based on existing automated plausibility checks, an efficient inspection and validation of check results for hundreds or thousands of time series is challenging. The main contribution of this paper is the validated design of Visplause, a system to support an efficient inspection of DQ problems for many time series. The key idea of Visplause is to utilize meta-information concerning the semantics of both the time series and the plausibility checks for structuring and summarizing results of DQ checks in a flexible way. Linked views enable users to inspect anomalies in detail and to generate hypotheses about possible causes. The design of Visplause was guided by goals derived from a comprehensive task analysis with domain experts in the energy sector. We reflect on the design process by discussing design decisions at four stages and we identify lessons learned. We also report feedback from domain experts after using Visplause for a period of one month. This feedback suggests significant efficiency gains for DQ assessment, increased confidence in the DQ, and the applicability of Visplause to summarize indicators also outside the context of DQ.\n\nSession Time Series \u2013 Thursday, Oct. 27, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970832", "name": "[VIS16 Preview] Visplause: Visual Data Quality Assessment of Many Time Series Using Plausibility Checks", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:57:27+00:00", "description": "InfoVis Paper (Best Paper Honorable Mention)\n\nAuthors: Yalong Yang, Tim Dwyer, Sarah Goodwin, Kim Marriott\n\nAbstract: Showing flows of people and resources between multiple geographic locations is a challenging visualisation problem. We conducted two quantitative user studies to evaluate different visual representations for such dense many-to-many flows. In our first study we compared a bundled node-link flow map representation and OD Maps with a new visualisation we call MapTrix. Like OD Maps, MapTrix overcomes the clutter associated with a traditional flow map while providing geographic embedding that is missing in standard OD matrix representations. We found that OD Maps and MapTrix had similar performance while bundled node-link flow map representations did not scale at all well. Our second study compared participant performance with OD Maps and MapTrix on larger data sets. Again performance was remarkably similar.\n\nSession Evaluation \u2013 Wednesday, Oct. 26, 10:30-12:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970812", "name": "[VIS16 Preview] Many-to-Many Geographically-Embedded Flow Visualisation: An Evaluation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:57:14+00:00", "description": "InfoVis Paper \n\nAuthors: Mona Hosseinkhani Loorak, Charles Perin, Christopher Collins, Sheelagh Carpendale\n\nAbstract: Heterogeneous multi-dimensional data are now sufficiently common that they can be referred to as ubiquitous. The most frequent approach to visualizing these data has been to propose new visualizations for representing these data. These new solutions are often inventive but tend to be unfamiliar. We take a different approach. We explore the possibility of extending well-known and familiar visualizations through including Heterogeneous Embedded Data Attributes (HEDA) in order to make familiar visualizations more powerful. We demonstrate how HEDA is a generic, interactive visualization component that can extend common visualization techniques while respecting the structure of the familiar layout. HEDA is a tabular visualization building block that enables individuals to visually observe, explore, and query their familiar visualizations through manipulation of embedded multivariate data. We describe the design space of HEDA by exploring its application to familiar visualizations in the D3 gallery. We characterize these familiar visualizations by the extent to which HEDA can facilitate data queries based on attribute reordering.\n\nSession Visualization Education \u2013 Friday, Oct. 28, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970781", "name": "[VIS16 Preview] Exploring the Possibilities of Embedding Heterogeneous Data Attributes in Familiar Visualizations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:56:59+00:00", "description": "InfoVis Paper \n\nAuthors: Kasper Dinkla, Hendrik Strobelt, Bryan Genest, Stephan Reiling, Mark Borowsky, Hanspeter Pfister\n\nAbstract: High-throughput and high-content screening enables large scale, cost-effective experiments in which cell cultures are exposed to a wide spectrum of drugs. The resulting multivariate data sets have a large but shallow hierarchical structure. The deepest level of this structure describes cells in terms of numeric features that are derived from image data. The subsequent level describes enveloping cell cultures in terms of imposed experiment conditions (exposure to drugs). We present Screenit, a visual analysis approach designed in close collaboration with screening experts. Screenit enables the navigation and analysis of multivariate data at multiple hierarchy levels and at multiple levels of detail. Screenit integrates the interactive modeling of cell physical states (phenotypes) and the effects of drugs on cell cultures (hits). In addition, quality control is enabled via the detection of anomalies that indicate low-quality data, while providing an interface that is designed according to typical workflows of screening experts. We demonstrate analyses for a real-world data set, CellMorph, with 6 million cells across 20,000 cell cultures.\n\nSession Applications \u2013 Wednesday, Oct. 26, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970753", "name": "[VIS16 Preview] Screenit:\\nVisual Analysis of Cellular Screens", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:56:42+00:00", "description": "InfoVis Paper \n\nAuthors: Sukwon Lee, Sung-Hee Kim, Bum Chul Kwon\n\nAbstract: The Information Visualization community has begun to pay attention to visualization literacy; however, researchers still lack instruments for measuring the visualization literacy of users. In order to address this gap, we systematically developed a visualization literacy assessment test (VLAT), especially for non-expert users in data visualization, by following the established procedure of test development in Psychological and Educational Measurement: (1) Test Blueprint Construction, (2) Test Item Generation, (3) Content Validity Evaluation, (4) Test Tryout and Item Analysis, (5) Test Item Selection, and (6) Reliability Evaluation. The VLAT consists of 12 data visualizations and 53 multiple-choice test items that cover eight data visualization tasks. The test items in the VLAT were evaluated with respect to their essentialness by five domain experts in Information Visualization and Visual Analytics (average content validity ratio = 0.66). The VLAT was also tried out on a sample of 191 test takers and showed high reliability (reliability coefficient omega = 0.76). In addition, we demonstrated the relationship between users' visualization literacy and aptitude for learning an unfamiliar visualization and showed that they had a fairly high positive relationship (correlation coefficient = 0.64). Finally, we discuss evidence for the validity of the VLAT and potential research areas that are related to the instrument.\n\nSession Visualization Education \u2013 Friday, Oct. 28, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970712", "name": "[VIS16 Preview] VLAT: Development of a Visualization Literacy Assessment Test", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:56:25+00:00", "description": "InfoVis Paper \n\nAuthors: Christoph Schulz, Arlind Nocaj, Jochen Goertler, Oliver Deussen, Ulrik Brandes, Daniel Weiskopf\n\nAbstract: We present a novel uncertain network visualization technique based on node-link diagrams. Nodes expand spatially in our probabilistic graph layout, depending on the underlying probability distributions of edges. The visualization is created by computing a two-dimensional graph embedding that combines samples from the probabilistic graph. A Monte Carlo process is used to decompose a probabilistic graph into its possible instances and to continue with our graph layout technique. Splatting and edge bundling are used to visualize point clouds and network topology. The results provide insights into probability distributions for the entire network-not only for individual nodes and edges. We validate our approach using three data sets that represent a wide range of network types: synthetic data, protein-protein interactions from the STRING database, and travel times extracted from Google Maps. Our approach reveals general limitations of the force-directed layout and allows the user to recognize that some nodes of the graph are at a specific position just by chance.\n\nSession Graphs \u2013 Wednesday, Oct. 26, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970686", "name": "[VIS16 Preview] Probabilistic Graph Layout for Uncertain Network Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:56:13+00:00", "description": "InfoVis Paper \n\nAuthors: Mi Feng, Cheng Deng, Evan Peck, Lane Harrison\n\nAbstract: Physical and digital objects often leave markers of our use. Website links turn purple after we visit them, for example, showing us information we have yet to explore. These \u201a\u00c4\u00fafootprints\u201a\u00c4\u00f9 of interaction offer substantial benefits in information saturated environments \u201a\u00c4\u00ec they enable us to easily revisit old information, systematically explore new information, and quickly resume tasks after interruption. While applying these design principles have been successful in HCI contexts, direct encodings of personal interaction history have received scarce attention in data visualization. One reason is that there is little guidance for integrating history into visualizations where many visual channels are already occupied by data. More importantly, there is not firm evidence that making users aware of their interaction history results in benefits with regards to exploration or insights. Following these observations, we propose HindSight - an umbrella term for the design space of representing interaction history directly in existing data visualizations. In this paper, we examine the value of HindSight principles by augmenting existing visualizations with visual indicators of user interaction history (e.g. How the Recession Shaped the Economy in 255 Charts, NYTimes). In controlled experiments of over 400 participants, we found that HindSight designs generally encouraged people to visit more data and recall different insights after interaction. The results of our experiments suggest that simple additions to visualizations can make users aware of their interaction history, and that these additions significantly impact users\u201a\u00c4\u00f4 exploration and insights.\n\nSession Interaction \u2013 Tuesday, Oct. 25, 10:30\u201312:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970669", "name": "[VIS16 Preview] HindSight: Encouraging Exploration through Direct Encoding of Personal Interaction History", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:56:01+00:00", "description": "InfoVis Paper \n\nAuthors: Bilal Alsallakh, Liu Ren\n\nAbstract: When analyzing a large amount of data, analysts often define groups over data elements that share certain properties. Using these groups as the unit of analysis not only reduces the data volume, but also allows detecting various patterns in the data. This involves analyzing intersection relations between these groups, and how the element attributes vary between these intersections. This kind of set-based analysis has various applications in a variety of domains, due to the generic and powerful notion of sets. However, visualizing intersections relations is challenging because their number grows exponentially with the number of sets. We present a novel technique based on Treemaps to provide a comprehensive overview of non-empty intersections in a set system in a scalable way. It enables gaining insight about how elements are distributed across these intersections as well as performing fine-grained analysis to explore and compare their attributes both in overview and in detail. Interaction allows querying and filtering these elements based on their set memberships. We demonstrate how our technique supports various use cases in data exploration and analysis by providing insights into set-based data, beyond the limits of state-of-the-art techniques.\n\nSession Interaction \u2013 Tuesday, Oct. 25, 10:30\u201312:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970649", "name": "[VIS16 Preview] PowerSet: A Comprehensive Visualization of Set Intersections", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:55:48+00:00", "description": "InfoVis Paper (Best Paper Award)\n\nAuthors: Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, Jeffrey Heer\n\nAbstract: We present Vega-Lite, a high-level grammar that enables rapid specification of interactive data visualizations. Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction. Users specify interactive semantics by composing selections. In Vega-Lite, a selection is an abstraction that defines input event processing, points of interest, and a predicate function for inclusion testing. Selections parameterize visual encodings by serving as input data, defining scale extents, or by driving conditional logic. The Vega-Lite compiler automatically synthesizes requisite data flow and event handling logic, which users can override for further customization. In contrast to existing reactive specifications, Vega-Lite selections decompose an interaction design into concise, enumerable semantic units. We evaluate Vega-Lite through a range of examples, demonstrating succinct specification of both customized interaction methods and common techniques such as panning, zooming, and linked selection.\n\nSession Interaction \u2013 Tuesday, Oct. 25, 10:30\u201312:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970629", "name": "[VIS16 Preview] Vega-Lite:\\nA Grammar of Interactive Graphics", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:55:36+00:00", "description": "InfoVis Paper \n\nAuthors: Connor C. Gramazio, David H. Laidlaw, Karen B. Schloss\n\nAbstract: We present an evaluation of Colorgorical, a web-based tool for creating discriminable and aesthetically preferable categorical color palettes. Colorgorical uses iterative semi-random sampling to pick colors from CIELAB space based on user-defined discriminability and preference importances. Colors are selected by assigning each a weighted sum score that applies the user- defined importances to Perceptual Distance, Name Difference, Name Uniqueness, and Pair Preference scoring functions, which compare a potential sample to already-picked palette colors. After, a color is added to the palette by randomly sampling from the highest scoring palettes. Users can also specify hue ranges or build off their own starting palettes. This procedure differs from previous approaches that do not allow customization (e.g., pre-made ColorBrewer palettes) or do not consider visualization design constraints (e.g., Adobe Color and ACE). In a Palette Score Evaluation, we verified that each scoring function measured different color information. Experiment 1 demonstrated that slider manipulation generates palettes that are consistent with the expected balance of discriminability and aesthetic preference for 3-, 5-, and 8-color palettes, and also shows that the number of colors may change the effectiveness of pair-based discriminability and preference scores. For instance, if the Pair Preference slider were upweighted, users would judge the palettes as more preferable on average. Experiment 2 compared Colorgorical palettes to benchmark palettes (ColorBrewer, Microsoft, Tableau, Random). Colorgorical palettes are as discriminable and are at least as preferable or more prefer- able than the alternative palette sets. In sum, Colorgorical allows users to make customized color palettes that are, on average, as effective as current industry standards by balancing the importance of discriminability and aesthetic preference. \\\n\nSession Storytelling / Presentation \u2013 Thursday, Oct. 27, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970612", "name": "[VIS16 Preview] Colorgorical: Creating discriminable and preferable color palettes for information visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:55:26+00:00", "description": "InfoVis Paper \n\nAuthors: Alex Bigelow, Steven Drucker, Danyel Fisher, Miriah Meyer\n\nAbstract: A common workflow for visualization designers begins with a generative tool, like D3 or Processing, to create the initial visualization; and proceeds to a drawing tool, like Adobe Illustrator or Inkscape, for editing and cleaning. Unfortunately, this is typically a one-way process: once a visualization is exported from the generative tool into a drawing tool, it is difficult to make further, data-driven changes. In this paper, we propose a bridge model to allow designers to bring their work back from the drawing tool to re-edit in the generative tool. Our key insight is to recast this iteration challenge as a merge problem - similar to when two people are editing a document and changes between them need to reconciled. We also present a specific instantiation of this model, a tool called Hanpuku, which bridges between D3 scripts and Illustrator. We show several examples of visualizations that are iteratively created using Hanpuku in order to illustrate the flexibility of the approach. We further describe several hypothetical tools that bridge between other visualization tools to emphasize the generality of the model.\n\nSession Storytelling / Presentation \u2013 Thursday, Oct. 27, 8:30\u201310:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970599", "name": "[VIS16 Preview] Iterating Between Tools to Create and Edit Visualizations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:55:08+00:00", "description": "InfoVis Paper \n\nAuthors: Maxime Cordeil, Tim Dwyer, Karsten Klein, Kim Marriott, Bireswar Laha, Bruce Thomas\n\nAbstract: High-quality immersive display technologies are becoming mainstream with the release of head-mounted displays (HMDs) such as the Oculus Rift. These devices potentially represent an affordable alternative to the more traditional, centralised CAVE-style immersive environments. One driver for the development of CAVE-style immersive environments has been collaborative sense-making. Despite this, there has been little research on the effectiveness of collaborative visualisation in CAVE-style facilities, especially with respect to abstract data visualisation tasks. Indeed, very few studies have focused on the use of these displays to explore and analyse abstract data such as networks and there have been no formal user studies investigating collaborative visualisation of abstract data in immersive environments. In this paper we present the results of the first such study. It explores the relative merits of HMD and CAVE-style immersive environments for collaborative analysis of network connectivity, a common and important task involving abstract data. We find significant differences between the two conditions in task completion time and the physical movements of the participants within the space: participants using the HMD were faster while the CAVE2 condition introduced an asymmetry in movement between collaborators. Otherwise, affordances for collaborative data analysis offered by the low-cost HMD condition were not found to be different for accuracy and communication with the CAVE2. These results are notable, given that the latest HMDs will soon be accessible (in terms of cost and potentially ubiquity) to a massive audience.\n\nSession Immersive Analytics \u2013 Tuesday, Oct. 25, 2:00\u20133:40 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970569", "name": "[VIS16 Preview] Immersive Collaborative Analysis of Network Connectivity: CAVE-style or Head-Mounted Display?", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:54:47+00:00", "description": "InfoVis Paper \n\nAuthors: Yanhong Wu, Nan Cao, Daniel Archambault, Qiaomu Shen, Huamin Qu, Weiwei Cui\n\nAbstract: Graph sampling is frequently used to address scalability issues when analyzing large graphs. Many algorithms have been proposed to sample graphs, and the performance of these algorithms has been quantified through metrics based on graph structural properties preserved by the sampling: degree distribution, clustering coefficient, and others. However, a perspective that is missing is the impact of these sampling strategies on the resultant visualizations. In this paper, we present the results of three user studies that investigate how sampling strategies influence node-link visualizations of graphs. In particular, five sampling strategies widely used in the graph mining literature are tested to determine how well they preserve visual features in node-link diagrams. Our results show that depending on the sampling strategy used different visual features are preserved. These results provide a complimentary view to metric evaluations conducted in the graph mining literature and provide an impetus to conduct future visualization studies.\n\nSession Evaluation \u2013 Wednesday, Oct. 26, 10:30-12:10 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970543", "name": "[VIS16 Preview] Evaluation of Graph Sampling: A Visualization Perspective", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:54:35+00:00", "description": "InfoVis Paper \n\nAuthors: Yifan Zhang, Ross Maciejewski\n\nAbstract: One critical visual task when using choropleth maps is to identify spatial clusters in the data. If spatial units have the same color and are in the same neighborhood, this region can be visually identified as a spatial cluster. However, the choice of classification method used to create the choropleth map determines the visual output. The critical map elements in the classification scheme are those that lie near the classification boundary as those elements could potentially belong to different classes with a slight adjustment of the classification boundary. Thus, these elements have the most potential to impact the visual features (i.e., spatial clusters) that occur in the choropleth map. We present a methodology to enable analysts and designers to identify spatial regions where the visual appearance may be the result of spurious data artifacts. The proposed methodology identifies the critical boundary cases that can impact the overall visual presentation of the choropleth map with respect to spatial autocorrelation. A classification metric is used for identifying map elements that are near class boundaries, and a metric for quantifying the visual impact of classification edge effects in choropleth maps is developed. Our results demonstrate the impact of boundary elements on the resulting visualization and suggest that special attention should be given to these elements during map design.\n\nSession Geovisualization \u2013 Tuesday, Oct. 25, 4:15\u20135:55 \u2013 Key 3+4+6", "uri": "https://vimeo.com/182970514", "name": "[VIS16 Preview] Quantifying the Visual Impact of Classification Boundaries in Choropleth Maps", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:54:05+00:00", "description": "SciVis Paper (Best Paper Honorable Mention)\n\nOrganizers / Panelists: Soumya Dutta, Chun-Ming Chen, Gregory Heinlein, Han-Wei Shen, Jenping Chen\n\nAbstract: Study of flow instability in turbine engine compressors is crucial to understand the inception and evolution of engine stall. Aerodynamics experts have been working on detecting the early signs of stall in order to devise novel stall suppression technologies. A state-of-the-art Navier-Stokes based, time-accurate computational fluid dynamics simulator, TURBO, has been developed in NASA to enhance the understanding of flow phenomena undergoing rotating stall. Despite the proven high modeling accuracy of TURBO, the excessive simulation data prohibits post-hoc analysis in both storage and I/O time. To address these issues and allow the expert to perform scalable stall analysis, we have designed an in situ distribution guided stall analysis technique. Our method summarizes statistics of important properties of the simulation data in situ using a probabilistic data modeling scheme. This data summarization enables statistical anomaly detection for flow instability in post analysis, which reveals the spatiotemporal trends of rotating stall for the expert to conceive new hypotheses. Furthermore, the verification of the hypotheses and exploratory visualization using the summarized data are realized using probabilistic visualization techniques such as uncertain isocontouring. Positive feedback from the domain scientist has indicated the efficacy of our system in exploratory stall analysis.", "uri": "https://vimeo.com/182970479", "name": "[VIS16 Preview] In Situ Distribution Guided Analysis and Visualization of Transonic Jet Engine Simulations", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:53:48+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Kui Wu, Aaron Knoll, Ben Isaac, Hamish Carr, Valerio Pascucci\n\nAbstract: Multifield data are common in visualization. However, reducing these data to comprehensible geometry is a challenging problem. Fiber surfaces, an analogy of isosurfaces to bivariate volume data, are a promising new mechanism for understanding multifield volumes. In this work, we explore direct ray casting of fiber surfaces from volume data without any explicit geometry extraction. We sample directly along rays in domain space, and perform geometric tests in range space where fibers are defined, using a signed distance field derived from the control polygons. Our method requires little preprocess, and enables real-time exploration of data, dynamic modification and pixel-exact rendering of fiber surfaces, and support for higher-order interpolation in domain space. We demonstrate this approach on several bivariate datasets, including analysis of multi-field combustion data.", "uri": "https://vimeo.com/182970466", "name": "[VIS16 Preview] Direct Multifield Volume Ray Casting of Fiber Surfaces", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:53:35+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Andrew H. Stevens, Thomas Butkiewicz, Colin Ware\n\nAbstract: Three-dimensional vector fields are common datasets throughout the sciences. Visualizing these fields is inherently difficult due to issues such as visual clutter and self-occlusion. Cutting planes are often used to overcome these issues by presenting more manageable slices of data. The existing literature provides many techniques for visualizing the flow through these cutting planes; however, there is a lack of empirical studies focused on the underlying perceptual cues that make popular techniques successful. This paper presents a quantitative human factors study that evaluates static monoscopic depth and orientation cues in the context of cutting plane glyph designs for exploring and analyzing 3D flow fields. The goal of the study was to ascertain the relative effectiveness of various techniques for portraying the direction of flow through a cutting plane at a given point, and to identify the visual cues and combinations of cues involved, and how they contribute to accurate performance. It was found that increasing the dimensionality of line-based glyphs into tubular structures enhances their ability to convey orientation through shading, and that increasing their diameter intensifies this effect. These tube-based glyphs were also less sensitive to visual clutter issues at higher densities. Adding shadows to lines was also found to increase perception of flow direction. Implications of the experimental results are discussed and extrapolated into a number of guidelines for designing more perceptually effective glyphs for 3D vector field visualizations.", "uri": "https://vimeo.com/182970447", "name": "[VIS16 Preview] Hairy Slices: Evaluating the Perceptual Effectiveness of Cutting Plane Glyphs for 3D Vector Fields", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:53:14+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Pedro Hermosilla, Jorge Estrada, Victor Guallar, Timo Ropinski, \u00c0lvar Vinacua, Pere-Pau V\u00e1zquez\n\nAbstract: Molecular simulations are used in many areas of biotechnology, such as drug design and enzyme engineering. Despite the development of automatic computational protocols, analysis of molecular interactions is still a major aspect where human comprehension and intuition are key to accelerate, analyze, and propose modifications to the molecule of interest. Most visualization algorithms help the users by providing an accurate depiction of the spatial arrangement: the atoms involved in inter-molecular contacts. There are few tools that provide visual information on the forces governing molecular docking. However, these tools, commonly restricted to close interaction between atoms, do not consider whole simulation paths, long-range distances and, importantly, do not provide visual cues for a quick and intuitive comprehension of the energy functions (modeling intermolecular interactions) involved. In this paper, we propose visualizations designed to enable the characterization of interaction forces by taking into account several relevant variables such as molecule-ligand distance and the energy function, which is essential to understand binding affinities. We put emphasis on mapping molecular docking paths obtained from Molecular Dynamics or Monte Carlo simulations, and provide time-dependent visualizations for different energy components and particle resolutions: atoms, groups or residues. The presented visualizations have the potential to support domain experts in a more efficient drug or enzyme design process.", "uri": "https://vimeo.com/182970417", "name": "[VIS16 Preview] Physics-based Visual Characterization of Molecular Interaction Forces", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:53:03+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Kai Lawonn, Erik Trostmann, Bernhard Preim, Klaus Hildebrandt\n\nAbstract: We present novel techniques for visualizing, illustrating, analyzing, and generating carvings in surfaces. In particular, we consider the carvings in the plaster of the cloister of the Madgeburg cathedral, which dates to the 13th century. Due to aging and weathering, the carvings have flattened. Historians and restorers are highly interested in using digitalization of carvings to get impressions and illustrations of their original shape and appearance. In addition, museums and churches are interested in such illustrations for presenting them to visitors. The techniques that we propose, allow for detecting, selecting, and visualizing carving structures. In addition, we introduce an example-based method for generating carvings. The resulting tool, which integrates all techniques, was evaluated by three experienced restorers to assess the usefulness and applicability. Furthermore, we compared our approach with exaggerated shading and other state-of-the-art methods.", "uri": "https://vimeo.com/182970407", "name": "[VIS16 Preview] Visualization and Extraction of Carvings for Heritage Conservation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:52:43+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Xin Tong, Cheng Li, Han-Wei Shen\n\nAbstract: Glyph as a powerful multivariate visualization technique is used to visualize data through its visual channels. To visualize 3D volumetric dataset, glyphs are usually placed on 2D surface, such as the slicing plane or the feature surface, to avoid occluding each other. However, the 3D spatial structure of some features may be missing. On the other hand, placing large number of glyphs over the entire 3D space results in occlusion and visual clutter that make the visualization ineffective. To avoid the occlusion, we propose a view-dependent interactive 3D lens that removes the occluding glyphs by pulling the glyphs aside through the animation. We provide two space deformation models and two lens shape models to displace the glyphs based on their spatial distributions. After the displacement, the glyphs around the user-interested region are still visible as the context information, and their spatial structures are preserved. Besides, we attenuate the brightness of the glyphs inside the lens based on their depths to provide more depth cue. Furthermore, we developed an interactive glyph visualization system to explore different glyph-based visualization applications. In the system, we provide a few lens utilities that allows users to pick a glyph or a feature and look at it from different view directions. We compare different display/interaction techniques to visualize/manipulate our lens and glyphs.", "uri": "https://vimeo.com/182970372", "name": "[VIS16 Preview] GlyphLens: View-dependent Occlusion Management in the Interactive Glyph Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:52:26+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Monique Meuschke, Samuel Voss, Oliver Beuing, Bernhard Preim, Kai Lawonn\n\nAbstract: We present the first visualization tool that combines patient-specific hemodynamics with information about the vessel wall deformation and wall thickness in cerebral aneurysms. Such aneurysms bear the risk of rupture, whereas their treatment also carries considerable risks for the patient. For the patient-specific rupture risk evaluation and treatment analysis, both morphological and hemodynamic data have to be investigated. Medical researchers emphasize the importance of analyzing correlations between wall properties such as the wall deformation and thickness, and hemodynamic attributes like the Wall Shear Stress and near-wall flow. Our method uses a linked 2.5D and 3D depiction of the aneurysm together with blood flow information that enables the simultaneous exploration of wall characteristics and hemodynamic attributes during the cardiac cycle. We thus offer medical researchers an effective visual exploration tool for aneurysm treatment risk assessment. The 2.5D view serves as an overview that comprises a projection of the vessel surface to a 2D map, providing an occlusion-free surface visualization combined with a glyph-based depiction of the local wall thickness. The 3D view represents the focus upon which the data exploration takes place. To support the time-dependent parameter exploration and expert collaboration, a camera path is calculated automatically, where the user can place landmarks for further exploration of the properties. We developed a GPU-based implementation of our visualizations with a flexible interactive data exploration mechanism. We designed our techniques in collaboration with domain experts, and provide details about the evaluation.", "uri": "https://vimeo.com/182970359", "name": "[VIS16 Preview] Combined Visualization of Vessel Deformation and Hemodynamics in Cerebral Aneurysms", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:52:15+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Ivan Kolesar, Ivan Viola, Stefan Bruckner, Helwig Hauser\n\nAbstract: The study of spatial data ensembles leads to substantial visualization challenges in a variety of applications. In this paper, we present a model for comparative visualization that supports the design of according ensemble visualization solutions by partial automation. We focus on applications, where the user is interested in preserving selected spatial data characteristics of the data as much as possible\u201a\u00c4\u00eeeven when many ensemble members should be jointly studied using comparative visualization. In our model, we separate the design challenge into a minimal set of user-specified parameters and an optimization component for the automatic configuration of the remaining design variables. We provide an illustrated formal description of our model and exemplify our approach in the context of several application examples from different domains in order to demonstrate its generality within the class of comparative visualization problems for spatial data ensembles.", "uri": "https://vimeo.com/182970341", "name": "[VIS16 Preview] A Fractional Cartesian Composition Model for Semi-spatial Comparative Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:52:03+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Ayan Biswas, Guang Lin, Xiaotong Liu, Han-Wei Shen\n\nAbstract: Uncertainty quantification in climate ensembles is an important topic for the domain scientists, especially for decision making in the real-world scenarios. With powerful computers, simulations now produce time-varying and multi-resolution ensemble data sets. It is of extreme importance to understand the model sensitivity given the input parameters such that more computation power can be allocated to the parameters with higher influence on the output. Also, when ensemble data is produced at different resolutions, understanding the accuracy of different resolutions helps the total time required to produce a desired quality solution with improved storage and computation cost. In this work, we propose to tackle these non-trivial problems on the Weather Research and Forecasting (WRF) model output. We employ a moment independent sensitivity measure to quantify and analyze parameter sensitivity across spatial regions and time domain. A comparison of clustering structures across three resolutions enables the users to investigate the sensitivity variation over the spatial regions of the five input parameters. The temporal trend in the sensitivity values is explored via an MDS view linked with a line chart for interactive brushing. The spatial and temporal views are connected to provide a full exploration system for complete spatio-temporal sensitivity analysis. To analyze the accuracy across varying resolutions, we formulate a Bayesian approach to identify which regions are better predicted at which resolutions compared to the observed precipitation. This information is aggregated over the time domain and finally encoded in an output image through a custom color map that guides the domain experts towards an adaptive grid implementation given a cost model. Users can select and further analyze the spatial and temporal error patterns for multi-resolution accuracy analysis via brushing and linking on the produced image. In this work, we collaborate with a domain expert whose feedback shows the effectiveness of our proposed exploration work-flow.", "uri": "https://vimeo.com/182970324", "name": "[VIS16 Preview] Visualization of Time-Varying Weather Ensembles Across Multiple Resolutions", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:51:52+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Fabio Miranda, Harish Doraiswamy, Marcos Lage, Kai Zhao, Bruno Gon\u00e7alves, Luc Wilson, Mondrian Hsieh, Claudio Silva\n\nAbstract: Cities are inherently dynamic. Interesting patterns of behavior typically manifest at several key areas of a city over multiple temporal resolutions. Studying these patterns can greatly help a variety of experts ranging from city planners and architects to human behavioral experts. Recent technological innovations have enabled the collection of enormous amounts of data that can help in these studies. However, techniques using these data sets typically focus on understanding the data in the context of the city, thus failing to capture the dynamic aspects of the city. The goal of this work is to instead understand the city in the context of multiple urban data sets. To do so, we define the concept of an \"urban pulse\" which captures the spatio-temporal activity in a city across multiple temporal resolutions. The prominent pulses in a city are obtained using the topology of the data sets, and are characterized as a set of beats. The beats are then used to analyze and compare different pulses. We also design a visual exploration framework that allows users to explore the pulses within and across multiple cities under different conditions. Finally, we present three case studies carried out by experts from two different domains that demonstrate the utility of our framework.", "uri": "https://vimeo.com/182970301", "name": "[VIS16 Preview] Urban Pulse: Capturing the Rhythm of Cities", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:32:35+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Andreas Johnsen Lind, Stefan Bruckner\n\nAbstract: Within the visualization community there are some well-known techniques for visualizing 3D spatial data and some general assumptions about how perception affects the performance of these techniques in practice. However, there is a lack of empirical research backing up the possible performance differences among the basic techniques for general tasks. One such assumption is that 3D renderings are better for obtaining an overview, whereas cross sectional visualizations such as the commonly used Multi- Planar Reformation (MPR) are better for supporting detailed analysis tasks. In the present study we investigated this common assumption by examining the difference in performance between MPR and 3D rendering for correctly identifying a known surface. We also examined whether prior experience working with image data affects the participant\u201a\u00c4\u00f4s performance, and whether there was any difference between interactive or static versions of the visualizations. Answering this question is important because it can be used as part of a scientific and empirical basis for determining when to use which of the two techniques. An advantage of the present study compared to other studies is that several factors were taken into account to compare the two techniques. The problem was examined through an experiment with 45 participants, where physical objects were used as the known surface (ground truth). Our findings showed that: 1. The 3D renderings largely outperformed the cross sections; 2. Interactive visualizations were partially more effective than static visualizations; and 3. The high experience group did not generally outperform the low experience group.", "uri": "https://vimeo.com/182968780", "name": "[VIS16 Preview] Comparing Cross-Sections and 3D Renderings for Surface Matching Tasks using Physical Ground Truths", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:32:19+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Ingo Wald, Gregory Johnson, Jefferson Amstutz, Carson Brownlee, Aaron Knoll, Jim Jeffers, Johannes Guenther, Paul Navratil\n\nAbstract: Scientific data is continually increasing in complexity, variety and size, making efficient visualization and specifically rendering an ongoing challenge. Traditional rasterization-based visualization approaches encounter performance and quality limitations, particularly in HPC environments without dedicated rendering hardware. In this paper, we present OSPRay, a turn-key CPU ray tracing framework oriented towards production-use scientific visualization which can utilize varying SIMD widths and multiple device backends found across diverse HPC resources. This framework provides a high-quality, efficient CPU-based solution for typical visualization workloads, which has already been integrated into several prevalent visualization packages. We show that this system delivers the performance, high-level API simplicity, and modular device support needed to provide a compelling new rendering framework for implementing efficient scientific visualization workflows.", "uri": "https://vimeo.com/182968762", "name": "[VIS16 Preview] OSPRay - A CPU Ray Tracing Framework for Scientific Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:32:04+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Jiaxi Hu, Hajar Hamidian, Zichun Zhong, Jing Hua\n\nAbstract: This paper presents a novel approach based on spectral geometry to quantify and visualize non-isometric deformations of 3D surfaces by mapping two manifolds. The proposed method can determine multi-scale, non-isometric deformations through the variation of Laplace-Beltrami spectrum of two shapes. Given two triangle meshes, the spectra can be varied from one to another with a scale function defined on each vertex. The variation is expressed as a linear interpolation of eigenvalues of the two shapes. In each iteration step, a quadratic programming problem is constructed, based on our derived spectrum variation theorem and smoothness energy constraint, to compute the spectrum variation. The derivation of the scale function is the solution of such a problem. Therefore, the final scale function can be solved by integral of the derivation from each step, which, in turn, quantitatively describes non-isometric deformations between the two shapes. To evaluate the method, we conduct extensive experiments on synthetic and real data. We employ real epilepsy patient imaging data to quantify the shape variation between the left and right hippocampus in epileptic brains. In addition, we use longitudinal Alzheimer data to compare the shape deformation of diseased and healthy hippocampus. In order to show the accuracy and effectiveness of the proposed method, we also compare it with a spatial registration-based method, non-rigid Iterative Closest Point (ICP). These experiments demonstrate the advantages of our method.", "uri": "https://vimeo.com/182968746", "name": "[VIS16 Preview] Visualizing Shape Deformations with Variation of Geometric Spectrum", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:31:51+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Tobias G\u00fcnther, Holger Theisel\n\nAbstract: Inertial particles are finite-sized objects that are carried by fluid flows and in contrast to massless tracer particles they are subject to inertia effects. In unsteady flows, the dynamics of tracer particles have been extensively studied by the extraction of Lagrangian coherent structures (LCS), such as hyperbolic LCS as ridges of the Finite-Time Lyapunov Exponent (FTLE). The extension of the rich LCS framework to inertial particles is currently a hot topic in the CFD literature and is actively under research. Recently, backward FTLE on tracer particles has been shown to correlate with the preferential particle settling of small inertial particles. For larger particles, inertial trajectories may deviate strongly from (massless) tracer trajectories, and thus for a better agreement, backward FTLE should be computed on inertial trajectories directly. Inertial backward integration, however, has not been possible until the recent introduction of the influence curve concept, which \u201a\u00c4\u00ec given an observation and an initial velocity \u201a\u00c4\u00ec allows to recover all sources of inertial particles as tangent curves of a derived vector field. In this paper, we show that FTLE on the influence curve vector field is in agreement with preferential particle settling and more importantly it is not only valid for small (near-tracer) particles. We further generalize the influence curve concept to general equations of motion in unsteady spatio-velocity phase spaces, which enables backward integration with more general equations of motion. Applying the influence curve concept to tracer particles in the spatio-velocity domain emits streaklines in massless flows as tangent curves of the influence curve vector field. We demonstrate the correlation between inertial backward FTLE and the preferential particle settling in a number of unsteady vector fields.", "uri": "https://vimeo.com/182968730", "name": "[VIS16 Preview] Backward Finite-Time Lyapunov Exponents in Inertial Flows", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:31:39+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Tim Gerrits, Christian R\u00f6ssl, Holger Theisel\n\nAbstract: Glyphs are a powerful tool for visualizing second-order tensors in a variety of scientic data as they allow to encode physical behavior in geometric properties. Most existing techniques focus on symmetric tensors and exclude non-symmetric tensors where the eigenvectors can be non-orthogonal or complex. We present a new construction of 2d and 3d tensor glyphs based on piecewise rational curves and surfaces with the following properties: invariance to (a) isometries and (b) scaling, (c) direct encoding of all real eigenvalues and eigenvectors, (d) one-to-one relation between the tensors and glyphs, (e) glyph continuity under changing the tensor. We apply the glyphs to visualize the Jacobian matrix fields of a number of 2d and 3d vector fields.", "uri": "https://vimeo.com/182968711", "name": "[VIS16 Preview] Glyphs for General Second-Order 2D and 3D Tensors", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:31:27+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Florian Ferstl, Mathias Kanzler, Marc Rautenhaus, R\u00fcdiger Westermann\n\nAbstract: We propose a new approach for analyzing the temporal growth of the uncertainty in ensembles of weather forecasts which are started from perturbed but similar initial conditions. As an alternative to traditional approaches in meteorology, which use juxtaposition and animation of spaghetti plots of iso-contours, we make use of contour clustering and provide means to encode forecast dynamics and spread in one single visualization. Based on a given ensemble clustering in a specified time window, we merge clusters in time-reversed order to indicate when and where forecast trajectories start to diverge. We present and compare different visualizations of the resulting time-hierarchical grouping, including space-time surfaces built by connecting cluster representatives over time, and stacked contour variability plots. We demonstrate the effectiveness of our visual encodings with forecast examples of the European Centre for Medium-Range Weather Forecasts, which convey the evolution of specific features in the data as well as the temporally increasing spatial variability.", "uri": "https://vimeo.com/182968690", "name": "[VIS16 Preview] Time-hierarchical Clustering and Visualization of Weather Forecast Ensembles", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:30:44+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Lonni Besan\u00e7on, Paul Issartel, Mehdi Ammi, Tobias Isenberg\n\nAbstract: We present the design and evaluation of an interface that combines tactile and tangible paradigms for 3D visualization. While studies have demonstrated that both tactile and tangible input can be efficient for a subset of 3D manipulation tasks, we reflect here on the possibility to combine the two complementary input types. Based on a field study and follow-up interviews, we present a conceptual framework of the use of these different interaction modalities for visualization both separately and combined---focusing on free exploration as well as precise control. We present a prototypical application of a subset of these combined mappings for fluid dynamics data visualization using a portable, position-aware device which offers both tactile input and tangible sensing. We evaluate our approach with domain experts and report on their qualitative feedback.", "uri": "https://vimeo.com/182968639", "name": "[VIS16 Preview] Hybrid Tactile/Tangible Interaction for 3D Data Exploration", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:30:33+00:00", "description": "SciVis Paper (Best Paper Award)\n\nOrganizers / Panelists: Julien Tierny, Hamish Carr\n\nAbstract: This paper presents an efficient algorithm for the computation of the Reeb space of an input bivariate piecewise linear scalar function f defined on a tetrahedral mesh. By extending and generalizing algorithmic concepts from the univariate case to the bivariate one, we report the first practical, output-sensitive algorithm for the exact computation of such a Reeb space. The algorithm starts by identifying the Jacobi set of f, the bivariate analogs of critical points in the univariate case. Next, the Reeb space is computed by segmenting the input mesh along the new notion of Jacobi Fiber Surfaces, the bivariate analog of critical contours in the univariate case. We additionally present a simplification heuristic that enables the progressive coarsening of the Reeb space. Our algorithm is simple to implement and most of its computations can be trivially parallelized. We report performance numbers demonstrating orders of magnitude speedups over previous approaches, enabling for the first time the tractable computation of bivariate Reeb spaces in practice. Moreover, unlike range-based quantization approaches (such as the Joint Contour Net), our algorithm is parameter-free. We demonstrate the utility of our approach by using the Reeb space as a semi-automatic segmentation tool for bivariate data. In particular, we introduce continuous scatterplot peeling, a technique which enables the reduction of the cluttering in the continuous scatterplot, by interactively selecting the features of the Reeb space to project. We provide a VTK-based C++ implementation of our algorithm that can be used for reproduction purposes or for the development of new Reeb space based visualization techniques. \\", "uri": "https://vimeo.com/182968630", "name": "[VIS16 Preview] Jacobi Fiber Surfaces for Bivariate Reeb Space Computation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:30:20+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Steffen Frey, Thomas Ertl\n\nAbstract: We present a novel technique to generate transformations between arbitrary volumes, providing both expressive distances and smooth interpolates. In contrast to conventional morphing or warping approaches, our technique requires no user guidance, intermediate representations (like extracted features), or blending, and imposes no restrictions regarding shape or structure. Our technique operates directly on the volumetric data representation, and while linear programming approaches could solve the underlying problem optimally, their polynomial complexity makes them infeasible for high-resolution volumes. We therefore propose a progressive refinement approach designed for parallel execution that is able to quickly deliver approximate results that are iteratively improved toward the optimum. On this basis, we further present a new approach for the streaming selection of time steps in temporal data that allows for the reconstruction of the full sequence with a user-specified error bound. We finally demonstrate the utility of our technique for different applications, compare our approach against alternatives, and evaluate its characteristics with a variety of different data sets.", "uri": "https://vimeo.com/182968610", "name": "[VIS16 Preview] Progressive Direct Volume-to-Volume Transformation", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:29:54+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Jens Schneider, Peter Rautek\n\nAbstract: In this paper we present a novel GPU-based data structure for spatial indexing. Based on Fenwick trees---a special type of binary indexed trees---our data structure allows construction in linear time. Updates and prefixes can be computed in logarithmic time, whereas point queries require only constant time on average. Unlike competing data structures such as summed-area tables and spatial hashing, our data structure requires a constant amount of bits for each data element, and it offers unconstrained point queries. This property makes our data structure ideally suited for applications requiring unconstrained indexing of large data, such as block-storage of large and block-sparse volumes. Finally, we provide asymptotic bounds on both run-time and memory requirements, and we show applications for which our new data structure is useful.", "uri": "https://vimeo.com/182968583", "name": "[VIS16 Preview] A Versatile and Efficient GPU Data Structure for Spatial Indexing", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:29:37+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Saad Nadeem, Joseph Marino, Xianfeng Gu, Arie Kaufman\n\nAbstract: We present a method for registration and visualization of corresponding supine and prone virtual colonoscopy scans based on eigenfunction analysis and fold modeling. In virtual colonoscopy, CT scans are acquired with the patient in two positions, and their registration is desirable so that physicians can corroborate findings between scans. Our algorithm performs this registration efficiently through the use of Fiedler vector representation (the second eigenfunction of the Laplace-Beltrami operator). This representation is employed to first perform global registration of the two colon positions. The registration is then locally refined using the haustral folds, which are automatically segmented using the 3D level sets of the Fielder vector. The use of Fiedler vectors and the segmented folds presents a precise way of visualizing corresponding regions across datasets and visual modalities. We present multiple methods of visualizing the results, including 2D flattened rendering and the corresponding 3D endoluminal views. The precise fold modeling is used to automatically find a suitable cut for the 2D flattening, which provides a less distorted visualization. Our approach is robust, and we demonstrate its efficiency and efficacy by showing matched views on both the 2D flattened colons and in the 3D endoluminal view. We analytically evaluate the results by measuring the distance between features on the registered colons, and we also assess our fold segmentation against 20 manually labeled datasets. We have compared our results analytically to previous methods, and have found our method to achieve superior results. We also prove the hot spots conjecture for modeling cylindrical topology using Fiedler vector representation, which allows our approach to be used for general cylindrical geometry modeling and feature extraction.", "uri": "https://vimeo.com/182968563", "name": "[VIS16 Preview] Corresponding Supine and Prone Colon Visualization Using Eigenfunction Analysis and Fold Modeling", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:29:23+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Jian Chen, Torsten Moeller\n\nAbstract: We present the results of a comprehensive multi-pass analysis of visualization paper keywords supplied by authors for their papers published in the IEEE Visualization conference series (now called IEEE VIS) between 1990--2015. From this analysis we derived a set of visualization topics that we discuss in the context of the current taxonomy that is used to categorize papers and assign reviewers in the IEEE VIS reviewing process. We point out missing and overemphasized topics in the current taxonomy and start a discussion on the importance of establishing common visualization terminology. Our analysis of research topics in visualization can, thus, serve as a starting point to (a) help create a common vocabulary to improve communication among different visualization sub-groups, (b) facilitate the process of understanding differences and commonalities of the various research sub-fields in visualization, (c) provide an understanding of emerging new research trends, (d) facilitate the crucial step of finding the right reviewers for research submissions, and (e) it can eventually lead to a comprehensive taxonomy of visualization research. One additional tangible outcome of our work is an online query tool (http://keyvis.org) that allows visualization researchers to easily browse the 3952 keywords used for IEEE VIS papers since 1990 to find related work or make informed keyword choices.", "uri": "https://vimeo.com/182968553", "name": "[VIS16 Preview] Visualization as Seen Through its Research Paper Keywords", "year": "2016", "event": "PREVIEW"}, {"created_time": "2016-09-16T08:29:12+00:00", "description": "SciVis Paper \n\nOrganizers / Panelists: Allan Rocha, Usman Alim, Julio Daniel Silva, Mario Costa Sousa\n\nAbstract: We introduce the use of decals for multivariate visualization design. Decals are visual representations that are used for communication; for example, a pattern, a text, a glyph, or a symbol, transferred from a 2D-image to a surface upon contact. By creating what we define as decal-maps, we can design a set of images or patterns that represent one or more data attributes. We place decals on the surface considering the data pertaining to the locations we choose. We propose a (texture mapping) local parametrization that allows placing decals on arbitrary surfaces interactively, even when dealing with a high number of decals. Moreover, we extend the concept of layering to allow the co-visualization of an increased number of attributes on arbitrary surfaces. By combining decal-maps, color-maps and a layered visualization, we aim to facilitate and encourage the creative process of designing multivariate visualizations. Finally, we demonstrate the general applicability of our technique by providing examples of its use in a variety of contexts.", "uri": "https://vimeo.com/182968537", "name": "[VIS16 Preview] Decal-maps: Real-Time Layering of Decals on Surfaces for Multivariate Visualization", "year": "2016", "event": "PREVIEW"}, {"created_time": "2015-09-18T06:14:52+00:00", "description": "[SciVis paper] \n\nAuthors: Sanghun Park, Hyunggoog Seo, Seunghoon Cha, Junyong Noh\n\nAbstract: We present a novel approach that utilizes a simple handheld camera to automatically calibrate multi-projector displays. Most existing studies adopt active structured light patterns to verify the relationship between the camera and the projectors. The utilized camera is typically expensive and requires an elaborate installation process depending on the scalability of its applications. Moreover, the observation of the entire area by the camera is almost impossible for a small space surrounded by walls as there is not enough distance for the camera to capture the entire scene. We tackle these issues by requiring only a portion of the walls to be visible to a handheld camera that is widely used these days. This becomes possible by the introduction of our new structured light pattern scheme based on a perfect submap and a geometric calibration that successfully utilizes the geometric information of multi-planar environments. We demonstrate that immersive display in a small space such as an ordinary room can be effectively created using images captured by a handheld camera.", "uri": "https://vimeo.com/139672340", "name": "VIS15 preview: Auto-Calibration of Multi-Projector Displays with a Single Handheld Camera", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-09-08T19:45:15+00:00", "description": "[Visualization in Practice]\n\nAuthors: Margaret Varga, Caroline Varga, and Ben Huston\n\nAbstract: Clostridium difficile (C. difficile) are bacteria that are present naturally in the gut of some adults and children. They do not cause any problems in healthy people; however, when gut bacteria are disrupted, for example by antibiotics, C. difficile can multiply and produce toxins. This results in illnesses such as diarrhoea and fever. The current lack of robust information on infectiousness; e.g. incubation and infectious periods, and infection impact on person-to-person spread; means that many transmissions not resulting in immediate disease are undetected or unconsidered. This paper reports a case study of the application of visual analytics and visualization techniques to explore and analyse data describing the transmission of C. difficile between patients within a hospital setting: most C. difficile infections occur in hospitals or care homes. The first key objective of the work was to transform and geo-temporally visualise non-visual spreadsheet patient record data covering a four year period across ten hospitals, so as to support the infection control healthcare specialist in the analysis of patients\u2019 movements, status (infection, incubation, infectousness, recovery etc.), and interactions in relation to the transmission and spread of C. difficile infections. The second key objective was the development of an empirical model of C. difficile infectiousness which did not previously exist.", "uri": "https://vimeo.com/138666743", "name": "VIS15 preview: Analysis and Visualization of Clostridium Difficile Hospital In-Ward Transmissions", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-09-08T19:44:40+00:00", "description": "[Visualization in Practice]\n\nAuthors: Carolina Nobre\n\nAbstract: The ocean is in constant motion. In order to collect long-term views of current movement, biological processes, as well as the changes in the ocean seafloor, scientists and engineers have devised ways to leave instruments out in the environment for extended periods of time. Moored observatories are platforms which allow us to observe how the ocean and seafloor change over time. This pa- per reports on the development of a pair of interactive visualization tools that facilitate both planning research cruises that deploy these instruments, as well as extracting ocean depth measurements in or- der to ensure optimal placement of the moorings. Existing soft- ware such as Ocean Data View (ODV) and SeaBird Data Process- ing (SBE) does provide the ability to visualize oceanographic data, such as ocean depth, in almost real-time. Howevever, these pro- grams are restricted to static plots and maps, and lack interactive features such as data selection, correction, or smoothing. The tools presented here address these shortcomings and provide interactive displays for both planning station locations and for correcting, sam- pling, and smoothing bathymetric data.", "uri": "https://vimeo.com/138666685", "name": "VIS15 preview: Deploying Moored Profilers on the Ocean Floor", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-09-08T19:43:52+00:00", "description": "[Visualization in Practice]\n\nAuthors: Christina Gillmann, Thomas Wischgoll, Hans Hagen\n\nAbstract: Cardiovascular diseases, mainly caused by lipids accumulating within the vessel wall creating plaque, are one of the leading causes of death in modern society. In advanced cases, surgery is required to avoid strokes or heart attacks. Therefore the aim of the presented approach is to assist in the en- tire surgery process, identify risks, discuss options, and examine the success. Unfortunately, the state of the art slice-by-slice reviewing method lacks in the ability to investigate CT (Computer Tomogra- phy) scans in their entirety, as it only shows a single slice of the raw CT scan at a time. This paper presents a tool to insert visual exploration in all stages of the surgery process. An interactive linked view system process is introduced that contains a standardized tree view of the coronary arteries and additional supporting view to orient oneself in space and in the recorded CT scan. This technique provides the ability to visually analyze a recorded CT scan to monitor the process of a surgery. As the visualization is derived as part of the medical workflow it addresses the user\u00d5s needs and opens the black box of vessel visualization to obtain user approval.", "uri": "https://vimeo.com/138666589", "name": "VIS15 preview: Visual Exploration in Surgery Monitoring for Coronary Vessels", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-09-08T19:42:50+00:00", "description": "[Poster] \n\nAuthors: Sergi Vives, Jason Dykes, Andrew Merryweather\n\nAbstract: We use the Design Study Methodology in a short term project to apply visualization to equity analysis with a major supplier of information to the finance industry. Our interactive prototype is positively received and our work suggests that this kind of design and the application of the DSM to short term visualization projects have potential in stock picking and more broadly in financial analysis.", "uri": "https://vimeo.com/138666472", "name": "VIS15 preview: Visualization for Equity Analysts: Using the DSM in Stock Picking", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-09-08T19:41:42+00:00", "description": "[Poster] \n\nAuthors: Younghun Jung, Geongi Gim, Myeongjae Kim, Yejin Kim, Kwangyun Wohn\n\nAbstract: In this work, we present a library, called Easy Screens and Play (ESP), for information visualization in tiled display systems. We describe the design of the ESP library in the tiled display environment for novice users and its unique features as compared to other distributed display libraries. We also demonstrate the efficacy and effectiveness of the ESP library by using several examples of information visualization. Discussions on the extension of the library and future improvements are presented as well.", "uri": "https://vimeo.com/138666116", "name": "VIS15 preview: Easy Screens and Play: A Library for Information Visualization on Tiled display Environments", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-09-02T10:31:31+00:00", "description": "Organizers: Theresia Gschwandtner, Adam Perer, J\u00fcrgen Bernard\n\nAbstract: As medical organizations move to electronic medical records and increasingly embrace health information technology, the amount of data available is growing at an unprecedented rate. This vast amount of healthcare data poses a challenging task (1) for medical experts trying to make sense of patients\u2019 conditions and understanding their medical history, (2) for patients trying to make sense of their health data, and (3) for analysts to conduct outcome research, such as exploring the effectiveness of different approaches. Visual Analytics and Information Visualization have the potential to provide great benefits to healthcare providers, patients, and data analysts. Given the strong turnout of this workshop in previous years, we propose to host a follow-up workshop at IEEE VIS 2015. In this workshop participants will have the opportunity to present ongoing work with short papers and demonstrations, and discuss user needs and challenges.", "uri": "https://vimeo.com/138066224", "name": "VIS 15 Workshop: Workshop on Visual Analytics in Healthcare", "year": "2015", "event": "WORKSHOP"}, {"created_time": "2015-09-02T10:26:12+00:00", "description": "[Poster] [HONORABLE MENTION]\n\nAuthors: Shenghui Cheng, Yue Wang, Dan Zhang, Zhifang Jiang, Klaus Mueller\n\nAbstract: In streaming acquisitions the data changes over time. Themeriver and line charts are common methods to display data over time. However, these methods can only show the values of the variables (or attributes) but not relationships among them over time. We propose a framework we call StreamVisND that can display these types streaming data relations. It first slices the data stream into different time slices, then it visualizes each slice with a sequence of multivariate 2D data layouts, and finally it flattens this series of displays into a parallel coordinate type display. Our framework is fully interactive and lends itself well to real-time displays.", "uri": "https://vimeo.com/138065885", "name": "VIS15 preview: StreamVisND: Visualizing Relationships in Streaming Multivariate Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-09-02T10:26:06+00:00", "description": "[Poster] \n\nAuthors: Stefan Gladisch, Ulrike Kister, Christian Tominski, Raimund Dachselt, Heidrun Schumann\n\nAbstract: Graph exploration and graph editing are still mostly considered independently and the available software solutions are typically not designed for today's interactive surfaces such as tablets or tabletops. This work is a step toward interactive systems for both graph exploration and graph editing. We deal with three questions. What tasks need to be supported? What interactions can be used? How to can tasks be mapped to interactions? We contribute a systematic review for the first two questions and present a formal mapping approach to address the third.", "uri": "https://vimeo.com/138065874", "name": "VIS15 preview: Mapping Tasks to Interactions for Graph Exploration and Editing", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-09-02T10:25:58+00:00", "description": "[Poster] \n\nAuthors: Stefan Gladisch, Heidrun Schumann, Christian Tominski\n\nAbstract: Node-link and matrix representations are widely applied for graph exploration. However, when it comes to editing graphs, matrix representations are mostly neglected. In this work, we investigate the suitability of matrix representations especially for graph editing. Based on a review of the characteristics of matrices in terms of representation and interaction, we propose first techniques for edge-based editing tasks. We combine our matrix-based techniques with classic node-link solutions in an interactive prototype for touch-enabled graph editing.", "uri": "https://vimeo.com/138065862", "name": "VIS15 preview: Toward Using Matrix Visualizations for Graph Editing", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-31T20:14:07+00:00", "description": "[Poster] \n\nAuthors: William Longabaugh\n\nAbstract: Extremely large networks are common both in biology as well as in other fields. Yet the typical approach for visualizing these networks, i.e. a node-link diagram laid out using a force-directed algorithm, often produces 'hairballs'\u009d that rarely provide useful insights. But two software tools developed at the Institute for Systems Biology, called BioTapestry and BioFabric, provide valuable alternatives to this standard technique. BioTapestry, which uses specific layout strategies designed to leverage the advantages of colored orthogonal directed hyperedges, can provide useful visualizations of large directed networks. BioFabric renders a node-link diagram using a novel approach that depicts nodes not as points, but as horizontal lines; edges are then represented as vertical lines. The additional degree of freedom provided by \"nodes as lines\" allows even very large networks to be visualized in a rational and organized way.", "uri": "https://vimeo.com/137880201", "name": "VIS15 preview: Visualizing Large Networks Using BioTapestry and BioFabric", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-31T19:39:37+00:00", "description": "[Poster] [HONORABLE MENTION]\n\nAuthors: Shenghui Cheng, Yue Wang, Dan Zhang, Zhifang Jiang, Klaus Mueller\n\nAbstract: In streaming acquisitions the data changes over time. Themeriver and line charts are common methods to display data over time. However, these methods can only show the values of the variables (or attributes) but not relationships among them over time. We propose a framework we call StreamVisND that can display these types streaming data relations. It first slices the data stream into different time slices, then it visualizes each slice with a sequence of multivariate 2D data layouts, and finally it flattens this series of displays into a parallel coordinate type display. Our framework is fully interactive and lends itself well to real-time displays.", "uri": "https://vimeo.com/137876345", "name": "VIS15 preview: StreamVisND: Visualizing Relationships in Streaming Multivariate Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-31T19:37:15+00:00", "description": "Organizers: Pak Chung Wong, David Haglin, David Bader, David Trimm\n\nAbstract: The workshop will explore the technical challenges and technology development opportunities of graph visual analytics found in the big data era with the goal of establishing a community of interest. Today\u2019s graph problems are increasingly multi-faceted and multi-disciplinary in nature. Many cutting-edge R&amp;D efforts are conducted independently in disparate domains such as bioinformatics, cybersecurity, and predictive machine learning. Although technology transfers in big graph visualization are recognized and growing, there has been little progress in establishing a community strategy for sharing and building knowledge. We invite researchers and practitioners with different interests to participate at the workshop by submitting position papers and, if accepted, presenting their ideas at the workshop co-located at IEEE VIS 2015. We agree that the data size that seems big today is different from what seemed big only a few years ago. While the workshop doesn\u2019t specify upper or lower bounds on the graph\u2019s size, we are particularly interested in emerging problems that challenge conventional wisdom in computation and interaction brought by the latest social-scale or web-scale graphs. This workshop is organized by a group of big graph analytics researchers and practitioners who share a common goal of establishing a substantial community to solve big problems with big graph data.", "uri": "https://vimeo.com/137876096", "name": "VIS 15 Workshop: Exploring Graphs At Scale (EGAS)", "year": "2015", "event": "WORKSHOP"}, {"created_time": "2015-08-31T19:36:02+00:00", "description": "Organizers: Charles Perin, Alice Thudt, Melanie Tory, Wesley Willett, Sheelagh Carpendale\n\nAbstract: Individuals recently began to seek how they can explore and understand the data that affect their personal lives. This includes biometric personal data such as health-related data, self-monitoring, sports performance, and data from online social networks, energy consumption, and photo collections. The main purpose of such data understanding is generating insights, and eventually making decisions to improve one's life, the ultimate purpose of visualization. Assessing individual's needs in the context of their personal data and designing appropriate tools to support visualization and analysis of this data is a crucial and emergent challenge. This workshop is intended to gather academics and industries concerned by the emergent topic of personal visualization and personal visual analytics. The intended outcomes of the workshop are 1) to gather the community working on the topic of personal visualization, and 2) to converge on a research agenda for the community.", "uri": "https://vimeo.com/137875964", "name": "VIS 15 Workshop: Personal Visualization: Exploring Data in Everyday Life", "year": "2015", "event": "WORKSHOP"}, {"created_time": "2015-08-25T06:39:31+00:00", "description": "[InfoVis paper] \n\nAuthors: Michelle Borkin, Zoya Bylinskii, Nam Wook Kim, Constance Bainbridge, Chelsea Yeh, Daniel Borkin, Hanspeter Pfister, Aude Oliva\n\nAbstract: In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant-generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable \"at-a-glance\" are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one.", "uri": "https://vimeo.com/137219625", "name": "VIS15 preview: Beyond Memorability: Visualization Recognition and Recall", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-25T06:34:35+00:00", "description": "[Poster] \n\nAuthors: Benjamin Bach, Nathalie Henry Riche, Roland Fernandez, Emmanoulis Giannisakis, Bongshin Lee, Jean-Daniel Fekete\n\nAbstract: Network visualizations support research in a range of scientific domains from biology to humanities. This paper presents our effort to create a platform to bridge the gap between domain scientists and visualisation researchers; NetworkCube aims in being a fast way to deploy experimental visualizations from research to domain experts analyzing dynamic networks. In turn, InfoVis researchers benefit from studying how their visualizations are used in the wild.", "uri": "https://vimeo.com/137219388", "name": "VIS15 preview: Networkcube: Bringing Dynamic Network Visualizations to Domain Scientists", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-25T06:33:44+00:00", "description": "[Poster] \n\nAuthors: Fateme Rajabiyazdi, Charles Perin, Sheelagh Carpendale\n\nAbstract: We present WESt, a visualization for surgery waiting times applied to the province of British Columbia in Canada. This project is motivated by the long waiting time for surgeries in BC, which has the potential to put patients in danger and put extra pressure on clinicians. WESt helps patients select a hospital and a physician, and to explore their options in order to get their surgery as soon as possible.", "uri": "https://vimeo.com/137219337", "name": "VIS15 preview: WESt: Visualizing non-Emergency Surgery Wait Times", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-24T09:54:03+00:00", "description": "[Poster] [BEST POSTER]\n\nAuthors: Stefan Luger, Holger Stitz, Samuel Gratzl, Nils Gehlenborg, Marc Streit\n\nAbstract: A major challenge of data-driven biomedical research lies in the collection and representation of provenance information to ensure reproducibility of the gained results. The Refinery Platform is an integrated data management, analysis, and visualization system designed to support reproducible biomedical research. In order to communicate and reproduce multi-step analyses on datasets that contain data of hundreds of samples, it is crucial to be able to visualize the provenance graph at different levels of detail. Most existing approaches are based on node-link diagrams, however, they usually do not scale well to large graphs. Our proposed visualization technique dynamically reduces the complexity of subgraphs through hierarchical aggregation and application of a degree-of-interest (DOI) function to each node. Triggered by user interactions, such as selecting a subset of analyses or a path in the graph, unaffected parts of the graph are dynamically aggregated into a glyph representation. We further reduce the complexity of the provenance graph visualization by layering identical or similar sequences of parallel analysis steps into an aggregated sequence.", "uri": "https://vimeo.com/137118409", "name": "VIS15 preview: Interactive Visualization of Provenance Graphs for Reproducible Biomedical Research", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-24T09:53:05+00:00", "description": "[Poster] \n\nAuthors: Erik Sund\u00e9n, Peter Steneteg, Sathish Kottravel, Daniel J\u00f6nsson, Rickard Englund, Martin Falk, Timo Ropinski\n\nAbstract: To enable visualization research impacting other scientific domains, the availability of easy-to-use visualization frameworks is essential. Nevertheless, an easy-to-use system also has to be adapted to the capabilities of modern hardware architectures, as only this allows for realizing interactive visualizations. With this trade-off in mind, we have designed and realized the cross-platform Inviwo (Interactive Visualization Workshop) visualization framework, that supports both interactive visualization research as well as efficient visualization application development and deployment. In this poster we give an overview of the architecture behind Inviwo, and show how its design enables us and other researchers to realize their visualization ideas efficiently. Inviwo consists of a modern and light-weight, graphics independent core, which is extended by optional modules that encapsulate visualization algorithms, well- known utility libraries and commonly used parallel-processing APIs (such as OpenGL and OpenCL). The core enables a simplistic structure for creating bridges between the different modules regarding data transfer across architecture and devices with an easy-to- use screen graph and minimalistic programming. Making the base structures in a modern way while providing intuitive methods of extending the functionality and creating modules based on other modules, we hope that Inviwo can help the visualization community to perform research through a rapid-prototyping design and GUI, while at the same time allowing users to take advantage of the results implemented in the system in any way they desire later on. Inviwo is publicly available at www.inviwo.org, and can be used freely by anyone under a permissive free software license (Simplified BSD).", "uri": "https://vimeo.com/137118352", "name": "VIS15 preview: Inviwo - An Extensible, Multi-Purpose Visualization Framework", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-24T09:52:23+00:00", "description": "[Poster] \n\nAuthors: Adrien Baland, Raghvendra Mall, Rocco Langone, Johan Suykens\n\nAbstract: In this paper we design a tool that allows visualization of hierarchical communities extracted from complex networks similar to Google maps i.e. the tool allows to zoom in and zoom out of communities at different levels of hierarchy. This tool uses the hierarchical structure of the communities to attack this problem in a recursive manner in order to avoid an ever-increasing complexity. We incorporated several additional functionalities that allows the user to modify the communities and/or their layout in the tool. The tool is implemented with the R library Shiny.", "uri": "https://vimeo.com/137118308", "name": "VIS15 preview: Visualization of Hierarchical Communities in Large Scale Networks", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-24T09:51:56+00:00", "description": "[Poster] \n\nAuthors: Radityo Eko Prasojo, Fariz Darari, Mouna Kacimi\n\nAbstract: We present ORCAESTRA, a tool for organizing and visualizing user comments about online news articles. ORCAESTRA enables the exploration of user comments based on (1) the entities they focus on and (2) the aspects, of those entities, they argue about. Moreover, it analyzes the sentiments expressed about each aspect to classify related user comments into positive, negative, or neutral. As a result, our tool enhances user experience by providing a better understanding of the different view points discussed in user comments.", "uri": "https://vimeo.com/137118280", "name": "VIS15 preview: ORCAESTRA: Organizing News Comments using Aspect, Entity and Sentiment Extraction", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-24T09:51:42+00:00", "description": "[Poster] \n\nAuthors: Robert Pienta, Zhiyuan Lin, Minsuk Kahng, Jilles Vreeken, Partha P. Talukdar, James Abello, Ganesh Parameswaran, Duen Horng (Polo) Chau\n\nAbstract: Visualization is a powerful paradigm for exploratory data analysis. Visualizing large graphs, however, often results in a meaningless hairball. In this paper, we propose a different approach that helps the user adaptively explore large million-node graphs from a local perspective. For nodes that the user investigates, we propose to only show the neighbors with the most subjectively interesting neighborhoods. We contribute novel ideas to measure this interestingness in terms of how surprising a neighborhood is given the background distribution, as well as how well it fits the nodes the user chose to explore. We are currently designing and developing AdaptiveNav, a fast and scalable method for visually exploring large graphs. By implementing our above ideas, it allows users to look into the forest through its trees.", "uri": "https://vimeo.com/137118260", "name": "VIS15 preview: AdaptiveNav: Adaptive Discovery of Interesting and Surprising Nodes in Large Graphs", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-19T20:42:46+00:00", "description": "[VAST paper] \n\nAuthors: Haeyong Chung, Chris North, Sarang Joshi, Jian Chen\n\nAbstract: The current proliferation of large displays and mobile devices presents a number of exciting opportunities for visual analytics and information visualization. The display ecology enables multiple displays to function in concert within a broader technological environment to accomplish visual analysis tasks. Based on a comprehensive survey of multi-display systems from a variety of fields, we propose four key considerations for visual analysis in display ecologies: 1) Display Composition, 2) Information Coodination/Transfer, 3) Information Connection, and 4) Display Membership. Different aspects of display ecologies stemming from these design considerations will enable users to transform and empower multiple displays as a single display ecology for a cohesive visual analysis space. We believe that our design considerations and related discussion can help visualization researchers in understanding, designing, and evaluating new visual analysis tools that maximize the potential of the numerous displays and devices.", "uri": "https://vimeo.com/136764192", "name": "VIS15 preview: Four Considerations for Supporting Visual Analysis in Display Ecologies", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-19T20:37:06+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Rahul C. Basole\n\nAbstract: Macroscopic insight into business ecosystems is becoming increasingly important. With the emergence of new digital business data, opportunities exist to develop rich, interactive visual\u00ad analytics tools. Georgia Institute of Technology researchers have been developing and implementing visual business ecosystem intelligence tools in corporate settings. This article discusses the challenges they faced, the lessons learned, and opportunities for future research.", "uri": "https://vimeo.com/136763673", "name": "VIS15 preview: Visual Business Ecosystem Intelligence: Lessons from the Field", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-17T19:57:45+00:00", "description": "[SciVis paper] \n\nAuthors: Chun-Ming Chen, Soumya Dutta, Xiaotong Liu, Gregory Heinlein, Han-Wei Shen, Jenping Chen\n\nAbstract: Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing, high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data in more detail. To achieve this, we propose efficient stall analysis algorithms derived from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.", "uri": "https://vimeo.com/136544942", "name": "VIS15 preview: Visualization and Analysis of Rotating Stall for Transonic Jet Engine Simulation", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-17T19:31:20+00:00", "description": "[SciVis paper] \n\nAuthors: Steffen Oeltze-Jafra, Juan R. Cebral, G\u00e0bor Janiga, Bernhard Preim\n\nAbstract: Computational fluid dynamic (CFD) simulations of blood flow provide new insights into the hemodynamics of vascular pathologies such as cerebral aneurysms. Understanding the relations between hemodynamics and aneurysm initiation, progression, and risk of rupture is crucial in diagnosis and treatment. Recent studies link the existence of vortices in the blood flow pattern to aneurysm rupture and report observations of embedded vortices -- a larger vortex encloses a smaller one flowing in the opposite direction -- whose implications are unclear. \nWe present a clustering-based approach for the visual analysis of vortical flow in simulated cerebral aneurysm hemodynamics. We show how embedded vortices develop at saddle-node bifurcations on vortex core lines and convey the participating flow at full manifestation of the vortex by a fast and smart grouping of streamlines and the visualization of group representatives. The grouping result may be refined based on spectral clustering generating a more detailed visualization of the flow pattern, especially further off the core lines. We aim at supporting CFD engineers researching the biological implications of embedded vortices.", "uri": "https://vimeo.com/136542350", "name": "VIS15 preview: Cluster Analysis of Vortical Flow in Simulations of Cerebral Aneurysm Hemodynamics", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-17T19:29:54+00:00", "description": "[SciVis paper] \n\nAuthors: Wenchao Wu, Jiayi Xu, Haipeng Zeng, Yixian Zheng, Bing Ni, Mingxuan Yuan, Lionel M. Ni, Huamin Qu\n\nAbstract: Understanding co-occurrence in urban human mobility (i.e. people from two regions visit an urban place during the same time span) is of great value in a variety of applications, such as urban planning, business intelligence, social behavior analysis, as well as containing contagious diseases. In recent years, the widespread use of mobile phones brings an unprecedented opportunity to capture large-scale and fine-grained data to study co-occurrence in human mobility. However, due to the lack of systematic and efficient methods, it is challenging for analysts to carry out in-depth analyses and extract valuable information. In this paper, we present TelCoVis, an interactive visual analytics system, which helps analysts leverage their domain knowledge to gain insight into the co-occurrence in urban human mobility based on telco data. Our system integrates visualization techniques with new designs and combines them in a novel way to enhance analysts' perception for a comprehensive exploration. In addition, we propose to study the correlations in co-occurrence (i.e. people from multiple regions visit different places during the same time span) by means of biclustering techniques that allow analysts to better explore coordinated relationships among different regions and identify interesting patterns. The case studies based on a real-world dataset and interviews with domain experts have demonstrated the effectiveness of our system in gaining insights into co-occurrence and facilitating various analytical tasks.", "uri": "https://vimeo.com/136542178", "name": "VIS15 preview: TelCoVis: Visual Exploration of Co-occurrence in Urban Human Mobility Based on Telco Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-17T19:29:13+00:00", "description": "[SciVis paper] \n\nAuthors: Hanqi Guo, Carolyn Phillips, Tom Peterka, Dmitry Karpeyev, Andreas Glatz\n\nAbstract: We propose a method for the vortex extraction and tracking of superconducting magnetic flux vortices for both structured and unstructured mesh data. In the Ginzburg-Landau theory, magnetic flux vortices are well-defined features in a complex-valued order parameter field, and their dynamics determine electromagnetic properties in type-II superconductors. Our method represents each vortex line (a 1D curve embedded in 3D space) as a connected graph extracted from the discretized field in both space and time. For a time-varying discrete dataset, our vortex extraction and tracking method is as accurate as the data discretization. We then apply 3D visualization and 2D event diagrams to the extraction and tracking results to help scientists understand vortex dynamics and macroscale superconductor behavior in greater detail than previously possible.", "uri": "https://vimeo.com/136542098", "name": "VIS15 preview: Extracting, Tracking, and Visualizing Magnetic Flux Vortices in 3D Complex-Valued Superconductor Simulation...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-17T19:17:01+00:00", "description": "Organizer: Martin Falk, Sebastian Grottel, Michael Krone, Guido Reina\n\nAbstract: We propose a half-day tutorial that covers fundamental techniques for interactive particle-based visualization. Particle data typically originates from measurements and simulations in various fields such as life sciences or physics. Often, the particles are visualized directly, that is, by simple representants like spheres. Interactive rendering facilitates the exploration and visual analysis of the data. With increasing data set sizes in terms of particle numbers, interactive high-quality visualization is a challenging task. This is especially true for dynamic data or abstract representations that are based on the raw particle data. Our intermediate-level tutorial will cover direct particle visualization using simple glyphs as well as abstractions that are application-driven such as clustering and aggregation. It targets visualization researchers and developers who are interested in visualization techniques for large, dynamic particle-based data. We will focus on GPU-accelerated algorithms for high-performance rendering and data processing that run in real-time on modern desktop hardware. Consequently, we will discuss the implementation of said algorithms and the required data structures to make use of the capabilities of modern graphics APIs. Furthermore, we will discuss GPU-accelerated methods for the generation of application-dependent abstract representations. This includes various representations commonly used in application areas such as structural biology, systems biology, thermodynamics, and astrophysics.", "uri": "https://vimeo.com/136540848", "name": "VIS15 Tutorial: Interactive GPU-based Visualization of Large Dynamic Particle Data", "year": "2015", "event": "TUTORIAL"}, {"created_time": "2015-08-17T19:13:56+00:00", "description": "Organizer: Steffen Oeltze-Jafra, Anders Ynnerman, Stefan Bruckner, Helwig Hauser\n\nAbstract: Medicine is one of the primary drivers of visualization research and medical visualization (MedViz) is a vibrant and successful field with a tradition of dozens of years. Traditionally, a lot of MedViz research has been focused on the visualization of a single, uni-modal patient dataset, being usually defined on a regular grid in 3D and capturing a selected part of the human anatomy. As a prominent example, volume rendering has been extensively studied, together with advanced lighting simulation, etc. In recent years, however, the most pressing challenges in MedViz have broadened, not at the least paralleling new developments in image acquisition, and being associated with a growing data complexity, and advances in medical diagnosis and patient treatment. It is now becoming increasingly common, that several datasets are acquired, also at different points in time, and that in-vivo information, related to physiology, is complementing the more traditional anatomical information. Different imaging modalities are applied and whole-body scans facilitate the screening for disease and amplify the opportunities of forensic pathology. Data may also be measured or computed in a numerical simulation over complex grids, e.g., in ultrasound imaging or in the simulation of blood flow in cerebral and aortic aneurysms. All this data needs to be integrated with the anatomical scans. While traditional MedViz usually focuses on data of a single patient, the large data pools that are acquired in longitudinal cohort studies, for example, in epidemiology, involving hundreds to thousands of individuals (the cohort) pose tremendous new challenges. These include the combined visualization of image and non-image data as well as the integrated visualization of heterogeneous data. The effective and efficient interactive exploration of large medical data requires innovative technology and dedicated interaction techniques such as table-top user interfaces and gesture-based interaction. In this tutorial, we discuss the above-mentioned modern challenges in MedViz, followed by examples of and strategies for the development of new solutions to cope with these challenges with respect to specific (clinical) problems. We explore a variety of advanced MedViz topics. In particular, we discuss the interactive visualization of whole-body medical volume data, visualization techniques addressing the readability problem of Ultrasound by enriching the data with other types of medical data, the visualization of more abstract physiological data in its anatomical context, and the interactive visual analysis of heterogeneous image-centric cohort study data. Sufficient room for discussion is also planned as part of this tutorial.", "uri": "https://vimeo.com/136540549", "name": "VIS15 Tutorial: Rejuvenated Medical Visualization -- Large-scale, whole-body visualization, visualizing physiology, non-...", "year": "2015", "event": "TUTORIAL"}, {"created_time": "2015-08-17T19:12:03+00:00", "description": "Organizer: Alexander Wiebel, Tobias Isenberg, Stefan Bruckner, Timo Ropinski\n\nAbstract: Natural sciences, medicine and engineering are only a small selection of application domains where volumetric data, continuous as well as scattered, are close to ubiquitous. While the visualization of such data itself is not straightforward, interaction with and manipulation of volumetric data - essential aspects of effective data analysis - pose even further challenges. Due to the three-dimensional nature of the data, it is not straightforward how to select features, pick positions, segment regions or otherwise interact with the rendering or the data themselves in an intuitive manner. In this tutorial we will present state of the art approaches and methods for addressing these challenges with a special focus on the users' analysis and interaction tasks, as well as on the application of the methods in a large variety of application domains. The tutorial will start by reviewing common classes of interaction tasks in volume visualization, motivating the need for direct interaction and manipulation, and describing the usually encountered difficulties. Interaction with visualization traditionally happens in PC-based environments with mouse and 2D displays. The second part of the tutorial discusses specific interaction methods that deal with the challenges in this context. Furthermore, an overview of the range of applications of these techniques is given to demonstrate their utility. The use of alternative paradigms for interaction with volumes is discussed in the third part. Such paradigms, e.g. in the context of touch interfaces or immersive environments, provide novel opportunities for volume exploration and manipulation, but also pose specific challenges themselves. The last part completes the tutorial's scope by a treatment of higher-level interaction techniques guiding users in navigation and exploration of the data using automatic or semi-automatic methods for identifying relevant parameter ranges. Such techniques employ additional, sometimes workflow-specific, information to assist in choosing effective volume visualization techniques and related attributes.", "uri": "https://vimeo.com/136540349", "name": "VIS15 Tutorial: Direct Volume Interaction for Visual Data Analysis", "year": "2015", "event": "TUTORIAL"}, {"created_time": "2015-08-17T19:10:45+00:00", "description": "Organizer: Theresa-Marie Rhyne\n\nAbstract: We examine the foundations of color theory &amp; how these methods apply to building effective visualizations. We define color harmony &amp; demonstrate the application of color harmony to case studies. Case studies include ensemble scientific visualizations, historic &amp; new infographics, correlation in biological data, rainbow color deficiency safe examples, &amp; time series animations. The Pantone Matching System, Munsell Color System and other hue systems are reviewed. The features of ColorBrewer, Adobe\u2019s Color app &amp; Josef Albers \u201cInteraction of Color\u201d app are examined. We also introduce \u201cGamut Mask\u201d &amp; \u201cColor Proportions of an Image\u201d analysis tools. Our tutorial concludes with a hands on session that teaches how to use online and mobile apps to successfully capture, analyze and store color schemes for future use in visual analytics. This includes the evaluations for color deficiencies using Coblis. These color suggestion tools are available online for your continued use in creating new visualizations. Please bring small JPEG examples of your visualizations for performing color analyses during the hands on session.", "uri": "https://vimeo.com/136540245", "name": "VIS15 Tutorial: Applying Color Theory to VIS", "year": "2015", "event": "TUTORIAL"}, {"created_time": "2015-08-17T19:07:58+00:00", "description": "[VAST paper] \n\nAuthors: Yanhong Wu, Naveen Pitipornvivat, Jian Zhao, Sixiao Yang, Guowei Huang, Huamin Qu\n\nAbstract: Ego-network, which represents relationships between a specific individual, i.e., the ego, and people connected to it, i.e., alters, is a critical target to study in social network analysis. Evolutionary patterns of ego-networks along time provide huge insights to many domains such as sociology, anthropology, and psychology. However, the analysis of dynamic ego-networks remains challenging due to its complicated time-varying graph structures, for example: alters come and leave, ties grow stronger and fade away, and alter communities merge and split. Most of the existing dynamic graph visualization techniques mainly focus on topological changes of the entire network, which is not adequate for egocentric analytical tasks. In this paper, we present egoSlider, a visual analysis system for exploring and comparing dynamic ego-networks. egoSlider provides a holistic picture of the data through multiple interactively coordinated views, revealing ego-network evolutionary patterns at three different layers: a macroscopic level for summarizing the entire ego-network data, a mesoscopic level for overviewing specific individuals' ego-network evolutions, and a microscopic level for displaying detailed temporal information of egos and their alters. We demonstrate the effectiveness of egoSlider with a usage scenario with the DBLP publication records. Also, a controlled user study indicates that in general egoSlider outperforms a baseline visualization of dynamic networks for completing egocentric analytical tasks.", "uri": "https://vimeo.com/136539964", "name": "VIS15 preview: egoSlider: Visual Analysis of Egocentric Network Evolution", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-17T19:06:42+00:00", "description": "[VAST paper] \n\nAuthors: Nan Cao, Conglei Shi, W. Sabrina Lin, Jie Lu, Yu-Ru Lin, Ching-Yung Lin\n\nAbstract: Users with anomalous behaviors in online communication systems (e.g. email and social medial platforms) are potential threats to society. Automated anomaly detection based on advanced machine learning techniques has been developed to combat this issue; challenges remain, though, due to the difficulty of obtaining proper ground truth for model training and evaluation. Therefore, substantial human judgment on the automated analysis results is often required to better adjust the performance of anomaly detection. Unfortunately, techniques that allow users to understand the analysis results more efficiently, to make a confident judgment about anomalies, and to explore data in their context, are still lacking. In this paper, we propose a novel visual analysis system, TargetVue, which detects anomalous users via an unsupervised learning model and visualizes the behaviors of suspicious users in behavior-rich context through novel visualization designs and multiple coordinated contextual views. Particularly, TargetVue incorporates three new ego-centric glyphs to visually summarize a user's behaviors which effectively present the user's communication activities, features, and social interactions. An efficient layout method is proposed to place these glyphs on a triangle grid, which captures similarities among users and facilitates comparisons of behaviors of different users. We demonstrate the power of TargetVue through its application in a social bot detection challenge using Twitter data, a case study based on email records, and an interview with expert users. Our evaluation shows that TargetVue is beneficial to the detection of users with anomalous communication behaviors.", "uri": "https://vimeo.com/136539821", "name": "VIS15 preview: TargetVue: Visual Analysis of Anomaly User Behaviors in Online Communication Systems", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-17T18:29:06+00:00", "description": "Organizers: Angus Forbes, Fanny Chevalier, Daria Tsoupikova\n\nAbstract: The IEEE VIS 2015 Arts Program, or VISAP'15, showcases innovative artwork and research that explores the exciting and increasingly prominent intersections between art and visualization. Through a dedicated papers track and an exhibition that run concurrently with the IEEE 2015 VIS conference, the Arts Program aims to foster new thinking, discussion, and collaboration between artists, designers, technologists, visualization scientists, and others working at the intersection of these fields.\n\nThe VISAP papers tracks will feature five exciting paper presentations, covering different topics related to the VISAP 2015 theme : Data Improvisations.\n\nRecently, in the visualization community, significant emphasis has been placed on understanding temporality, on how to represent and reason about information that changes over time. Advances in parallel algorithms and high-performance computing hardware have made it possible to process and analyze greater and greater sizes of data in real-time, and new visualization techniques strive to develop a means for human viewers to more effectively observe and reason about these streams of information. Computational artworks also have explored dynamics as a primary theme: how social media feeds erupt with new content, how robotic architecture can acknowledge movement or emotion, or how compositional algorithms can creatively respond to live inputs of musical sound. Moreover, there is a rich vocabulary about how meaning evolves over time that is present in other contemporary mediums: kinetic sculptures that react to natural forces or human interaction, cinematographers who capture the changing qualities of light, and jazz musicians who trade musical fragments to build new compositional ideas.\n\nCan these types of artistic explorations offer insight into thinking about the effective representation of time in visualization research contexts? Complementarily, can the advances in visualization research present new opportunities for artists to think about the creative coupling of data to meaning? Could scientific research and the arts improvise to create new spaces of interaction and analysis? This year's papers explore the relationships between visualization research and artistic practice, and present or discuss creative visual techniques that emphasize the temporal and dynamic aspects of scientific or cultural exploration.\n\nMore information at : http://visap.uic.edu/\n\ncopyright creative commons license for sound composition : Frange by Tron Sepia", "uri": "https://vimeo.com/136536154", "name": "VIS Arts Program (VISAP) 2015 - Papers", "year": "2015", "event": "ARTS PROGRAM"}, {"created_time": "2015-08-17T18:26:16+00:00", "description": "Organizers: Angus Forbes, Fanny Chevalier, Daria Tsoupikova\n\nAbstract: The IEEE VIS 2015 Arts Program, or VISAP'15, showcases innovative artwork and research that explores the exciting and increasingly prominent intersections between art and visualization. Through a dedicated papers track and an exhibition that run concurrently with the IEEE 2015 VIS conference, the Arts Program aims to foster new thinking, discussion, and collaboration between artists, designers, technologists, visualization scientists, and others working at the intersection of these fields.\n\nThe VISAP Art Exhibition, showcased at the School of the Art Institute of Chicago, will feature artworks in LeRoy Neiman Centrer, open to the general public. Nineteen installations, demos and live performance will cover different topics related to the VISAP 2015 theme : Data Improvisations.\n\nRecently, in the visualization community, significant emphasis has been placed on understanding temporality, on how to represent and reason about information that changes over time. Advances in parallel algorithms and high-performance computing hardware have made it possible to process and analyze greater and greater sizes of data in real-time, and new visualization techniques strive to develop a means for human viewers to more effectively observe and reason about these streams of information. Computational artworks also have explored dynamics as a primary theme: how social media feeds erupt with new content, how robotic architecture can acknowledge movement or emotion, or how compositional algorithms can creatively respond to live inputs of musical sound. Moreover, there is a rich vocabulary about how meaning evolves over time that is present in other contemporary mediums: kinetic sculptures that react to natural forces or human interaction, cinematographers who capture the changing qualities of light, and jazz musicians who trade musical fragments to build new compositional ideas.\n\nCan these types of artistic explorations offer insight into thinking about the effective representation of time in visualization research contexts? Complementarily, can the advances in visualization research present new opportunities for artists to think about the creative coupling of data to meaning? Could scientific research and the arts improvise to create new spaces of interaction and analysis? This year's papers explore the relationships between visualization research and artistic practice, and present or discuss creative visual techniques that emphasize the temporal and dynamic aspects of scientific or cultural exploration.\n\nMore information at : http://visap.uic.edu/\n\ncopyright creative commons license for sound composition : Frange by Tron Sepia", "uri": "https://vimeo.com/136535886", "name": "VIS Arts Program (VISAP) 2015 - Art Exhibition", "year": "2015", "event": "ARTS PROGRAM"}, {"created_time": "2015-08-17T18:14:34+00:00", "description": "[VAST paper] \n\nAuthors: Hannah Kim, Jaegul Choo, Haesun Park, Alex Endert\n\nAbstract: Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.", "uri": "https://vimeo.com/136534743", "name": "VIS15 preview: InterAxis: Steering Scatterplot Axes via Observation-Level Interaction", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-17T17:54:42+00:00", "description": "Organizers: Robert S. Laramee\n\nAbstract: Evaluation, solved and unsolved problems, and future directions are popular themes pervading the visualization research community over the last decade. The top unsolved problems in both scientific and information visualization was the subject of an IEEE Visualization Conference panel in 2004 (Rhyne et al 2004). The future of graphics hardware was another important topic of discussion the same year (Johnson et al 2004). The subject of how to evaluate visualization returned a few years later (House et al., 2005, Van Wijk 2005). Chris Johnson published a list of top problems in scientific visualization research (Johnson 2004) in 2004. This was followed up by report of both past achievements and future challenges in visualization research as well as financial support recommendations to the National Science Foundation (NSF) and National Institute of Health (NIH) (Johnson et al 2006). C. Chen published the first list of top unsolved information visualization problems (Chen 2005) in 2005. Future research directions in topology-based visualization were also a major theme of a workshop on topology-based visualization methods (Hauser et al., 2005, Scheuermann et al., 2005). Laramee and Kosara published a list of top future challenges in human-centered visualization (Laramee and Kosara 2007) in 2007. Laramee et al presented a list of top unsolved problems and future challenges in multi- field visualization (Laramee et al., 2014). These pervasive themes coincide roughly with the 20th anniversary of what is often recognized as the start of visualization in computing as a distinct field of research (McCormick et al., 1987).\nHowever, these lists, panels, and papers imply that that some fundamental problems have been solved in visualization. After 30 years, what have we as a visualization research community solved? Is there any consensus on solved problems in visualization? This panel addresses some very difficult, core, fundamental questions such as (but not limited to):\n\u2022 What visualization (scientific and information) problems have we, the visualization research community, solved?\n\u2022 Is there any consensus on what problems have been solved?\n\u2022 How can we as a community define an \u201copen\u201d or \u201csolved\u201d problem?\n\u2022 Have any of the top challenges identified 10 or 20 years ago been solved?\n\u2022 What about visual analytics?", "uri": "https://vimeo.com/136532705", "name": "VIS15 Panel: Solved Problems in Visualization", "year": "2015", "event": "PANEL"}, {"created_time": "2015-08-17T17:46:20+00:00", "description": "Organizers: Vetria Byrd\n\nAbstract: Visualization is fundamental in understanding and analyzing complex data from all aspects and most disciplines of research and scholarship. Using visualization, researchers convert raw, simulated or observed information into a graphical format. The need to diversify a field with such far-reaching influences is imperative. This panel brings together a diverse group of visualization scientists. The main goal of the panel is to facilitate a timely discussion in VisWeek 2015 about potential mechanisms to broaden participation of women and members of underrepresented groups in visualization for the purpose of encouraging more diversity in the field of visualization. As a secondary benefit, this panel will raise awareness about efforts that are being made to broaden participation in visualization.", "uri": "https://vimeo.com/136531881", "name": "VIS15 Panel: Could Visualization Provide a Pathway to STEM?", "year": "2015", "event": "PANEL"}, {"created_time": "2015-08-15T08:12:15+00:00", "description": "Organizers: Laura McNamara\n\nAbstract: IEEE VIS comprises three co-located complementary but distinct conferences. SciVis focuses on visualizing science data, while InfoVis visualizes abstract information and VAST takes a scientific approach to understanding analysis processes. This panel considers an alternative taxonomy based on the institutional situation of the researcher/developer; i.e., their \u201cecological niche\u201d in the field. Our panelists represent visualization practice in three key \u201cecological niches\u201d that span the SciVis, InfoVis and VAST communities: government, industry, and academia. Together, we would like to explore the identity and practices of the \u201cvisualization researcher\u201d in each of these niches, comparing and contrasting experience to understand the permutations of VIS knowledge in our various professional environments.\nPanelists include two government researchers who work with government clients, two interactive information visualization researchers representing the commercial sector, and two university researchers with experience collaborating with counterparts in the previously mentioned two niches. We draw on our collective professional experience to open a conversation about the role of professional and institutional affiliation as shaping forces in the practice and products of our research.", "uri": "https://vimeo.com/136369208", "name": "VIS15 Panel: The Professional Ecology of Visualization", "year": "2015", "event": "PANEL"}, {"created_time": "2015-08-15T08:11:54+00:00", "description": "Organizers: Marti A. Hearst, Eytan Adar\n\nAbstract: Information visualization has escaped the research lab and is now widely used by practitioners across a wide spectrum of \ufb01elds. New software tools and programming frameworks appear on a monthly basis. New design paradigms are rapidly gaining acceptance and evolving. At the same time, methods for teaching in the classroom and beyond are being challenged and in\ufb02uenced by online offerings such as Khan Academy and Massive Open Online Courses (MOOCs), the adoption of \ufb02ipped classrooms, and the adaptation of instructional environments used in other communities. Pedagogy geared towards mastery learning that makes use of active learning and peer learning are being introduced in more and more contexts, re\ufb02ecting the results of decades of research showing the bene\ufb01ts of these techniques, as well as their suitability for today\u2019s connected students who expect a more interactive learning experience. As the role of information visualization grows and changes in the world of practice, new methods are needed to teach this dynamic topic. This panel brings together experts with different perspectives to talk about how they are rising to the challenge to teach information visualization in this new world. They are asked to take into account speci\ufb01cally:\n\u2022 Practice vs research (infoviz is now huge in the \u201creal world\u201d)\n\u2022 Distracted students, need active learning\n\u2022 Many software tools and frameworks that instructors should build on \nand address the question of how we incorporate and balance these issues into modern infoviz courses? This panel asks instructors who teach across the spectrum from purely research-oriented courses to more applied courses, and with a wide range of styles, how they meet these challenges. Two panelists teach in Computer Science departments, two teach in interdisciplinary Schools of Information, and one is a senior research associate in a non-pro\ufb01t policy center. The moderator is a former academic and practi- tioner thought-leader who now leads innovation at a leading software developer of visualization tools.", "uri": "https://vimeo.com/136369193", "name": "VIS15 Panel: Vis, The Next Generation: Teaching Across the Researcher-Practitioner Gap", "year": "2015", "event": "PANEL"}, {"created_time": "2015-08-15T08:04:28+00:00", "description": "Organizers: Theresa-Marie Rhyne\n\nAbstract: In this panel, we highlight optimal solutions for designing and building color maps in visualization applications and presentations. Our panelists represent artists, software engineers, cartographers, color scientists, perceptual psychologists, and visualization researchers who have contributed effective solutions to applying color to data visualization. Each panelist will highlight their perspective as well as tips and tricks for color map solutions. Drawing on perspectives from many disciplines, the panel will identify gaps in our understanding about the use of color in visualization and will identify future research directions.", "uri": "https://vimeo.com/136368966", "name": "VIS15 Panel: Color Mapping in VIS: Perspectives on Optimal Solutions", "year": "2015", "event": "PANEL"}, {"created_time": "2015-08-15T07:56:38+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Lyn Bartram\n\nAbstract: Domestic energy conservation is considered critical to reducing energy demand and greenhouse gas emissions. Personal visualization has a role to play in the design of appropriate feedback for encouraging more effective home energy use, but the unique nature of residential energy informatics introduces new design issues. We review current approaches and discuss challenges as yet unaddressed in this emerging space of personal visualization related to better conceptual models, context, enhanced communicative scope and functional aesthetics.", "uri": "https://vimeo.com/136368778", "name": "VIS15 preview: Engaging with Energy in the Informative Home: Challenges and opportunities for eco-feedback", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-15T07:56:10+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Eun Kyoung Choe, Bongshin Lee, M. C. Schraefel\n\nAbstract: Data visualization and analytics research has great potential to empower people to improve their lives by leveraging their own personal data. However, most Quantified\u00adSelfers are neither visualization experts nor data scientists. Consequently, their visualizations of their data are often not ideal for conveying their insights. Aiming to design a visualization system to help non\u00ad experts explore and present their personal data, we conducted a pre\u00addesign empirical study. Through the lens of Quantified\u00adSelfers, we examined what insights people gain specifically from their personal data and how they use visualizations to communicate their insights. Based on our analysis of 30 Quantified Self presentations, we characterized eight insight types (detail, self\u00ad reflection, trend, comparison, correlation, data summary, distribution, outlier) and mapped the visual annotations used to communicate them. We further discussed four areas for the design of personal visualization systems, including support for encouraging self\u00adreflection, gaining valid insight, communicating insight, and using visual annotations.", "uri": "https://vimeo.com/136368770", "name": "VIS15 preview: Characterizing Visualization Insights from Quantified-Selfers' Personal Data Presentations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-15T07:55:44+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Rahul C. Basole, Ahsan Qamar, Hyunwoo Park, Christiaan J.J. Paredis, Leon F. McGinnis\n\nAbstract: This article reports on our ongoing experiences in developing visual analytics tools for real\u00adworld CESs. Our work focuses on the early design phase during which a large design space is explored, poor alternatives are pruned, and valuable alternatives are considered further. Visual analytics tools can provide interactive discovery, exploration, and understanding of real\u00adworld complex engineered systems (CES). The proposed tool, which focuses on the early design phase, can help users perform routine CES design analysis tasks and offer stakeholder\u00adspecific visual representations of complex design models.", "uri": "https://vimeo.com/136368761", "name": "VIS15 preview: Visual Analytics for Early-Phase Complex Engineered System Design Support", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-15T07:55:03+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Charilaos Papadopoulos, Kaloian Petkov, Arie E. Kaufman, Klaus Mueller\n\nAbstract: The Reality Deck is a visualization facility offering state\u00adof\u00adthe\u00adart aggregate resolution and immersion. Its a 1.5\u00adGpixel immersive tiled display with a full 360\u00addegree horizontal field of view. Comprising 416 high\u00ad-density LED\u00ad-backlit LCD displays, it visualizes gigapixel\u00ad-resolution data while providing 20/20 visual acuity for most of the visualization space.", "uri": "https://vimeo.com/136368743", "name": "VIS15 preview: The Reality Deck - Immersive Gigapixel Display", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-14T06:19:45+00:00", "description": "[InfoVis paper] \n\nAuthors: Hendrik Strobelt, Bilal Alsallakh, Joseph Botros, Brant Peterson, Mark Borowsky, Hanspeter Pfister, Alexander Lex\n\nAbstract: Alternative splicing is a process by which the same DNA sequence is used to assemble different proteins, called protein isoforms. Alternative splicing works by selectively omitting some of the coding regions (exons) typically associated with a gene. Detection of alternative splicing is difficult and uses a combination of advanced data acquisition methods and statistical inference. Knowledge about the abundance of isoforms is important for understanding both normal processes and diseases and to eventually improve treatment through targeted therapies. The data however, is complex and current visualizations for isoforms are neither perceptually efficient nor scalable. To remedy this, we developed Vials, a novel visual analysis tool that enables analysts to explore the various components that scientists use to make judgements about isoforms: the abundance of reads associated with the coding regions of the gene, evidence for junctions, i.e., edges connecting the coding regions, and predictions of isoform frequencies. Vials is scalable as it allows the simultaneous analysis of many samples in multiple groups. Our tool thus enables experts to (a) identify patterns of isoform abundance in groups of samples and (b) evaluate the quality of the data. We show the value of our tool in case studies using publicly available datasets.", "uri": "https://vimeo.com/136278933", "name": "VIS15 preview: Vials: Visualizing Alternative Splicing of Genes", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-14T06:19:13+00:00", "description": "[InfoVis paper] \n\nAuthors: Hendrik Strobelt, Daniela Oelke, Bum Chul Kwon, Tobias Schreck, Hanspeter Pfister\n\nAbstract: Semi-automatic text analysis involves manual inspection of text. Often, different text annotations (like part of speech or named entities) are indicated by using distinctive text highlighting techniques. In typesetting there exist well-known formatting conventions, such as bold typeface, italics, or background coloring, that are usefluf for highlighting certain parts of a given text. Also, many advanced techniques for visualization and highlighting of text exist; yet, standard typesetting is common, and the effects of standard typesetting on the perception of text are not fully understood. As such, we surveyed and tested the effectiveness of common text highlighting techniques, both individually and in combination, to discover how to maximize pop-out effects while minimizing visual interference between techniques. To validate our findings, we conducted a series of crowdsourced experiments to determine: i) a ranking of nine commonly-used text highlighting techniques; ii) the degree of visual interference between pairs of text highlighting techniques; iii) the effectiveness of techniques for visual conjunctive search. Our results show that increasing font size works best as a single highlighting technique, and that there are significant visual interferences between some pairs of highlighting techniques. We discuss pros and cons of different combinations as a design guideline to choose text highlighting techniques for text viewers.", "uri": "https://vimeo.com/136278904", "name": "VIS15 preview: Guidelines for Effective Usage of Text Highlighting Techniques", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-14T06:18:30+00:00", "description": "[InfoVis paper] \n\nAuthors: Anushka Anand, Justin Talbot\n\nAbstract: Effective small multiple displays are created by partitioning a visualization on variables that reveal interesting conditional structure in the data. We propose a method that automatically ranks partitioning variables, allowing analysts to focus on the most promising small multiple displays. Our approach is based on a randomized, non-parametric permutation test, which allows us to handle a wide range of quality measures for visual patterns defined on many different visualization types, while discounting spurious patterns. We demonstrate the effectiveness of our approach on scatterplots of real-world, multidimensional datasets.", "uri": "https://vimeo.com/136278883", "name": "VIS15 preview: Automatic Selection of Partitioning Variables for Small Multiple Displays", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-14T06:17:24+00:00", "description": "[VAST paper] \n\nAuthors: Cesar Palomo, Zhan Guo, Claudio Silva, Juliana Freire\n\nAbstract: Public transportation schedules are designed by agencies to optimize service quality under multiple constraints. However, real service usually deviates from the plan. Therefore, transportation analysts need to identify, compare and explain both eventual and systemic performance issues that must be addressed so that better timetables can be created. The purely statistical tools commonly used by analysts pose many difficulties due to the large number of attributes at trip- and station-level for planned and real service. Also challenging is the need for models at multiple scales to search for patterns at different times and stations, since analysts do not know exactly where or when relevant patterns might emerge and need to compute statistical summaries for multiple attributes at different granularities. To aid in this analysis, we worked in close collaboration with a transportation expert to design TR-EX, a visual exploration tool developed to identify, inspect and compare spatio-temporal patterns for planned and real transportation service. TR-EX combines two new visual encodings inspired by Marey's Train Schedule: Trips Explorer for trip-level analysis of frequency, deviation and speed; and Stops Explorer for station-level study of delay, wait time, reliability and performance deficiencies such as bunching. To tackle overplotting and to provide a robust representation for a large numbers of trips and stops at multiple scales, the system supports variable kernel bandwidths to achieve the level of detail required by users for different tasks. We justify our design decisions based on specific analysis needs of transportation analysts. We provide anecdotal evidence of the efficacy of TR-EX through a series of case studies that explore NYC subway service, which illustrate how TR-EX can be used to confirm hypotheses and derive new insights through visual exploration.", "uri": "https://vimeo.com/136278839", "name": "VIS15 preview: Visually Exploring Transportation Schedules", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-14T06:15:30+00:00", "description": "[VAST paper] \n\nAuthors: Shenghui Cheng, Klaus Mueller\n\nAbstract: Numerous methods have been described that allow the visualization of the data matrix. But all suffer from a common problem - observing the data points in the context of the attributes is either impossible or inaccurate. We describe a method that allows these types of comprehensive layouts. We achieve it by combining two similarity matrices typically used in isolation - the matrix encoding the similarity of the attributes and the matrix encoding the similarity of the data points. This combined matrix yields two of the four submatrices needed for a full multi-dimensional scaling type layout. The remaining two submatrices are obtained by creating a fused similarity matrix - one that measures the similarity of the data points with respect to the attributes, and vice versa. The resulting layout places the data objects in direct context of the attributes and hence we call it the data context map. It allows users to simultaneously appreciate (1) the similarity of data objects, (2) the similarity of attributes in the specific scope of the collection of data objects, and (3) the relationships of data objects with attributes and vice versa. The contextual layout also allows data regions to be segmented and labeled based on the locations of the attributes. This enables, for example, the map's application in selection tasks where users seek to identify one or more data objects that best fit a certain configuration of factors, using the map to visually balance the tradeoffs.", "uri": "https://vimeo.com/136278727", "name": "VIS15 preview: The Data Context Map: Fusing Data and Attributes into a Unified Display", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-14T06:15:14+00:00", "description": "[VAST paper] \n\nAuthors: Tatiana von Landesberger, Felix Brodkorb, Philipp Roskosch, Gennady Andrienko, Natalia Andrienko, Andreas Kerren\n\nAbstract: Learning more about people mobility is an important task for official decision makers and urban planners. Mobility data sets characterize the variation of the presence of people in different places over time as well as movements (or flows) of people between the places. The analysis of mobility data is challenging due to the need to analyze and compare spatial situations (i.e., presence and flows of people in certain time intervals) and to gain an understanding of the spatio-temporal changes (variations of situations over time). Traditional flow visualizations usually fail due to massive clutter. Modern approaches offer limited support for investigating the complex variation of the movements over longer time periods. \nWe propose a visual analytics methodology that solves these issues by combined spatial and temporal simplifications. We have developed a graph-based method, called MobilityGraphs, which reveals movement patterns that were occluded in flow maps. Our method enables the visual representation of the spatio-temporal variation of movements for long time series of spatial situations originally containing a large number of intersecting flows. The interactive system supports data exploration from various perspectives and at various levels of detail by interactive setting of clustering parameters. The feasibility our approach was tested on aggregated mobility data derived from a set of geolocated Twitter posts within the Greater London city area and mobile phone call data records in Abidjan, Ivory Coast. We could show that MobilityGraphs support the identification of regular daily and weekly movement patterns of resident population.", "uri": "https://vimeo.com/136278721", "name": "VIS15 preview: MobilityGraphs: Visual Analysis of Mass Mobility Dynamics via Spatio-Temporal Graphs and Clustering", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-14T06:14:25+00:00", "description": "[VAST paper] \n\nAuthors: Quan Li, Peng Xu, Huamin Qu\n\nAbstract: The rate at which frames are rendered in a computer game directly influences both game playability and enjoyability. Players frequently have to deal with the trade-off between high frame rates and good resolution. Analyzing patterns in frame rate data and their correlation with the overall game performance is important in designing games (e.g., graphic card/display setting suggestion and game performance measurement). However, this task is challenging because game frame rates vary both temporally and spatially. In addition, players may adjust their display settings based on their gaming experience and hardware conditions, which further contributes to the unpredictability of frame rates. In this paper, we present a comprehensive visual analytics system FPSSeer, to help game designers gain insight into frame rate data. Our system consists of four major views: 1) a frame rate view to show the overall distribution in a geographic scale, 2) a grid view to show the frame rate distribution and grid element clusters based on their similarity, 3) a FootRiver view to reveal the temporal patterns in game condition changes and potential spatiotemporal correlations, and 4) a comparison view to evaluate game performance discrepancy under different game tests. The real-world case studies demonstrate the effectiveness of our system. The system has been applied to an online commercial game to monitor its performance and to provide feedbacks to designers and developers.", "uri": "https://vimeo.com/136278689", "name": "VIS15 preview: FPSSeer: Visual Analysis of Game Frame Rate Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-14T06:13:26+00:00", "description": "[SciVis paper] \n\nAuthors: Daniel J\u00f6nsson, Martin Falk, Anders Ynnerman\n\nAbstract: In this work we present a volume exploration method designed to be used by novice users and visitors to science centers and museums. The volumetric digitalization of artifacts in museums is of rapidly increasing interest as enhanced user experience through interactive data visualization can be achieved. This is, however, a challenging task since the vast majority of visitors are not familiar with the concepts commonly used in data exploration, such as mapping of visual properties from values in the data domain using transfer functions. Interacting in the data domain is an effective way to filter away undesired information but it is difficult to predict where the values lie in the spatial domain. In this work we make extensive use of dynamic previews instantly generated as the user explores the data domain. The previews allow the user to predict what effect changes in the data domain will have on the rendered image without being aware that visual parameters are set in the data domain. Each preview represents a subrange of the data domain where overview and details are given on demand through zooming and panning. The method has been designed with touch interfaces as the target platform for interaction. We provide a qualitative evaluation performed with visitors to a science center to show the utility of the approach.", "uri": "https://vimeo.com/136278660", "name": "VIS15 preview: Intuitive Exploration of Volumetric Data Using Dynamic Galleries", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:22:02+00:00", "description": "[Poster] \n\nAuthors: Roger A. Leite, Theresia Gschwandtner, Silvia Miksch\n\nAbstract: One of the primary concerns of financial institutions is to guarantee security and legitimacy in their services. Being able to detect and avoid fraudulent schemes also enhances the credibility of these institutions. Currently, fraud detection approaches still lack Visual Analytics techniques. We propose a Visual Analytics process that tackles the main challenges in the area of fraud detection.", "uri": "https://vimeo.com/136255234", "name": "VIS15 preview: Visual Analytics for Fraud Detection and Monitoring", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:21:25+00:00", "description": "[Poster] \n\nAuthors: Mershack Okoe, Radu Jianu\n\nAbstract: Quantitative user studies are often reviewed and judged by the magnitude of detected effects and basic soundness of their evaluation procedure. Here, we advocate for an increased focus on the ecological validity of tasks, interactions, and data chosen for evaluation. We revisit Ghoniem et al.'s highly cited study of node-link vs. matrix representations of graph data [2], discuss the ecological validity of its design using a formal framework, and show quantitatively how minor changes in task and interaction phrasing can lead to significantly different outcomes and insights.", "uri": "https://vimeo.com/136255187", "name": "VIS15 preview: Ecological Validity in Quantitative User Studies \u2013 a Case Study in Graph Evaluation", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:21:14+00:00", "description": "[Poster] \n\nAuthors: Kristin Cook, Jean Scholtz, Mark Whiting\n\nAbstract: The VAST Challenge has been a popular venue for academic and industry participants for over ten years. Many participants comment that the majority of their time in preparing VAST Challenge entries is discovering elements in their software environments that need to be redesigned in order to solve the given task. Fortunately, there is no need to wait until the VAST Challenge is announced to test out software systems. The Visual Analytics Benchmark Repository contains all past VAST Challenge tasks, data, solutions and submissions. In this poster we describe how developers can perform informal evaluations of various aspects of their visual analytics environments using VAST Challenge information.", "uri": "https://vimeo.com/136255168", "name": "VIS15 preview: A Software Developer\u00e2\u20ac\u2122s Guide to Informal Evaluation of Visual Analytics Environments Using VAST Challenge.", "year": "2015", "event": "VAST, PREVIEW"}, {"created_time": "2015-08-13T22:19:30+00:00", "description": "[Poster] \n\nAuthors: David Manzano, Juan Salamanca, Carlos Arce-Lopera\n\nAbstract: Public Transportation Systems with robust information platforms allow their customers to access vehicle related information expecting that well-informed customers have a better riding experience or perceive waiting times shorter. Unfortunately that is not the case of users of urban transportation systems in large developing cities who often report poor user experience. We present the results of a user study of a visualization project that complements the information available to bus dispatchers at the control room with a graphic representation of the on-going customer experience. The aim of the project is to promptly support dispatcher's response to unforeseen situations that undermine quality of service. From the evaluation of a visualization prototype we derive some design guidelines for the inclusion of the customer experience in the operation visualization displayed at the dispatcher's control room.", "uri": "https://vimeo.com/136255023", "name": "VIS15 preview: Visualizing Riding Experience to Support Real Time Decision-Making in Urban Transportation Systems", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:18:01+00:00", "description": "[Poster] \n\nAuthors: Jonathan P. Leidig, Santhosh Dharmapuri\n\nAbstract: Modeling and simulation is often used to predict future events and plan accordingly. Experiments in this domain often produce thousands of results from individual simulations, based on slightly varying input parameters. Geo-spatial visualizations can be a powerful tool to help health researchers and decision-makers to take measures during catastrophic and epidemic events such as Ebola outbreaks. The work produced a web-based geo-visualization tool to visualize and compare the spread of Ebola in the West African countries Ivory Coast and Senegal based on multiple simulation results. The visualization is not Ebola specific and may visualize any time-varying frequencies for given geo-locations.", "uri": "https://vimeo.com/136254902", "name": "VIS15 preview: Automated Visualization Workflow for Simulation Experiments", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:16:31+00:00", "description": "[Poster] \n\nAuthors: Jiang Zhang, Hanqi Guo, Xiaoru Yuan\n\nAbstract: We present a novel model based on high-order access dependencies for high performance pathline computation in flow field. The high-order access dependencies are defined as transition probabilities from one data block to other blocks based on a few historical data accesses. Compared with existing methods which employed first-order access dependencies, our approach takes the advantages of high order access dependencies with higher accuracy and reliability in data access prediction. In our work, high-order access dependencies are calculated by tracing densely-seeded pathlines. The efficiency of our proposed approach is demonstrated through a parallel particle tracing framework with high-order data prefetching. Results show that our method can achieve higher data locality than the first-order access dependencies based method, thereby reducing the I/O requests and improving the efficiency of pathline computation in various applications.", "uri": "https://vimeo.com/136254798", "name": "VIS15 preview: High Performance Flow Field Visualization with High-Order Access Dependencies", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:16:15+00:00", "description": "[Poster] \n\nAuthors: Sujan Anreddy\n\nAbstract: Weather Research and Forecasting (WRF) models simulate weather conditions by generating 2D numerical weather prediction ensemble members either through perturbing initial conditions or by changing different parameterization schemes, e.g., cumulus and microphysics schemes. These simulations are often used by weather analysts to analyze the nature of uncertainty attributed by these simulations to forecast weather conditions with good accuracy. The number of simulations used for forecasting is growing with the advent of increase in computing power. Hence, there is a need for providing better visual insights of uncertainty with growing number of ensemble members. We propose a geo visual analytical framework that uses visual analytics approach to resolve visual scalability of these ensemble members. Our approach naturally fits with the workflow of an analyst analyzing ensemble spatial uncertainty. Meteorologists evaluated our framework qualitatively and found it to be effective in acquiring insights of spatial uncertainty associated with multiple ensemble runs that are simulated using multiple parameterization schemes.", "uri": "https://vimeo.com/136254778", "name": "VIS15 preview: Visual Scalability of Spatial Ensemble Uncertainty", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:15:58+00:00", "description": "[Poster] \n\nAuthors: Cecilia di Sciascio, Vedran Sabol, Eduardo Veas\n\nAbstract: uRank is a Web-based tool combining lightweight text analytics and visual methods for topic-wise exploration of document sets. It includes a view summarizing the content of the document set in meaningful terms, a dynamic document ranking view and a detailed view for further inspection of individual documents. Its major strength lies in how it supports users in reorganizing documents on-the-fly as their information interests change. We present a preliminary evaluation showing that uRank helps to reduce cognitive load compared to a traditional list-based representation.", "uri": "https://vimeo.com/136254755", "name": "VIS15 preview: uRank: Visual Analytics Approach for Search Result Exploration", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:15:39+00:00", "description": "[Poster] \n\nAuthors: Mai Elshehaly, Denis Gracanin, Mohammed Gad, Junpeng Wang, Hicham Elmongui\n\nAbstract: The study of physical phenomena and their dynamic evolution is supported by the analysis and visualization of time-enabled data. In many applications, available data are sparsely distributed in the space-time domain, which leads to incomprehensible visualiza- tions. We present an interactive approach for the dynamic track- ing and visualization of measured data particles through advection in a simulated flow. We introduce a fully GPU-based technique for efficient spatio-temporal interpolation, using a kd-tree forest for ac- celeration. As the user interacts with the system using a time slider, particle positions are reconstructed for the time selected by the user. Our results show that the proposed technique achieves highly ac- curate parallel tracking for thousands of particles. The rendering performance is mainly affected by the size of the query set.", "uri": "https://vimeo.com/136254732", "name": "VIS15 preview: Real-Time Interactive Time Correction on the GPU", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:15:07+00:00", "description": "[Poster] \n\nAuthors: Volker Ahlers\n\nAbstract: Information visualization is a subject that has attracted growing interest in recent years,both in academic and general public literature such as newspapers. We reflect our experience with visualization courses at Master's level and find that the subject provides a playground for innovative teaching methods. It is shown that typical visualization (and human computer interaction) topics such as perception and cognition offer particular potential for using classroom response systems (CRS). Besides using ConcepTests for testing the understanding of fundamental concepts taught in the lecture, we employ CRS to demonstrate certain perception phenomena and to evaluate the effectivity of visualization solutions with regard to the students. Laboratory projects allowing the application of visualization methods are generally enjoyed by students, who can bring their own data and get immediate visual feedback. In our laboratory projects the JavaScript library D3.js is used, which builds upon a declarative programming paradigm. This lets students experience several new concepts in computer science, such as web-based programming, declarative programming, and web services for access to data sources.", "uri": "https://vimeo.com/136254689", "name": "VIS15 preview: Teaching Information Visualization: A Playground for Classroom Response Systems and Declarative Programming...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:14:11+00:00", "description": "[Poster] \n\nAuthors: Sayeed Safayet Alam, Radu Jianu\n\nAbstract: We demonstrate the feasibility of real-time analysis of eye-tracking data captured from multiple people concurrently using an interactive visualization. We leverage the Data of Interest (DOI) approach, in which gaze coordinates are related to a visualization's content as it is rendered, to output data objects that users are interested in at any given time. We show in a controlled user study that subjects could interpret real-time DOI streaming from multiple users concurrently, to determine in real time what tasks those users were doing. We briefly discuss potential applications of our methods in teaching and in developing gaze-contingent visualizations.", "uri": "https://vimeo.com/136254607", "name": "VIS15 preview: What Are They Doing? : Real-time Analysis of Eye-Tracking Data.", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:13:54+00:00", "description": "[Poster] \n\nAuthors: Christine Nothelfer, Michael Gleicher, Steven Franconeri\n\nAbstract: Data visualizations allow viewers to compare one dataset to another. The visual marks that represent these datasets, or classes, are visually distinguished from one another by salient visual feature differences, such as color or shape. A designer of a graph or map might encode one class of marks as either red, or circular, and another class as either green, or square. One common technique is to combine these cues in a redundant fashion, encoding one class as red and circular, and the other as green and square, under the assumption that a larger difference (via multiple differing features) should help. Despite the ubiquity of this practice, we know of no empirical demonstration that reveals evidence of a potential benefit. Across two experiments, we demonstrate that redundant coding can improve visual segmentation of a simulated dataset in a crowded display (Experiment 1) and that redundant coding also leads to stronger visual grouping of elements (Experiment 2).", "uri": "https://vimeo.com/136254592", "name": "VIS15 preview: Redundant Coding Can Improve Segmentation in Multiclass Displays", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:13:39+00:00", "description": "[Poster] \n\nAuthors: Mark Livingston, Jonathan Decker, Zhuming Ai\n\nAbstract: We revisited past user study data on multivariate visualizations, looking at whether image processing measures offer any insight into user performance. While we find statistically significant correlations, some of the greatest insights into user performance came from variables that have strong ties to two key properties of multivariate representations. We discuss our analysis and propose a taxonomy of multivariate visualizations that arises.", "uri": "https://vimeo.com/136254577", "name": "VIS15 preview: A Proposed Multivariate Visualization Taxonomy from User Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:13:10+00:00", "description": "[Poster] \n\nAuthors: Mark Livingston, Derek Brock, Dennis Perzanowski, Tucker Maney, Wende Frost\n\nAbstract: Situation reports are commonly used to report on current conditions and resources. While format varies, many use prose, tables, and diagrams to express information. Many methods assess readability of prose; no analogous tools seemed as strong for tables and diagrams. We adapted the Sentence Verification Technique and conducted an initial study to assess its applicability.", "uri": "https://vimeo.com/136254544", "name": "VIS15 preview: An Initial Study on Assessing Comprehension of Information Visualization with the Sentence Verification...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:12:49+00:00", "description": "[Poster] \n\nAuthors: Andrew Dai, Ramik Sadana, Charles Stolper, John Stasko\n\nAbstract: We have developed an updated version of the Dust and Magnet visualization technique for large, multitouch displays. Multiple users can interactively manipulate magnets (data attributes) to observe how iron dust (the data items) changes its positions, and thus gain insight about the data. This type of visualization provides a very direct engagement with the data and thus a very 'hands on' analytic experience.", "uri": "https://vimeo.com/136254529", "name": "VIS15 preview: Hands On, Large Display Visual Data Exploration", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:09:52+00:00", "description": "[Poster] \n\nAuthors: Rafael Henkin, Aidan Slingsby, Jason Dykes\n\nAbstract: Time can be considered as a continuous phenomenon, but it is often discretised and aggregated into different temporal granularities. Domains and tasks demand the analysis of temporal data at different granularities. Data aggregated and visualized with some granularities may reveal patterns hidden in other units. Choosing the appropriate visualization depends on various temporal aspects and the tasks to be solved. We describe an initial investigation of the use of interactive visualization techniques to explore temporal aspects of granularities and define future work to be done.", "uri": "https://vimeo.com/136254348", "name": "VIS15 preview: Exploring Temporal Granularities with Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:09:33+00:00", "description": "[Poster] \n\nAuthors: Andrew H. Stevens, Colin Ware\n\nAbstract: Studies have found conflicting results regarding the effectiveness of tube-like structures for representing 3D flow data. This paper presents the findings of a small-scale pilot study contrasting static monoscopic depth cues to ascertain their importance in perceiving the orientation of a three-dimensional glyph with respect to a cutting plane. A simple striped texture and shading were found to reduce judgement errors when used with a 3D tube glyph as compared to plain or shaded line glyphs. A discussion of considerations for a full-scale study and possible future work follows.", "uri": "https://vimeo.com/136254319", "name": "VIS15 preview: Visualizing 3D Flow through Cutting Planes", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:08:54+00:00", "description": "[Poster] \n\nAuthors: Fangyan Zhang, Song Zhang, Pak Chung Wong\n\nAbstract: Effectively visualizing large graphs containing millions of nodes and edges is challenging. Simplification algorithms developed to observe and analyze large graphs in a more feasible manner are indispensable for this task. Several varieties of sampling approaches for graph simplification have been proposed. It is still an open question, however, which single sampling technique produces the best representative sample. The goal of this paper is to evaluate commonly used sampling methods through a combined visual and statistical comparison. The visual comparison is facilitated by employing a fixed graph layout. The resulting benchmark can be incorporated into graph visualization and analysis tools.", "uri": "https://vimeo.com/136254257", "name": "VIS15 preview: A Visual and Statistical Benchmark for Graph Sampling Methods", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:08:30+00:00", "description": "[Poster] \n\nAuthors: Windhager Florian, Schreder G\u00fcnther, Smuc Michael, Mayr Eva\n\nAbstract: The use of information visualizations has been investigated with focus on perception, interaction and sensemaking aspects. But how do users organize internal representations of abstract, multidimensional data on a global level? And which design recommendations could be derived for corresponding external representations - especially for visualization interfaces making use of multiple coordinated views? We build on the theory of mental models and discuss consequences for the design of visualization interfaces. 'Mental models' are internal representations of external realities and were investigated - beside others - in problem solving, text comprehension and spatial orientation. A coherent mental model integrates different aspects and perspectives proportionally and allows perspective-taking, reorientation, and inferences generation. In contrast, if information integration fails, a cognitive collage results, that is, a distorted, fragmentary mix-up of partial information. The conceptual coherence of external representations is a critical factor for the successful creation of a coherent mental model. Especially for multidimensional data, it is a challenge to design conceptually consistent interfaces of multiple coordinated views so that they support local and global coherence. We explore and assemble different design methods to support the construction and elaboration of mental models of abstract data.", "uri": "https://vimeo.com/136254220", "name": "VIS15 preview: Drawing Things Together: Supporting Information Visualizations' Coherence across Multiple Views", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:08:18+00:00", "description": "[Poster] [BEST POSTER]\n\nAuthors: Alexander Bock, Michal Marcinkowski, Joakim Kilby, Carter Emmart, Anders Ynnerman\n\nAbstract: This work presents a visualization system and its application to space missions. The system allows the public to disseminate the scientific findings of space craft and gain a greater understanding thereof. Instruments\u2019 field-of-views and their measurements are embedded in an accurate 3 dimensional rendering of the solar system to provide context to past measurements or the planning of future events. We tested our system with NASA\u2019s New Horizons at the Pluto Pallooza event in New York and will expose it to the greater public on the upcoming July 14th Pluto flyby.", "uri": "https://vimeo.com/136254211", "name": "VIS15 preview: OpenSpace: Public Dissemination of Space Mission Profiles", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:06:55+00:00", "description": "[Poster] \n\nAuthors: Jillian Aurisano, Amruta Nanavaty, Isabel F. Cruz\n\nAbstract: We present AlignmentVis, a visualization tool that supports users in the process of interactive ontology matching, which is a central data integration component. AlignmentVis enables tasks such as the exploration of the resulting mappings, the assessment of the performance of the matching algorithms, and the identification of sets of ontology entities that share similar matching characteristics with the purpose of facilitating the diagnosis of matching errors and of algorithm optimization. We offer a user-centric application design to enable these tasks through multiple coordinated views.", "uri": "https://vimeo.com/136254110", "name": "VIS15 preview: AlignmentVis: Visual Analytics for Ontology Matching", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:06:44+00:00", "description": "[Poster] \n\nAuthors: Jessica Peter, Steve Szigeti, Sara Diamond\n\nAbstract: The interactive visualization of topic models is a promising approach to summarizing large sets of textual data. Topicks is the working title for a means to visualize topic modelling outputs. Incorporating a radial layout, users can view the relationships between topics, terms and the corpus as a whole. Interacting with topic and term nodes, as well as a related bar chart, provides the user with various ways to manipulate the visualization and explore the data. We describe the visualization and potential user interactions before discussing future work.", "uri": "https://vimeo.com/136254101", "name": "VIS15 preview: Topicks: Visualizing Complex Topic Models for User Comprehension", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:05:17+00:00", "description": "[Poster] \n\nAuthors: Benafsh Husain, Alfredo Gime\u00cc\u0081nez, Joshua Levine, Todd Gamblin, Peer-Timo Bremer\n\nAbstract: Developing efficient simulation codes on HPC systems is a challenging task, requiring a careful analysis of the interplay between software and hardware. We investigate memory access patterns by developing visual representations of fine-grained memory access data. Specifically, we examine how function call graphs can be linked with underlying CPU hardware architecture layouts. These views assist in diagnosing performance problems in real world applications. As an exemplar, we consider LULESH, a hydrodynamics simulation code. Our technique allows programmers to link in- efficient memory access patterns from the source code to the under- lying architecture.", "uri": "https://vimeo.com/136253988", "name": "VIS15 preview: Visualizing Fine-Grained Memory Accesses using Linked Software and Hardware Views", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:04:05+00:00", "description": "[Poster] \n\nAuthors: Jean-Daniel Fekete\n\nAbstract: Reordering, also known as seriation, and linear ordering, consists in assigning an order to rows/columns of a table or to vertices of a network to reveal structures. When visualizing a table as a heatmap or when visualizing a network as an adjacency matrix, finding a good order is essential to revealing high-level patterns in the data. The literature on reordering algorithms is dense but few implementations are easily available. Reorder.js is a novel JavaScript library, provided in source code with a BSD license, which provides several of these algorithms and variants to facilitate the use, adoption, and experimentation of these algorithms. Reorder.js is used in several existing Web-based applications such as Bertifier, NodeTrix, and Multipiles. It can be used with e.g. D3.js for adjacency matrices, Heatmaps, and Parallel-coordinates.", "uri": "https://vimeo.com/136253887", "name": "VIS15 preview: Reorder.js: a JavaScript Library to Reorder Tables and Networks", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:03:06+00:00", "description": "[Poster] \n\nAuthors: Bharathi Asokarajan, June Abbas, Sam Huskey, Chris Weaver\n\nAbstract: Classics scholars working on ancient manuscripts view each text as a treasure trove of puzzles waiting to be solved. With the help of interactive features and query capabilities, new visualization designs can help scholars improve the efficiency of analysis and quickly identify patterns and distinct characteristics of details in the text. We are developing a new pixel-based text visualization techniques to summarize and analyze the complex structure of primary texts with their rich apparatus metadata in an accessible and comparable fashion. In this poster we will present how pixel-based representations can link the influences of past manuscripts on a scholar\u00e2\u20ac\u2122s reproduction of lost ancient texts. We will also describe our ongoing work to incorporate new focus+context and other advanced features to help scholars drill deeply into texts.", "uri": "https://vimeo.com/136253820", "name": "VIS15 preview: Poster: Pixel-oriented Visualization for Analyzing Classical Latin Texts", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T22:01:29+00:00", "description": "[Poster] [HONORABLE MENTION]\n\nAuthors: Alex Godwin, John Stasko\n\nAbstract: Spatial data is frequently used in urban planning contexts to support decision-making. This data can be used to plan routes through the city for public transportation or to designate zones for planning new businesses and residences. Unfortunately, few tools exist that allow non-programmers to rapidly utilize spatial data to create such plans. In this poster, we present SpaceSketch, a tool for rapidly constructing map-based visualizations using spatial data. SpaceSketch utilizes a sketch-based approach to visualization specification in which hand-drawn trajectories and regions are encoded using a user-specified model. Using SpaceSketch, map-based plans can be rapidly constructed, compared, and altered on a multitouch canvas.", "uri": "https://vimeo.com/136253707", "name": "VIS15 preview: Drawing Data on Maps: Sketch-Based Spatiotemporal Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:59:58+00:00", "description": "[Poster] \n\nAuthors: Miriam Perkins, Yanlai Chen\n\nAbstract: The Centers for Medicare and Medicaid Services (CMS) has made public a data set showing what hospitals charged and what Medicare paid for the one hundred most common inpatient stays. Here we present the application of Reduced Basis Decomposition (RBD), an efficient novel dimension reduction algorithm for data processing, to the CMS data. This was paired with a comparative visual exploration of the results when put into context with characteristics of the hospitals and marketplaces in which they operate. We used Weave Analyst, a new web-based analysis and visualization environment, to visualize the relationship between the hospital groups, their charge levels, and distinguishing indicator variables. Particular insights to the relatively small number of underlying factors that exert greatest influence on hospital pricing surfaced thanks to the combined synergetic integration of the modeling, reduction, and visualization techniques.", "uri": "https://vimeo.com/136253562", "name": "VIS15 preview: Using Visualization and Analysis with Efficient Dimension Reduction to Determine Underlying Factors in...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:59:08+00:00", "description": "[Poster] \n\nAuthors: Eugenia Lee\n\nAbstract: As information visualisation enters mainstream use in data journalism, a conflict exists between purist theories of visualisation and the emerging visual discourses of the networked news media. To explore this threshold and how techniques of visual storytelling can translate scientific data to a general non-expert audience, I aim to conduct a multimodal analysis on both static and interactive online visualisations of climate change. To do so, the research aims to formulate a novel method of evaluation by critiquing and comparing the dominant formations within the disciplines of journalism studies and information visualisation, particularly Kress and Van Leeuwen\u2019s multimodal theory and Tufte\u2019s information visualisation principles. This research argues that an interdisciplinary analysis is necessary to offer theorisations of the practice but also to explore the concept of stories in data.", "uri": "https://vimeo.com/136253510", "name": "VIS15 preview: Stories in the Data: A Multimodal Analysis on the Threshold of Data Journalism and Narrative Visualisation", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:58:46+00:00", "description": "[Poster] [HONORABLE MENTION]\n\nAuthors: Zuchao Wang, Xiaoru Yuan, Tangzhi Ye, Youfeng Hao, Siming Chen, Jie Liang, Qiusheng Li, Haiyang Wang, Yadong Wu\n\nAbstract: We present a novel visual analysis method to systematically discover data quality problems in raw taxi GPS data. It combines semi-supervised active learning and interactive visual exploration. It helps analysts interactively discover unknown data quality problems, and automatically extract known problems. We report analysis results on Beijing taxi GPS data.", "uri": "https://vimeo.com/136253474", "name": "VIS15 preview: Visual Data Quality Analysis for Taxi GPS Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:58:06+00:00", "description": "[Poster] \n\nAuthors: Kaisa Lahtinen, Aidan Slingsby, Jason Dykes, Sarah Butt, Rory Fitzgerald\n\nAbstract: Through an ongoing process of co-design and co-discovery we are developing and using visualization to explore large amounts of auxiliary data from unfamiliar sources to understand non-response bias in social surveys. We present auxiliary data in their geographical contexts and show how this can complement traditional data analysis and provide a more comprehensive understanding of the data. This is helping select variables for non-response modelling. These processes are not just limited to non-response analysis, but have potential to be used in wider quantitative analysis in social science.", "uri": "https://vimeo.com/136253403", "name": "VIS15 preview: Informing Non-Response Bias Model Creation in Social Surveys with Visualisation", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:57:13+00:00", "description": "[Poster] \n\nAuthors: Shejuti Silvia, June Abbas, Sam Huskey, Chris Weaver\n\nAbstract: Storyline visualizations help users understand and analyze the interactions between entities in a story and explore how entity relationships evolve over time. Existing storyline techniques support limited or no user interaction due to the high cost of layout. Typical design considerations for storyline layout include minimizing line crossing and line wiggling, which are NP-hard or NP-complete problems. Interactive generation of layouts, such as in response to dynamic querying, is a substantial performance challenge. Motivated by application to visualization of classical Latin texts, we present work in progress on a new approach that uses force directed layout to dynamically position storyline elements in an agile, legible fashion in real time. We outline how this approach can support appealing layout even in response to diverse user interactions including rapid panning and zooming and dynamic filtering of storyline elements.", "uri": "https://vimeo.com/136253317", "name": "VIS15 preview: Storyline Visualization with Force Directed Layout", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:57:03+00:00", "description": "[Poster] \n\nAuthors: Min Lu, Chufan Lai, Tangzhi Ye, Jie Liang, Xiaoru Yuan\n\nAbstract: There are often multiple routes between regions. Many factors potentially affect driver's route choice, such as expected time cost, length etc. In this work, we present a visual analysis system to explore driver's route choice behaviour based on taxi GPS trajectory data. With interactive trajectory filtering, the system constructs feasible routes between regions of interest. Using a rank-based visualization, the attributes of multiple routes are explored and compared. Based on a statistical model, the system supports to verify trajectory-related factors' impact on route choice behaviour. The effectiveness of the system is demonstrated by applying to real trajectory dataset.", "uri": "https://vimeo.com/136253307", "name": "VIS15 preview: Visual Analysis of Route Choice Behaviour based on GPS Trajectories", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:56:51+00:00", "description": "[Poster] \n\nAuthors: Masahiko Itoh, Daisaku Yokoyama, Masashi Toyoda\n\nAbstract: It is vital for the transportation industry, which performs most of its work by automobiles, to reduce its accident rate. This paper proposes a 3D visual interaction method for exploring caution areas from large-scale vehicle recorder data. Our method provides (i) a flexible filtering interface for driving operations such as braking or handling operations by various combinations of their attribute values such as velocity and acceleration, and (ii) a 3D visual environment for spatio-temporal exploration of caution areas. The proposed method was able to extract caution areas where some accidents have actually occurred or that are on very narrow roads with bad visibility by using real data given by one of the biggest transportation companies in Japan.", "uri": "https://vimeo.com/136253292", "name": "VIS15 preview: A System for Visual Exploration of Caution Spots from Drive Recorder Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:56:13+00:00", "description": "[Poster] \n\nAuthors: Jihye Lee, Geon Hur, Yongkyun Lee, Yerin Ga, LeeKyung Hong, Kyungwon Lee\n\nAbstract: Given that staying informed about the trends within a field and analyzing them is critical to researchers, related prior researches centered on statistical analyses of the cumulative body of theses on specific topics have existed before. This research takes a step further, collating and visualizing topic words from papers published in Korea during the designated years to find interdisciplinary research topics dealt with by major disciplines. The outcome of this research will prove useful to researchers in Korea allowing them to discover the extent to which their own research topic is studied in other disciplines. Keyword data from two years, 2010 and 2012 has been visualized and select keywords have been extracted from the data pool to be analyzed in greater detail through our case study.", "uri": "https://vimeo.com/136253235", "name": "VIS15 preview: Research Trend Case Study: For Understanding Interdisciplinary Keywords in South Korea", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:55:12+00:00", "description": "[Poster] \n\nAuthors: Richen Liu, Hanqi Guo, Xiaoru Yuan\n\nAbstract: Most of the existing approaches to visualize vector field ensembles are achieved by visualizing the uncertainty of individual variables from different simulation runs. However, the comparison of the derived feature or user-defined feature, such as the vortex in ensemble flow is also of vital significance since they often make more sense according to the domain knowledge. In this work, we present a framework to extract user-defined feature from different simulation runs. Specially, we use a bottom-up searching scheme to help to extract vortex with a user-defined shape, and further compute the geometry information including the size, and the geo-spatial location of the extracted vortex. Finally we design some linked views to compare the feature between different runs.", "uri": "https://vimeo.com/136253136", "name": "VIS15 preview: A Bottom-Up Scheme for User-Defined Feature Exploration in Vector Field Ensembles", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:54:57+00:00", "description": "[Poster] \n\nAuthors: Surendar Nambirajan, Ronald Metoyer, Sachin Pandya\n\nAbstract: Attorney behavior is of interest to law researchers who often view lawyers as managers of case portfolios. They are interested in the makeup of these portfolios and how they evolve over time. We have designed an interactive visualization tool for analyzing attorney litigation portfolios using a diversity index as the primary metric for comparison of portfolios. We employ Munzner's nested model to guide our visualization design process and present the components of our in-progress design in this poster submission.", "uri": "https://vimeo.com/136253116", "name": "VIS15 preview: Visual Analysis of Attorney Portfolio Diversity", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:54:43+00:00", "description": "[Poster] [HONORABLE MENTION]\n\nAuthors: Jillian Aurisano, Abhinav Kumar, Alberto Gonzales, Khairi Reda, Jason Leigh, Barbara Di Eugenio, Andrew Johnson\n\nAbstract: Visual data exploration poses challenges for 'InfoVis Novices'. A 'conversational interface' which would enable users to generate and interact with visualizations through natural language and gestures, while maintaining a history of the data exploration, has the potential to ameliorate many of these challenges. We performed an exploratory, observational study designed to examine the role of such a conversational interface in visual data exploration. We simulated a conversational interface, using a remote human mediator, with multiple cycles of visualization construction. We believe analysis of this data will yield concrete design goals for conversational interfaces in information visualization.", "uri": "https://vimeo.com/136253095", "name": "VIS15 preview: \"Show me data.\" Observational study of a conversational interface in visual data exploration.", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:54:28+00:00", "description": "[Poster] \n\nAuthors: Tanyoung Kim\n\nAbstract: Beer is good and fun to explore. With a self-logging smart phone application (app) \u201cUntappd,\u201d people can check-in when they drink beer with their own rating of the beer, retrieving the location information, and the metadata of the beer including style, alcohol percentage, and brewery. Harnessing these personal beer drinking data, we develop a web app called \u201cBeer Match\u201d that analyzes single users\u2019 beer taste and drinking behavior and compares two users to generate new datasets. The app showcases two views, Single and Match, each of which has a series of visualizations. Beer Match enables users to learn more about their everyday life around an interesting but otherwise hidden subject and to examine the comparison between their and others\u2019 behavior.", "uri": "https://vimeo.com/136253070", "name": "VIS15 preview: Comparative Visualization of Personal Beer Taste", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:54:13+00:00", "description": "[Poster] \n\nAuthors: Emily Grimes, Chris Weaver\n\nAbstract: Our work attempts to bridge a gap in understanding about the essential forms and functions of interaction in visualization. Here, we introduce our first step toward constructing an intricate model of data interaction as it is applied in information visualization tool designs in research and practice. The primary purpose of this model is to inform a declarative language of data gesture specification that can be applied in visualization design in much the same way as languages for specifying visual representations and encodings. As a means to this end, we are conducting a comprehensive survey of interaction in the literature on visualization. Here, we present our approach for conducting the survey and for characterizing interactions in detail from their descriptions in the literature. We feature an overview of several representative and remarkable selection techniques from our review thus far.", "uri": "https://vimeo.com/136253049", "name": "VIS15 preview: Poster: Examining the Many Faces of Selection", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:53:22+00:00", "description": "[Poster] \n\nAuthors: Charles Perin, Sheelagh Carpendale\n\nAbstract: Members understanding the organization of a community is useful for any community. Correspondingly this applies to researchers involved in the Infovis community. We present Who Rules Infovis?, a visualization designed to be a static infographic showing the temporal evolution of the internal organization of the Infovis conference track. It shows all persons who have been a chair or in a related committee over the past 20 years and makes it more possible to understand the internal organization of the community.", "uri": "https://vimeo.com/136253002", "name": "VIS15 preview: Unwrapping Infovis Organization", "year": "2015", "event": "INFOVIS, PREVIEW"}, {"created_time": "2015-08-13T21:51:38+00:00", "description": "[Poster] \n\nAuthors: Jonas Parnow, Marian D\u00f6rk\n\nAbstract: At the intersection of information visualization and typography lies the design space of micro visualization, a family of basic techniques enriching text in regard of its accessibility, comprehensibility, and memorability. We propose a taxonomy that differentiates specific types of visualizations applied to text design and layout. We elaborate two main approaches to aligning the visual appearance of a text and its content. The first explores the addition of graphical elements embedded into or adjacent to a text, while the other approach explores the visual modification of a text by means of typographic visualization. For this we evaluate how different techniques can be used as visual variables.", "uri": "https://vimeo.com/136252870", "name": "VIS15 preview: Micro Visualizations: Data-driven typography and graphical text enhancement", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:51:18+00:00", "description": "[Poster] \n\nAuthors: Hyunjoo Song, Jeongjin Lee, Tae Jung Kim, Kyoung Ho Lee, Bohyoung Kim, Jinwook Seo\n\nAbstract: Comparative gaze analysis has been performed widely with a diversity of eye tracking data visualizations. In the medical field, volumetric images from CT and MRI have been used as stimuli in eye tracking studies. However, prior work on such images focused mostly on effective visualization of eye tracking data only with either spatial or temporal aspects to resolve complexity stemming from the additional spatial dimension of stimuli. Thus, the proposed visual representations suffer from a neglected aspect and lack of interactivity in the comparison of eye tracking data. In this paper, a context-embedded interactive scatterplot (CIS) with other scatterplot-based visual representations is introduced for comparative analysis. Furthermore, additional crucial contextual information during diagnosis (e.g., brightness and contrast of image) is incorporated. Case studies with chest and abdominal radiologists were conducted to prove the effectiveness of the CIS.", "uri": "https://vimeo.com/136252837", "name": "VIS15 preview: Coordinated Interactive Scatterplots for Comparative Gaze Analysis with Volumetric Medical Images", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:50:28+00:00", "description": "[Poster] [HONORABLE MENTION]\n\nAuthors: Mathias Goldau, Andr\u00e9 Reichenbach, Mario Hlawitschka\n\nAbstract: Diffusion weighted magnetic resonance imaging (dMRI) together with tractography algorithms allow to probe for principal white matter tracts in the living human brain. Specifically, probabilistic tractography quantifies the existence of physical connections to a given seed region as a 3D scalar map of confidence scores. Fiber-Stippling is a visualization for probabilistic tracts that effectively communicates the diffusion pattern, connectivity score, and anatomical context. Unfortunately, it cannot handle multiple diffusion orientations per voxel, which exist in high angular resolution diffusion imaging (HARDI) data. Such data is needed to resolve tracts in complex configurations, such as crossings. In this work, we suggest a visualization based on Fiber-Stippling but sensible to multiple diffusion orientations from HARDI-based diffusion models. With such a technique, it is now possible to visualize probabilistic tracts from HARDI-based tractography algorithms. This implies that tract crossings may now be visualized as crossing stipples, which is an essential step towards an accurate visualization of the neuroanatomy, as crossing tracts are widespread phenomena in the brain.", "uri": "https://vimeo.com/136252762", "name": "VIS15 preview: Visualizing Crossing Probabilistic Tracts", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:49:58+00:00", "description": "[Poster] \n\nAuthors: Omniah Nagoor, Markus Hadwiger, Madhusudhanan Srinivasan\n\nAbstract: PathlinesExplorer is a novel image-based tool, which has been designed to visualize large scale pathline fields on a single computer. PathlinesExplorer integrates explorable images (EI) technique with order-independent transparency (OIT) method. What makes this method different is that it allows users to handle large data on a single workstation. Although it is a view-dependent method, PathlinesExplorer combines both exploration and modification of visual aspects without re-accessing the original huge data. Our approach is based on constructing a per-pixel linked list data structure in which each pixel contains a list of pathline segments. With this view-dependent method, it is possible to filter, color-code, and explore large-scale flow data in real-time. In addition, optimization techniques such as early-ray termination and deferred shading are applied, which further improves the performance and scalability of our approach.", "uri": "https://vimeo.com/136252731", "name": "VIS15 preview: PathlinesExplorer Image-based Exploration of Large-Scale Pathline Fields", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:48:23+00:00", "description": "[Poster] \n\nAuthors: Roger Beecham, Jason Dykes, Aidan Slingsby, Cagatay Turkay\n\nAbstract: We describe and discuss a visual analysis prototype to support volume crime analysis, a form of exploratory data analysis that aims to identify and describe patterns of criminality using historical and recent crime reports. Analysis requirements are relatively familiar: analysts wish to identify, define and compare sets of crime reports across multiple attributes (space, time and description). A challenge particular to the domain, identified through workshops with Police analysts in Belgium and the UK, is in developing exploratory data analysis software that offers some sophistication in data selection, aggregation and comparison, but with interaction techniques and representations that can be easily understood, navigated and communicated. In light of ongoing discussion with Police analysts, we propose four visual design and interaction maxims that relate to this challenge and discuss an early visual analysis prototype that we hope conforms to these maxims.", "uri": "https://vimeo.com/136252627", "name": "VIS15 preview: Supporting crime analysis through visual design", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:47:32+00:00", "description": "[Poster] \n\nAuthors: Chris Rooney, Roger Beecham, Jason Dykes, Cagatay Turkay, Aidan Slingsby, Jo Wood, William Wong\n\nAbstract: Many datasets have multiple perspectives \u2013 e.g. time, space and description \u2013 and often summaries are required for these multiple perspectives concurrently. A design challenge is to provide such a concurrent summary of perspectives in ways that neither clutter nor visually overload. We report on a framework that helps us do this. We demonstrate its use with an implementation based on a Crime Pattern Analysis usage scenario that produces synoptic summarises of spatial, temporal and descriptive information in crime reports. Our work with crime analysts suggests that the framework offers some potential for Crime Pattern Analysis.", "uri": "https://vimeo.com/136252553", "name": "VIS15 preview: Multi-Perspective Synopsis with Faceted Views of Varying Emphasis", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:46:27+00:00", "description": "[Poster] \n\nAuthors: Prithiviraj Muthumanickam, Katerina Vrotsou, Matthew Cooper, Jimmy Johansson\n\nAbstract: Searching for all possible patterns in a time series graph is a computationally complex problem. User-sketched pattern matching is an effective semi-automatic approach to address this problem but the search space is still very large the accuracy of the pattern search must be considered. Our method, SMART series, uses a ratio-based approximation of the raw time series and transforms it into a symbolic representation. The user can then draw patterns of interest in a separate sketching space and search, in real time, for matches within the symbolic space. Accuracy relaxation in the matching is provided through a further reduction of the symbolic space.", "uri": "https://vimeo.com/136252471", "name": "VIS15 preview: SMART Series: Sketch-based Matching through Approximated Ratios in Time Series", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:46:01+00:00", "description": "[Poster] \n\nAuthors: Cameron C. Gray, Jonathan C. Roberts, Panagiotis D. Ritsos\n\nAbstract: Network administrators often wish to ascertain where network attackers are located; therefore it would be useful to display the network map from the context of either the attacker's potential location or the attacked host. As part of a bigger project we are investigating how to best visualize contextual network data. We use a dataset of station adjacencies with journey times as edge weights, to explore which visualization design is most suitable, and also ascertain the best network shortest-path metric. This short paper presents our initial findings, and a visualization for Contextual Navigation using circular, centered-phylogram projections of the network. Our visualizations are interactive allowing users to explore different scenarios and observe relative distances in the data.", "uri": "https://vimeo.com/136252426", "name": "VIS15 preview: Where Can I Go From Here? Drawing Contextual Navigation Maps of the London Underground", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:45:31+00:00", "description": "[Poster] [BEST POSTER]\n\nAuthors: Markus B\u00f6gl, Wolfgang Aigner, Peter Filzmoser, Theresia Gschwandtner, Tim Lammarsch, Silvia Miksch, Alexander Rind\n\nAbstract: Missing values are a problem in many real world applications, for example failing sensor measurements. For further analysis these missing values need to be imputed. Thus, imputation of such missing values is important in a wide range of applications. We propose a visually and statistically guided imputation approach, that allows applying different imputation techniques to estimate the missing values as well as evaluating and fine tuning the imputation by visual guidance. In our approach we include additional visual information about uncertainty and employ the cyclic structure of time inherent in the data. Including this cyclic structure enables visually judging the adequateness of the estimated values with respect to the uncertainty/error boundaries and according to the patterns of the neighbouring time points in linear and cyclic (e.g., the months of the year) time.", "uri": "https://vimeo.com/136252390", "name": "VIS15 preview: Visually and Statistically Guided Imputation of Missing Values in Univariate Seasonal Time Series", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:45:19+00:00", "description": "[Poster] [HONORABLE MENTION]\n\nAuthors: Holger Stitz, Samuel Gratzl, Wolfgang Aigner, Marc Streit\n\nAbstract: Multi-attribute time-series data plays a vital role in many different domains. An important task when making sense of such data is to provide users with an overview to identify items that show an interesting development over time. However, this is not well sup- ported by existing visualization techniques. To address this issue, we present ThermalPlot, a visualization technique that summarizes complex combinations of multiple attributes over time using an item\u2019s position, the most salient visual variable. More precisely, the x-position in the ThermalPlot is based on a user-defined degree-of- interest (DoI) function that combines multiple attributes over time. The y-position is determined by the relative change in the DoI value (\u2206DoI) within a user-specified time window. Animating this map- ping via a moving time window gives rise to circular movements of items over time\u2014as in thermal systems. To help the user to iden- tify important items that match user-defined temporal patterns and to increase the technique\u2019s scalability, we adapt the items\u2019 level of detail based on the DoI value. We demonstrate the effectiveness of our technique in a stock market usage scenario.", "uri": "https://vimeo.com/136252376", "name": "VIS15 preview: ThermalPlot: Visualizing Multi-Attribute Time-Series Data Using a Thermal Metaphor", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:44:44+00:00", "description": "[Poster] \n\nAuthors: Scott Houde, Shelia Bonde, David H. Laidlaw\n\nAbstract: This project explores the representation of uncertainty in visualizations for archaeological research and provides insights obtained from user feedback. Our 3D models brought together information from standing architecture and excavated remains, surveyed plans, ground penetrating radar (GPR) data from the Carthusian monastery of Bourgfontaine in northern France. We also included information from comparative Carthusian sites and a bird's eye representation of the site in an early modern painting. Each source was assigned a certainty value which was then mapped to a color or texture for the model. Certainty values between one and zero were assigned by one subject matter expert and should be considered qualitative. Students and faculty from the fields of architectural history and archaeology at two institutions interacted with the models and answered a short survey with four questions about each. We discovered equal preference for color and transparency and a strong dislike for the texture model. Discoveries during model building also led to changes of the excavation plans for summer 2015.", "uri": "https://vimeo.com/136252312", "name": "VIS15 preview: An Evaluation of Three Methods for Visualizing Uncertainty in Architecture and Archaeology", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:43:41+00:00", "description": "[Poster] \n\nAuthors: Yi Du, Danhuai Guo, Qianyu Liu, Yuanchun Zhou, Jianhui Li\n\nAbstract: We present DVIZ, a visualization generation system based on DVDL. This system allows users to select data source, configure visualization properties, publish and share result. It also supports real-time generation and interaction between multiple visualization. Besides, DVIZ is web-based, and supports various social platforms for easy distribution of result.", "uri": "https://vimeo.com/136252208", "name": "VIS15 preview: DVIZ: A Model-driven Visualization Generation System", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:43:12+00:00", "description": "[Poster] \n\nAuthors: Veronika Solteszova, Marco Foscato, Sondre Eliasson, Vidar Jensen\n\nAbstract: De novo design is a computational-chemistry method, where a computer program utilizes an optimization method, in our case an evolutionary algorithm, to design compounds with desired chemical properties. The optimization is performed with respect to a quantity called fitness, defined by the chemists. We present a tool that connects interactive visual analysis and evolutionary algorithm-based molecular design. We employ linked views to communicate different aspects of the data: the statistical distribution of molecule fitness, connections between individual molecules during the evolution and 3D molecular structure. The application is already used by chemists to explore and analyze the results of their evolution experiments and has proved to be highly useful.", "uri": "https://vimeo.com/136252181", "name": "VIS15 preview: Evolution Inspector: Interactive Visual Analysis for Evolutionary Molecular Design", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:42:49+00:00", "description": "[Poster] \n\nAuthors: Michael M.A. Wu, Tamara Munzner\n\nAbstract: Eye tracking is becoming widely used in HCI and many other fields to study user behaviour. Eye tracking data explains user attention patterns in great detail, but analysis is a challenge because of its volume and complexity. Various visualizations have been designed to aid such data analysis, but none of them focuses on the sequential patterns in eye gaze, which can reveal insights in user behaviours. We present the SEQIT visualization system designed for sequence analysis of eye tracking data. Using pre-defined areas of interest (AOI), SEQIT aggregates fixations into AOI visits and presents sequences of AOI visits in a timeline view. It supports comparisons between multiple sequences and exploration of sequence patterns associated with user characteristics.", "uri": "https://vimeo.com/136252149", "name": "VIS15 preview: SEQIT: Visualizing Sequences of Interest in Eye Tracking Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:41:47+00:00", "description": "[Poster] \n\nAuthors: Yi-Chih Tsai, Liang Chi Hsieh, Wen-Feng Cheng, Yin-Hsi Kuo, Winston Hsu, Wen-Chin Chen\n\nAbstract: Although many visualization tools provide us plenty of ways to view the data, users can not easily find the trending events and their explanation from the data. In this work, we address the issue by leveraging the real music streaming log data as an example to better understand a million-scale dataset. Trending event explanation turns out to be challenging when it comes to categorical log data. Therefore, we propose to use a learning-based method with an interface design to uncover the trending event compositions for time-series categorical log data, which can be extend to other datasets, e.g., the hashtags in social media. First, we perform \"trending pool\" operation to save the memory and time cost. Second, we apply sparse coding to learn important trending candidate combination sets instead of traditional brute-force way or manual investigation for generating combinations. Besides the contributions above, we also observe some interesting user behaviors by exploring detected trending candidate combinations visually through our interface.", "uri": "https://vimeo.com/136252060", "name": "VIS15 preview: Trending Pool: Visual Analytics for Trending Event Compositions for Time-Series Categorical Log Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:41:15+00:00", "description": "[Poster] \n\nAuthors: Mar\u00eda Luj\u00e1n Ganuza, Mar\u00eda Florencia Gargiulo, Gabriela Ferracutti, Silvia Mabel Castro, Ernesto Bjerg, Eduard Gr\u00f6ller, Kresimir Matkovic\n\nAbstract: Spinel group minerals are excellent indicators of geological environments (tectonic settings). In 2001, Barnes and Roeder defined a set of contours corresponding to compositional fields for spinel group minerals. Geologists typically use this contours to estimate the tectonic environment where a particular spinel composition could have been formed. This task is prone to errors and requires tedious manual comparison of overlapping diagrams. We introduce a semi-automatic, interactive detection of tectonic settings for an arbitrary dataset based on the Barnes and Roeder contours. The new approach integrates the mentioned contours and includes a novel interaction called contour brush. The new methodology is integrated in the Spinel Explorer system and it improves the scientist's workflow significantly.", "uri": "https://vimeo.com/136252015", "name": "VIS15 preview: Interactive Semi-Automatic Categorization for Spinel Group Minerals", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:40:42+00:00", "description": "[Poster] \n\nAuthors: David Borland, Eugenia McPeek Hinz, Leigh Ann Herhold, Vivian L. West, W. Ed Hammond\n\nAbstract: Understanding how a quantity changes over time for multiple entities is a common task when analyzing time-varying data sets. Various temporal visualization techniques exist, however many are ineffective for large data sets. We introduce the path map, a temporal visualization technique designed to effectively handle data sets with many entities. The path map is a rectangular space with columns representing temporal samples and rows representing individual data entities. Rectangular cells with a single color-mapped value are generated from adjacent rows based on their vertical order. An interactive sorting interface reveals patterns in the data by reordering rows vertically based on their values at user-selected columns. Additional contributions include missing data display and aggregation methods to handle larger data sets. We demonstrate path maps using lab data from over 500 and over 3500 diabetic patients.", "uri": "https://vimeo.com/136251962", "name": "VIS15 preview: Path Maps: Visualization of Trajectories in Large-Scale Temporal Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:40:30+00:00", "description": "[Poster] \n\nAuthors: Haoling Dong, Siliang Tang, Si Li, Fei Wu, Yueting Zhuang\n\nAbstract: Topic model has been an active research area for many years, it can be used for discovering latent semantics and finding hidden knowledge in unstructured data corpus. In this paper, we investigated the problems in visualizing hierarchical topic and their evolution. The contribution of this paper is threefold, first we explore the static visualization of hierarchical topics using the `nested circle' layout, and then in order to present the topic evolution over time, we extended a hierarchical topic model and employ topic transformation visualizations to track the arising, splitting and disappearing of certain topics under the dynamic topical hierarchy. Finally, a Hierarchical Topic Model Visualization System (HTMVS) is designed to take advantage of both static and dynamic hierarchical topic visualization.", "uri": "https://vimeo.com/136251941", "name": "VIS15 preview: HTMVS: Visualizing Hierarchical Topics and Their Evolution", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:39:09+00:00", "description": "[Poster] \n\nAuthors: Samuel Gratzl, Nils Gehlenborg, Alexander Lex, Hendrik Strobelt, Christian Partl, Marc Streit\n\nAbstract: In many science domains data analysis has replaced data acquisition, generation, and storage as the main challenge. This challenge stems not only from volume but also from complexity and heterogeneity of the data. Molecular biology is a prime example for this trend. We identified six key aspects that a visual analysis platform for biological data should support. We address these aspects with the development of Caleydo Web. In this poster we describe its architecture and give an example of how it can be used to create StratomeX.js, a sophisticated visualization technique for cancer subtype characterization.", "uri": "https://vimeo.com/136251810", "name": "VIS15 preview: Caleydo Web: An Integrated Visual Analysis Platform for Biomedical Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:37:39+00:00", "description": "[Poster] \n\nAuthors: Lauren Samuels, Robert Greevy\n\nAbstract: Observational studies are a widely used and challenging class of studies. A key challenge is selecting a study cohort from the available data, or 'pruning' the data, in a way that produces both sufficient balance in pre-treatment covariates and an easily described cohort from which results can be generalized. Even with advanced pruning methods, it is often difficult for researchers to see how the cohort is being selected; consequently, these methods are underutilized in research. Visual Pruner is a free, easy-to-use web application that can improve both the credibility and generalizability of observational studies by letting analysts use updatable visual displays of estimated propensity scores and key baseline covariates to refine inclusion criteria. By helping researchers see how covariate distributions in their data relate to the estimated probabilities of treatment assignment, the app lets researchers make pruning decisions based on pre-treatment covariate patterns that are otherwise hard to discover. The app yields a set of inclusion criteria that can be used in conjunction with further statistical analysis in any statistical software.", "uri": "https://vimeo.com/136251665", "name": "VIS15 preview: Visual Pruner: Visually guided cohort selection for observational studies", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:37:13+00:00", "description": "[Poster] \n\nAuthors: Christian Richter, Martin Luboschik, Martin R\u00f6hlig, Heidrun Schuhmann\n\nAbstract: Exploring and comparing categorical time series and finding temporal patterns are complex tasks in the field of time series data mining. Although different analysis approaches exist, these tasks remain challenging, especially when numerous time series are considered at once. We propose a visual analysis approach that supports exploring such data by ordering time series in meaningful ways. We provide interaction techniques to steer the automated arrangement and to allow users to investigate patterns in detail.", "uri": "https://vimeo.com/136251606", "name": "VIS15 preview: Sequencing of Categorical Time Series", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:36:56+00:00", "description": "[Poster] \n\nAuthors: Emily S. Cramer, Lyn Bartram, Jill N. Warren\n\nAbstract: Research into the potential for \u201csketchy\u201d visualizations to communicate uncertainty or encourage engagement has yielded mixed results, suggesting a role for context in users' reactions to sketchiness. To explore these effects, we conducted two mixed-methods studies that compared presenters\u2019 and recipients' interpretations of sketchiness across sixteen presentation contexts. We found that presenters and recipients disagree about which contexts warrant sketchy visualizations. Our findings motivate and can guide a closer investigation of the contextual-sensitivity of user reactions to alternate rendering styles.", "uri": "https://vimeo.com/136251587", "name": "VIS15 preview: Discrepancies in the Intention and Interpretation of Sketchy Visualizations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:36:35+00:00", "description": "[Poster] \n\nAuthors: Peter Polack, Shang-Tse Chen, Minsuk Kahng, Moushumi Sharmin, Duen Chang\n\nAbstract: Whereas event-based timelines for healthcare enable users to visualize the chronology of events surrounding events of interest, they are often not designed to aid the discovery, construction, or comparison of associated cohorts. We present TimeStitch, a system that helps health researchers discover and understand events that may cause abstinent smokers to lapse. TimeStitch extracts common sequences of events performed by abstinent smokers from large amounts of mobile health sensor data, and offers a suite of interactive and visualization techniques to enable cohort discovery, construction, and comparison, using extracted sequences as interactive elements. We are extending TimeStitch to support more complex health conditions with high mortality risk, such as reducing hospital readmission in congestive heart failure.", "uri": "https://vimeo.com/136251565", "name": "VIS15 preview: TimeStitch: Interactive Multi-focus Cohort Discovery and Comparison", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:36:14+00:00", "description": "[Poster] \n\nAuthors: Najmeh Abedzadeh, J. Edward Swan, Andrew Mercer\n\nAbstract: One of the most vital challenges for weather forecasters is the correlation between two geographical phenomena that are distributed continuously in multidimensional multivariate time-varying datasets. In this research, we have visualized the correlation between Pressure and Temperature in the climate datasets. Pearson correlation is used in this study to measure the major linear relationship between two variables in the dataset. Using glyphs in the spatial location, we highlighted the significant association between variables. Based on the positive or negative slope of correlation lines, we can conclude how much they are correlated. The principal of this research is visualizing the local trend of variables versus each other in multidimensional multivariate time-varying datasets, which needs to be visualized with their spatial locations in meteorological datasets. Using glyphs, not only can we visualize the correlation between two variables in the coordinate system, but we can also discern whether any of these variables is separately increasing or decreasing. Moreover, we can visualize the background color as another variable and see the correlation lines around of a particular zone such as storm area.", "uri": "https://vimeo.com/136251542", "name": "VIS15 preview: Correlation Analysis in Multidimensional Multivariate Time-varying Datasets", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:35:32+00:00", "description": "[Poster] \n\nAuthors: Yen-Chia Hsu, Paul Dille, Randy Sargent, Christopher Bartley, Illah Nourbakhsh\n\nAbstract: Scientists, journalists, and photographers have used advanced camera technology to capture extremely high resolution timelapses and developed information visualization tools for data exploration and analysis. However, it takes a great deal of effort for professionals to form and tell stories after exploring data, since these tools usually provide little aids in creating visual elements. We present a web-based timelapse editor to support the creation of guided video tours and interactive slideshows from a collection of large-scale spacial and temporal images. Professionals can embed these two visual elements into webpages in conjunction with various forms of digital media to tell multimodal and interactive stories.", "uri": "https://vimeo.com/136251467", "name": "VIS15 preview: A Web-based Large-scale Timelapse Editor for Creating and Sharing Guided Video Tours and Interactive...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:10:05+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Shimin Wang, Yuzuru Tanahashi, Nick Leaf, Kwan-Liu Ma\n\nAbstract: This paper presents a study on how users react to different personal visualization designs. First, we present three distinct personal visualization designs: Timeline, Spark, and Bouquet, for visualizing Facebook user data. Each of these three respectively embodies a specific style of visualization \u2013 traditional, illustrative, and artistic visualization. Next, we conduct a user study consisting of a series of interviews. These interviews were based on three scenarios: 1) exploring the visualization of the users\u2019 own data, 2) comparing two personal visualizations and 3) analyzing a series of personal visualizations. Our analysis of the recorded conversations suggests that the illustrative design was the most well balanced of all three designs with respect to relaying information, motivating data exploration, and providing personal insights to the users. We believe this study will provide future developers with some guidance into what elements need to be taken into consideration when designing personal visualizations.", "uri": "https://vimeo.com/136249228", "name": "VIS15 preview: Design and Effects of Personal Visualizations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:09:07+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Kuno Kurzhals, Daniel Weiskopf\n\nAbstract: In many research fields, eye tracking has become an established method to analyze the distribution of visual attention in various scenarios. With the trend toward increasingly affordable and easy\u00adto\u00aduse consumer hardware, we expect mobile eye tracking to become ubiquitous, recording massive amounts of gaze data on a regular basis in everyday personal situations. To make use of this data, new approaches for personal visual analytics will be necessary to make the data accessible for non\u00adexpert users for self\u00adreflection and re\u00adexperiencing interesting events. We discuss how eye tracking fits in the context of personal visual analytics, the challenges that arise with its application to everyday situations, and the research perspectives of personal eye tracking. Therefore, the extraction and representation of areas of interest (AOIs) in the recorded data is a crucial part of data processing. We present a new technique to represent these AOIs from multiple videos: the AOI cloud. In our example, we apply this technique to examine the personal encounters of a user with other persons. The technique provides an accessible user interface that is also applicable to touch devices and therefore suitable for an integration into the everyday life of a user.", "uri": "https://vimeo.com/136249147", "name": "VIS15 preview: Eye Tracking for Personal Visual Analytics", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:08:47+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Wesley Willett, Pascal Goffin, Petra Isenberg\n\nAbstract: We present results and design implications from a study of digital note\u00adtaking practice to examine how visualization can support revisitation, reflection, and collaboration around notes. As digital notebooks become common forms of external memory, keeping track of volumes of content is increasingly difficult. Information visualization tools can help give note\u00adtakers an overview of their content and allow them to explore diverse sets of notes, find and organize related content, and compare their notes with their collaborators. To ground the design of such tools, we conducted a detailed mixed\u00admethods study of digital note\u00adtaking practice. We identify a variety of different editing, organization, and sharing methods used by digital note\u00adtakers, many of which result in notes becoming \"lost in the pile''. These findings form the basis for our design considerations that examine how visualization can support the revisitation, organization, and sharing of digital notes.", "uri": "https://vimeo.com/136249125", "name": "VIS15 preview: Understanding Digital Note-Taking Practice for Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:07:34+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Jo Wood\n\nAbstract: The potential for creating personal visualization of participation in sports cycling is explored as a design study. Examples are provided to show riders\u2019 personal narratives and performance relative to other participants in long\u00addistance cycling events. Minimalist cartographic design is applied in the automatic generation of Profile Maps that allow personal textual narratives to be attached to visualizations of 3d variation in terrain. Changes in relative position and time\u00adin\u00adhand during mass participation events are shown as Position Charts while animations of rider density over time are used to visualize the progress of larger groups of riders in an event. The designs are focussed in representing those aspects of participation that evoked an emotional response in order to engage readers with the visualization. It is argued here that such an emotional connection with the data representation is an important characteristic of effective personal visualization not present in more scientific or professional contexts.", "uri": "https://vimeo.com/136249008", "name": "VIS15 preview: Visualizing Personal Progress in Participatory Sports Cycling Events", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:06:49+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: David H. S. Chung, Matthew L. Parry, Iwan W. Griffiths, Robert S. Laramee, Rhodri Bown, Philip A. Legg, Min Chen\n\nAbstract: Organizing sport video data for performance analysis can be challenging, especially when this involves multiple attributes, and the criteria for sorting frequently changes depending on the user's task. In this work, we propose a visual analytic system to convert a user's knowledge on rankings to support such a process. The system enables users to specify a sort requirement in a flexible manner without depending on specific knowledge about individual sort keys. We use regression techniques to train different analytical models for different types of sorting requirements. We use visualization to facilitate the discovery of knowledge at different stages of the visual analytic process. This includes visualizing the parameters of the ranking model, visualizing the results of a sort query for interactive exploration, and the playback of sorted video clips. We demonstrate the system with a case study in rugby to find key instances for analyzing team and player performance.", "uri": "https://vimeo.com/136248932", "name": "VIS15 preview: Knowledge-Assisted Ranking: A Visual Analytic Application for Sport Event Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:06:19+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Enik\u00f6 Sz\u00e9kely, Arnaud Sallaberry, Faraz Zaidi, Pascal Poncelet\n\nAbstract: Detection of outliers and anomalous behavior is a well\u00adknown problem in the data mining and statistics fields. Although the problem of identifying single outliers has been extensively studied in the literature, little effort has been devoted to detecting small groups of outliers that are similar to each other but markedly different from the entire population. Many real\u00adworld scenarios have small groups of outliers\u00ad\u00adfor example, a group of students who excel in a classroom or a group of spammers in an online social network. In this article, the authors propose a novel method to solve this challenging problem that lies at the frontiers of outlier detection and clustering of similar groups. The method transforms a multidimensional dataset into a graph, applies a network metric to detect clusters, and renders a representation for visual assessment to find rare events. The authors tested the proposed method to detect pathologic cells in the biomedical science domain. The results are promising and confirm the available ground truth provided by the domain experts.", "uri": "https://vimeo.com/136248872", "name": "VIS15 preview: A Graph-based Method to Detect Rare Events: An Application to Identify Pathologic Cells", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:05:32+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Jonathan C. Roberts, Panagiotis D. Ritsos, Sriram Karthik Badam, Dominique Brodbeck, Jessie Kennedy, Niklas Elmqvist\n\nAbstract: Visualization is coming of age. With visual depictions being seamlessly integrated into documents, and data visualization techniques being used to understand increasingly large and complex datasets, the term \"visualization\"' is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today's new devices and tomorrow's technology. Today, people interact with visual depictions through a mouse. Tomorrow, they'll be touching, swiping, grasping, feeling, hearing, smelling, and even tasting data. The next big thing is multisensory visualization that goes beyond the desktop.", "uri": "https://vimeo.com/136248781", "name": "VIS15 preview: Visualization Beyond the Desktop - the next big thing", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:05:06+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Paolo Angelelli, Steffen Oeltze, Judit Ha\u00e1sz, Cagatay Turkay, Erlend Hodneland, Arvid Lundervold, Astri J. Lundervold, Bernhard Preim, Helwig Hauser\n\nAbstract: This approach enables the visual exploration and analysis of large amounts of heterogeneous data, helping to generate and validate hypotheses. It uses a data\u00adcube\u00adbased model to handle overlapping data subsets. This enables seamless integration of the data during visualization and the linking of spatial and nonspatial views of the data.", "uri": "https://vimeo.com/136248738", "name": "VIS15 preview: Interactive Visual Analysis of Heterogeneous Cohort Study Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T21:04:25+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Yafeng Lu, Feng Wang, Ross Maciejewski\n\nAbstract: With over 16 million tweets per hour, 600 new blog posts per minute, and 400 million active users on Facebook, businesses have begun searching for ways to turn real\u00adtime consumer\u00ad based posts into actionable intelligence. The goal is to extract information from this noisy, unstructured data and use it for trend analysis and prediction. Current practices support the idea that visual analytics (VA) can help enable the effective analysis of such data. However, empirical evidence demonstrating the effectiveness of a VA solution is still lacking. A proposed VA toolkit extracts data from Bitly and Twitter to predict movie revenue and ratings. Results from the 2013 VAST Box Office Challenge demonstrate the benefit of an interactive environment for predictive analysis, compared to a purely statistical modeling approach. The VA approach used by the toolkit is generalizable to other domains involving social media data, such as sales forecasting and advertisement analysis.", "uri": "https://vimeo.com/136248682", "name": "VIS15 preview: Business Intelligence from Social Media: A Study from the VAST Box Office Challenge", "year": "2015", "event": "VAST, PREVIEW"}, {"created_time": "2015-08-13T21:03:35+00:00", "description": "[CG&amp;A paper presentation] \n\nAuthors: Eser Kandogan, Aruna Balakrishnan, Eben M. Haber, Jeffrey S. Pierce\n\nAbstract: With greater availability of data, businesses are increasingly becoming data\u00addriven enterprises, establishing standards for data acquisition, processing, infrastructure, and decision making. Enterprises now have people dedicated to performing analytic work to support decision makers. To better understand analytic work, particularly the role of enterprise business analysts, researchers interviewed 34 analysts at a large corporation. Analytical work occurred in an ecosystem of data, tools, and people; the ecosystem's overall quality and efficiency depended on the amount of coordination and collaboration. Analysts were the bridge between business and IT, closing the semantic gap between datasets, tools, and people. This article provides an overview of the analytic work in the enterprise, describing challenges in data, tools, and practices and identifying opportunities for new tools for collaborative analytics.", "uri": "https://vimeo.com/136248596", "name": "VIS15 preview: From Data to Insight: Work Practices of Analysts in the Enterprise", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:42:17+00:00", "description": "[SciVis paper] [BEST PAPER]\n\nAuthors: David Schroeder, Daniel Keefe\n\nAbstract: We present Visualization-by-Sketching, a direct-manipulation user interface for designing new data visualizations. The goals are twofold: First, make the process of creating real, animated, data-driven visualizations of complex information more accessible to artists, graphic designers, and other visual experts with traditional, non-technical training. Second, support and enhance the role of human creativity in visualization design, enabling visual experimentation and workflows similar to what is possible with traditional artistic media. The approach is to conceive of visualization design as a combination of processes that are already closely linked with visual creativity: sketching, digital painting, image editing, and reacting to exemplars. Rather than studying and tweaking low-level algorithms and their parameters, designers create new visualizations by painting directly on top of a digital data canvas, sketching data glyphs, and arranging and blending together multiple layers of animated 2D graphics. This requires new algorithms and techniques to interpret painterly user input relative to data \"under\"\u009d the canvas, balance artistic freedom with the need to produce accurate data visualizations, and interactively explore large (e.g., terabyte-sized) multivariate datasets. Results demonstrate a variety of multivariate data visualization techniques can be rapidly recreated using the interface. More importantly, results and feedback from artists support the potential for interfaces in this style to attract new, creative users to the challenging task of designing more effective data visualizations and to help these users stay \"in the creative zone\"\u009d as they work.", "uri": "https://vimeo.com/136211325", "name": "VIS15 preview: Visualization-by-Sketching: An Artist's Interface for Creating Multivariate Time-Varying Data Visualizations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:42:12+00:00", "description": "[SciVis paper] \n\nAuthors: Gordon Kindlmann, Charisee Chiw, Nicholas Seltzer, Lamont Samuels, John Reppy\n\nAbstract: Many algorithms for scientific visualization and image analysis are rooted in the world of continuous scalar, vector, and tensor fields, but are programmed in low-level languages and libraries that obscure their mathematical foundations. Diderot is a parallel domain-specific language that is designed to bridge this semantic gap by providing the programmer with a high-level, mathematical programming notation that allows direct expression of mathematical concepts in code. Furthermore, Diderot provides parallel performance that takes advantage of modern multicore processors and GPUs. The high-level notation allows a concise and natural expression of the algorithms and the parallelism allows efficient execution on real-world datasets.", "uri": "https://vimeo.com/136211303", "name": "VIS15 preview: Diderot: a Domain-Specific Language for Portably Parallel Scientific Visualization and Image Analysis", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:42:07+00:00", "description": "[SciVis paper] \n\nAuthors: Ali Al-Awami, Johanna Beyer, Daniel Haehn, Narayanan Kasthuri, Jeff W. Lichtman, Hanspeter Pfister, Markus Hadwiger\n\nAbstract: In the fi\u0081eld of connectomics, neuroscientists acquire electron microscopy volumes at nanometer resolution in order to reconstruct a detailed wiring diagram of the neurons in the brain. The resulting image volumes, which often are hundreds of terabytes in size, need to be segmented to identify cell boundaries, synapses, and important cell organelles. However, the segmentation process of a single volume is very complex, time-intensive, and usually performed using a diverse set of tools and many users. To tackle the associated challenges, this paper presents NeuroBlocks, which is a novel visualization system for tracking the state, progress, and evolution of very large volumetric segmentation data in neuroscience. NeuroBlocks is a multi-user web-based application that seamlessly integrates the diverse set of tools that neuroscientists currently use for manual and semi-automatic segmentation, proofreading, visualization, and analysis. NeuroBlocks is the fi\u0081rst system that integrates this heterogeneous tool set, providing crucial support for the management, provenance, accountability, and auditing of large-scale segmentations. We describe the design of NeuroBlocks, starting with an analysis of the domain-specifi\u0081c tasks, their inherent challenges, and our subsequent task abstraction and visual representation. We demonstrate the utility of our design based on two case studies that focus on different user roles and their respective requirements for performing and tracking the progress of segmentation and proofreading in a large real-world connectomics project.", "uri": "https://vimeo.com/136211296", "name": "VIS15 preview: NeuroBlocks: Visual Tracking of Segmentation and Proof-Reading for Large Connectomics Projects", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:42:00+00:00", "description": "[SciVis paper] \n\nAuthors: Soumya Dutta, Han-Wei Shen\n\nAbstract: Effective analysis of features in time-varying data is essential in numerous scientific applications. Feature extraction and tracking are two important tasks scientists rely upon to get insights about the dynamic nature of the large scale time-varying data. However, often the complexity of the scientific phenomena only allows scientists to vaguely define their feature of interest. Furthermore, such features can have varying motion patterns and dynamic evolution over time. As a result, automatic extraction and tracking of features becomes a non-trivial task. In this work, we investigate these issues and propose a distribution driven approach which allows us to construct novel algorithms for reliable feature extraction and tracking with high confidence in the absence of accurate feature definition. We exploit two key properties of an object, motion and similarity to the target feature, and fuse the information gained from them to generate a robust feature-aware classification field at every time step. Tracking of features is done using such classified fields which enhances the accuracy and robustness of the proposed algorithm. The efficacy of our method is demonstrated by successfully applying it on several scientific data sets containing a wide range of dynamic time-varying features.", "uri": "https://vimeo.com/136211283", "name": "VIS15 preview: Distribution Driven Extraction and Tracking of Features for Time-varying Data Analysis", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:53+00:00", "description": "[SciVis paper] \n\nAuthors: Changgong Zhang, Thomas Schultz, Kai Lawonn, Elmar Eisemann, Anna Vilanova\n\nAbstract: Diffusion Tensor Imaging (DTI) is a magnetic resonance imaging modality that enables the in-vivo reconstruction and visualization of fibrous structures. To inspect the local and individual diffusion tensors, expressed as \\\\(3\\\\times3\\\\) symmetric and positive-definite matrices, glyph-based visualizations are commonly used since they are able to effectively convey full aspects of the diffusion tensor. For several applications, it is necessary to compare tensor fields, e.g., to study the effects of acquisition parameters, or to investigate the influence of pathologies on white matter structures. The comparison is commonly done by extracting scalar information out of the tensor fields and then comparing these scalar fields, which leads to a loss of information. If the glyph representation is kept, juxtaposition or superposition can be used, but neither facilitates the identification and interpretation of the differences between the tensor fields. Inspired by the checkerboard-style visualization and the superquadric tensor glyph, we designed a new glyph to locally visualize differences between two diffusion tensors by combining juxtaposition and explicit encoding. The new glyphs allow us to efficiently and effectively identify the diffusion tensor (a) scale, (b) anisotropy type, and (c) orientation differences as demonstrated in a user study. Tensor scale, anisotropy and orientation are related to anatomical information that is relevant in most DTI applications. We applied our glyphs to investigate the differences between two DTI datasets of the human brain acquired with different b-values, and from a healthy subject and a HIV-infected subject respectively.", "uri": "https://vimeo.com/136211266", "name": "VIS15 preview: Glyph-based Comparative Visualization for Diffusion Tensor Fields", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:47+00:00", "description": "[SciVis paper] \n\nAuthors: Jan By\u0161ka, Mathieu Le Muzic, Eduard Gr\u00f6ller, Ivan Viola, Barbora Kozlikova\n\nAbstract: In this paper we propose a novel method for the interactive exploration of protein tunnels. The basic principle of our approach is that we entirely abstract from the 3D/4D space the simulated phenomenon is embedded in. A complex 3D structure and its curvature information is represented only by a straightened tunnel centerline and its width profile. This representation focuses on a key aspect of the studied geometry and frees up graphical estate to key chemical and physical properties represented by surrounding amino acids. The method shows the detailed tunnel profile and its temporal aggregation. The profile is interactively linked with a visual overview of all amino acids which are lining the tunnel over time. In this overview, each amino acid is represented by a set of colored lines depicting the spatial and temporal impact of the amino acid on the corresponding tunnel. This representation clearly shows the importance of amino acids with respect to selected criteria. It helps the biochemists to select the candidate amino acids for mutation which changes the protein function in a desired way. The AnimoAminoMiner was designed in close cooperation with domain experts. Its usefulness is documented by their feedback and a case study, which are included.", "uri": "https://vimeo.com/136211254", "name": "VIS15 preview: AnimoAminoMiner: Exploration of Protein Tunnels and their Properties in Molecular Dynamics", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:41+00:00", "description": "[SciVis paper] \n\nAuthors: Daisuke Sakurai, Osamu Saeki, Hamish Carr, Hsiang-Yun Wu, Takahiro Yamamoto, David Duke, Shigeo Takahashi\n\nAbstract: Scalar topology in the form of Morse theory has provided computational tools that analyze and visualize data from scientific and engineering tasks. Contracting isocontours to single points encapsulates variations in isocontour connectivity in the Reeb graph. For multivariate data, isocontours generalize to fibers-inverse images of points in the range, and this area is therefore known as fiber topology. However, fiber topology is less fully developed than Morse theory, and current efforts rely on manual visualizations. This paper presents how to accelerate and semi-automate this task through an interface for visualizing fiber singularities of multivariate functions R^3 -&gt; R^2. This interface exploits existing conventions of fiber topology, but also introduces a 3D view based on the extension of Reeb graphs to Reeb spaces. Using the Joint Contour Net, a quantized approximation of the Reeb space, this accelerates topological visualization and permits online perturbation to reduce or remove degeneracies in functions under study. Validation of the interface is performed by assessing whether the interface supports the mathematical workflow both of experts and of less experienced mathematicians.", "uri": "https://vimeo.com/136211245", "name": "VIS15 preview: Interactive Visualization for Singular Fibers of Functions f: R3-&gt;R2", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:37+00:00", "description": "[SciVis paper] \n\nAuthors: Jorge Poco, Harish Doraiswamy, Marian Talbert, Jeffrey Morisette, Claudio Silva\n\nAbstract: Species distribution models (SDM) are used to help understand what drives the distribution of various plant and animal species. These models are typically high dimensional scalar functions, where the dimensions of the domain correspond to predictor variables of the model algorithm. Understanding and exploring the differences between models help ecologists understand areas where their data or understanding of the system is incomplete and will help guide further investigation in these regions. These differences can also indicate an important source of model to model uncertainty. However, it is cumbersome and often impractical to perform this analysis using existing tools, which allows for manual exploration of the models usually as 1-dimensional curves. In this paper, we propose a topology-based framework to help ecologists explore the differences in various SDMs directly in the high dimensional domain. In order to accomplish this, we introduce the concept of maximum topology matching that computes a locality-aware correspondence between similar extrema of two scalar functions. The matching is then used to compute the similarity between two functions. We also design a visualization interface that allows ecologists to explore SDMs using their topological features and to study the differences between pairs of models found using maximum topological matching. We demonstrate the utility of the proposed framework through several use cases using different data sets and report the feedback obtained from ecologists.", "uri": "https://vimeo.com/136211232", "name": "VIS15 preview: Using Maximum Topology Matching to Explore Differences in Species Distribution Models", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:31+00:00", "description": "[SciVis paper] \n\nAuthors: Attila Gyulassy, Aaron Knoll, Peer-Timo Bremer, Bei Wang, Kah Chun Lau, Michael Papka, Larry Curtiss, Valerio Pascucci\n\nAbstract: Large-scale molecular dynamics (MD) simulations are commonly used for simulating the synthesis and ion diffusion of battery materials. A good battery anode material is determined by its capacity to store ion or other diffusers. However, modeling of ion diffusion dynamics and transport properties at large length and long time scales would be impossible with current MD codes. To analyze the fundamental properties of these materials, therefore, we turn to geometric and topological analysis of their structure. In this paper, we apply a novel technique inspired by discrete Morse theory to the Delaunay triangulation of the simulated geometry of a thermally annealed carbon nanosphere. We utilize our computed structures to drive further geometric analysis to extract the interstitial diffusion structure as a single mesh. Our results provide a new approach to analyze the geometry of the simulated carbon nanosphere, and new insights into the role of carbon defect size and distribution in determining the charge capacity and charge dynamics of these carbon based battery materials.", "uri": "https://vimeo.com/136211217", "name": "VIS15 preview: Interstitial and Interlayer Ion Diffusion Geometry Extraction in Graphitic Nanosphere Battery Materials", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:25+00:00", "description": "[SciVis paper] \n\nAuthors: Xiaotong Liu, Han-Wei Shen\n\nAbstract: The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data's multi-faceted properties. In this paper, we present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.", "uri": "https://vimeo.com/136211205", "name": "VIS15 preview: Association Analysis for Visual Exploration of Multivariate Scientific Data Sets", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:14+00:00", "description": "[SciVis paper] \n\nAuthors: Joseph Marino, Arie Kaufman\n\nAbstract: We present a novel method to create planar visualizations of treelike structures (e.g., blood vessels and airway trees) where the shape of the object is well preserved, allowing for easy recognition by users familiar with the structures. Based on the extracted skeleton through the treelike object, a radial planar embedding is first obtained such that there are no self-intersections of the skeleton which would have resulted in occlusions in the final view. An optimization procedure which adjusts the angular positions of the skeleton nodes is then used to reconstruct the shape as closely as possible to the original, according to a specified view plane, which thus preserves the geometric context of the object. Using this shape recovered embedded skeleton, the object surface is then flattened to the plane without occlusions using harmonic mapping. The boundary of the mesh is adjusted during the flattening step to account for regions where the mesh is stretched over a gap. This parameterized surface can then be used either as a map for guidance during endoluminal navigation or directly for interrogation and decision making. Depth cues are provided with a grayscale border to aid the user. Examples are presented using real world datasets, and the results are evaluated quantitatively and with a user study.", "uri": "https://vimeo.com/136211185", "name": "VIS15 preview: Planar Visualization of Treelike Structures", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:09+00:00", "description": "[SciVis paper] \n\nAuthors: Kenneth Weiss, Peter Lindstrom\n\nAbstract: Multilinear interpolants are at the foundation of many key visualization techniques, including isosurfacing, direct volume rendering and texture mapping, which assume piecewise multilinear interpolants over the cells of a mesh. However, despite their importance, there has not been much focus within the visualization community on techniques that efficiently generate and encode globally continuous functions defined by the union of piecewise multilinear cells. Wavelets provide a rich context for analyzing and processing complicated datasets. In this paper, we exploit adaptive regular refinement as a means of representing and evaluating functions described by a subset of their nonzero wavelet coefficients. We analyze the dependencies involved in the transform and describe how to generate the coarsest adaptive mesh with nodal function values such that the inverse wavelet transform is exactly reproduced via simple interpolation (subdivision) over the mesh elements. This allows for an adaptive, sparse representation of the function with on-demand evaluation at any point in the domain. We focus on the popular wavelets formed by tensor products of linear B-splines, resulting in an adaptive, nonconforming but crack-free quad- or octree mesh that allows reproducing globally continuous functions via multilinear interpolation over elements.", "uri": "https://vimeo.com/136211175", "name": "VIS15 preview: Adaptive multilinear tensor product wavelets", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:41:01+00:00", "description": "[SciVis paper] \n\nAuthors: Jonathan Woodring, Mark Petersen, Andre Schmeisser, John Patchett, James Ahrens, Hans Hagen\n\nAbstract: An eddy is a feature associated with a rotating body of fluid, surrounded by a ring of shearing fluid. In the ocean, eddies are 10 to 150 km in diameter, are spawned by boundary currents and baroclinic instabilities, may live for hundreds of days, and travel for hundreds of kilometers. Eddies are important in climate studies because they transport heat, salt, and nutrients through the world's oceans and are vessels of biological productivity. The study of eddies in global ocean-climate models requires large-scale, high-resolution simulations. This poses a problem for feasible (timely) eddy analysis, as ocean simulations generate massive amounts of data, causing a bottleneck for traditional analysis workflows. To enable eddy studies, we have developed an in situ workflow for the quantitative and qualitative analysis of MPAS-Ocean, a high-resolution ocean climate model, in collaboration with the ocean model research and development process. Planned eddy analysis at high-spatial and temporal resolutions will not be possible with a post-processing workflow due to constraints, but the in situ workflow enables it and scales well to ten-thousand processing elements.", "uri": "https://vimeo.com/136211155", "name": "VIS15 preview: In Situ Eddy Analysis in a High-Resolution Ocean Climate Model", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:40:48+00:00", "description": "[SciVis paper] \n\nAuthors: Matthias Labsch\u00fctz, Stefan Bruckner, Eduard Gr\u00f6ller, Markus Hadwiger, Peter Rautek\n\nAbstract: Sparse volume data structures enable the efficient representation of large but sparse volumes in GPU memory for computation and visualization. However, the choice of a specific data structure for a given data set depends on several factors, such as the memory budget, the sparsity of the data, and data access patterns. In general, there is no single optimal sparse data structure, but a set of several candidates with individual strengths and drawbacks. One solution to this problem are hybrid data structures which locally adapt themselves to the sparsity. However, they typically suffer from increased traversal overhead which limits their utility in many applications. This paper presents JiTTree, a novel sparse hybrid volume data structure that uses just-in-time compilation to overcome these problems. By combining multiple sparse data structures and reducing traversal overhead we leverage their individual advantages. We demonstrate that hybrid data structures adapt well to a large range of data sets. They are especially superior to other sparse data structures for data sets that locally vary in sparsity. Possible optimization criteria are memory, performance and a combination thereof. Through just-in-time compilation, JiTTree reduces the traversal overhead of the resulting optimal data structure. As a result, our hybrid volume data structure enables efficient computations on the GPU, while being superior in terms of memory usage when compared to non-hybrid data structures.", "uri": "https://vimeo.com/136211137", "name": "VIS15 preview: JiTTree: A Just-in-Time Compiled Sparse GPU Volume Data Structure", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:40:41+00:00", "description": "[SciVis paper] \n\nAuthors: Ingo Wald, Aaron Knoll, Gregory P Johnson, Will Usher, Valerio Pascucci, Michael Papka\n\nAbstract: We present a novel approach to rendering large particle data sets from molecular dynamics, astrophysics and other sources. We employ a new data structure adapted from the original balanced k-d tree, which allows for representation of data with trivial or no overhead. In the OSPRay visualization framework, we have developed an efficient CPU algorithm for traversing, classifying and ray tracing these data. Our approach is able to render up to billions of particles on a typical workstation, purely on the CPU, without any approximations or level-of-detail techniques, and optionally with attribute-based color mapping, dynamic range query, and advanced lighting models such as ambient occlusion and path tracing.", "uri": "https://vimeo.com/136211121", "name": "VIS15 preview: CPU Ray Tracing Large Particle Data with Balanced P-k-d Trees", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:40:33+00:00", "description": "[SciVis paper] \n\nAuthors: Zhongjie Wang, Hans-Peter Seidel, Tino Weinkauf\n\nAbstract: We present an approach to pattern matching in 3D multi-field scalar data. Existing pattern matching algorithms work on single scalar or vector fields only, yet many numerical simulations output multi-field data where only a joint analysis of multiple fields describes the underlying phenomenon fully. Our method takes this into account by bundling information from multiple fields into the description of a pattern. First, we extract a sparse set of features for each 3D scalar field using the 3D SIFT algorithm (Scale-Invariant Feature Transform). This allows for a memory-saving description of prominent features in the data with invariance to translation, rotation, and scaling. Second, the user defines a pattern as a set of SIFT features in multiple fields by e.g. brushing a region of interest. Third, we locate and rank matching patterns in the entire data set. Experiments show that our algorithm is efficient in terms of required memory and computational efforts.", "uri": "https://vimeo.com/136211103", "name": "VIS15 preview: Multi-field Pattern Matching based on Sparse Feature Sampling", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:40:28+00:00", "description": "[SciVis paper] \n\nAuthors: Robin Sk\u00e5nberg, Pere-Pau V\u00e1zquez, Victor Guallar, Timo Ropinski\n\nAbstract: Today molecular simulations produce complex data sets capturing the interactions of molecules in detail. Due to the complexity of this time-varying data, advanced visualization techniques are required to support its visual analysis. Current molecular visualization techniques utilize ambient occlusion as a global illumination approximation to improve spatial comprehension. Besides these shadow-like effects, interreflections are also known to improve the spatial comprehension of complex geometric structures. Unfortunately, the inherent computational complexity of interreflections would forbid interactive exploration, which is is mandatory in many scenarios dealing with static and time-varying data. In this paper, we introduce a novel analytic approach for capturing interreflections of molecular structures in real-time. By exploiting the knowledge of the underlying space filling representations, we are able to reduce the required parameters and can thus apply symbolic regression to obtain an analytic expression for interreflections. We show how to obtain the data required for the symbolic regression analysis, and how to exploit our analytic solution to enhance interactive molecular visualizations.", "uri": "https://vimeo.com/136211098", "name": "VIS15 preview: Real-Time Molecular Visualization Supporting Diffuse Illumination and Ambient Occlusion", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:40:23+00:00", "description": "[SciVis paper] \n\nAuthors: Christopher Paul Kappe, Lucas Sch\u00fctz, Stefan Gunther, Lars Hufnagel, Steffen Lemke, Heike Leitte\n\nAbstract: Animal development is marked by the repeated reorganization of cells and cell populations, which ultimately determine form and shape of the growing organism. One of the central questions in developmental biology is to understand precisely how cells reorganize, as well as how and to what extent this reorganization is coordinated. While modern microscopes can record video data for every cell during animal development in 3D+t, analyzing these videos remains a major challenge: reconstruction of comprehensive cell tracks turned out to be very demanding especially with decreasing data quality and increasing cell densities. In this paper, we present an analysis pipeline for coordinated cellular motions in developing embryos based on the optical flow of a series of 3D images. We use numerical integration to reconstruct cellular long-term motions in the optical flow of the video, we take care of data validation, and we derive a LIC-based, dense flow visualization for the resulting pathlines. This approach allows us to handle low video quality such as noisy data or poorly separated cells, and it allows the biologists to get a comprehensive understanding of their data by capturing dynamic growth processes in stills. We validate our methods using three videos of growing fruit fly embryos.", "uri": "https://vimeo.com/136211094", "name": "VIS15 preview: Reconstruction and Visualization of Coordinated 3D Cell Migration Based on Optical Flow", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:40:16+00:00", "description": "[SciVis paper] \n\nAuthors: Florian Ferstl, Kai B\u00fcrger, R\u00fcdiger Westermann\n\nAbstract: We present a new method to visualize from an ensemble of flow fields the statistical properties of streamlines passing through a selected location. We use principal component analysis to transform the set of streamlines into a low-dimensional Euclidean space. In this space the streamlines are clustered into major trends, and each cluster is in turn approximated by a multivariate Gaussian distribution. This yields a probabilistic mixture model for the streamline distribution, from which confidence regions can be derived in which the streamlines are most likely to reside. This is achieved by transforming the Gaussian random distributions from the low-dimensional Euclidean space into a streamline distribution that follows the statistical model, and by visualizing confidence regions in this distribution via iso-contours. We further make use of the principal component representation to introduce a new concept of streamline-median, based on existing median concepts in multidimensional Euclidean spaces. We demonstrate the potential of our method in a number of real-world examples, and we compare our results to alternative clustering approaches for particle trajectories as well as curve boxplots.", "uri": "https://vimeo.com/136211085", "name": "VIS15 preview: Streamline Variability Plots for Characterizing the Uncertainty in Vector Field Ensembles", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:40:10+00:00", "description": "[SciVis paper] \n\nAuthors: Lingyun Yu, Konstantinos Efstathiou, Petra Isenberg, Tobias Isenberg\n\nAbstract: We present a family of three interactive Context-Aware Selection Techniques (CAST) for the analysis of large 3D particle datasets. For these datasets, spatial selection is an essential prerequisite to many other analysis tasks. Traditionally, such interactive target selection has been particularly challenging when the data subsets of interest were implicitly defined in the form of complicated structures of thousands of particles. Our new techniques SpaceCast, TraceCast, and PointCast improve usability and speed of spatial selection in point clouds through novel context-aware algorithms. They are able to infer a user\u00e2\u20ac\u2122s subtle selection intention from gestural input, can deal with complex situations such as partially occluded point clusters or multiple cluster layers, and can all be fine-tuned after the selection interaction has been completed. Together, they provide an effective and efficient tool set for the fast exploratory analysis of large datasets. In addition to presenting Cast, we report on a formal user study that compares our new techniques not only to each other but also to existing state-of-the-art selection methods. Our results show that Cast family members are virtually always faster than existing methods without tradeoffs in accuracy. In addition, qualitative feedback shows that PointCast and TraceCast were strongly favored by our participants for intuitiveness and efficiency.", "uri": "https://vimeo.com/136211077", "name": "VIS15 preview: CAST: Effective and Efficient User Interaction for Context-Aware Selection in 3D Particle Clouds", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:40:04+00:00", "description": "[SciVis paper] [HONORABLE MENTION]\n\nAuthors: Tobias G\u00fcnther, Maik Schulze, Holger Theisel\n\nAbstract: We propose a new class of vortex definitions for flows that are induced by rotating mechanical parts, such as stirring devices, helicopters, hydrocyclones, centrifugal pumps, or ventilators. Instead of a Galilean invariance, we enforce a rotation invariance, i.e., the invariance of a vortex under a uniform-speed rotation of the underlying coordinate system around a fixed axis. We provide a general approach to transform a Galilean invariant vortex concept to a rotation invariant one by simply adding a closed form matrix to the Jacobian. In particular, we present rotation invariant versions of the well-known Sujudi-Haimes, Lambda-2, and Q vortex criteria. We apply them to a number of artificial and real rotating flows, showing that for these cases rotation invariant vortices give better results than their Galilean invariant counterparts.", "uri": "https://vimeo.com/136211061", "name": "VIS15 preview: Rotation Invariant Vortices for Flow Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:53+00:00", "description": "[SciVis paper] \n\nAuthors: Valentin Zobel, Markus Stommel, Gerik Scheuermann\n\nAbstract: Virtual testing is an integral part of modern product development in mechanical engineering. Numerical structure simulations allow the computation of local stresses which are given as tensor fields. For homogeneous materials, the tensor information is usually reduced to a scalar field like the von Mises stress. A material-dependent threshold defines the material failure answering the key question of engineers. This leads to a rather simple feature-based visualisation. For composite materials like short fiber reinforced polymers, the situation is much more complex. The material property is determined by the fiber distribution at every position, often described as fiber orientation tensor field. Essentially, the material's ability to cope with stress becomes anisotropic and inhomogeneous. We show how to combine stress field and fiber orientation field in such cases, leading to a feature-based visualization of tensor fields for composite materials. The resulting features inform the engineer about potential improvements in the product development.", "uri": "https://vimeo.com/136211040", "name": "VIS15 preview: Feature-Based Tensor Field Visualization for Fiber Reinforced Polymers", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:47+00:00", "description": "[SciVis paper] \n\nAuthors: Max Hermann, Anja C. Schunke, Thomas Schultz, Reinhard Klein\n\nAbstract: Large image deformations pose a challenging problem for the visualization and statistical analysis of 3D image ensembles which have a multitude of applications in biology and medicine. Simple linear interpolation in the tangent space of the ensemble introduces artifactual anatomical structures that hamper the application of targeted visual shape analysis techniques. In this work we make use of the theory of stationary velocity fields to facilitate interactive non-linear image interpolation and plausible extrapolation for high quality rendering of large deformations and devise an efficient image warping method on the GPU. This does not only improve quality of existing visualization techniques, but opens up a field of novel interactive methods for shape ensemble analysis. Taking advantage of the efficient non-linear 3D image warping, we showcase four visualizations: 1) browsing on-the-fly computed group mean shapes to learn about shape differences between specific classes, 2) interactive reformation to investigate complex morphologies in a single view, 3) likelihood volumes to gain a concise overview of variability and 4) streamline visualization to show variation in detail, specifically uncovering its component tangential to a reference surface. Evaluation on a real world dataset shows that the presented method outperforms the state-of-the-art in terms of visual quality while retaining interactive frame rates. A case study with a domain expert was performed in which the novel analysis and visualization methods are applied on standard model structures, namely skull and mandible of different rodents, to investigate and compare influence of phylogeny, diet and geography on shape. The visualizations enable for instance to distinguish (population-)normal and pathological morphology, assist in uncovering correlation to extrinsic factors and potentially support assessment of model quality.", "uri": "https://vimeo.com/136211028", "name": "VIS15 preview: Accurate Interactive Visualization of Large Deformations and Variability in Biomedical Image Ensembles", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:42+00:00", "description": "[SciVis paper] \n\nAuthors: Amin Abbasloo, Vitalis Wiens, Max Hermann, Thomas Schultz\n\nAbstract: Despite the widely recognized importance of symmetric second order tensor fields in medicine and engineering, the visualization of data uncertainty in tensor fields is still in its infancy. A recently proposed tensorial normal distribution, involving a fourth order covariance tensor, provides a mathematical description of how different aspects of the tensor field, such as trace, anisotropy, or orientation, vary and covary at each point. However, this wealth of information is far too rich for a human analyst to take in at a single glance, and no suitable visualization tools are available. We propose a novel approach that facilitates visual analysis of tensor covariance at multiple levels of detail. We start with a visual abstraction that uses slice views and direct volume rendering to indicate large-scale changes in the covariance structure, and locations with high overall variance. We then provide tools for interactive exploration, making it possible to drill down into different types of variability, such as in shape or orientation. Finally, we allow the analyst to focus on specific locations of the field, and provide tensor glyph animations and overlays that intuitively depict confidence intervals at those points. Our system is demonstrated by investigating the effects of measurement noise on diffusion tensor MRI, and by analyzing two ensembles of stress tensor fields from solid mechanics.", "uri": "https://vimeo.com/136211017", "name": "VIS15 preview: Visualizing Tensor Normal Distributions at Multiple Levels of Detail", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:38+00:00", "description": "[SciVis paper] \n\nAuthors: Christopher Healey, Lihua Hao, Steffen Bass\n\nAbstract: An ensemble is a collection of related datasets, called members, built from a series of runs of a simulation or an experiment. Ensembles are large, temporal, multidimensional, and multivariate, making them difficult to analyze. Another important challenge is visualizing ensembles that vary both in space and time. Initial visualization techniques displayed ensembles with a small number of members, or presented an overview of an entire ensemble, but without potentially important details. Recently, researchers have suggested combining these two directions, allowing users to choose subsets of members to visualization. This manual selection process places the burden on the user to identify which members to explore. We first introduce a static ensemble visualization system that automatically helps users locate interesting subsets of members to visualize. We next extend the system to support analysis and visualization of temporal ensembles. We employ 3D shape comparison, cluster tree visualization, and glyph based visualization to represent different levels of detail within an ensemble. This strategy is used to provide two approaches for temporal ensemble analysis: (1) segment based ensemble analysis, to capture important shape transition time-steps, clusters groups of similar members, and identify common shape changes over time across multiple members; and (2) time-step based ensemble analysis, which assumes ensemble members are aligned in time by combining similar shapes at common time-steps. Both approaches enable users to interactively visualize and analyze a temporal ensemble from different perspectives at different levels of detail. We demonstrate our techniques on an ensemble studying matter transition from hadronic gas to quark-gluon plasma during gold-on-gold particle collisions.", "uri": "https://vimeo.com/136211002", "name": "VIS15 preview: Effective Visualization of Temporal Ensembles", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:32+00:00", "description": "[SciVis paper] \n\nAuthors: Victor Matvienko, Jens Krueger\n\nAbstract: In this work we propose an effective method for frequency-controlled dense flow visualization derived from a generalization of the Line Integral Convolution (LIC) technique. Our approach consists in considering the spectral properties of the dense flow visualization process as an integral operator defined in a local curvilinear coordinate system aligned with the flow. \nExploring LIC from this point of view, we suggest a systematic way to design a flow visualization process with particular local spatial frequency properties of the resulting image. Our method is efficient, intuitive, and based on a long-standing model developed as a result of numerous perception studies. The method can be described as an iterative application of line integral convolution, followed by a one-dimensional Gabor filtering orthogonal to the flow. To demonstrate the utility of the technique, we generated novel adaptive multi-frequency flow visualizations, that according to our evaluation, feature a higher level of frequency control and higher quality scores than traditional approaches in texture-based flow visualization.", "uri": "https://vimeo.com/136210989", "name": "VIS15 preview: Explicit Frequency Control for High-Quality Texture-Based Flow Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:25+00:00", "description": "[SciVis paper] \n\nAuthors: Alexander Bock, Asher Pembroke, M. Leila Mays, Lutz Rastaetter, Anders Ynnerman, Timo Ropinski\n\nAbstract: We propose a system to analyze and contextualize simulations of coronal mass ejections. As current simulation techniques require manual input, uncertainty is introduced into the simulation pipeline leading to inaccurate predictions that can be mitigated through ensemble simulations. We provide the space weather analyst with a multi-view system providing visualizations to: 1. compare ensemble members against ground truth measurements, 2. inspect time-dependent information derived from optical flow analysis of satellite images, and 3. combine satellite images with a volumetric rendering of the simulations. This three-tier workflow provides experts with tools to discover correlations between errors in predictions and simulation parameters, thus increasing knowledge about the evolution and propagation of coronal mass ejections that pose a danger to Earth and interplanetary travel.", "uri": "https://vimeo.com/136210973", "name": "VIS15 preview: Visual Verification of Space Weather Ensemble Simulations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:20+00:00", "description": "[SciVis paper] \n\nAuthors: Hongsen Liao, Yingcai Wu, Li Chen, Thomas M. Hamill, Yunhai Wang, Kan Dai, Hui Zhang, Wei Chen\n\nAbstract: Numerical weather predictions have been widely used for weather forecasting. Many large meteorological centers are producing highly accurate ensemble forecasts routinely to provide effective weather forecast services. However, biases frequently exist in forecast products because of various reasons, such as the imperfection of the weather forecast models. Failure to identify and neutralize the biases would result in unreliable forecast products that might mislead analysts; consequently, unreliable weather predictions are produced. The analog method has been commonly used to overcome the biases. Nevertheless, this method has some serious limitations including the difficulty in finding effective similar past forecasts, the large search space for proper parameters and the lack of support for interactive, real-time analysis. In this study, we develop a visual analytics system based on a novel voting framework to circumvent the problems. The framework adopts the idea of majority voting to combine judiciously the different variants of analog methods towards effective retrieval of the proper analogs for calibration. The system seamlessly integrates the analog methods into an interactive visualization pipeline with a set of coordinated views that characterizes the different methods. Instant visual hints are interactively provided in the views to guide users in finding and refining analogs. We have worked closely with the domain experts in the meteorological research to design and develop the system. The effectiveness of the system is demonstrated using two case studies. An informal evaluation with the experts proves the usability and usefulness of the system.", "uri": "https://vimeo.com/136210969", "name": "VIS15 preview: A Visual Voting Framework for Weather Forecast Calibration", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:14+00:00", "description": "[SciVis paper] \n\nAuthors: Kuno Kurzhals, Marcel Hlawatsch, Florian Heimerl, Michael Burch, Thomas Ertl, Daniel Weiskopf\n\nAbstract: We present a new visualization approach for displaying eye tracking data from multiple participants. We aim to show the spatio-temporal data of the gaze points in the context of the underlying image or video stimulus without occlusion. Our technique, denoted as gaze stripes, does not require the explicit definition of areas of interest but directly uses the image data around the gaze points, similar to thumbnails for images. A gaze stripe consists of a sequence of such gaze point images, oriented along a horizontal timeline. By displaying multiple aligned gaze stripes, it is possible to analyze and compare the viewing behavior of the participants over time. Since the analysis is carried out directly on the image data, expensive post-processing or manual annotation are not required. Therefore, not only patterns and outliers in the participants' scanpaths can be detected, but the context of the stimulus is available as well. Furthermore, our approach is especially well suited for dynamic stimuli due to the non-aggregated temporal mapping. Complementary views, i.e., markers, notes, screenshots, histograms, and results from automatic clustering, can be added to the visualization to display analysis results. We illustrate the usefulness of our technique on static and dynamic stimuli. Furthermore, we discuss the limitations and scalability of our approach in comparison to established visualization techniques.", "uri": "https://vimeo.com/136210958", "name": "VIS15 preview: Gaze Stripes: Image-Based Visualization of Eye Tracking Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:08+00:00", "description": "[SciVis paper] \n\nAuthors: Yi Gu, Chaoli Wang, Tom Peterka, Robert Jacob, Seung Hyun Kim\n\nAbstract: A notable recent trend in time-varying volumetric data analysis and visualization is to extract data relationships and represent them in a low-dimensional abstract graph view for visual understanding and making connections to the underlying data. Nevertheless, the ever-growing size and complexity of data demands novel techniques that go beyond standard brushing and linking to allow significant reduction of cognition overhead and interaction cost. In this paper, we present a mining approach that automatically extracts meaningful features from a graph-based representation for exploring time-varying volumetric data. This is achieved through the utilization of a series of graph analysis techniques including graph simplification, community detection, and visual recommendation. We investigate the most important transition relationships for time-varying data and evaluate our solution with several time-varying data sets of different sizes and characteristics. For gaining insights from the data, we show that our solution is more efficient and effective than simply asking users to extract relationships via standard interaction techniques, especially when the data set is large and the relationships are complex. We also collect expert feedback to confirm the usefulness of our approach.", "uri": "https://vimeo.com/136210947", "name": "VIS15 preview: Mining Graphs for Understanding Time-Varying Volumetric Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:39:02+00:00", "description": "[SciVis paper] \n\nAuthors: Marco Ament, Carsten Dachsbacher\n\nAbstract: We present a novel method to compute anisotropic shading for direct volume rendering to improve the perception of the orientation and shape of surface-like structures. We determine the scale-aware anisotropy of a shading point by analyzing its ambient region. We sample adjacent points with similar scalar values to perform a principal component analysis by computing the eigenvectors and eigenvalues of the covariance matrix. In particular, we estimate the tangent directions, which serve as the tangent frame for anisotropic bidirectional reflectance distribution functions. Moreover, we exploit the ratio of the eigenvalues to measure the magnitude of the anisotropy at each shading point. Altogether, this allows us to model a data-driven, smooth transition from isotropic to strongly anisotropic volume shading. In this way, the shape of volumetric features can be enhanced significantly by aligning specular highlights along the principal direction of anisotropy. Our algorithm is independent of the transfer function, which allows us to compute all shading parameters once and store them with the data set. We integrated our method in a GPU-based volume renderer, which offers interactive control of the transfer function, light source positions, and viewpoint. Our results demonstrate the benefit of anisotropic shading for visualization to achieve data-driven local illumination for improved perception compared to isotropic shading.", "uri": "https://vimeo.com/136210939", "name": "VIS15 preview: Anisotropic Ambient Volume Shading", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:56+00:00", "description": "[SciVis paper] \n\nAuthors: Thomas Butkiewicz, Andrew H. Stevens\n\nAbstract: Previous perceptual research and human factors studies have identified several effective methods for texturing 3D surfaces to ensure that their curvature is accurately perceived by viewers. However, most of these studies examined the application of these techniques to static surfaces. This paper explores the effectiveness of applying these techniques to dynamically changing surfaces. When these surfaces change shape, common texturing methods, such as grids and contours, induce a range of different motion cues, which can draw attention and provide information about the size, shape, and rate of change. A human factors study was conducted to evaluate the relative effectiveness of these methods when applied to dynamically changing pseudo-terrain surfaces. The results indicate that, while no technique is most effective for all cases, contour lines generally perform best, and that the pseudo-contour lines induced by banded color scales convey the same benefits.", "uri": "https://vimeo.com/136210927", "name": "VIS15 preview: Effectiveness of Structured Textures on Dynamically Changing Terrain-like Surfaces", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:44+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Giusy Di Lorenzo, Marco Luca Sbodio, Francesco Calabrese, Michele Berlingerio, Fabio Pinelli, Rahul Nair\n\nAbstract: The deep penetration of mobile phones offers cities the ability to opportunistically monitor citizens\u2019mobility and use datadriven insights to better plan and manage services. With large scale data onmobility patterns, operators can move away from the costly, mostly survey based, transportation planning processes, to a more data-centric view, that places the instrumented user at the center of development. In this framework, using mobile phone data to perform transit analysis and optimization represents a new frontier with significant societal impact, especially in developing countries. In this paper we present AllAboard, an intelligent tool that analyses cellphone data to help city authorities in visually exploring urban mobility and optimizing public transport. This is performed within a self contained tool, as opposed to the current solutions which rely on a combination of several distinct tools for analysis, reporting, optimisation and planning. An interactive user interface allows transit operatorsto visually explore the travel demand in both space and time, correlate it with the transit network, and evaluate the quality of service that a transit network provides to the citizens at very fine grain. Operators can visually test scenarios for transit network improvements, and compare the expected impact on the travellers\u2019 experience. The system has been tested using real telecommunication data for the city of Abidjan, Ivory Coast, and evaluated from a data mining, optimisation and user prospective.", "uri": "https://vimeo.com/136210900", "name": "VIS15 preview: AllAboard: Visual Exploration of Cellphone Mobility Data to Optimise Public Transport", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:39+00:00", "description": "[TVCG paper presentation]\n\nAuthors: David L\u00f3pez, Lora Oehlberg, Candemir Doger, Tobias Isenberg\n\nAbstract: We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopicviewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of thestereoscopic and monoscopic displays. We show how this mapping supports interactive dataexploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users\u2019 movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.", "uri": "https://vimeo.com/136210888", "name": "VIS15 preview: Towards an Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:34+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Hua Guo, Jeff Huang, David H. Laidlaw\n\nAbstract: When visualizing data with uncertainty, a common approach is to treat uncertainty as an additional dimension and encode it using a visual variable. The effectiveness of this approach depends on how the visual variables chosen for representing uncertainty and other attributes interact to influence the user\u2019s perception of each variable. We report a user study on the perception of graph edge attributes when uncertainty associated with each edge and the main edge attribute are visualized simultaneously using two separate visual variables. The study covers four visual variables that are commonly used for visualizing uncertainty on line graphical primitives: lightness, grain, fuzziness, and transparency. We select width, hue, and saturation for visualizing the main edge attribute and hypothesize that we can observe interference between the visual variable chosen to encode the main edge attribute and that to encode uncertainty, as suggested by the concept of dimensional integrality. Grouping the seven visualvariables as color-based, focus-based, or geometry-based, we further hypothesize that the degree of interference is affected by the groups to which the two visual variables belong. We consider two further factors in the study: discriminability level for each visual variable as a factor intrinsic to the visualvariables and graph-task type (visual search versus comparison) as a factor extrinsic to the visualvariables. Our results show that the effectiveness of a visual variable in depicting uncertainty is strongly mediated by all the factors examined here. Focus-based visual variables (fuzziness, grain, and transparency) are robust to the choice of visual variables for encoding the main edge attribute, though fuzziness has stronger negative impact on the perception of width and transparency has stronger negative impact on the perception of hue than the other uncertainty visual variables. We found that interference between hue and lightness is much greater tha- that between saturation and lightness, though all three are color-based visual variables. We also found a compound relationship between discriminability level and the degree of dimensional integrality. We discuss the generalizability and limitation of the results and conclude with design considerations for visualizing graph uncertaintyderived from these results, including recommended choices of visual variables when the relative importance of data attributes and graph tasks is known.", "uri": "https://vimeo.com/136210879", "name": "VIS15 preview: Representing Uncertainty in Graph Edges: An Evaluation of Paired Visual Variables", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:27+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Jungu Choi, Deok Gun Park, Yuet Ling Wong, Eli Raymond Fisher, Niklas Elmqvist\n\nAbstract: Standard user applications provide a range of cross-cutting interaction techniques that are common to virtually all such tools: selection, filtering, navigation, layer management, and cut-and-paste. We presentVisDock, a JavaScript mixin library that provides a core set of these cross-cutting interactiontechniques for visualization, including selection (lasso, paths, shape selection, etc), layer management (visibility, transparency, set operations, etc), navigation (pan, zoom, overview, magnifying lenses, etc), and annotation (point-based, region-based, data-space based, etc). To showcase the utility of the library, we have released it as Open Source and integrated it with a large number of existing web-basedvisualizations. Furthermore, we have evaluated VisDock using qualitative studies with both developers utilizing the toolkit to build new web-based visualizations, as well as with end-users utilizing it to explore movie ratings data. Results from these studies highlight the usability and effectiveness of the toolkitfrom both developer and end-user perspectives.", "uri": "https://vimeo.com/136210843", "name": "VIS15 preview: VisDock: A Toolkit for Cross-Cutting Interactions in Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:22+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Puripant Ruchikachorn, Klaus Mueller\n\nAbstract: We propose the concept of teaching (and learning) unfamiliar visualizations by analogy, that is, demonstrating an unfamiliar visualization method by linking it to another more familiar one, where the in-betweens are designed to bridge the gap of these two visualizations and explain the difference in a gradual manner. As opposed to a textual description, our morphing explains an unfamiliar visualizationthrough purely visual means. We demonstrate our idea by ways of four visualization pair examples: data table and parallel coordinates, scatterplot matrix and hyperbox, linear chart and spiral chart, and hierarchical pie chart and treemap. The analogy is commutative i.e. any member of the pair can be the unfamiliar visualization. A series of studies showed that this new paradigm can be an effective teaching tool. The participants could understand the unfamiliar visualization methods in all of the four pairs either fully or at least significantly better after they observed or interacted with the transitions from the familiar counterpart. The four examples suggest how helpful visualization pairings be identified and they will hopefully inspire other visualization morphings and associated transition strategies to be identified.", "uri": "https://vimeo.com/136210835", "name": "VIS15 preview: Learning Visualizations by Analogy: Promoting Visual Literacy through Visualization Morphing", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:15+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Hongfeng Yu, Jinrong Xie,  Kwan-Liu Ma, Hemanth Kolla, Jacqueline H. Chen\n\nAbstract: Computing distance fields is fundamental to many scientific and engineering applications. Distancefields can be used to direct analysis and reduce data. In this paper, we present a highly scalablemethod for computing 3D distance fields on massively parallel distributedmemory machines. A new distributed spatial data structure, named parallel distance tree, is introduced to manage the level sets of data and facilitate surface tracking over time, resulting in significantly reduced computation and communication costs for calculating the distance to the surface of interest from any spatial locations. Our method supports several data types and distance metrics from realworld applications. We demonstrate its efficiency and scalability on state-of-the-art supercomputers using both large-scalevolume data sets and surface models. We also demonstrate in-situ distance field computation on dynamic turbulent flame surfaces for a petascale combustion simulation. Our work greatly extends the usability of distance fields for demanding applications.", "uri": "https://vimeo.com/136210810", "name": "VIS15 preview: Scalable Parallel Distance Field Construction for Large-Scale Applications", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:09+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Aritra Dasgupta, Jorge Poco, Yaxing Wei, Robert Cook, Enrico Bertini, Claudio T. Silva\n\nAbstract: Evaluation methodologies in visualization have mostly focused on how well the tools and techniques cater to the analytical needs of the user. While this is important in determining the effectiveness of the tools and advancing the state-of-the-art in visualization research, a key area that has mostly been overlooked is how well established visualization theories and principles are instantiated in practice. This is especially relevant when domain experts, and not visualization researchers, design visualizations for analysis of their data or for broader dissemination of scientific knowledge. There is very little research on exploring the synergistic capabilities of cross-domain collaboration between domain experts andvisualization researchers. To fill this gap, in this paper we describe the results of an exploratory studyof climate data visualizations conducted in tight collaboration with a pool of climate scientists. The studyanalyzes a large set of static climate data visualizations for identifying their shortcomings in terms ofvisualization design. The outcome of the study is a classification scheme that categorizes the designproblems in the form of a descriptive taxonomy. The taxonomy is a first attempt for systematically categorizing the types, causes, and consequences of design problems in visualizations created by domain experts. We demonstrate the use of the taxonomy for a number of purposes, such as, improving the existing climate data visualizations, reflecting on the impact of the problems for enabling domain experts in designing better visualizations, and also learning about the gaps and opportunities for future visualization research. We demonstrate the applicability of our taxonomy through a number of examples and discuss the lessons learnt and implications of our findings.", "uri": "https://vimeo.com/136210800", "name": "VIS15 preview: Bridging Theory with Practice: An Exploratory Study of Visualization Use and Design for Climate Model...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:38:03+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Maarten H. Everts, Henk Bekker, Jos B.T.M. Roerdink, Tobias Isenberg\n\nAbstract: We present a visualization technique for brain fiber tracts from DTI data that provides insight into thestructure of white matter through visual abstraction. We achieve this abstraction by analyzing the localsimilarity of tract segment directions at different scales using a stepwise increase of the search range. Next, locally similar tract segments are moved toward each other in an iterative process, resulting in alocal contraction of tracts perpendicular to the local tract direction at a given scale. This not only leads to the abstraction of the global structure of the white matter as represented by the tracts, but also creates volumetric voids. This increase of empty space decreases the mutual occlusion of tracts and, consequently, results in a better understanding of the brain's three-dimensional fiber tract structure. Our implementation supports an interactive and continuous transition between the original and the abstracted representations via various scale levels of similarity. We also support the selection of groups of tracts, which are highlighted and rendered with the abstracted visualization as context.", "uri": "https://vimeo.com/136210785", "name": "VIS15 preview: Exploration of the Brain's White Matter Structure through Visual Abstraction and Multi-Scale Local Fiber...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:37:58+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Zhiyuan Zhang; Kevin T. McDonnell, Erez Zadok, Klaus Mueller\n\nAbstract: Correlation analysis can reveal the complex relationships that often exist among the variables in multivariate data. However, as the number of variables grows, it can be difficult to gain a good understanding of the correlation landscape and important intricate relationships might be missed. We previously introduced a technique that arranged the variables into a 2D layout, encoding their pairwisecorrelations. We then used this layout as a network for the interactive ordering of axes in parallel coordinate displays. Our current work expresses the layout as a correlation map and employs it forvisual correlation analysis. In contrast to matrix displays where correlations are indicated at intersections of rows and columns, our map conveys correlations by spatial proximity which is more direct and more focused on the variables in play. We make the following new contributions, some unique to our map: (1) we devise mechanisms that handle both categorical and numerical variables within a unified framework, (2) we achieve scalability for large numbers of variables via a multi-scale semantic zooming approach, (3) we provide interactive techniques for exploring the impact of value bracketing on correlations, and (4) we visualize data relations within the sub-spaces spanned by correlated variables by projecting the data into a corresponding tessellation of the map.", "uri": "https://vimeo.com/136210778", "name": "VIS15 preview: Visual Correlation Analysis of Numerical and Categorical Data on the Correlation Map", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:37:51+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Yuzuru Tanahashi, Chien-Hsin Hsueh, Kwan-Liu Ma\n\nAbstract: This paper presents a novel framework for applying storyline visualizations to streaming data. Theframework includes three components: a new data management scheme for processing and storing the incoming data, a layout construction algorithm specifically designed for incrementally generatingstorylines from streaming data, and a layout refinement algorithm for improving the legibility of thevisualization. By dividing the layout computation to two separate components, one for constructing and another for refining, our framework effectively provides the users with the ability to follow and reason dynamic data. The evaluation studies of our storyline visualization framework demonstrate its efficacy to present streaming data as well as its superior performance over existing methods in terms of both computational efficiency and visual clarity.", "uri": "https://vimeo.com/136210767", "name": "VIS15 preview: An Efficient Framework for Generating Storyline Visualizations from Streaming Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:37:45+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Samah Gad, Waqas Javed, Sohaib Ghani, Niklas Elmqvist, Tom Ewing, Keith N. Hampton, Naren Ramakrishnan\n\nAbstract: We present ThemeDelta, a visual analytics system for extracting and visualizing temporal trends, clustering, and reorganization in time-indexed textual datasets. ThemeDelta is supported by a dynamictemporal segmentation algorithm that integrates with topic modeling algorithms to identify change points where significant shifts in topics occur. This algorithm detects not only the clustering and associations of keywords in a time period, but also their convergence into topics (groups of keywords) that may later diverge into new groups. The visual representation of ThemeDelta uses sinuous, variable-width lines to show this evolution on a timeline, utilizing color for categories, and line width for keyword strength. We demonstrate how interaction with ThemeDelta helps capture the rise and fall of topics by analyzing archives of historical newspapers, of U.S. presidential campaign speeches, and of social messages collected through iNeighbors, a web-based social website. ThemeDelta is evaluated using a qualitative expert user study involving three researchers from rhetoric and history using the historical newspapers corpus.", "uri": "https://vimeo.com/136210757", "name": "VIS15 preview: ThemeDelta: Dynamic Segmentations over Temporal Topic Models", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:37:39+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Maxine Dumas, Michael McGuffin, Patrick Chasse\n\nAbstract: We investigate the selection of curves within a 2D visualization by specifying their angle or slope. Suchangular selection has applications in parallel coordinates, time series visualizations, spatio-temporal movement data, etc. Our interaction technique specifies a region of interest in the visualization (with a position and diameter), a direction, and an angular tolerance, all with a single drag. We experimentally compared this angular selection technique with other techniques for selecting curves, and found thatangular selection resulted in a higher number of trials that were successful on the first attempt and fewer incorrectly selected curves, and was also subjectively preferred by participants. We then present the design of a popup lens widget, called the VectorLens, that allows for easy angular selection and also allows the user to perform additional filtering operations based on type of curve. MultipleVectorLens widgets can also be instantiated to combine the results of their filtering operations with boolean operators.", "uri": "https://vimeo.com/136210748", "name": "VIS15 preview: VectorLens: Angular Selection of Curves within 2D Dense Visualizations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:37:34+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Ronak Etemadpour, Robson Motta, Jose Gustavo de Souza Paiva, Rosane Minghim, Maria Cristina Ferreira de Oliveira, Lars Linsen\n\nAbstract: Similarity-based layouts generated by multidimensional projections or other dimension reduction techniques are commonly used to visualize high-dimensional data. Many projection techniques have been recently proposed addressing different objectives and application domains. Nonetheless, very little is known about the effectiveness of the generated layouts from a user's perspective, how distinct layouts from the same data compare regarding the typical visualization tasks they support, or how domain-specific issues affect the outcome of the techniques. Learning more about projection usage is an important step towards both consolidating their role in high-dimensional data analysis and taking informed decisions when choosing techniques. This work provides a contribution towards this goal. We describe the results of an investigation on the performance of layouts generated by projectiontechniques as perceived by their users. We conducted a controlled user study to test against the following hypotheses: (1) projection performance is task-dependent; (2) certain projections perform better on certain types of tasks; (3) projection performance depends on the nature of the data; and (4) subjects prefer projections with good segregation capability. We generated layouts of high-dimensionaldata with five techniques representative of different projection approaches. As application domains we investigated image and document data. We identified eight typical tasks, three of them related to segregation capability of the projection, three related to projection precision, and two related to incurred visual cluttering. Answers to questions were compared for correctness against `ground truth' computed directly from the data. We also looked at subject confidence and task completion times. Statistical analysis of the collected data resulted in Hypotheses 1 and 3 being confirmed, Hypothesis 2 being confirmed partially and Hypotheses 4 could not be confirmed. We discuss our findings in com- arison with some numerical measures of projection layout quality. Our results offer interesting insight on the use of projection layouts in data visualization tasks and provide a departing point for further systematic investigations.", "uri": "https://vimeo.com/136210739", "name": "VIS15 preview: Perception-based Evaluation of Projection Methods for Multidimensional Data Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:37:28+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Amy L. Griffin, Anthony C. Robinson\n\nAbstract: In most coordinated view geovisualization tools, a transient visual effect is used to highlightobservations across views when brushed with a mouse or other input device. Most current geovisualization and information visualization systems use colored outlines or fills to highlightobservations, but there remain a wide range of alternative visual strategies that can also be implemented and compared to color highlighting to evaluate user performance. This paper describes the results of an experiment designed to compare user performance with two highlighting methods; colorand leader lines. Our study methodology uses eye-tracking to capture participant eye fixations while they answer questions that require attention to highlighted observations in multiple views. Our results show that participants extract information as efficiently from coordinated view displays that use leaderline highlighting to link information as they do from those that use a specific color to highlight items. We also found no significant differences when changing the color of the highlighting effect from red to black. We conclude that leader lines show significant potential for use as an alternative highlighting method incoordinated multiple view visualizations, allowing color to be reserved for representing thematic attributes of data.", "uri": "https://vimeo.com/136210730", "name": "VIS15 preview: Comparing Color and Leader Line Highlighting Strategies in Coordinated View Geovisualizations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:37:23+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Dandan Huang , Melanie Tory, Bon Adriel Aseniero, Lyn Bartram, Scott Bateman, Sheelagh Carpendale, Anthony Tang, Rob Woodbury\n\nAbstract: Data surrounds each and every one of us in our daily lives, ranging from exercise logs, to archives of our interactions with others on social media, to online resources pertaining to our hobbies. There is enormous potential for us to use these data to understand ourselves better and make positive changes in our lives. Visualization (Vis) and visual analytics (VA) offer substantial opportunities to help individuals gain insights about themselves, their communities and their interests; however, designing tools to support data analysis in non-professional life brings a unique set of research and design challenges. We investigate the requirements and research directions required to take full advantage of Vis and VA in a personal context. We develop a taxonomy of design dimensions to provide a coherent vocabulary for discussing personal visualization and personal visual analytics. By identifying and exploring clusters in the design space, we discuss challenges and share perspectives on future research. This work brings together research that was previously scattered across disciplines. Our goal is to call research attention to this space and engage researchers to explore the enabling techniques and technology that will support people to better understand data relevant to their personal lives, interests, and needs.", "uri": "https://vimeo.com/136210716", "name": "VIS15 preview: Personal Visualization and Personal Visual Analytics", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T14:37:17+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Katerina Vrotsou, Halldor Janetzko, Carlo Navarra, Georg Fuchs, David Spretke, Florian Mansmann, Natalia Andrienko, Gennady Andrienko\n\nAbstract: Movement data sets collected using today's advanced tracking devices consist of complex trajectoriesin terms of length, shape, and number of recorded positions. Multiple additional attributes characterizing the movement and its environment are often also included making the level of complexity even higher.Simplification of trajectories can improve the visibility of relevant information by reducing less relevant details while maintaining important movement patterns. We propose a systematic stepwisemethodology for simplifying and thematically enhancing trajectories in order to support their visual analysis. The methodology is applied iteratively and is composed of: (a) a simplification step applied to reduce the morphological complexity of the trajectories, (b) a thematic enhancement step which aims at accentuating patterns of movement, and (c) the representation and interactive exploration of the results in order to make interpretations of the findings and further refinement to the simplification andenhancement process. We illustrate our methodology through an analysis example of two different types of tracks, aircraft and pedestrian movement.", "uri": "https://vimeo.com/136210705", "name": "VIS15 preview: A Methodology for Simplification and Thematic Enhancement of Trajectories", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:55:51+00:00", "description": "[TVCG paper presentation]\n\nAuthors: Nan Cao, Yu-Ru Lin, David Gotz\n\nAbstract: Data with multiple probabilistic labels are common in many situations. For example, a movie may be associated with multiple genres with different levels of confidence. Despite their ubiquity, the problem of visualizing probabilistic labels has not been adequately addressed. Existing approaches often either discard the probabilistic information, or map the data to a low-dimensional subspace where their associations with original labels are obscured. In this paper, we propose a novel visual technique,UnTangle Map, for visualizing probabilistic multi-labels. In our proposed visualization, data items are placed inside a web of connected triangles, with labels assigned to the triangle vertices such that nearby labels are more relevant to each other. The positions of the data items are determined based on the probabilistic associations between items and labels. UnTangle Map provides both (a) an automaticlabel placement algorithm, and (b) adaptive interactions that allow users to control the label positioning for different information needs. Our work makes a unique contribution by providing an effective way to investigate the relationship between data items and their probabilistic labels, as well as the relationships among labels. Our user study suggests that the visualization effectively helps users discover emergent patterns and compare the nuances of probabilistic information in the data labels.", "uri": "https://vimeo.com/136206607", "name": "VIS15 preview: UnTangle Map: Visual Analysis of Probabilistic Multi-Label Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:54:10+00:00", "description": "[VAST paper] \n\nAuthors: Josua Krause, Adam Perer, Harry Stavropoulos\n\nAbstract: Many researchers across diverse disciplines aim to analyze the behavior of cohorts whose behaviors are recorded in large event databases. However, extracting cohorts from databases is a difficult yet important step, often overlooked in many analytical solutions. This is especially true when researchers wish to restrict their cohorts to exhibit a particular temporal pattern of interest. In order to fill this gap, we designed COQUITO, a visual interface that assists users defining cohorts with temporal constraints. COQUITO was designed to be comprehensible to domain experts with no preknowledge of database queries and also to encourage exploration. We then demonstrate the utility of COQUITO via two case studies, involving medical and social media researchers.", "uri": "https://vimeo.com/136206435", "name": "VIS15 preview: Supporting Iterative Cohort Construction with Visual Temporal Queries", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:54:02+00:00", "description": "[VAST paper] \n\nAuthors: Michael Brooks, Saleema Amershi, Bongshin Lee, Steven Drucker, Ashish Kapoor, Patrice Simard\n\nAbstract: Machine learning requires an effective combination of data, features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, an interactive visual analytics tool for building new dictionary features (semantically related groups of words) for text classification problems. FeatureInsight supports an error-driven feature ideation process and provides interactive visual summaries of sets of misclassified documents. We conducted a controlled experiment evaluating both visual summaries and sets of errors in FeatureInsight. Our results show that visual summaries significantly improve feature ideation, especially in combination with sets of errors. Users preferred visual summaries over viewing raw data, and only preferred examining sets when visual summaries were provided. We discuss extensions of both approaches to data types other than text, and point to important areas for future research.", "uri": "https://vimeo.com/136206426", "name": "VIS15 preview: FeatureInsight: Visual Support for Error-Driven Feature Ideation in Text Classification", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:57+00:00", "description": "[VAST paper] \n\nAuthors: Carlos Scheidegger, Gordon Woodhull, Stephen North, Simon Urbanek\n\nAbstract: Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.", "uri": "https://vimeo.com/136206416", "name": "VIS15 preview: Collaborative Visual Analysis on the Web with RCloud", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:51+00:00", "description": "[VAST paper] \n\nAuthors: Florian Heimerl, Qi Han, Steffen Koch, Thomas Ertl\n\nAbstract: The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.", "uri": "https://vimeo.com/136206405", "name": "VIS15 preview: CiteRivers: Visual Analytics of Citation Patterns", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:45+00:00", "description": "[VAST paper] \n\nAuthors: Rainer Splechtna, Kresimir Matkovic, Denis Gracanin, Mario Jelovic, Helwig Hauser\n\nAbstract: Multi-level simulation models, i.e., models where different components are simulated using sub-models of varying levels of complexity, belong to the current state-of-the-art in simulation. The existing analysis practice for multi-level simulation results is to manually compare results from different levels of complexity, amounting to a very tedious and error-prone, trial-and-error exploration process. In this paper, we introduce hierarchical visual steering, a new approach to the exploration and design of complex systems. Hierarchical visual steering makes it possible to explore and analyze hierarchical simulation ensembles at different levels of complexity. At each level, we deal with a dynamic simulation ensemble --- the ensemble grows during the exploration process. There is at least one such ensemble per simulation level, resulting in a collection of dynamic ensembles, analyzed simultaneously. The key challenge is to map the multi-dimensional parameter space of one ensemble to the multi-dimensional parameter space of another ensemble (from another level). In order to support the interactive visual analysis of such complex data we propose a novel approach to interactive and semi-automatic parameter space segmentation and comparison. The approach combines a novel interaction technique and automatic, computational methods - clustering, concave hull computation, and concave polygon overlapping - to support the analysts in the cross-ensemble parameter space mapping. In addition to the novel parameter space segmentation we also deploy coordinated multiple views with standard plots. We describe the abstract analysis tasks, identified during a case study, i.e., the design of a variable valve actuation system of a car engine. The study is conducted in cooperation with experts from the automotive industry. Very positive feedback indicates the usefulness and efficiency of the newly proposed approach.", "uri": "https://vimeo.com/136206397", "name": "VIS15 preview: Interactive Visual Steering of Hierarchical Simulation Ensembles", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:39+00:00", "description": "[VAST paper] \n\nAuthors: Stefan J\u00e4nicke, Josef Focht, Gerik Scheuermann\n\nAbstract: Determining similar objects based upon the features of an object of interest is a common task for visual analytics systems. This process is called profiling, if the object of interest is a person with individual attributes. The profiling of musicians similar to a musician of interest with the aid of visual means became an interesting research question for musicologists working with the Bavarian Musicians Encyclopedia Online. This paper illustrates the development of a visual analytics profiling system that is used to address such research questions. Taking musicological knowledge into account, we outline various steps of our collaborative digital humanities project, priority (1) the definition of various measures to determine the similarity of musicians' attributes, and (2) the design of an interactive profiling system that supports musicologists in iteratively determining similar musicians. The utility of the profiling system is emphasized by various usage scenarios illustrating current research questions in musicology.", "uri": "https://vimeo.com/136206387", "name": "VIS15 preview: Interactive Visual Profiling of Musicians", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:35+00:00", "description": "[VAST paper] \n\nAuthors: Chris Bryan, Xue Wu, Susan Mniszewski, Kwan-Liu Ma\n\nAbstract: The Epidemic Simulation System (EpiSimS) is a scalable, complex modeling tool for analyzing disease within the United States. Due to its high input dimensionality, time requirements, and resource constraints, simulating over the entire parameter space is unfeasible. One solution is to take a granular sampling of the input space and use simpler predictive models (emulators) in between. The quality of the implemented emulator depends on many factors: its robustness, sophistication, configuration settings, and suitability to the input data. Visual analytics (VA) can be leveraged to provide guidance and understanding to the user. In this paper, we have implemented a novel VA interface and workflow for emulator building and use. We introduce a workflow to build emulators, make predictions, and then analyze the results. Our prediction process first predicts temporal time series, and uses these to derive predicted spatial densities. Integrated into the EpiSimS framework, we target users who are non-experts at statistical modeling. This approach allows for a high level of analysis into the state of the built emulators and their resultant predictions. We present our workflow, models and the associated VA system, and evaluate the overall utility with feedback from EpiSimS scientists.", "uri": "https://vimeo.com/136206382", "name": "VIS15 preview: Integrating Predictive Analytics into a Spatio-Temporal Epidemic Simulation", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:30+00:00", "description": "[VAST paper] \n\nAuthors: Siming Chen, Zhenhuang Wang, Cong Guo, Zuchao Wang, Jie Liang, Xiaoru Yuan, Xiaolong (Luke) Zhang, Jiawan Zhang\n\nAbstract: Social media data with geotags can be used to track people's movements in their daily lives. By providing both rich text and movement information, visual analysis on social media data can be both interesting and challenging. In contrast to traditional movement data, the sparseness and irregularity of social media data increase the difficulty of extracting movement patterns. To facilitate the understanding of people's movements, we present an interactive visual analytics system to support the exploration of sparsely sampled trajectory data from social media. We propose a heuristic model to reduce the uncertainty caused by the nature of social media data. \u00c2\u00ad\u00c2\u00adIn the proposed system, users can filter and select reliable data from each derived movement category, based on the guidance of uncertainty model and interactive selection tools. By iteratively analyzing filtered movements, users can explore the semantics of movements, including the transportation methods, frequent visiting sequences and keyword descriptions. We provide two cases to demonstrate how our system can help users to explore the movement patterns.", "uri": "https://vimeo.com/136206374", "name": "VIS15 preview: Interactive Visual Movement Patterns Discovering from Sparsely Sampled Geo-tagged Social Media Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:23+00:00", "description": "[VAST paper] \n\nAuthors: Nivan Ferreira, Marcos Lage, Harish Doraiswamy, Huy Vo, Luc Wilson, Heidi Werner, Muchan Park, Claudio Silva\n\nAbstract: Architects working with developers and city planners typically rely on experience, precedent and data analyzed in isolation when making decisions that impact the character of a city. These decisions are critical in enabling vibrant, sustainable environments but must also negotiate a range of complex political and social forces. This requires those shaping the built environment to balance maximizing the value of a new development with its impact on the character of a neighborhood. As a result architects are focused on two issues throughout the decision making process: a) what defines the character of a neighborhood? and b) how will a new development change its neighborhood? In the first, character can be influenced by a variety of factors and understanding the interplay between diverse data sets is crucial; including safety, transportation access, school quality and access to entertainment. In the second, the impact of a new development is measured, for example, by how it impacts the view from the buildings that surround it. In this paper, we work in collaboration with architects to design Urbane, a 3-dimensional multi-resolution framework that enables a data- driven approach for decision making in the design of new urban development. This is accomplished by integrating multiple data layers and impact analysis techniques facilitating architects to explore and assess the effect of these attributes on the character and value of a neighborhood. Several of these data layers, as well as impact analysis, involve working in 3-dimensions and operating in real time. Efficient computation and visualization is accomplished through the use of techniques from computer graphics. We demonstrate the effectiveness of Urbane through a case study of development in Manhattan depicting how a data-driven understanding of the value and impact of speculative buildings can benefit the design-development process between architects, planners and developers.", "uri": "https://vimeo.com/136206358", "name": "VIS15 preview: Urbane: A 3D Framework to Support Data Driven Decision Making in Urban Development", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:18+00:00", "description": "[VAST paper] \n\nAuthors: Paola Valdivia, Fabio Dias, Fabiano Petronetto, Claudio Silva, Luis Gustavo Nonato\n\nAbstract: Visualizing time-varying data, defined on the nodes of a graph, is a challenging problem that has been faced with different approaches. Although techniques based on aggregation, topology, and topic modeling have proven their usefulness, the visual analysis of smooth and or abrupt data variations as well as the evolution of such variations over time are aspects not properly tackled by existing methods. In this work we propose a novel visualization methodology that relies on graph wavelet theory and stacked graph metaphor to enable the visual analysis of time-varying data defined on the nodes of a graph. The proposed method is able to identify regions where data presents abrupt and mild spacial and/or temporal variation while still been able to show how such changes evolve over time, making the identification of events an easier task. The usefulness of our approach is shown through a set of results using synthetic as well as a real data set involving taxi trips in downtown Manhattan. The methodology was able to reveal interesting phenomena and events such as the identification of specific locations with abrupt variation in the number of taxi pickups.", "uri": "https://vimeo.com/136206353", "name": "VIS15 preview: Wavelet-based Visualization of Time-Varying Data on Graphs", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:12+00:00", "description": "[VAST paper] \n\nAuthors: Eric Alexander, Michael Gleicher\n\nAbstract: Topic modeling, a method of statistically extracting thematic content from a large collection of texts, is used for a wide variety of tasks within text analysis. Though there are a growing number of tools and techniques for exploring single models, comparisons between models are generally reduced to a small set of numerical metrics. These metrics may or may not reflect a model's performance on the analyst's intended task, and can therefore be insufficient to diagnose what causes differences between models. In this paper, we explore task-centric topic model comparison, considering how we can both provide detail for a more nuanced understanding of differences and address the wealth of tasks for which topic models are used. We derive comparison tasks from single-model uses of topic models, which predominantly fall into the categories of understanding topics, understanding similarity, and understanding change. Finally, we provide several visualization techniques that facilitate these tasks, including buddy plots, which combine color and position encodings to allow analysts to readily view changes in document similarity.", "uri": "https://vimeo.com/136206343", "name": "VIS15 preview: Task-Driven Comparison of Topic Models", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:07+00:00", "description": "[VAST paper] [HONORABLE MENTION]\n\nAuthors: Hua Guo, Steven Gomez, Caroline Ziemkiewicz, David Laidlaw\n\nAbstract: We present results from an experiment aimed at using logs of interactions with a visual analytics application to better understand how interactions lead to insight generation. We performed an insight-based user study of a visual analytics application and ran post hoc quantitative analyses of participants' measured insight metrics and interaction logs. The quantitative analyses identified features of interaction that were correlated with insight characteristics, and we confirmed these findings using a qualitative analysis of video captured during the user study. Results of the experiment include design guidelines for the visual analytics application aimed at supporting insight generation. Furthermore, we demonstrated an analysis method using interaction logs that identified which interaction patterns led to insights, going beyond insight-based evaluations that only quantify insight characteristics. We also discuss choices and pitfalls encountered when applying this analysis method, such as the benefits and costs of applying an abstraction framework to application-specific actions before further analysis. Our method can be applied to evaluations of other visualization tools to inform the design of insight-promoting interactions and to better understand analyst behaviors.", "uri": "https://vimeo.com/136206336", "name": "VIS15 preview: A Case Study Using Visualization Interaction Logs and Insight Metrics to Understand How Analysts Arrive at...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:53:01+00:00", "description": "[VAST paper] \n\nAuthors: Thomas L\u00f6we, Emmy-Charlotte F\u00f6rster, Georgia Albuquerque, Jens-Peter Kreiss, Marcus Magnor\n\nAbstract: Order selection of autoregressive processes is an active research topic in time series analysis, and the development and evaluation of automatic order selection criteria remains a challenging task for domain experts. We propose a visual analytics approach, to guide the analysis and development of such criteria. A flexible synthetic model generator--combined with specialized responsive visualizations--allows comprehensive interactive evaluation. Our fast framework allows feedback-driven development and fine-tuning of new order selection criteria in real-time. We demonstrate the applicability of our approach in three use-cases for two general as well as a real-world example.", "uri": "https://vimeo.com/136206328", "name": "VIS15 preview: Visual Analytics for Development and Evaluation of Order Selection Criteria for Autoregressive Processes", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:54+00:00", "description": "[VAST paper] \n\nAuthors: Phong Nguyen, Kai Xu, Ashley Wheat, William Wong, Simon Attfield, Bob Fields\n\nAbstract: Sensemaking is described as the process of comprehension, finding meaning and gaining insight from information, producing new knowledge and informing further action. Understanding the sensemaking process allows building effective visual analytics tools to make sense of large and complex datasets. Currently, it is often a manual and time-consuming undertaking to comprehend this: researchers collect observation data, transcribe screen capture videos and think-aloud recordings, identify recurring patterns, and eventually abstract the sensemaking process into a general model. In this paper, we propose a general approach to facilitate such a qualitative analysis process, and introduce a prototype, SensePath, to demonstrate the application of this approach with a focus on browser-based online sensemaking. The approach is based on a study of a number of qualitative research sessions including observations of users performing sensemaking tasks and post hoc analyses to uncover their sensemaking processes. Based on the study results and a follow-up participatory design session with HCI researchers, we decided to focus on the transcription and coding stages of thematic analysis. SensePath automatically captures user's sensemaking actions, i.e., analytic provenance, and provides multi-linked views to support their further analysis. A number of other requirements elicited from the design session are also implemented in SensePath, such as easy integration with existing qualitative analysis workflow and non-intrusive for participants. The tool was used by an experienced HCI researcher to analyze two sensemaking sessions. The researcher found the tool intuitive and considerably reduced analysis time, allowing better understanding of the sensemaking process.", "uri": "https://vimeo.com/136206317", "name": "VIS15 preview: SensePath: Understanding Sensemaking Process Through Analytic Provenance", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:49+00:00", "description": "[VAST paper] \n\nAuthors: Dominik Sacha, Hansi Senaratne, Bum Chul Kwon, Geoffrey Ellis, Daniel Keim\n\nAbstract: Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected, collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed to generate a visual representation of the data. These often introduce their own uncertainties, in addition to the ones inherent in the data, and these propagated and compounded uncertainties can result in impaired decision making. The user's confidence or trust in the results depends on the extent of user's awareness of the underlying uncertainties generated on the system side. This paper unpacks the uncertainties that propagate through visual analytics systems, illustrates how human's perceptual and cognitive biases influence the user's awareness of such uncertainties, and how this affects the user's trust building. The knowledge generation model for visual analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design of uncertainty-aware systems are presented that can aid the user in better decision making.", "uri": "https://vimeo.com/136206312", "name": "VIS15 preview: The Role of Uncertainty, Awareness, and Trust in Visual Analytics", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:44+00:00", "description": "[VAST paper] \n\nAuthors: Bum Chul Kwon, Sung-Hee Kim, Sukwon Lee, Jaegul Choo, Jina Huh, Ji Soo Yi\n\nAbstract: Through online health communities (OHCs), patients and caregivers exchange their illness experiences and strategies for overcoming the illness, and provide emotional support. To facilitate healthy and lively conversations in these communities, their members should be continuously monitored and nurtured by OHC administrators. The main challenge of OHC administrators' tasks lies in understanding the diverse dimensions of conversation threads that lead to productive discussions in their communities. In this paper, we present a design study in which three domain expert groups participated, an OHC researcher and two OHC administrators of online health communities, which was conducted to find with a visual analytic solution. Through our design study, we characterized the domain goals of OHC administrators and derived tasks to achieve these goals. As a result of this study, we propose a system called VisOHC, which visualizes individual OHC conversation threads as collapsed boxes--a visual metaphor of conversation threads. In addition, we augmented the posters' reply authorship network with marks and/or beams to show conversation dynamics within threads. We also developed unique measures tailored to the characteristics of OHCs, which can be encoded for thread visualizations at the users' requests. Our observation of the two administrators while using VisOHC showed that it supports their tasks and reveals interesting insights into online health communities. Finally, we share our methodological lessons on probing visual designs together with domain experts by allowing them to freely encode measurements into visual variables.", "uri": "https://vimeo.com/136206304", "name": "VIS15 preview: VisOHC: Designing Visual Analytics for Online Health Communities", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:38+00:00", "description": "[VAST paper] \n\nAuthors: Mengchen Liu, Shixia Liu, Xizhou Zhu, Qinying Liao, Furu Wei, Shimei Pan\n\nAbstract: Although there has been a great deal of interest in analyzing customer opinions and breaking news in microblogs, progress has been hampered by the lack of an effective mechanism to discover and retrieve data of interest from microblogs. To address this problem, we have developed an uncertainty-aware visual analytics approach to retrieve salient posts, users, and hashtags. We extend an existing ranking technique to compute a multifaceted retrieval result: the mutual reinforcement rank of a graph node, the uncertainty of each rank, and the propagation of uncertainty among different graph nodes. To illustrate the three facets, we have also designed a composite visualization with three visual components: a graph visualization, an uncertainty glyph, and a flow map. The graph visualization with glyphs, the flow map, and the uncertainty analysis together enable analysts to effectively find the most uncertain results and interactively refine them. We have applied our approach to several Twitter datasets. Qualitative evaluation and two real-world case studies demonstrate the promise of our approach for retrieving high-quality microblog data.", "uri": "https://vimeo.com/136206291", "name": "VIS15 preview: An Uncertainty-Aware Approach for Exploratory Microblog Retrieval", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:32+00:00", "description": "[VAST paper] \n\nAuthors: Dominik J\u00e4ckle, Fabian Fischer, Tobias Schreck, Daniel Keim\n\nAbstract: Multivariate time series data can be found in many application domains. Examples include data from computer networks, healthcare, social networks, or fi\u0081nancial markets. Often, patterns in such data evolve over time among multiple dimensions and are hard to detect. Dimensionality reduction methods such as PCA and MDS allow analysis and visualization of multivariate data, but per se do not provide means to explore multivariate patterns over time. We propose Temporal Multidimensional Scaling (TMDS), a novel visualization technique that computes temporal one-dimensional MDS plots for multivariate data which evolve over time. Using a sliding window approach, MDS is computed for each data window separately, and the results are plotted sequentially along the time axis, taking care of plot alignment. Our TMDS plots enable visual identi\u00ef\u00ac\u0081cation of patterns based on multidimensional similarity of the data evolving over time. We demonstrate the usefulness of our approach in the \u00ef\u00ac\u0081eld of network security and show in two case studies how users can iteratively explore the data to identify previously unknown, temporally evolving patterns.", "uri": "https://vimeo.com/136206282", "name": "VIS15 preview: Temporal MDS Plots for Analysis of Multivariate Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:27+00:00", "description": "[VAST paper] \n\nAuthors: Mihaela Jarema, Ismail Demir, Johannes Kehrer, R\u00fcdiger Westermann\n\nAbstract: We present a new visual analysis approach to support the comparative exploration of 2D vector-valued ensemble fields. Our approach enables the user to quickly identify the most similar groups of ensemble members, as well as the locations where the variation among the members is high. We further provide means to visualize the main features of the potentially multimodal directional distributions at user-selected locations. For this purpose, directional data is modelled using mixtures of probability density functions (pdfs), which allows us to characterize and classify complex distributions with relatively few parameters. The resulting mixture models are used to determine the degree of similarity between ensemble members, and to construct glyphs showing the direction, spread, and strength of the principal modes of the directional distributions. We also propose several similarity measures, based on which we compute pairwise member similarities in the spatial domain and form clusters of similar members. The hierarchical clustering is shown using dendrograms and similarity matrices, which can be used to select particular members and visualize their variations. A user interface providing multiple linked views enables the simultaneous visualization of aggregated global and detailed local variations, as well as the selection of members for a detailed comparison.", "uri": "https://vimeo.com/136206269", "name": "VIS15 preview: Comparative Visual Analysis of Vector Field Ensembles", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:21+00:00", "description": "[VAST paper] \n\nAuthors: Kristin Cook, Alex Endert, Nicholas Cramer, Joe Bruce, David Israel, Michael Wolverton, Edwin Russell Burtner Russ Burtner\n\nAbstract: Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.", "uri": "https://vimeo.com/136206260", "name": "VIS15 preview: Mixed-Initiative Visual Analytics Using Task-Driven Recommendations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:16+00:00", "description": "[VAST paper] \n\nAuthors: Eric Ragan, Alex Endert, Jian Chen, Jibonananda Sanyal\n\nAbstract: While the primary goal of visual analytics research is to improve the quality of insights and findings, a substantial amount of research in provenance has focused on the history of changes and advances throughout the analysis process. The term, provenance, has been used in a variety of ways to describe different types of records and histories related to visualization. The existing body of provenance research has grown to a point where the consolidation of design knowledge requires cross-referencing a variety of projects and studies spanning multiple domain areas. We present an organizational framework of the different types of provenance information and purposes for why they are desired in the field of visual analytics. Our organization is intended to serve as a framework to help researchers specify types of provenance and coordinate design knowledge across projects. We also discuss the relationships between these factors and the methods used to capture provenance information. In addition, our organization can be used to guide the selection of evaluation methodology and the comparison of study outcomes in provenance research.", "uri": "https://vimeo.com/136206245", "name": "VIS15 preview: Characterizing Provenance in Visualization and Data Analysis: An Organizational Framework of Provenance Types...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:11+00:00", "description": "[VAST paper] \n\nAuthors: Johanna Fulda, Matthew Brehmer, Tamara Munzner\n\nAbstract: We present TimeLineCurator, a browser-based authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing and encodes them along a visual timeline. Our goal is to facilitate the timeline creation process for journalists and others who tell temporal stories online. Current solutions involve manually extracting and formatting event data from source documents, a process that tends to be tedious and error prone. With TimeLineCurator, a prospective timeline author can quickly identify the extent of time encompassed by a document, as well as the distribution of events occurring along this timeline. Authors can speculatively browse possible documents to quickly determine whether they are appropriate sources of timeline material. TimeLineCurator provides controls for curating and editing events on a timeline, the ability to combine timelines from multiple source documents, and export curated timelines for online deployment. We evaluate TimeLineCurator through a benchmark comparison of entity extraction error against a manual timeline curation process, a preliminary evaluation of the user experience of timeline authoring, a brief qualitative analysis of its visual output, and a discussion of prospective use cases suggested by members of the target author communities following its deployment.", "uri": "https://vimeo.com/136206237", "name": "VIS15 preview: TimeLineCurator: Interactive Authoring of Visual Timelines from Unstructured Text", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:06+00:00", "description": "[VAST paper] \n\nAuthors: Sujin Jang, Niklas Elmqvist, Karthik Ramani\n\nAbstract: Pattern analysis of human motions, which is useful in many research areas, requires understanding and comparison of different styles of motion patterns. However, working with human motion tracking data to support such analysis poses great challenges. In this paper, we propose MotionFlow, a visual analytics system that provides an effective overview of various motion patterns based on an interactive flow visualization. This visualization formulates a motion sequence as transitions between static poses, and aggregates these sequences into a tree diagram to construct a set of motion patterns. The system also allows the users to directly reflect the context of data and their perception of pose similarities in generating representative pose states. We provide local and global controls over the partition-based clustering process. To support the users in organizing unstructured motion data into pattern groups, we designed a set of interactions that enables searching for similar motion sequences from the data, detailed exploration of data subsets, and creating and modifying the group of motion patterns. To evaluate the usability of MotionFlow, we conducted a user study with six researchers with expertise in gesture-based interaction design. They used MotionFlow to explore and organize unstructured motion tracking data. Results show that the researchers were able to easily learn how to use MotionFlow, and the system effectively supported their pattern analysis activities, including leveraging their perception and domain knowledge.", "uri": "https://vimeo.com/136206227", "name": "VIS15 preview: MotionFlow: Visual Abstraction and Aggregation of Sequential Patterns in Human Motion Tracking Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:52:00+00:00", "description": "[VAST paper] \n\nAuthors: Paul Klemm, Kai Lawonn, Sylvia Glasser, Uli Niemann, Kathrin Hegenscheid, Henry V\u00f6lzke, Bernhard Preim\n\nAbstract: Epidemiological studies comprise heterogeneous data about a subject group to define disease-specific risk factors. These data contain information (features) about a subject's lifestyle, medical status as well as medical image data. Statistical regression analysis is used to evaluate these features and to identify feature combinations indicating a disease (the target feature). \nWe propose an analysis approach of epidemiological data sets by incorporating all features in an exhaustive regression-based analysis. This approach combines all independent features w.r.t. a target feature. It provides a visualization that reveals insights into the data by highlighting relationships. The 3D Regression Heat Map, a novel 3D visual encoding, acts as an overview of the whole data set. It shows all combinations of two to three independent features with a specific target disease. Slicing through the 3D Regression Heat Map allows for the detailed analysis of the underlying relationships. Expert knowledge about disease-specific hypotheses can be included into the analysis by adjusting the regression model formulas. Furthermore, the influences of features can be assessed using a difference view comparing different calculation results. We applied our 3D Regression Heat Map method to a hepatic steatosis data set to reproduce results from a data mining-driven analysis. A qualitative analysis was conducted on a breast density data set. We were able to derive new hypotheses about relations between breast density and breast lesions with breast cancer. With the 3D Regression Heat Map, we present a visual overview of epidemiological data that allows for the first time an interactive regression-based analysis of large feature sets with respect to a disease.", "uri": "https://vimeo.com/136206221", "name": "VIS15 preview: 3D Regression Heat Map Analysis of Population Study Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:51:54+00:00", "description": "[VAST paper] [BEST PAPER]\n\nAuthors: Stef van den Elzen, Danny Holten, Jorik Blaas, Jarke van Wijk\n\nAbstract: We propose a visual analytics approach for the exploration and analysis of dynamic networks. We consider snapshots of the network as points in high-dimensional space and project these to two dimensions for visualization and interaction using two juxtaposed views: one for showing a snapshot and one for showing the evolution of the network. With this approach users are enabled to detect stable states, recurring states, outlier topologies, and gain knowledge about the transitions between states and the network evolution in general. The components of our approach are discretization, vectorization and normalization, dimensionality reduction, and visualization and interaction, which are discussed in detail. The effectiveness of the approach is shown by applying it to artificial and real-world dynamic networks.", "uri": "https://vimeo.com/136206214", "name": "VIS15 preview: Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:51:39+00:00", "description": "[VAST paper] \n\nAuthors: Johannes Sorger, Thomas Ortner, Christian Luksch, Michael Schw\u00e4rzler, Eduard Gr\u00f6ller, Harald Piringer\n\nAbstract: State-of-the-art lighting design is based on physically accurate lighting simulations of scenes such as offices. The simulation results support lighting designers in the creation of lighting configurations, which must meet contradicting customer objectives regarding quality and price while conforming to industry standards. However, current tools for lighting design impede rapid feedback cycles. On the one side, they decouple analysis and simulation specification. On the other side, they lack capabilities for a detailed comparison of multiple configurations. The primary contribution of this paper is a design study of LiteVis, a system for efficient decision support in lighting design. LiteVis tightly integrates global illumination-based lighting simulation, a spatial representation of the scene, and non-spatial visualizations of parameters and result indicators. This enables an efficient iterative cycle of simulation parametrization and analysis. Specifically, a novel visualization supports decision making by ranking simulated lighting configurations with regard to a weight-based prioritization of objectives that considers both spatial and non-spatial characteristics. In the spatial domain, novel concepts support a detailed comparison of illumination scenarios. We demonstrate LiteVis using a real-world use case and report qualitative feedback of lighting designers. This feedback indicates that LiteVis successfully supports lighting designers to achieve key tasks more efficiently and with greater certainty.", "uri": "https://vimeo.com/136206189", "name": "VIS15 preview: LiteVis: Integrated Visualization for Simulation-Based Decision Support in Lighting Design", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:51:32+00:00", "description": "[VAST paper] \n\nAuthors: Charilaos Papadopoulos, Ievgeniia Gutenko, Arie Kaufman\n\nAbstract: Empirical, hypothesis-driven, experimentation is at the heart of the scientific discovery process and has become com- monplace in human-factors related fields. To enable the integration of visual analytics in such experiments, we introduce VEEVVIE, the Visual Explorer for Empirical Visualization, VR and Interaction Experiments. VEEVVIE is comprised of a back-end ontology which can model several experimental designs encountered in these fields. This formalization allows VEEVVIE to capture experimental data in a query-able form and makes it accessible through a front-end interface. This front-end offers several multi-dimensional vi- sualization widgets with built-in filtering and highlighting functionality. VEEVVIE is also expandable to support custom experimental measurements and data types through a plug-in visualization widget architecture. We demonstrate VEEVVIE through several case studies of visual analysis, performed on the design and data collected during an experiment on the scalability of high-resolution, immersive, tiled-display walls.", "uri": "https://vimeo.com/136206178", "name": "VIS15 preview: VEEVVIE - Visual Explorer for Empirical Visualization, VR and Interaction Experiments", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:51:26+00:00", "description": "[VAST paper] \n\nAuthors: Yafeng Lu, Michael Steptoe, Sarah Burke, Hong Wang, Jiun-Yi Tsai, Hasan Davulcu, Douglas Montgomery, Steven R. Corman, Ross Maciejewski\n\nAbstract: Online news, microblogs and other media documents all contain valuable insight regarding events and responses to events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously) to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential relationships between climate change framing and conflicts in Africa.", "uri": "https://vimeo.com/136206168", "name": "VIS15 preview: Exploring Evolving Media Discourse Through Event Cueing", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:51:16+00:00", "description": "[VAST paper] \n\nAuthors: Isaac Cho, Wenwen Dou, Xiaoyu Wang, Eric Sauda, Bill Ribarsky\n\nAbstract: Learning and gaining knowledge of Roman history is an area of interest for students and citizens at large. This is an example of a subject with great sweep (with many interrelated sub-topics over, in this case, a 3,000 year history) that is hard to grasp by any individual and, in its full detail, is not available as a coherent story. In this paper, we propose a visual analytics approach to construct a data driven view of Roman history based on a large collection of Wikipedia articles. Extracting and enabling the discovery of useful knowledge on events, places, times, and their connections from large amounts of textual data has always been a challenging task. To this aim, we introduce VAiRoma, a visual analytics system that couples state-of-the-art text analysis methods with an intuitive visual interface to help users make sense of events, places, times, and more importantly, the relationships between them. VAiRoma goes beyond textual content exploration, as it permits users to compare, make connections, and externalize the findings all within the visual interface. As a result, VAiRoma allows users to learn and create new knowledge regarding Roman history in an informed way. We evaluated VAiRoma with 16 participants through a user study, with the task being to learn about roman piazzas through finding relevant articles and new relationships. Our study results showed that the VAiRoma system enables the participants to find more relevant articles and connections compared to Web searches and literature search conducted in a roman library. Subjective feedback on VAiRoma was also very positive. In addition, we ran two case studies that demonstrate how VAiRoma can be used for deeper analysis, permitting the rapid discovery and analysis of a small number of key documents even when the original collection contains hundreds of thousands of documents.", "uri": "https://vimeo.com/136206158", "name": "VIS15 preview: VAiRoma: A Visual Analytics System for Making Sense of Places, Times, and Events in Roman History", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:51:12+00:00", "description": "[VAST paper] \n\nAuthors: Wenwen Dou, Isaac Cho, Omar ElTayeby, Jaegul Choo, Xiaoyu Wang, Bill Ribarsky\n\nAbstract: The wide-spread use of social media provides unprecedented sources of written language that can be used to model and infer online demographics. In this paper, we introduce a novel visual text analytics system, DemographicVis, to aid interactive analysis of such demographic information based on user-generated contents. Our approach connects categorical data (demographic information) with textual data, allowing users to understand the characteristics of different demographic groups in a transparent and explorative manner. The modeling and visualization are based on ground truth demographic information collected via a survey conducted on Reddit.com. Detailed user information is taken into our modeling process that connects the demographic groups with features that best describe the distinguishing characteristics of each group. Features including topical and linguistic are generated from the user-generated contents. Such features are then analyzed and ranked based on their ability to predict the users' demographic information. To enable interactive demographic analysis, we introduce a web-based visual interface to present the relationship of the demographic groups, their topic interests, as well as the predictive power of various features. We present multiple case studies to showcase the utility of our visual analytics approach in exploring and understanding the interests of different demographic groups. We also report results from a comparative evaluation, showing that the DemographicVis is quantitatively superior or competitive and subjectively preferred when compared to a commercial text analysis tool.", "uri": "https://vimeo.com/136206149", "name": "VIS15 preview: DemographicVis: Analyzing Demographic Information based on User Generated Content", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:50:59+00:00", "description": "[VAST paper] \n\nAuthors: Jun Wang, Klaus Mueller\n\nAbstract: Uncovering the causal relations that exist among variables in multivariate datasets is one of the ultimate goals in data analytics. Causation is related to correlation but correlation does not imply causation. While a number of casual discovery algorithms have been devised that eliminate spurious correlations from a network, there are no guarantees that all of the inferred causations are indeed true. Hence, bringing a domain expert into the casual reasoning loop can be of great benefit in identifying erroneous casual relationships suggested by the discovery algorithm. To address this need we present the Visual Causal Analyst - a novel visual causal reasoning framework that allows users to apply their expertise, verify and edit causal links, and collaborate with the causal discovery algorithm to identify a valid causal network. Its interface consists of both an interactive 2D graph view and a numerical presentation of salient statistical parameters, such as regression coefficients, p-values, and others. Both help users in gaining a good understanding of the landscape of causal structures particularly when the number of variables is large. Our framework is also novel in that it can handle both numerical and categorical variables within one unified model and return plausible results. We demonstrate its use via a set of case studies using multiple practical datasets.", "uri": "https://vimeo.com/136206127", "name": "VIS15 preview: The Visual Causality Analyst: An Interactive Interface for Causal Reasoning", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:50:52+00:00", "description": "[VAST paper] \n\nAuthors: Martin R\u00f6hlig, Martin Luboschik, Markus B\u00f6gl, Frank Kr\u00fcger, Bilal Alsallakh, Silvia Miksch, Thomas Kirste, Heidrun Schumann\n\nAbstract: Recognizing activities has become increasingly relevant in many application domains, such as security or ambient assisted living. To handle different scenarios, the underlying automated algorithms are configured using multiple input parameters. However, the influence and interplay of these parameters is often not clear, making exhaustive evaluations necessary. On this account, we propose a visual analytics approach to supporting users in understanding the complex relationships among parameters, recognized activities, and associated accuracies. First, representative parameter settings are determined. Then, the respective output is computed and statistically analyzed to assess parameters' influence in general. Finally, visualizing the parameter settings along with the activities provides overview and allows to investigate the computed results in detail. Coordinated interaction helps to explore dependencies, compare different settings, and examine individual activities. By integrating automated, visual, and interactive means users can select parameter values that meet desired quality criteria. We demonstrate the application of our solution in a use case with realistic complexity, involving a study of human protagonists in daily living with respect to hundreds of parameter settings.", "uri": "https://vimeo.com/136206106", "name": "VIS15 preview: Supporting Activity Recognition by Visual Analytics", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:50:46+00:00", "description": "[VAST paper] [HONORABLE MENTION]\n\nAuthors: Tanja Blascheck, Markus John, Kuno Kurzhals, Steffen Koch, Thomas Ertl\n\nAbstract: Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.", "uri": "https://vimeo.com/136206091", "name": "VIS15 preview: VA^2: A Visual Analytics Approach for Evaluating Visual Analytics Applications", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:50:42+00:00", "description": "[VAST paper] \n\nAuthors: Maoyuan Sun, Peng Mi, Chris North, Naren Ramakrishnan\n\nAbstract: Identifying coordinated relationships is an important task in data analytics. For example, an intelligence analyst might want to discover three suspicious people who all visited the same four cities. Existing techniques that display individual relationships, such as between lists of entities, require repetitious manual selection and significant mental aggregation in cluttered visualizations to find coordinated relationships. In this paper, we present BiSet, a visual analytics technique to support interactive exploration of coordinated relationships. In BiSet, we model coordinated relationships as biclusters and algorithmically mine them from a dataset. Then, we visualize the biclusters in context as bundled edges between sets of related entities. Thus, bundles enable analysts to infer task-oriented semantic insights about potentially coordinated activities. We make bundles as first class objects and add a new layer, \"in-between\"\u009d, to contain these bundle objects. Based on this, bundles serve to organize entities represented in lists and visually reveal their membership. Users can interact with edge bundles to organize related entities, and vice versa, for sensemaking purposes. With a usage scenario, we demonstrate how BiSet supports the exploration of coordinated relationships in text analytics.", "uri": "https://vimeo.com/136206081", "name": "VIS15 preview: BiSet: Semantic Edge Bundling with Biclusters for Sensemaking", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:50:37+00:00", "description": "[VAST paper] \n\nAuthors: Xiaoke Huang, Ye Zhao, Jing Yang, Chong Zhang, Chao Ma, Xinyue Ye\n\nAbstract: We propose TrajGraph, a new visual analytics method, for studying urban mobility patterns by integrating graph modeling and visual analysis with taxi trajectory data. A special graph is created to store and manifest real traffic information recorded by taxi trajectories over city streets. It conveys urban transportation dynamics which can be discovered by applying graph analysis algorithms. To support interactive, multiscale visual analytics, a graph partitioning algorithm is applied to create region-level graphs which have smaller size than the original street-level graph. Graph centralities, including Pagerank and betweenness, are computed to characterize the time-varying importance of different urban regions. The centralities are visualized by three coordinated views including a node-link graph view, a map view and a temporal information view. Users can interactively examine the importance of streets to discover and assess city traffic patterns. We have implemented a fully working prototype of this approach and evaluated it using massive taxi trajectories of Shenzhen, China. TrajGraph's capability in revealing the importance of city streets was evaluated by comparing the calculated centralities with the subjective evaluations from a group of drivers in Shenzhen. Feedback from a domain expert was collected. The effectiveness of the visual interface was evaluated through a formal user study. We also present several examples and a case study to demonstrate the usefulness of TrajGraph in urban transportation analysis.", "uri": "https://vimeo.com/136206069", "name": "VIS15 preview: TrajGraph: A Graph-Based Visual Analytics Approach to Studying Urban Network Centrality Using Taxi Trajectory...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:50:29+00:00", "description": "[VAST paper] \n\nAuthors: Fabian Beck, Sebastian Koch, Daniel Weiskopf\n\nAbstract: Bibliographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.", "uri": "https://vimeo.com/136206061", "name": "VIS15 preview: Visual Analysis and Dissemination of Scientific Literature Collections with SurVis", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:50:06+00:00", "description": "[VAST paper] \n\nAuthors: Qingsong Liu, Yifan Hu, Lei Shi, Xinzhu Mu, Yutao Zhang, Jie Tang\n\nAbstract: Event-based egocentric dynamic networks are an important class of networks widely seen in many domains. In this paper, we present a visual analytics approach for these networks by combining data-driven network simplifications with a novel visualization design - EgoNetCloud. In particular, an integrated data processing pipeline is proposed to prune, compress and filter the networks into smaller but salient abstractions. To accommodate the simplified network into the visual design, we introduce a constrained graph layout algorithm on the dynamic network. Through a real-life case study as well as conversations with the domain expert, we demonstrate the effectiveness of the EgoNetCloud design and system in completing analysis tasks on event-based dynamic networks. The user study comparing EgoNetCloud with a working system on academic search confirms the effectiveness and convenience of our visual analytics based approach.", "uri": "https://vimeo.com/136206030", "name": "VIS15 preview: EgoNetCloud: Event-based Egocentric Dynamic Network Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:49:54+00:00", "description": "[InfoVis paper] \n\nAuthors: Paolo Simonetto, Daniel Archambault, Carlos Scheidegger\n\nAbstract: General methods for drawing Euler diagrams tend to generate irregular polygons. Yet, empirical evidence indicates that smoother contours make these diagrams easier to read. In this paper, we present a simple method to smooth the boundaries of any Euler diagram drawing. When refining the diagram, the method must ensure that set elements remain inside their appropriate boundaries and that no region is removed or created in the diagram. Our approach uses a force system that improves the diagram while at the same time ensuring its topological structure does not change. We demonstrate the effectiveness of the approach through case studies and quantitative evaluations.", "uri": "https://vimeo.com/136206015", "name": "VIS15 preview: A Simple Approach for Boundary Improvement of Euler Diagrams", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:49:43+00:00", "description": "[InfoVis paper] \n\nAuthors: Yvonne Jansen, Kasper Hornb\u00e6k\n\nAbstract: Physical visualizations, or data physicalizations, encode data in attributes of physical shapes. Despite a considerable body of work on visual variables, \"physical variables\"\u009d remain poorly understood. One of them is physical size. A difficulty for solid elements is that \"size\"\u009d is ambiguous - it can refer to either length/diameter, surface, or volume. Thus, it is unclear for designers of physicalizations how to effectively encode quantities in physical size. To investigate, we ran an experiment where participants estimated ratios between quantities represented by solid bars and spheres. Our results suggest that solid bars are compared based on their length, consistent with previous findings for 2D and 3D bars on flat media. But for spheres, participants' estimates are rather proportional to their surface. Depending on the estimation method used, judgments are rather consistent across participants, thus the use of perceptually-optimized size scales seems possible. We conclude by discussing implications for the design of data physicalizations and the need for more empirical studies on physical variables.", "uri": "https://vimeo.com/136205993", "name": "VIS15 preview: A Psychophysical Investigation of Size as a Physical Variable", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:49:35+00:00", "description": "[InfoVis paper] \n\nAuthors: Renata Raidou, Martin Eisemann, Marcel Breeuwer, Elmar Eisemann, Anna Vilanova\n\nAbstract: Parallel Coordinate Plots (PCPs) is one of the most powerful techniques for the visualization of multivariate data. However, for large datasets, the representation suffers from clutter due to overplotting. In this case, discerning the underlying data information and selecting specific interesting patterns can become difficult. We propose a new and simple technique to improve the display of PCPs by emphasizing the underlying data structure. Our Orientation-enhanced Parallel Coordinate Plots (OPCPs) improve pattern and outlier discernibility by visually enhancing parts of each PCP polyline with respect to its slope. This enhancement also allows us to introduce a novel and efficient selection method, the Orientation-enhanced Brushing (O-Brushing). Our solution is particularly useful when multiple patterns are present or when the view on certain patterns is obstructed by noise. We present the results of our approach with several synthetic and real-world datasets. Finally, we conducted a user evaluation, which verifies the advantages of the OPCPs in terms of discernibility of information in complex data. It also confirms that O-Brushing eases the selection of data patterns in PCPs and reduces the amount of necessary user interactions compared to state-of-the-art brushing techniques.", "uri": "https://vimeo.com/136205977", "name": "VIS15 preview: Orientation-Enhanced Parallel Coordinate Plots (OPCPs)", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:49:29+00:00", "description": "[InfoVis paper] \n\nAuthors: Benjamin Bach, Conglei Shi, Nicolas Heulot, Tara Madhayastha, Thomas Grabowsi, Pierre Dragicevic\n\nAbstract: We introduce time curves as a general approach for visualizing patterns of evolution in temporal data. Examples of such patterns include slow and regular progressions, large sudden changes, and reversals to previous states. These patterns can be of interest in a range of domains, such as collaborative document editing, dynamic network analysis, and video analysis. Time curves employ the metaphor of folding a timeline visualization into itself so as to bring similar time points close to each other. This metaphor can be applied to any dataset where a similarity metric between temporal snapshots can be defined, thus it is largely datatype-agnostic. We illustrate how time curves can visually reveal informative patterns in a range of different datasets.", "uri": "https://vimeo.com/136205973", "name": "VIS15 preview: Time Curves: Folding Time to Visualize Patterns of Temporal Evolution in Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:49:24+00:00", "description": "[InfoVis paper] \n\nAuthors: Alice Thudt, Dominikus Baur, Samuel Huron, Sheelagh Carpendale\n\nAbstract: In this paper we discuss the creation of visual mementos as a new application area for visualization. We define visual mementos as visualizations of personally relevant data for the purpose of reminiscing, and sharing of life experiences. Today more people collect digital information about their life than ever before. The shift from physical to digital archives poses new challenges and opportunities for self-reflection and self-representation. Drawing on research on autobiographical memory and on the role of artifacts in reminiscing, we identified design challenges for visual mementos: mapping data to evoke familiarity, expressing subjectivity, and obscuring sensitive details for sharing. Visual mementos can make use of the known strengths of visualization in revealing patterns to show the familiar instead of the unexpected, and extend representational mappings beyond the objective to include the more subjective. To understand whether people's subjective views on their past can be reflected in a visual representation, we developed, deployed and studied a technology probe that exemplifies our concept of visual mementos. Our results show how reminiscing has been supported and reveal promising new directions for self-reflection and sharing through visual mementos of personal experiences.", "uri": "https://vimeo.com/136205968", "name": "VIS15 preview: Visual Mementos: Reflecting Memories with Personal Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:49:17+00:00", "description": "[InfoVis paper] \n\nAuthors: Nina McCurdy, Julie Lein, Katharine Coles, Miriah Meyer\n\nAbstract: The digital humanities have experienced tremendous growth within the last decade, mostly in the context of developing computational tools that support what is called distant reading - collecting and analyzing huge amounts of textual data for synoptic evaluation. On the other end of the spectrum is a practice at the heart of the traditional humanities, close reading - the careful, in-depth analysis of a single text in order to extract, engage, and even generate as much productive meaning as possible. The true value of computation to close reading is still very much an open question. During a two-year design study we explored this question with several poetry scholars, focusing on an investigation of sound and linguistic devices in poetry. The contributions of our design study include a problem characterization and data abstraction of the use of sound in poetry as well as Poemage, a visualization tool for interactively exploring the sonic topology of a poem. The design of Poemage is grounded in the evaluation of a series of technology probes we deployed to our poetry collaborators, and we validate the final design with several case studies that illustrate the disruptive impact technology can have on poetry scholarship. Finally, we also contribute a reflection on the challenges we faced conducting visualization research in literary studies.", "uri": "https://vimeo.com/136205958", "name": "VIS15 preview: Poemage: Visualizing the Sonic Topology of a Poem", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:49:12+00:00", "description": "[InfoVis paper] \n\nAuthors: Alvitta Ottley, Evan Peck, Lane Harrison, Daniel Afergan, Caroline Ziemkiewicz, Holly A. Taylor, Paul K. J. Han, Remco Chang\n\nAbstract: Decades of research have repeatedly shown that people perform poorly at estimating and understanding conditional probabilities that are inherent in Bayesian reasoning problems. Yet in the medical domain, both physicians and patients make daily, life-critical judgments based on conditional probability. Although there have been a number of attempts to develop more effective ways to facilitate Bayesian reasoning, reports of these findings tend to be inconsistent and sometimes even contradictory. For instance, the reported accuracies for individuals being able to correctly estimate conditional probability range from 6% to 62%. In this work, we show that problem representation can significantly affect accuracies. By controlling the amount of information presented to the user, we demonstrate how text and visualization designs can increase overall accuracies to as high as 77%. Additionally, we found that for users with high spatial ability, our designs can further improve their accuracies to as high as 100%. By and large, our findings provide explanations for the inconsistent reports on accuracy in Bayesian reasoning tasks and show a significant improvement over existing methods. We believe that these findings can have immediate impact on risk communication in health-related fields.", "uri": "https://vimeo.com/136205947", "name": "VIS15 preview: Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:49:02+00:00", "description": "[InfoVis paper] \n\nAuthors: P. Samuel Quinan, Miriah Meyer\n\nAbstract: Meteorologists process and analyze weather forecasts using visualization in order to examine the behaviors of and relationships among weather features. In this design study conducted with meteorologists in decision support roles, we identified and attempted to address two significant common challenges in weather visualization: the employment of inconsistent and often ineffective visual encoding practices across a wide range of visualizations, and a lack of support for directly visualizing how different weather features relate across an ensemble of possible forecast outcomes. In this work, we present a characterization of the problems and data associated with meteorological forecasting, we propose a set of informed default encoding choices that integrate existing meteorological conventions with effective visualization practice, and we extend a set of techniques as an initial step toward directly visualizing the interactions of multiple features over an ensemble forecast. We discuss the integration of these contributions into a functional prototype tool, and also reflect on the many practical challenges that arise when working with weather data.", "uri": "https://vimeo.com/136205936", "name": "VIS15 preview: Visually Comparing Weather Features in Forecasts", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:55+00:00", "description": "[InfoVis paper] \n\nAuthors: Theresia Gschwandtner, Markus B\u00f6gl, Paolo Federico, Silvia Miksch\n\nAbstract: A number of studies have investigated different ways of visualizing uncertainty. However, in the temporal dimension, it is still an open question how to best represent uncertainty, since the special characteristics of time require special visual encodings and may provoke different interpretations. Thus, we have conducted a comprehensive study comparing alternative visual encodings of intervals with uncertain start and end times: gradient plots, violin plots, accumulated probability plots, error bars, centered error bars, and ambiguation. Our results reveal significant differences in error rates and time for these different visualization types and different tasks. We recommend using ambiguation - using a lighter color value to represent uncertain regions - or error bars for judging durations and temporal bounds, and gradient plots - using fading color or transparency - for judging probability values.", "uri": "https://vimeo.com/136205918", "name": "VIS15 preview: Visual Encodings of Temporal Uncertainty: A Comparative User Study", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:50+00:00", "description": "[InfoVis paper] \n\nAuthors: James Walker, Rita Borgo, Mark Jones\n\nAbstract: Collecting sensor data results in large temporal data sets which need to be visualized, analysed, and presented. One-dimensional time-series charts are used, but these present problems when screen resolution is small in comparison to the data. This can result in severe over-plotting, giving rise for the requirement to provide effective rendering and methods to allow interaction with the detailed data. Common solutions can be categorized as multi-scale representations, frequency based, and lens based interaction techniques. \nIn this paper, we comparatively evaluate existing methods, such as Stack Zoom and ChronoLenses, giving a graphical overview of each and classifying their ability to explore and interact with data. We propose new visualizations and other extensions to the existing approaches. We undertake and report an empirical study and a field study using these techniques.", "uri": "https://vimeo.com/136205907", "name": "VIS15 preview: TimeNotes: A Study on Effective Chart Visualization and Interaction Techniques for Time-Series Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:43+00:00", "description": "[InfoVis paper] \n\nAuthors: Julian Stahnke, Marian D\u00f6rk, Boris M\u00fcller, Andreas Thom\n\nAbstract: We introduce a set of integrated interaction techniques to interpret and interrogate dimensionality-reduced data. Projection techniques generally aim to make a high-dimensional information space visible in form of a planar layout. However, the meaning of the resulting data projections can be hard to grasp. It is seldom clear why elements are placed far apart or close together and the inevitable approximation errors of any projection technique are not exposed to the viewer. Previous research on dimensionality reduction focuses on the efficient generation of data projections, interactive customisation of the model, and comparison of different projection techniques. There has been only little research on how the visualization resulting from data projection is interacted with. We propose a set of interactive visualization methods to examine the dimensionality-reduced data as well as the projection itself. The methods let viewers see approximation errors, question the positioning of elements, compare them to each other, and visualize the influence of data dimensions on the projection space. We created a web-based system implementing these methods, and report on findings from an evaluation with data analysts using the prototype to examine multidimensional datasets.", "uri": "https://vimeo.com/136205894", "name": "VIS15 preview: Probing Projections: Interaction Techniques for Interpreting Arrangements and Errors of Dimensionality...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:38+00:00", "description": "[InfoVis paper] \n\nAuthors: Yong Wang, Qiaomu Shen, Daniel Archambault, Zhiguang Zhou, Min Zhu, Sixiao Yang, Huamin Qu\n\nAbstract: Node-link diagrams provide an intuitive way to explore networks and have inspired a large number of automated graph layout strategies that optimize visual aesthetic criteria. However, any particular drawing approach cannot fully satisfy all these criteria simultaneously, producing drawings with visual ambiguities that can impede the understanding of network structure. To bring attention to these potentially problematic areas present in the drawing, this paper presents a technique that highlights common types of visual ambiguities: ambiguous spatial relationships between nodes and edges, visual overlap between community structures, and ambiguity in edge bundling and metanodes. Metrics, including newly proposed metrics for abnormal edge lengths, visual overlap in community structures and node/edge aggregation, are proposed to quantify areas of ambiguity in the drawing. These metrics and others are then displayed using a heatmap-based visualization that provides visual feedback to developers of graph drawing and visualization approaches, allowing them to quickly identify misleading areas. The novel metrics and the heatmap-based visualization allow a user to explore ambiguities in graph layouts from multiple perspectives in order to make reasonable graph layout choices. The effectiveness of the technique is demonstrated through case studies and expert reviews.", "uri": "https://vimeo.com/136205887", "name": "VIS15 preview: AmbiguityVis: Visualization of Ambiguity in Graph Layouts", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:32+00:00", "description": "[InfoVis paper] [HONORABLE MENTION]\n\nAuthors: Matthew Kay, Jeffrey Heer\n\nAbstract: Models of human perception - including perceptual \"laws\"\u009d - can be valuable tools for deriving visualization design recommendations. However, it is important to assess the explanatory power of such models when using them to inform design. We present a secondary analysis of data previously used to rank the effectiveness of bivariate visualizations for assessing correlation (measured with Pearson's r) according to the well-known Weber-Fechner Law. Beginning with the model of Harrison et al. [1], we present a sequence of refinements including incorporation of individual differences, log transformation, censored regression, and adoption of Bayesian statistics. Our model incorporates all observations dropped from the original analysis, including data near ceilings caused by the data collection process and entire visualizations dropped due to large numbers of observations worse than chance. This model deviates from Weber's Law, but provides improved predictive accuracy and generalization. Using Bayesian credibility intervals, we derive a partial ranking that groups visualizations with similar performance, and we give precise estimates of the difference in performance between these groups. We conclude with a discussion of the value of data sharing and replication, and share implications for modeling similar experimental data.", "uri": "https://vimeo.com/136205873", "name": "VIS15 preview: Beyond Weber's Law: A Second Look at Ranking Visualizations of Correlation", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:26+00:00", "description": "[InfoVis paper] \n\nAuthors: Vidya Setlur, Maureen Stone\n\nAbstract: When data categories have strong color associations, it is useful to use these semantically meaningful concept-color associations in data visualizations. In this paper, we explore how linguistic information about the terms defining the data can be used to generate semantically meaningful colors. To do this effectively, we need first to establish that a term has a strong semantic color association, then discover which color or colors express it. Using co-occurrence measures of color name frequencies from Google n-grams, we define a measure for colorability that describes how strongly associated a given term is to any of a set of basic color terms. We then show how this colorability score can be used with additional semantic analysis to rank and retrieve a representative color from Google Images. Alternatively, we use symbolic relationships defined by WordNet to select identity colors for categories such as countries or brands. To create visually distinct color palettes, we use k-means clustering to create visually distinct sets, iteratively reassigning terms with multiple basic color associations as needed. This can be additionally constrained to use colors only in a predefined palette.", "uri": "https://vimeo.com/136205858", "name": "VIS15 preview: A Linguistic Approach to Categorical Color Assignment for Data Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:19+00:00", "description": "[InfoVis paper] \n\nAuthors: Uta Hinrichs, Stefania Forlini, Bridget Moynihan\n\nAbstract: In this paper we exemplify how information visualization supports speculative thinking, hypotheses testing, and preliminary interpretation processes as part of literary research. While InfoVis has become a buzz topic in the digital humanities, skepticism remains about how effectively it integrates into and expands on traditional humanities research approaches. From an InfoVis perspective, we lack case studies that show the specific design challenges that make literary studies and humanities research at large a unique application area for information visualization. We examine these questions through our case study of the Speculative W@nderverse, a visualization tool that was designed to enable the analysis and exploration of an untapped literary collection consisting of thousands of science fiction short stories. We present the results of two empirical studies that involved general-interest readers and literary scholars who used the evolving visualization prototype as part of their research for over a year. Our findings suggest a design space for visualizing literary collections that is defined by (1) their academic and public relevance, (2) the tension between qualitative vs. quantitative methods of interpretation (3) result- vs. process-driven approaches to InfoVis, and (4) the unique material and visual qualities of cultural collections. Through the Speculative W@nderverse we demonstrate how visualization can bridge these sometimes contradictory perspectives by cultivating curiosity and providing entry points into literary collections while, at the same time, supporting multiple aspects of humanities' research processes.", "uri": "https://vimeo.com/136205848", "name": "VIS15 preview: Speculative Practices: Utilizing InfoVis to Explore Untapped Literary Collections", "year": "2015", "event": "INFOVIS, PREVIEW"}, {"created_time": "2015-08-13T13:48:14+00:00", "description": "[InfoVis paper] [BEST PAPER]\n\nAuthors: Steve Kieffer, Tim Dwyer, Kim Marriott, Michael Wybrow\n\nAbstract: Over the last 50 years a wide variety of automatic network layout algorithms have been developed. Some are fast heuristic techniques suitable for networks with hundreds of thousands of nodes while others are multi-stage frameworks for higher-quality layout of smaller networks. However, despite decades of research currently no algorithm produces layout of comparable quality to that of a human. We give a new \"human-centred\" methodology for automatic network layout algorithm design that is intended to overcome this deficiency. User studies are first used to identify the aesthetic criteria algorithms should encode, then an algorithm is developed that is informed by these criteria and finally, a follow-up study evaluates the algorithm output. We have used this new methodology to develop an automatic orthogonal network layout method, HOLA, that achieves measurably better (by user study) layout than the best available orthogonal layout algorithm and which produces layouts of comparable quality to those produced by hand.", "uri": "https://vimeo.com/136205841", "name": "VIS15 preview: HOLA: Human-like Orthogonal Network Layout", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:09+00:00", "description": "[InfoVis paper] \n\nAuthors: Mona Hosseinkhani Loorak, Charles Perin, Noreen Kamal, Michael Hill, Sheelagh Carpendale\n\nAbstract: We present TimeSpan, an exploratory visualization tool designed to gain a better understanding of the temporal aspects of the stroke treatment process. Working with stroke experts, we seek to provide a tool to help improve outcomes for stroke victims. Time is of critical importance in the treatment of acute ischemic stroke patients. Every minute that the artery stays blocked, an estimated 1.9 million neurons and 12 km of myelinated axons are destroyed. Consequently, there is a critical need for efficiency of stroke treatment processes. Optimizing time to treatment requires a deep understanding of interval times. Stroke health care professionals must analyze the impact of procedures, events, and patient attributes on time-ultimately, to save lives and improve quality of life after stroke. First, we interviewed eight domain experts, and closely collaborated with two of them to inform the design of TimeSpan. We classify the analytical tasks which a visualization tool should support and extract design goals from the interviews and field observations. Based on these tasks and the understanding gained from the collaboration, we designed TimeSpan, a web-based tool for exploring multi-dimensional and temporal stroke data. We describe how TimeSpan incorporates factors from stacked bar graphs, line charts, histograms, and a matrix visualization to create an interactive hybrid view of temporal data. From feedback collected from domain experts in a focus group session, we reflect on the lessons we learned from abstracting the tasks and iteratively designing TimeSpan.", "uri": "https://vimeo.com/136205834", "name": "VIS15 preview: TimeSpan: Using Visualization to Explore Temporal Multidimensional Data of Stroke Patients", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:48:02+00:00", "description": "[InfoVis paper] \n\nAuthors: Manuel Rubio-S\u00e1nchez, Laura Raya, Francisco D\u00edaz, Alberto Sanchez\n\nAbstract: RadViz and star coordinates are two of the most popular projection-based multivariate visualization techniques that arrange variables in radial layouts. Formally, the main difference between them consists of a nonlinear normalization step inherent in RadViz. In this paper we show that, although RadViz can be useful when analyzing sparse data, in general this design choice limits its applicability and introduces several drawbacks for exploratory data analysis. In particular, we observe that the normalization step introduces nonlinear distortions, can encumber outlier detection, prevents associating the plots with useful linear mappings, and impedes estimating original data attributes accurately. In addition, users have greater flexibility when choosing different layouts and views of the data in star coordinates. Therefore, we suggest that analysts and researchers should carefully consider whether RadViz's normalization step is beneficial regarding the data sets' characteristics and analysis tasks.", "uri": "https://vimeo.com/136205825", "name": "VIS15 preview: A comparative study between RadViz and Star Coordinates", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:47:55+00:00", "description": "[InfoVis paper] \n\nAuthors: Yael Albo, Joel Lanir, Peter Bak, Sheizaf Rafaeli\n\nAbstract: A composite indicator (CI) is a measuring and benchmark tool used to capture multi-dimensional concepts, such as Information and Communication Technology (ICT) usage. Individual indicators are selected and combined to reflect a phenomena being measured. Visualization of a composite indicator is recommended as a tool to enable interested stakeholders, as well as the public audience, to better understand the indicator components and evolution over time. However, existing CI visualizations introduce a variety of solutions and there is a lack in CI's visualization guidelines. Radial visualizations are popular among these solutions because of CI's inherent multi-dimensionality. Although in dispute, Radar-charts are often used for CI presentation. However, no empirical evidence on Radar's effectiveness and efficiency for common CI tasks is available. In this paper, we aim to fill this gap by reporting on a controlled experiment that compares the Radar chart technique with two other radial visualization methods: Flower- charts as used in the well-known OECD Betterlife index, and Circle-charts which could be adopted for this purpose. Examples of these charts in the current context are shown in Figure 1. We evaluated these charts, showing the same data with each of the mentioned techniques applying small multiple views for different dimensions of the data. We compared users' performance and preference empirically under a formal task-taxonomy. Results indicate that the Radar chart was the least effective and least liked, while performance of the two other options were mixed and dependent on the task. Results also showed strong preference of participants toward the Flower chart. Summarizing our results, we provide specific design guidelines for composite indicator visualization.", "uri": "https://vimeo.com/136205811", "name": "VIS15 preview: Off the radar: comparative evaluation of radial visualization solutions for composite indicators", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:47:50+00:00", "description": "[InfoVis paper] \n\nAuthors: Lydia Byrne, Daniel Angus, Janet Wiles\n\nAbstract: While information visualization frameworks and heuristics have traditionally been reluctant to include acquired codes of meaning, designers are making use of them in a wide variety of ways. Acquired codes leverage a user's experience to understand the meaning of a visualization. They range from figurative visualizations which rely on the reader's recognition of shapes, to conventional arrangements of graphic elements which represent particular subjects. In this study, we used content analysis to codify acquired meaning in visualization. We applied the content analysis to a set of infographics and data visualizations which are exemplars of innovative and effective design. 88% of the infographics and 71% of data visualizations in the sample contain at least one use of figurative visualization. Conventions on the arrangement of graphics are also widespread in the sample. In particular, a comparison of representations of time and other quantitative data showed that conventions can be specific to a subject. These results suggest that there is a need for information visualization research to expand its scope beyond perceptual channels, to include social and culturally constructed meaning. Our paper demonstrates a viable method for identifying figurative techniques and graphic conventions and integrating them into heuristics for visualization design.", "uri": "https://vimeo.com/136205802", "name": "VIS15 preview: Acquired Codes of Meaning in Data Visualization and Infographics: Beyond Perceptual Primitives", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:47:43+00:00", "description": "[InfoVis paper] \n\nAuthors: Jonathan Roberts, Christopher Headleand, Panagiotis Ritsos\n\nAbstract: Sketching designs has been shown to be a useful way of planning and considering alternative solutions. The use of lo-fidelity prototyping, especially paper-based sketching, can save time, money and converge to better solutions more quickly. However, this design process is often viewed to be too informal. Consequently users do not know how to manage their thoughts and ideas (to first think divergently, to then finally converge on a suitable solution). We present the Five Design Sheet (FdS) methodology. The methodology enables users to create information visualization interfaces through lo-fidelity methods. Users sketch and plan their ideas, helping them express different possibilities, think through these ideas to consider their potential effectiveness as solutions to the task (sheet 1); they create three principle designs (sheets 2,3 and 4); before converging on a final realization design that can then be implemented (sheet 5). In this article, we present (i) a review of the use of sketching as a planning method for visualization and the benefits of sketching, (ii) a detailed description of the Five Design Sheet (FdS) methodology, and (iii) an evaluation of the FdS using the System Usability Scale, along with a case-study of its use in industry and experience of its use in teaching.", "uri": "https://vimeo.com/136205787", "name": "VIS15 preview: Sketching designs using the Five Design-Sheet methodology", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:47:33+00:00", "description": "[InfoVis paper] \n\nAuthors: Vahan Yoghourdjian, Tim Dwyer, Graeme Gange, Steve Kieffer, Karsten Klein, Kim Marriott\n\nAbstract: Prior research into network layout has focused on fast heuristic techniques for layout of large networks, or complex multi-stage pipelines for higher quality layout of small graphs. Improvements to these pipeline techniques, especially for orthogonal-style layout, are difficult and practical results have been slight in recent years. Yet, as discussed in this paper, there remain significant issues in the quality of the layouts produced by these techniques, even for quite small networks. This is especially true when layout with additional grouping constraints is required. The first contribution of this paper is to investigate an ultra-compact, grid-like network layout aesthetic that is motivated by the grid arrangements that are used almost universally by designers in typographical layout. Since the time when these heuristic and pipeline-based graph-layout methods were conceived, generic technologies (MIP, CP and SAT) for solving combinatorial and mixed-integer optimization problems have improved massively. The second contribution of this paper is to reassess whether these techniques can be used for high-quality layout of small graphs. While they are fast enough for graphs of up to 50 nodes we found these methods do not scale up. Our third contribution is a large-neighborhood search meta-heuristic approach that is scalable to larger networks.", "uri": "https://vimeo.com/136205779", "name": "VIS15 preview: High-Quality Ultra-Compact Grid Layout of Grouped Networks", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:47:20+00:00", "description": "[InfoVis paper] \n\nAuthors: Jeremy Boy, Louis Eveillard, Fran\u00e7oise Detienne, Jean-Daniel Fekete\n\nAbstract: In this article, we investigate methods for suggesting the interactivity of online visualizations embedded with text. We first assess the need for such methods by conducting three initial experiments on Amazon's Mechanical Turk. We then present a design space for Suggested Interactivity (i.e., visual cues used as perceived affordances--SI), based on a survey of 382 HTML5 and visualization websites. Finally, we assess the effectiveness of three SI cues we designed for suggesting the interactivity of bar charts embedded with text. Our results show that only one cue (SI3) was successful in inciting participants to interact with the visualizations, and we hypothesize this is because this particular cue provided feedforward.", "uri": "https://vimeo.com/136205747", "name": "VIS15 preview: Suggested Interactivity: Seeking Perceived Affordances for Information Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:47:13+00:00", "description": "[InfoVis paper] \n\nAuthors: Susan VanderPlas, Heike Hofmann\n\nAbstract: Graphics convey numerical information very efficiently, but rely on a different set of mental processes than tabular displays. Here, we present a study relating demographic characteristics and visual skills to perception of graphical lineups. We conclude that lineups are essentially a classification test in a visual domain, and that performance on the lineup protocol is associated with general aptitude, rather than specific tasks such as card rotation and spatial manipulation. We also examine the possibility that specific graphical tasks may be associated with certain visual skills and conclude that more research is necessary to understand which visual skills are required in order to understand certain plot types.", "uri": "https://vimeo.com/136205737", "name": "VIS15 preview: Spatial Reasoning and Data Displays", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:46:52+00:00", "description": "[InfoVis paper] \n\nAuthors: Sarah Goodwin, Jason Dykes, Aidan Slingsby, Cagatay Turkay\n\nAbstract: Comparing multiple variables to select those that effectively characterize complex entities is important in a wide variety of domains - geodemographics for example. Identifying variables that correlate is a common practice to remove redundancy, but correlation varies across space, with scale and over time, and the frequently used global statistics hide potentially important differentiating local variation. For more comprehensive and robust insights into multivariate relations, these local correlations need to be assessed through various means of defining locality. We explore the geography of this issue, and use novel interactive visualization to identify interdependencies in multivariate data sets to support geographically informed multivariate analysis. We offer terminology for considering scale and locality, visual techniques for establishing the effects of scale on correlation and a theoretical framework through which variation in geographic correlation with scale and locality are addressed explicitly. Prototype software demonstrates how these contributions act together. These techniques enable multiple variables and their geographic characteristics to be considered concurrently as we extend visual parameter space analysis (vPSA) to the spatial domain. We find variable correlations to be sensitive to scale and geography to varying degrees in the context of energy-based geodemographics. This sensitivity depends upon the calculation of locality as well as the geographical and statistical structure of the variable.", "uri": "https://vimeo.com/136205701", "name": "VIS15 preview: Visualizing Multiple Variables Across Scale and Geography", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:46:44+00:00", "description": "[InfoVis paper] \n\nAuthors: Sukwon Lee, Sung-Hee Kim, Ya-Hsin Hung, Heidi Lam, Youn-ah Kang, Ji Soo Yi\n\nAbstract: In this paper, we would like to investigate how people make sense of unfamiliar information visualizations. In order to achieve the research goal, we conducted a qualitative study by observing 13 participants when they endeavored to make sense of three unfamiliar visualizations (i.e., a parallel-coordinates plot, a chord diagram, and a treemap) that they encountered for the first time. We collected data including audio/video record of think-aloud sessions and semi-structured interview; and analyzed the data using the grounded theory method. The primary result of this study is a grounded model of NOvice's information VIsualization Sensemaking (NOVIS model), which consists of the five major cognitive activities: (1) encountering visualization, (2) constructing a frame, (3) exploring visualization, (4) questioning the frame, and (5) floundering on visualization. We introduce the NOVIS model by explaining the five activities with representative quotes from our participants. We also explore the dynamics in the model. Lastly, we compare with other existing models and share further research directions that arose from our observations.", "uri": "https://vimeo.com/136205695", "name": "VIS15 preview: How do People Make Sense of Unfamiliar Visualizations?: A Grounded Model of Novice's Information...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:46:37+00:00", "description": "[InfoVis paper] \n\nAuthors: Kanit Wongsuphasawat, Dominik Moritz, Anushka Anand, Jock Mackinlay, Bill Howe, Jeffrey Heer\n\nAbstract: General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "uri": "https://vimeo.com/136205686", "name": "VIS15 preview: Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:46:31+00:00", "description": "[InfoVis paper] \n\nAuthors: Dirk Joachim Lehmann, Holger Theisel\n\nAbstract: Finding good projections of high-dimensional data sets into a 2D visualization domain is one of the most important prob- lems in Information Visualization. Users are interested in getting a maximal insight into the data by exploring a minimal number of projections. However, if the number is too small or improper projections are used, then important data patterns might be overlooked. We propose a data-driven approach to find minimal sets of projections that uniquely show certain data patterns. For this we introduce a distance measure of data projections that discards affine transformations of projections and this way prevents to view repetitions of the same data patterns. Based on this, we provide complete data tours of at most n/2 projections. Furthermore, we propose optimal paths of projection matrices for an interactive data exploration. We illustrate our technique with a set of state-of-the-art real high-dimensional benchmark data sets.", "uri": "https://vimeo.com/136205671", "name": "VIS15 preview: Optimal Sets of Projections of High-Dimensional Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:46:22+00:00", "description": "[InfoVis paper] \n\nAuthors: Roeland Scheepens, Christophe Hurter, Huub van de Wetering, Jarke van Wijk\n\nAbstract: Visualization of the trajectories of moving objects leads to dense and cluttered images, which hinders exploration and understanding. It also hinders adding additional visual information, such as direction, and makes it difficult to interactively extract traffic flows, i.e., subsets of trajectories. In this paper we present our approach to visualize traffic flows and provide interaction tools to support their exploration. We show an overview of the traffic using a density map. The directions of traffic flows are visualized using a particle system on top of the density map. The user can extract traffic flows using a novel selection widget that allows for the intuitive selection of an area, and filtering on a range of directions and any additional attributes. Using simple, visual set expressions, the user can construct more complicated selections. The dynamic behaviors of selected flows may then be shown in annotation windows in which they can be interactively explored and compared. We validate our approach through use cases where we explore and analyze the temporal behavior of aircraft and vessel trajectories, e.g., landing and takeoff sequences, or the evolution of flight route density. The aircraft use cases have been developed and validated in collaboration with domain experts.", "uri": "https://vimeo.com/136205658", "name": "VIS15 preview: Visualization, Selection, and Analysis of Traffic Flows", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-13T13:46:15+00:00", "description": "[InfoVis paper] \n\nAuthors: Arvind Satyanarayan, Ryan Russell, Jane Hoffswell, Jeffrey Heer\n\nAbstract: We present Reactive Vega, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization. Starting from a single declarative specification, Reactive Vega constructs a dataflow graph in which input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. To support expressive interactive visualizations that may involve time-varying scalar, relational, or hierarchical data, Reactive Vega's dataflow graph can dynamically re-write itself at runtime by extending or pruning branches in a data-driven fashion. We discuss both compile- and run-time optimizations applied within Reactive Vega, and share the results of benchmark studies that indicate superior interactive performance to both D3 and the original, non-reactive Vega system.", "uri": "https://vimeo.com/136205652", "name": "VIS15 preview: Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:21:55+00:00", "description": "[VAST paper] \n\nAuthors: Michael Glueck, Peter Hamilton, Fanny Chevalier, Simon Breslav, Azam Khan, Daniel Wigdor, Michael Brudno\n\nAbstract: The differential diagnosis of hereditary disorders is a challenging task for clinicians due to the heterogeneity of phenotypes that can be observed in patients. Existing clinical tools are often text-based and do not emphasize consistency, completeness, or granularity of phenotype reporting. This can impede clinical diagnosis and limit their utility to genetics researchers. Herein, we present PhenoBlocks, a novel visual analytics tool that supports the comparison of phenotypes between patients, or between a patient and the hallmark features of a disorder. An informal evaluation of PhenoBlocks with expert clinicians suggested that the visualization effectively guides the process of differential diagnosis and could reinforce the importance of complete, granular phenotypic reporting.", "uri": "https://vimeo.com/136147446", "name": "VIS15 preview: PhenoBlocks: Phenotype Comparison Visualizations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:21:50+00:00", "description": "[VAST paper] \n\nAuthors: Liang Yu, Wei Wu, Xiaohui Li, Guangxia Li, Wee Siong Ng, See Kiong Ng, Zhongwen Huang, Anushiya Arunan, Hui Min Watt\n\nAbstract: Using transport smart card transaction data to understand the home-work dynamics of a city for urban planning is emerging as an alternative to traditional surveys which may be conducted every few years are no longer effective and efficient for the rapidly transforming modern cities. As commuters' travel patterns are highly diverse, existing rule-based methods are not fully adequate. In this paper, we present iVizTRANS - a tool which combines an interactive visual analytics (VA) component to aid urban planners to analyse complex travel patterns and decipher activity locations for single public transport commuters. It is coupled with a machine learning component that iteratively learns from the planners' classifications to train a classifier. The classifier is then applied to the city-wide smart card data to derive the dynamics for all public transport commuters. Our evaluation shows it outperforms the rule-based methods in previous work.", "uri": "https://vimeo.com/136147436", "name": "VIS15 preview: iVizTRANS: Interactive Visual Learning for Home and Work Place Detection from Massive Public Transportation...", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:21:43+00:00", "description": "[SciVis paper] \n\nAuthors: Kai Lawonn, Sylvia Glasser, Anna Vilanova, Bernhard Preim, Tobias Isenberg\n\nAbstract: We present the first visualization tool that combines pathlines from blood flow and wall thickness information. Our method uses illustrative techniques to provide occlusion-free visualization of the flow. We thus offer medical researchers an effective visual analysis tool for aneurysm treatment risk assessment. Such aneurysms bear a high risk of rupture and significant treatment-related risks. Therefore, to get a fully informed decision it is essential to both investigate the vessel morphology and the hemodynamic data. Ongoing research emphasizes the importance of analyzing the wall thickness in risk assessment. Our combination of blood flow visualization and wall thickness representation is a significant improvement for the exploration and analysis of aneurysms. As all presented information is spatially intertwined, occlusion problems occur. We solve these occlusion problems by dynamic cutaway surfaces. We combine this approach with a glyph-based blood flow representation and a visual mapping of wall thickness onto the vessel surface. We developed a GPU-based implementation of our visualizations which facilitates wall thickness analysis through real-time rendering and flexible interactive data exploration mechanisms. We designed our techniques in collaboration with domain experts, and we provide details about the evaluation of the technique and tool.", "uri": "https://vimeo.com/136147417", "name": "VIS15 preview: Occlusion-free Blood Flow Animation with Wall Thickness Visualization", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:21:38+00:00", "description": "[SciVis paper] \n\nAuthors: Bireswar Laha, Doug Bowman, David Laidlaw, John Socha\n\nAbstract: Empirical findings from studies in one scientific domain have very limited applicability to other domains, unless we formally establish deeper insights on the generalizability of task types. We present a domain-independent classification of visual analysis tasks with volume visualizations. This task taxonomy will help researchers design experiments, ensure coverage, and generate hypotheses in empirical studies with volume datasets. To develop a taxonomy, we first interviewed scientists working with spatial data in disparate domains. We then ran a survey to evaluate the design. Scientists and professionals from around the world, working with volume data in various scientific domains, participated in the survey. The respondents agreed substantially with our taxonomy design, but also suggested important refinements. We report the results in the form of a goal-based generic categorization of visual analysis tasks with volume visualizations. Our taxonomy comprehensively covers tasks performed with a wide variety of volume datasets.", "uri": "https://vimeo.com/136147410", "name": "VIS15 preview: A Classification of User Tasks in Visual Analysis of Volume Data", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:21:30+00:00", "description": "[SciVis paper] \n\nAuthors: Tushar Athawale, Elham Sakhaee, Alireza Entezari\n\nAbstract: The problem of isosurface extraction in uncertain data is an important research problem and may be approached in two ways. One can extract statistics (e.g., mean) from uncertain data points and visualize the extracted field. Alternatively, data uncertainty, characterized by probability distributions, can be propagated through the isosurface extraction process. We analyze the impact of data uncertainty on topology and geometry extraction algorithms. A novel, edge-crossing probability based approach is proposed to predict underlying isosurface topology for uncertain data. We derive a probabilistic version of the midpoint decider that resolves ambiguities that arise in identifying topological configurations. Moreover, the probability density function characterizing positional uncertainty in isosurfaces is derived analytically for a broad class of nonparametric distributions. This analytic characterization can be used for efficient closed-form computation of the expected value and variation in geometry. Our experiments show the computational advantages of our analytic approach over Monte-Carlo sampling for characterizing positional uncertainty. We also show the advantage of modeling underlying error densities in a nonparametric statistical framework as opposed to a parametric statistical framework through our experiments on ensemble datasets and uncertain scalar fields.", "uri": "https://vimeo.com/136147397", "name": "VIS15 preview: Isosurface Visualization of Data with Nonparametric Models for Uncertainty", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:21:24+00:00", "description": "[SciVis paper] \n\nAuthors: Christian Schulte zu Berge, Denis Declara, Christoph Hennersperger, Maximilian Baust, Nassir Navab\n\nAbstract: B-mode ultrasound is a very well established imaging modality and is widely used in many of today's clinical routines. However, acquiring good images and interpreting them correctly is a challenging task due to the complex ultrasound image formation process depending on a large number of parameters. To facilitate ultrasound acquisitions, we introduce a novel framework for real-time uncertainty visualization in B-mode images. We compute real-time per-pixel ultrasound Confidence Maps, which we fuse with the original ultrasound image in order to provide the user with an interactive feedback on the quality and credibility of the image. In addition to a standard color overlay mode, primarily intended for educational purposes, we propose two perceptional visualization schemes to be used in clinical practice. Our mapping of uncertainty to chroma uses the perceptionally uniform L*a*b* color space to ensure that the perceived brightness of B-mode ultrasound remains the same. The alternative mapping of uncertainty to fuzziness keeps the B-mode image in its original grayscale domain and locally blurs or sharpens the image based on the uncertainty distribution. An elaborate evaluation of our system and user studies on both medical students and expert sonographers demonstrate the usefulness of our proposed technique. In particular for ultrasound novices, such as medical students, our technique yields powerful visual cues to evaluate the image quality and thereby learn the ultrasound image formation process. Furthermore, seeing the distribution of uncertainty adjust to the transducer positioning in real-time, provides also expert clinicians with a strong visual feedback on their actions. This helps them to optimize the acoustic window and can improve the general clinical value of ultrasound.", "uri": "https://vimeo.com/136147386", "name": "VIS15 preview: Real-time Uncertainty Visualization for B-Mode Ultrasound", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:13:16+00:00", "description": "[InfoVis paper] \n\nAuthors: Mehmet Adil Yalcin, Niklas Elmqvist, Ben Bederson\n\nAbstract: Datasets commonly include multi-value (set-typed) attributes that describe set memberships over elements, such as genres per movie or courses taken per student. Set-typed attributes describe rich relations across elements, sets, and the set intersections. Increasing the number of sets results in a combinatorial growth of relations and creates scalability challenges. Exploratory tasks (e.g. selection, comparison) have commonly been designed in separation for set-typed attributes, which reduces interface consistency. To improve on scalability and to support rich, contextual exploration of set-typed data, we present AggreSet. AggreSet creates aggregations for each data dimension: sets, set-degrees, set-pair intersections, and other attributes. It visualizes the element count per aggregate using a matrix plot for set-pair intersections, and histograms for set lists, set-degrees and other attributes. Its non-overlapping visual design is scalable to numerous and large sets. AggreSet supports selection, filtering, and comparison as core exploratory tasks. It allows analysis of set relations inluding subsets, disjoint sets and set intersection strength, and also features perceptual set ordering for detecting patterns in set matrices. Its interaction is designed for rich and rapid data exploration. We demonstrate results on a wide range of datasets from different domains with varying characteristics, and report on expert reviews and a case study using student enrollment and degree data with assistant deans at a major public university.", "uri": "https://vimeo.com/136146649", "name": "VIS15 preview: AggreSet: Rich and Scalable Set Exploration using Visualizations of Element Aggregations", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:13:10+00:00", "description": "[InfoVis paper] \n\nAuthors: Aur\u00e9lie Cohe, Bastien Liutkus, Gilles Bailly, James Eagan, Eric Lecolinet\n\nAbstract: System schematics, such as those used for electrical or hydraulic systems, can be large and complex. Fisheye techniques can help navigate such large documents by maintaining the context around a focus region, but the distortion introduced by traditional fisheye techniques can impair the readability of the diagram. We present SchemeLens, a vector-based, topology-aware fisheye technique which aims to maintain the readability of the diagram. Vector-based scaling reduces distortion to components, but distorts layout. We present several strategies to reduce this distortion by using the structure of the topology, including orthogonality and alignment, and a model of user intention to foster smooth and predictable navigation. We evaluate this approach through two user studies: Results show that (1) SchemeLens is 16-27% faster than both round and rectangular flat-top fisheye lenses to find and identify a target along one or several paths in a network diagram; (2) augmenting SchemeLens with a lens adapting itself to user intentions aids in learning the network topology.", "uri": "https://vimeo.com/136146638", "name": "VIS15 preview: SchemeLens: A Structural Fisheye Technique for Large Network Diagrams Preserving Topology and Legibility", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:00:22+00:00", "description": "[InfoVis paper] \n\nAuthors: Jimmy Johansson, Camilla Forsell\n\nAbstract: The parallel coordinates technique is widely used for the analysis of multivariate data. During recent decades significant research efforts have been devoted to exploring the applicability of the technique and to expand upon it, resulting in a variety of extensions. Of these many research activities, a surprisingly small number concerns user-centred evaluations investigating actual use and usability issues for different tasks, data and domains. The result is a clear lack of convincing evidence to support and guide uptake by users as well as future research directions. To address these issues this paper contributes a thorough literature survey of what has been done in the area of user-centred evaluation of parallel coordinates. These evaluations are divided into four categories based on characterization of use, derived from the survey. Based on the data from the survey and the categorization combined with the authors' experience of working with parallel coordinates, a set of guidelines for future research directions is proposed.", "uri": "https://vimeo.com/136145376", "name": "VIS15 preview: Evaluation of Parallel Coordinates: Overview, Categorization and Guidelines for Future Research", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-08-12T21:00:10+00:00", "description": "[InfoVis paper] \n\nAuthors: Matthew Brehmer, Jocelyn Ng, Kevin Tate, Tamara Munzner\n\nAbstract: The energy performance of large building portfolios is challenging to analyze and monitor, as current analysis tools are not scalable or they present derived and aggregated data at too coarse of a level. We conducted a visualization design study, beginning with a thorough work domain analysis and a characterization of data and task abstractions. We describe generalizable visual encoding design choices for time-oriented data framed in terms of matches and mismatches, as well as considerations for workflow design. Our designs address several research questions pertaining to scalability, view coordination, and the inappropriateness of line charts for derived and aggregated data due to a combination of data semantics and domain convention. We also present guidelines relating to familiarity and trust, as well as methodological considerations for visualization design studies. Our designs were adopted by our collaborators and incorporated into the design of an energy analysis software application that will be deployed to tens of thousands of energy workers in their client base.", "uri": "https://vimeo.com/136145346", "name": "VIS15 preview: Matches, Mismatches, and Methods: Multiple-View Workflows for Energy Portfolio Analysis", "year": "2015", "event": "PREVIEW"}, {"created_time": "2015-03-02T16:09:23+00:00", "description": "Authors: Fei Wang, Wei Chen, Feiran Wu, Ye Zhao, Han Hong, Tianyu Gu, Long Wang, Ronghua Liang, Hujun Bao", "uri": "https://vimeo.com/121041308", "name": "VAST 2014: A Visual Reasoning Approach for Data-driven Transport Assessment on Urban Roads", "year": "2015", "event": "VAST"}, {"created_time": "2015-03-02T15:59:35+00:00", "description": "Authors: Olav Lenz, Frank Keul, Sebastian Bremm, Kay Hamacher, Tatiana von Landesberger", "uri": "https://vimeo.com/121040238", "name": "VAST 2014: Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs", "year": "2015", "event": "VAST"}, {"created_time": "2015-03-02T15:43:09+00:00", "description": "Authors: Sungahn Ko, Shehzad Afzal, Simon Walton, Yang Yang, Junghoon Chae, Abish Malik, Yun Jang, Min Chen, David Ebert", "uri": "https://vimeo.com/121038387", "name": "VAST 2014: Analyzing High-dimensional Multivariate Network Links with Integrated Anomaly Detection, Highlighting and Exploration", "year": "2015", "event": "VAST"}, {"created_time": "2015-03-02T15:27:48+00:00", "description": "Authors: Johannes Landstorfer, Ivo Herrmann, Jan-Erik Stange, Marian D\u00f6rk, Reto Wettach", "uri": "https://vimeo.com/121036651", "name": "VAST 2014: Weaving a Carpet from Log Entries: a Network Security Visualization Built with Co-Creation", "year": "2015", "event": "VAST"}, {"created_time": "2015-03-02T15:13:27+00:00", "description": "Authors: Steven Gomez, Hua Guo, Caroline Ziemkiewicz, David H. Laidlaw", "uri": "https://vimeo.com/121035154", "name": "VAST 2014: An Insight- and Task-based Methodology for Evaluating Spatiotemporal Visual Analytics", "year": "2015", "event": "VAST"}, {"created_time": "2015-03-01T09:32:49+00:00", "description": "Authors: Sujal Bista, Jiachen Zhou, Rao Gullapalli, Amitabh Varshney", "uri": "https://vimeo.com/120932568", "name": "SciVis 2014: Visualization of Brain Microstructure through Spherical Harmonics Illumination of High Fidelity Spatio-Angular Fiel", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T09:07:29+00:00", "description": "Authors: Sylvia Glasser, Kai Lawonn, Thomas Hoffmann, Martin Skalej, Bernhard Preim", "uri": "https://vimeo.com/120931451", "name": "SciVis 2014: Combined Visualization of Wall Thickness and Wall Shear Stress for the Evaluation of Aneurysms", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T08:43:43+00:00", "description": "Authors: Jan Kretschmer, Grzegorz Soza, Christian Tietjen, Michael Suehling, Bernhard Preim, Marc Stamminger", "uri": "https://vimeo.com/120930434", "name": "SciVis 2014: ADR - Anatomy-Driven Reformation", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T08:12:29+00:00", "description": "Authors: Norbert Lindow, Daniel Baum, Hans-Christian Hege", "uri": "https://vimeo.com/120929204", "name": "SciVis 2014: Ligand Excluded Surface : A New Type of Molecular Surface", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T07:48:28+00:00", "description": "Authors: David Guenther, Roberto Alvarez Boto, Julia Contreras Garcia, Jean-Philip Piquemal, Julien Tierny", "uri": "https://vimeo.com/120928208", "name": "SciVis 2014: Characterizing Molecular Interactions in Chemical Systems", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T07:22:35+00:00", "description": "Authors: Fan Hong, Chufan Lai, Hanqi Guo, Xiaoru Yuan, Enya Shen, Sikun Li", "uri": "https://vimeo.com/120927061", "name": "SciVis 2014: FLDA: Latent Dirichlet Allocation Based Unsteady Flow Analysis", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T07:11:24+00:00", "description": "Authors: Harsh Bhatia, Valerio Pascucci, and Peer-Timo Bremer", "uri": "/channels/721847https://vimeo.com/120926594", "name": "SciVis 2014: [TVCG Invited] The Natural Helmholtz-Hodge Decomposition For Open-Boundary Flow Analysis", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T06:52:52+00:00", "description": "Authors: Tobias G\u00fcnther, Holger Theisel", "uri": "https://vimeo.com/120925812", "name": "SciVis 2014: Vortex Cores of Inertial Particles", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T04:05:56+00:00", "description": "Authors: Franz Sauer, Hongfeng Yu, Kwan-Liu Ma", "uri": "https://vimeo.com/120918220", "name": "SciVis 2014: Trajectory-based Flow Feature Tracking in Joint Particle Volume Datasets", "year": "2015", "event": "SCIVIS"}, {"created_time": "2015-03-01T03:52:25+00:00", "description": "Authors: Hanqi Guo, Jiang Zhang, Richen Liu, Lu Liu, Xiaoru Yuan, Jian Huang, Xiangfei Meng, Jingshan Pan", "uri": "https://vimeo.com/120917560", "name": "SciVis 2014: Advection-Based Sparse Data Management for Visualizing Unsteady Flow", "year": "2015", "event": "SCIVIS"}, {"created_time": "2014-12-02T09:56:48+00:00", "description": "Authors: Andre Schollmeyer, Bernd Froehlich", "uri": "https://vimeo.com/113373268", "name": "SciVis 2014: Direct Isosurface Ray Casting of NURBS-based Isogeometric Analysis", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T09:36:39+00:00", "description": "Authors: Stefan Lindholm, Daniel J\u00f6nsson, Charles Hansen, Anders Ynnerman", "uri": "https://vimeo.com/113371942", "name": "SciVis 2014: Boundary Aware Reconstruction of Scalar Fields", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T09:11:57+00:00", "description": "Authors: Marco Ament, Filip Sadlo, Carsten Dachsbacher, Daniel Weiskopf", "uri": "https://vimeo.com/113370543", "name": "SciVis 2014: Low-Pass Filtered Volumetric Shadows", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T08:42:14+00:00", "description": "Authors: Dilip Thomas, Vijay Natarajan", "uri": "https://vimeo.com/113369006", "name": "SciVis 2014: Multiscale Symmetry Detection in Scalar Fields by Clustering Contours", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T08:22:18+00:00", "description": "Authors: Ronell Sicat, Jens Krueger, Torsten M\u00f6ller, Markus Hadwiger", "uri": "https://vimeo.com/113368100", "name": "SciVis 2014: Sparse PDF Volumes for Consistent Multi-Resolution Volume Rendering", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T08:01:40+00:00", "description": "Authors: Daniel Haehn, Seymour Knowles-Barley, Mike Roberts, Johanna Beyer, Narayanan Kasthuri, Jeff W. Lichtman, Hanspeter Pfister", "uri": "https://vimeo.com/113367077", "name": "SciVis 2014: Design and Evaluation of Interactive Proofreading Tools for Connectomics", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T07:37:30+00:00", "description": "Authors: Manuela Waldner, mathieu le muzic, Matthias Bernhard, Werner Purgathofer, Ivan Viola", "uri": "https://vimeo.com/113365950", "name": "SciVis 2014: Attractive Flicker: Guiding Attention in Dynamic Narrative Visualizations", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T07:17:27+00:00", "description": "Authors: Victor Matvienko and Jens Kruger", "uri": "https://vimeo.com/113365038", "name": "SciVis 2014: A Metric for the Evaluation of Dense Vector Field Visualizations", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T07:01:16+00:00", "description": "Authors: Asmund Birkeland, Cagatay Turkay, and Ivan Viola", "uri": "https://vimeo.com/113364150", "name": "SciVis 2014: Perceptually Uniform Motion Space", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-02T06:44:17+00:00", "description": "Authors: Marta Kersten-Oertel, Sean J. Chen, D. Louis Collins", "uri": "https://vimeo.com/113363361", "name": "SciVis 2014: An Evaluation of Depth Enhancing Perceptual Cues for Vascular Volume Visualization in Neurosurgery", "year": "2014", "event": "SCIVIS"}, {"created_time": "2014-12-01T19:24:21+00:00", "description": "Authors: Ramik Sadana, Timothy Major, Alistair Dove, John Stasko", "uri": "https://vimeo.com/113313251", "name": "InfoVis 2014: OnSet: A visualization technique for large-scale binary set data", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T18:43:56+00:00", "description": "Authors: Brittany Kondo, Christopher Collins", "uri": "https://vimeo.com/113309265", "name": "InfoVis 2014: DimpVis: Exploring Time-varying Information Visualizations by Direct Manipulation", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T18:28:23+00:00", "description": "Authors: Manuel Rubio-S\u00e1nchez, Alberto Sanchez", "uri": "https://vimeo.com/113307720", "name": "InfoVis 2014: Axis Calibration for Improving Data Attribute Estimation in Star Coordinates Plots", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T18:14:00+00:00", "description": "Authors: Samuel Gratzl, Nils Gehlenborg, Alexander Lex, Hanspeter Pfister, Marc Streit \n\n**InfoVis Honorable Mention Award**", "uri": "https://vimeo.com/113306290", "name": "InfoVis 2014: Domino: Extracting, Comparing, and Manipulating Subsets across Multiple Tabular Datasets", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T17:56:31+00:00", "description": "InfoVis Paper Chairs: Helwig Hauser (University of Bergen), Jeff Heer (University of Washington), Melanie Tory (University of Victoria)", "uri": "https://vimeo.com/113304516", "name": "InfoVis 2014: Opening", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T02:17:44+00:00", "description": "Authors: Martijn Tennekes, Edwin de Jonge", "uri": "https://vimeo.com/113240453", "name": "InfoVis 2014: Tree Colors: Color Schemes for Tree-Structured Data", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T02:08:26+00:00", "description": "Authors: Felipe S. L. G. Duarte, Fabio Sikansi, Francisco M. Fatore, Samuel G. Fadel, Fernando V. Paulovich", "uri": "https://vimeo.com/113239918", "name": "InfoVis 2014: Nmap: A Novel Neighborhood Preservation Space-filling Algorithm", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T01:56:08+00:00", "description": "Authors: Arthur van Goethem, Andreas Reimer, Bettina Speckmann, Jo Wood", "uri": "https://vimeo.com/113239278", "name": "InfoVis 2014: Stenomaps: Shorthand for Shapes", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T01:41:44+00:00", "description": "Authors: Diansheng Guo, Xi Zhu", "uri": "https://vimeo.com/113238478", "name": "InfoVis 2014: Origin-Destination Flow Data Smoothing and Mapping", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T01:21:02+00:00", "description": "Authors: Cagatay Turkay, Aidan Slingsby, Helwig Hauser, Jo Wood, Jason Dykes", "uri": "https://vimeo.com/113237309", "name": "InfoVis 2014: Attribute Signatures: Dynamic Visual Summaries for Analyzing Multivariate Geographical Data", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T01:02:46+00:00", "description": "Authors: Radu Jianu, Adrian Rusu, Yifan Hu, Douglas Taggart", "uri": "https://vimeo.com/113236263", "name": "InfoVis 2014: How to Display Group Information on Node-Link Diagrams: an Evaluation", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T00:46:27+00:00", "description": "Authors: Marcel Hlawatsch, Michael Burch, Daniel Weiskopf", "uri": "https://vimeo.com/113235326", "name": "InfoVis 2014: Visual Adjacency Lists for Dynamic Graphs", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T00:33:50+00:00", "description": "Authors: Benjamin Bach, Emmanuel Pietriga, Jean-Daniel Fekete", "uri": "https://vimeo.com/113234643", "name": "InfoVis 2014: GraphDiaries: Animated Transitions and Temporal Navigation for Dynamic Networks", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T00:19:18+00:00", "description": "Authors: James Abello, Steffen Hadlak, Heidrun Schumann, and Hans-J\u00f6rg Schulz", "uri": "https://vimeo.com/113233823", "name": "InfoVis 2014: A Modular Degree-of-Interest Specification for the Visual Analysis of Large Dynamic Networks", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-12-01T00:03:50+00:00", "description": "Authors: Charles D. Stolper, Minsuk Kahng, Zhiyuan Lin, Florian Foerster, Aakash Goel, John Stasko, Duen Horng Chau", "uri": "https://vimeo.com/113232967", "name": "InfoVis 2014: GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-11-30T23:44:42+00:00", "description": "Authors: Stef van den Elzen, Jarke J. van Wijk\n\n**InfoVis Best Paper Award**", "uri": "https://vimeo.com/113231962", "name": "InfoVis 2014: Multivariate Network Exploration and Presentation: From Detail to Overview via Selections and Aggregations", "year": "2014", "event": "INFOVIS"}, {"created_time": "2014-11-28T21:06:22+00:00", "description": "Thanks to everyone contributing to the VIS 25 timeline!\n\nWe enjoyed seeing you all in Paris for IEEE VIS 2014!\n\nKeep our collection growing at http://ieeevis.org/vis25timeline\n\nCredits: Benjamin Bach, Jeremy Boy, Theresa-Marie Rhyne, Mathieu Le Goc, John Stasko\nMusic by Broke for Free \"As Colorful as Ever\", Creative Commons Licence, freemusicarchive.org", "uri": "https://vimeo.com/113119116", "name": "VIS 25th Anniversary Celebration Video", "year": "2014", "event": ""}, {"created_time": "2014-10-01T18:30:41+00:00", "description": "Authors: J. Lee, K. McDonnell, A. Zelenyuk, D. Imre, K. Mueller\n\nAbstract - Although the Euclidean distance does well in measuring data distances within high-dimensional clusters, it does poorly when it comes to gauging inter-cluster distances. This significantly impacts the quality of global, low-dimensional space embedding procedures such as the popular multi-dimensional scaling (MDS) where one can often observe non-intuitive layouts. We were inspired by the perceptual processes evoked in the method of parallel coordinates which enables users to visually aggregate the data by the patterns the polylines exhibit across the dimension axes. We call the path of such a polyline its structure and suggest a metric that captures this structure directly in high-dimensional space. This allows us to better gauge the distances of spatially distant data constellations and so achieve data aggregations in MDS plots that are more cognizant of existing high-dimensional structure similarities. Our bi-scale framework distinguishes far-distances from near-distances. The coarser scale uses the structural similarity metric to separate data aggregates obtained by prior classification or clustering, while the finer scale employs the appropriate Euclidean distance.", "uri": "https://vimeo.com/107739561", "name": "A Structure-Based Distance Metric for High-Dimensional Space Exploration with Multi-Dimensional Scaling", "year": "2014", "event": ""}, {"created_time": "2014-09-09T11:30:27+00:00", "description": "Authors: L\u00e9o Allemand-Giorgis, Georges-Pierre Bonneau, Stefanie Hahmann\n\nAbstract: Simulation of phenomena like climate often deals with large datasets. A process to extract the most salient features is needed to assist in the understanding of the dataset. The Morse-Smale (MS) complex is a topological structure defined on scalar functions which extracts critical points of the function and the links between them. Furthermore, it encodes a hierarchy between critical points, and less important critical points can be deleted in order to simplify the structure. Starting from an initial function f : R2 \u2192 R, the Morse-Smale complex of this function is computed, then simplified. From this simplified structure, we aim to construct a new function, which corresponds to this structure, closed to the initial function, thus approximating the initial data set by preserving the most salient features. The main difficulty we face, lies in the fact that both, the boundary curves (corresponding to the 1-cells of the MS complex) and the surface patches inside each 2-cell have to be monotonic functions. Furthermore, the geometry of the 2-cells may be very complex, see Fig. 1 (middle and right). This poster proposes a novel approach for computing a scalar function coherent with a given simplified MS complex that privileges the use of piecewise polynomial functions. Based on techniques borrowed from Shape Preserving Design in Computer Aided Geometric Design, our method constructs the surface 2-cell by 2-cell using piecewise polynomials curves and surfaces. We first compute monotonic boundary curves as quartic B-splines. With the aid of a parametrization of the 2-cell\u2019s domain onto the unit square we then construct a piecewise polynomial monotonic surface inside the 2-cell interpolating the boundary curves and the critical points provided by the simplified MS complex.", "uri": "https://vimeo.com/105646744", "name": "Piecewise Polynomial Reconstruction of Functions from Simplified Morse-Smale complex", "year": "2014", "event": ""}, {"created_time": "2014-08-26T16:05:39+00:00", "description": "Organisers: Chuck Hansen and Thierry Carrard\n\nIn many areas of science, simulations and experiments begin to generate many petabytes of data, with some sciences facing exabytes of data near term. Similarly, the collection of information about the Internet applications and users for a variety of purposes is generating only more data. Our ability to manage, mine, analyze, and visualize the data is fundamental to the knowledge discovery process. That is, the value of data at extreme scale can be fully realized only if we have an end-to-end solution, which demands a collective, inter-disciplinary effort to develop. This new symposium, held in conjunction with IEEE VIS 2014, aims at bringing together domain scientists, data analytics and visualization researchers, and users, and fostering the needed exchange to develop the next-generation data-intensive analysis and visualization technology. Attendees will be introduced to the latest and greatest research innovations in large data management, analysis, and visualization, learn how these innovations impact data intensive computing and knowledge discovery, and also learn about the critical issues in creating a complete solution through both invited and contributed talks, and panel discussion. Paper submissions are solicited for a long paper event that describes large data visualization techniques and systems, and a short paper event for practitioners to describe and present their large data visualization applications. Topic emphasis is on algorithms, languages, systems and hardware that supports the analysis and visualization of large data.", "uri": "https://vimeo.com/104416760", "name": "LDAV 2014 - The 4th IEEE Symposium on Large Data Analysis and Visualization", "year": "2014", "event": "LDAV"}, {"created_time": "2014-08-20T07:33:35+00:00", "description": "Authors: Jose Gustavo S. Paiva, William Robson Schwartz, Helio Pedrini, Rosane Minghim\n\nAbstract: Automatic data classification is a computationally intensive task that presents variable precision and is considerably sensitive to the classifier configuration and to data representation, particularly for evolving data sets. Some of these issues can best be handled by methods that support users\u2019 control over the classification steps. In this paper, we propose a visual data classification methodology that supports users in tasks related to categorization such as training set selection; model creation, application and verification; and classifier tuning. The approach is then well suited for incremental classification, present in many applications with evolving data sets. Data set visualization is accomplished by means of point placement strategies, and we exemplify the method through multidimensional projections and Neighbor Joining trees. The same methodology can be employed by a user who wishes to create his or her own ground truth (or perspective) from a previously unlabeled data set. We validate the methodology through its application to categorization scenarios of image and text data sets, involving the creation, application, verification, and adjustment of classification models.", "uri": "https://vimeo.com/103884940", "name": "An Approach to Supporting Incremental Visual Data Classification", "year": "2014", "event": ""}, {"created_time": "2014-08-20T07:33:34+00:00", "description": "Authors: Sriram Karthik Badam, Eli Raymond Fisher, and Niklas Elmqvist\n\nAbstract: We present Munin, a software framework for building ubiquitous analytics environments consisting of multiple input and output surfaces, such as tabletop displays, wall-mounted displays, and mobile devices. Munin utilizes a service-based model where each device provides one or more dynamically loaded services for input, display, or computation. Using a peer-to-peer model for communication, it leverages IP multicast to replicate the shared state among the peers. Input is handled through a shared event channel that lets input and output devices be fully decoupled. It also provides a data-driven scene graph to delegate rendering to peers, thus creating a robust, fault-tolerant, decentralized system. In this paper, we describe Munin\u2019s general design and architecture, provide several examples of how we are using the framework for ubiquitous analytics and visualization, and present a case study on building a Munin assembly for multidimensional visualization. We also present performance results and anecdotal user feedback for the framework that suggests that combining a service-oriented, data-driven model with middleware support for data sharing and event handling eases the design and execution of high performance distributed visualizations.", "uri": "https://vimeo.com/103884939", "name": "Munin: A Peer-to-Peer Middleware for Ubiquitous Analytics and Visualization Spaces", "year": "2014", "event": ""}, {"created_time": "2014-08-20T07:33:34+00:00", "description": "Authors: Andre Schollmeyer, Bernd Froehlich\n\nAbstract: In NURBS-based isogeometric analysis, the basis functions of a 3D model\u2019s geometric description also form the basis for the solution space of variational formulations of partial differential equations. In order to visualize the results of a NURBS-based isogeometric analysis, we developed a novel GPU-based multi-pass isosurface visualization technique which performs directly on an equivalent rational B\u00e9zier representation without the need for discretization or approximation. Our approach utilizes rasterization to generate a list of intervals along the ray that each potentially contain boundary or isosurface intersections. Depth-sorting this list for each ray allows us to proceed in front-to-back order and enables early ray termination. We detect multiple intersections of a ray with the higher-order surface of the model using a sampling-based root-isolation method. The model\u2019s surfaces and the isosurfaces always appear smooth, independent of the zoom level due to our pixel-precise processing scheme. Our adaptive sampling strategy minimizes costs for point evaluations and intersection computations. The implementation shows that the proposed approach interactively visualizes volume meshes containing hundreds of thousands of B\u00e9zier elements on current graphics hardware. A comparison to a GPU-based ray casting implementation using spatial data structures indicates that our approach generally performs significantly faster while being more accurate.", "uri": "https://vimeo.com/103884937", "name": "Direct Isosurface Ray Casting of NURBS-based Isogeometric Analysis", "year": "2014", "event": ""}, {"created_time": "2014-08-20T07:33:33+00:00", "description": "Authors: Marcel Hlawatsch, Michael Burch, Daniel Weiskopf\n\nAbstract: We present a visual representation for dynamic, weighted graphs based on the concept of adjacency lists. Two orthogonal axes are used: one for all nodes of the displayed graph, the other for the corresponding links. Colors and labels are employed to identify the nodes. The usage of color allows us to scale the visualization to single pixel level for large graphs. In contrast to other techniques, we employ an asymmetric mapping that results in an aligned and compact representation of links. Our approach is independent of the specific properties of the graph to be visualized, but certain graphs and tasks benefit from the asymmetry. As we show in our results, the strength of our technique is the visualization of dynamic graphs. In particular, sparse graphs benefit from the compact representation. Furthermore, our approach uses visual encoding by size to represent weights and therefore allows easy quantification and comparison. We evaluate our approach in a quantitative user study that confirms the suitability for dynamic and weighted graphs. Finally, we demonstrate our approach for two examples of dynamic graphs.", "uri": "https://vimeo.com/103884936", "name": "Visual Adjacency Lists for Dynamic Graphs", "year": "2014", "event": ""}, {"created_time": "2014-08-20T07:33:33+00:00", "description": "Authors: Marta Kersten-Oertel, Sean J. Chen, D. Louis Collins\n\nAbstract: Cerebral vascular images obtained through angiography are used by neurosurgeons for diagnosis, surgical planning and intra-operative guidance. The intricate branching of the vessels and furcations, however, make the task of understanding the spatial three-dimensional layout of these mages challenging. In this paper, we present empirical studies on the effect of different perceptual cues (fog, pseudo-chromadepth, kinetic depth, and depicting edges) both individually and in combination on the depth perception of cerebral vascular volumes and compare these to the cue of stereopsis. Two experiments with novices and one experiment with experts were performed. The results with novices showed that the pseudo-chromadepth and fog cues were stronger cues than that of stereopsis. Furthermore, the addition of the stereopsis cue to the other cues did not improve relative depth perception in cerebral vascular volumes. In contrast to novices, the experts also performed well with the edge cue. In terms of both novice and expert subjects, pseudo-chromadepth and fog allow for the best relative depth perception, although experts unlike novices also performed well with the edge cue. By using such cues to improve depth perception of cerebral vasculature we may improve diagnosis, surgical planning, and intra-operative guidance.", "uri": "https://vimeo.com/103884934", "name": "An Evaluation of Depth Enhancing Perceptual Cues for Vascular Volume Visualization in Neurosurgery", "year": "2014", "event": ""}, {"created_time": "2014-08-20T07:25:12+00:00", "description": "Authors: Ingrid Hotz, Anna Vilanova, Thomas Schultz, Eugene Zhang\n\nAbstract: Tensors provide a powerful language to describe physical phenomena. Consequently, they play a fundamental role in physics and engineering. They appear in many numerical simulations, either as the final result or as intermediate product. Application areas are medicine, geology, astrophysics, continuum mechanics and many more. Compared to their significance  tensor fields are an underrepresented topic in visualization. Often a certain reservation with respect to this topic can be observed. The reasons for this may be the complexity of the field but also the fact that there is no long tradition in tensor field visualization, e.g. compared to vector fields. Many domain scientists are just starting to consider these data as interesting. As a consequence there is no uniform language  and the relevant information, needed to get started, is scattered in various textbooks. \\  \\ The intention of the tutorial is to motivate more visualization scientists to get involved in this field and ease the entry into this topic. Our experience, in particular when working with students, has shown  that such an introduction is strongly needed. In summary, the major goals can be summarized as: Introduction into the fundamental concepts of tensors and presentation of the different view on tensor fields; Introduction of the main application areas from diffusion tensor imaging to mechanical engineering applications; Pointing out the major challenges when processing tensor data; Finally, giving an overview about existing visualization methods.", "uri": "https://vimeo.com/103884481", "name": "Tutorial - Introduction to Tensor Field Visualization: Concepts, Processing and Visualization", "year": "2014", "event": "TUTORIAL"}, {"created_time": "2014-08-20T07:25:12+00:00", "description": "Authors: Rita Borgo, Min Chen, Eamon Maguire, Sine McDougall, Matthew Ward\n\nAbstract: Multivariate data visualization (MDV) is nowadays a common requirement across different disciplines, from the sciences to the social sciences, from engineering to arts and humanities, and from media to industry. Glyph-based visualization is a form of MDV. In comparison with other forms of MDV, such as parallel coordinates plots, glyph-based visualization exhibits some unique advantages as well as limitations. For example, glyphs can be superimposed on top of their spatial or temporal contexts (e.g., a map or a time series plot), while most other forms of MDV cannot. Well-designed glyphs can facilitate effective visual search and pattern identification, and are intuitive to learn and use. Meanwhile, because of their sizes, the visual channels used in glyphs have relatively limited bandwidth capacities. The costs of designing a glyph set for a specific application, and the demands for familiarization and memorization are usually the main stumbling block hindering the deployment of this technique. There has been a resurging interest in glyph-based visualization in recent years. This tutorial is built on a series of surveys on glyph-based visualization [Ward 2002 [20], Ward 2008 [19], Borgo et al. 2013 [4]], and offers a timely introduction to the fundamentals, techniques and applications of glyph-based visualization.", "uri": "https://vimeo.com/103884480", "name": "VIS 2014: Tutorial: Glyph-based Visualization", "year": "2014", "event": "TUTORIAL"}, {"created_time": "2014-08-20T07:25:11+00:00", "description": "Authors: Tamara Munzner\n\nAbstract: This introductory tutorial will provide a broad foundation for thinking systematically about visualization systems, built around the idea that becoming familiar with analyzing existing systems is a good springboard for designing new ones. The major data types of concern in visual analytics, information visualization, and scientific visualization will all be covered: tables, networks, and sampled spatial data. This tutorial is focused on data and task abstractions, and the design choices for visual encoding and interaction; it will not cover algorithms. No background in computer science or visualization is assumed.", "uri": "https://vimeo.com/103884479", "name": "Tutorial - Visualization Analysis and Design", "year": "2014", "event": "TUTORIAL"}, {"created_time": "2014-08-20T07:25:11+00:00", "description": "Authors: Theresa-Marie Rhyne\n\nAbstract: We examine the foundations of color theory and how these methods apply to building effective and compelling visualizations. We define color harmony and demonstrate the application of color harmony to visualization case studies. These case studies include ensemble scientific visualizations, historic information visualizations, correlation in molecular biological data, rainbow color deficiency safe examples, and time series animations. We review the features of ColorBrewer and discuss automated color suggestion systems for use in building visualizations. Adobe\u2019s new mobile Kuler app is demonstrated for color design and analyses. To aid in the design of interactive visual analytics, we demonstrate the new \u201cInteraction of Color\u201d app that is based on Josef Albers\u2019 interactive color studies. We also introduce \u201cColor Proportions of an Image\u201d analysis tools. Our tutorial concludes with a hands on session that teaches how to use online and mobile apps to successfully capture, analyze and store color schemes for future use in visualizations. This includes the evaluations for color deficiencies using Vischeck and similar mobile apps. These color suggestion tools are available online for your continued use in creating visualizations. Please bring small JPEG examples of your visualizations for performing color analyses during the hands on session. Since IEEE VIS 2014 is in Paris, we encourage you to capture color schemes as the French Impressionists might have done for their work.", "uri": "https://vimeo.com/103884478", "name": "Tutorial - Applying Color Theory to Visualization", "year": "2014", "event": "TUTORIAL"}, {"created_time": "2014-08-20T07:25:10+00:00", "description": "Authors: Hans-J\u00f6rg Schulz, Tatiana von Landesberger, Dominikus Baur\n\nAbstract: Interaction (or human-computer interaction/HCI) is a key ingredient of modern visualization and visual analysis systems. It allows the user to manage the data and to explore its different aspects, as well as to shape its visual representation and to observe it from different perspectives - ultimately to pursue the user's analytical goal. Yet, so far interaction is perceived from either of three different angles: through interaction activities (e.g., visualization tasks or input events), through interaction architectures (e.g., model-view-controller), or through interaction metaphors (e.g., direct manipulation). This 1/2-day tutorial for beginning audiences aims to bring these three perspectives on interaction together by first detailing the state-of-the-art for each of them and then putting them in the context of each other. This will be accompanied by a discussion of real world examples - i.e., actual interactive visualization techniques and systems - to highlight the benefits and challenges of such a comprehensive view on interaction. After completing this tutorial, participants can expect to have gained an extensive overview of interaction in visualization that will allow them to consider all three of the perspectives when designing and evaluating interactions.", "uri": "https://vimeo.com/103884477", "name": "Tutorial - Opening the Black Box of Interaction in Visualization", "year": "2014", "event": "TUTORIAL"}, {"created_time": "2014-08-20T07:16:25+00:00", "description": "Authors: Noeska Smit, Cees-Willem Hofstede, Annelot Kraima, Daniel Jansma, Marco deRuiter, Elmar Eisemann, Charl Botha, Anna Vilanova\n\nAbstract: Human anatomy is complex and as early as the late bronze age, people have been trying to gain insights in the functioning of the human body. Nowadays, resources such as books and software are available to educate medical students, but these media usually have some restrictions; anatomical images in books present information from fixed views and do not allow readers to freely explore the information, while software tools often present an idealized average human anatomy. In this work, we introduce the Online Anatomical Human (OAH), an online viewer and annotation system for anatomical information. It is based on real human anatomy and incorporates medical image data in linked 2D and 3D views. The goals of this anatomical online resource are two-fold. First of all, the OAH will serve as an educational platform available to anyone that has access to a modern web browser. Secondly, by making our work accessible to medical experts, we can ensure an increasing amount of information and, hereby, a gain in educational value of our tool.", "uri": "https://vimeo.com/103884057", "name": "The Online Anatomical Human: Anatomical Knowledge Exchange on the Web", "year": "2014", "event": ""}, {"created_time": "2014-08-20T07:16:25+00:00", "description": "Authors: Roxana Bujack, Ingrid Hotz, Jens Kasten, Gerik Scheuermann, Eckhard Hitzer\n\nAbstract: Moment invariants are popular descriptors for real valued functions. Their independence from certain transformations makes them a powerful tool for the recognition of patterns and shapes. It has recently been demonstrated that the basic ideas can also be transferred to vector valued functions. Vector moment invariants can be used to define and search for interesting flow structures. A generalization to three-dimensional vector valued functions so far has not been investigated at all.  In this paper, we approach that problem. We introduce a definition of moments for three-dimensional vector fields and present how flow field invariants can be constructed from the normalization of the first order vector moment tensor.", "uri": "https://vimeo.com/103884056", "name": "Moment Invariants for 3D Flow Fields", "year": "2014", "event": ""}, {"created_time": "2014-08-20T07:16:25+00:00", "description": "Authors: Anne Stevens, Hudson Pridham, Steve Szigeti, Sara Diamond, Bhuvaneswari Arunachalan\n\nAbstract: Visual analytic tools, combined with social networks and mobile platforms, make it possible to create multi-dimensional, holistic pictures of people's health care and condition and expand the scope of information addressed in medical records. The Care and Condition Monitor (CCM) is a tablet-based, networked visual analytics tool for collecting, structuring and analyzing informal and qualitative healthcare data and visualizing it in a circular format. It illustrates how social communication within teams of caregivers enables capturing of longitudinal informal data that can (a) result in rich and meaningful information visualizations, (b) improve comprehension of healthcare data and changes in condition over time, and (c) support medical decision making.", "uri": "https://vimeo.com/103884055", "name": "The Care and Condition Monitor: a Tablet Based Tool for Visualizing Informal Qualitative Health Care Data", "year": "2014", "event": ""}, {"created_time": "2014-08-20T07:13:11+00:00", "description": "Organisers: Angus Forbes, Fanny Chevalier\n\nAbstract: The IEEE VIS 2014 Arts Program, or VISAP'14, showcases high-quality artwork and research that explores the exciting and increasingly prominent intersections between art and visualization. Through a dedicated papers track and an art show that runs concurrently with the IEEE 2014 VIS conference, the Arts Program aims to foster new thinking, discussion, and collaboration between artists, designers, technologists, visualization scientists, and others working at the intersection of these fields.\nThe theme for VISAP'14 is Art+Interpretation. In certain ways artists and visualization researchers share common goals: to make things visible which are normally difficult to see; and to enable reasoning about information that we might otherwise remain ignorant of. A conventional explanation of the differences between art practice and visualization research is that artistic exploration raises new questions, while visualization research aims to help domain experts answer existing questions. However, these categorizations may be oversimplified. Media artists create opportunities for reflecting on cultural issues, but also highlight how we absorb technology and explore how the exposure to tremendous amounts of data affects our daily lives. Recently, in the visualization community, significant emphasis has been placed on notions such as indicating uncertainty, accurately portraying data provenance, and using narrative techniques to aid in transmitting information more effectively. Visualization systems not only provide a representation of data collections, but also, wittingly or unwittingly, provide an interpretation of that data. Hence, potential areas of overlap between art and research practices are becoming more discernible, which raises the following key questions: Can artistic practice offer insight into thinking about the effective interpretability of complex data? Conversely, can visualization research offer quantifiable methods to artists seeking to investigate and represent cultural phenomena? In this year's call for entries, we are looking especially for projects and papers that explore the relationships between visualization research and artistic practice, and that present or discuss creative visual techniques that emphasize the interpretative or narrative aspects of scientific or cultural exploration.", "uri": "https://vimeo.com/103883893", "name": "VIS Art Program (VISAP)", "year": "2014", "event": "ARTS PROGRAM"}, {"created_time": "2014-08-20T07:10:43+00:00", "description": "Moderator: J\u00f6rn Kohlhammer\nPanelists: Philippe Quevauviller, Teresa de Martino, William Wong, Daniel Keim, Brian Fisher\n\nAbstract: IEEE VIS comes to Europe for the first time in 25 years. In the same year, the new framework program for research in the EU, called Horizon 2020, had its first calls targeting a wide range of topics related to ICT, including visualization. The breadth of calls and sub-programs can be truly overwhelming, and it is hard to get an overview of the opportunities for researchers in information visualization, visual analytics, and scientific visualization. There is also the possibility to include partners from outside the EU, called international collaboration in Horizon 2020, which should be of interest for many participants of VIS.\nThis panel aims to give an overview of the current EU funding opportunities in visualization.We have confirmation by two EU representatives from Brussels, Belgium, who are or were overlooking projects with a visualization and visual analytics emphasis. They agreed to introduce two areas of EU funding in Horizon 2020: the specific security research unit, and the broader, basic research-oriented FET area. We also invited several project coordinators of EU projects that give examples for successful collaboration or potential pitfalls during projects. \nIn addition to the panelists we also invited representatives of international funding agencies, including Dr. Joe Kielman, Department of Homeland Security (DHS) and Dr. Maria Zemankova, National Science Foundation (NSF), to give their views as part of the discussion on international collaboration.", "uri": "https://vimeo.com/103883780", "name": "Panel: Funding Opportunities for V.I.S. Research in Horizon 2020", "year": "2014", "event": "PANEL"}, {"created_time": "2014-08-13T16:32:51+00:00", "description": "Authors: Daniela Oelke, Judith Eckle-Kohler, Iryna Gurevych\n\nAbstract: This poster paper describes experiences and results of a collaboration between computational linguists and visualization experts whose goal was to design a visualization for a web interface of the large-scale linked lexical resource UBY. Besides introducing the problem and the resulting design, we reflect on the iterative design process, thereby focusing on lessons learned that are applicable to all kinds of interdisciplinary visualization projects (including industrial projects). Furthermore, we briefly discuss the value of detours when working as a visualization expert with practitioners.", "uri": "https://vimeo.com/103348398", "name": "Is there a Value in Detours? - Experiences with Designing a Visual Browser for the Linked Lexical Resource UBY", "year": "2014", "event": ""}, {"created_time": "2014-08-11T20:03:02+00:00", "description": "Authors: J\u00fcrgen Bernard, David Sessler, Michael Behrisch, Marco Hutter, Tobias Schreck, J\u00f6rn Kohlhammer\n\nAbstract: The creation of similarity functions based on visual-interactive user feedback is a promising means to capture the mental similarity notion in the heads of domain experts. In particular, concepts exist where users arrange multivariate data objects on a 2D data landscape in order to learn new similarity functions. While systems that incorporate numerical data attributes have been presented in the past, the remaining overall goal may be to develop systems also for mixed data sets. In this work, we present a feedback model for categorical data which can be used alongside of numerical feedback models in future.", "uri": "https://vimeo.com/103169827", "name": "Towards a User-Defined Visual-Interactive Definition of Similarity Functions for Mixed Data", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:17:10+00:00", "description": "Authors: Ekaterina Galkina, Georges Grinstein\n\nAbstract: Growing evidence points to serious medical conditions associated with airborne pollutant exposure, including neurologically harmful effects, increases in hospitalizations and mortality due to cardiovascular and respiratory diseases. The US Environmental Protection Agency (EPA) regularly monitors levels of lead, ozone, particulate and other air pollution. However, monitor sites are unequally distributed, necessitating spatial interpolation of pollutant concentrations at poorly sampled locations. Our study focuses on epidemiological data aggregated to only the first 3-digits of the postal ZIP code. Consequently, the varying land area coverage of the information-sensitive residential geographies creates an additional exposure assessment challenge. We compared four common interpolation methods for predicting particulate matter concentrations in the state of California alone, including kernel smoothing, inverse distance weighting (IDW), Voronoi partitioning and kriging. We show that methods that produce a prediction standard error map are the most reliable (i.e. kriging) and that a consensus in the prediction maps can be reached only at the smaller geographical units, i.e. county-level.", "uri": "https://vimeo.com/103147280", "name": "Comparison of interpolation methods for estimating spatially aggregated pollution exposure", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:12:15+00:00", "description": "Authors: Alexander Bock, M. Leila Mays, Lutz Rastaetter, Anders Ynnerman, Timo Ropinski\n\nAbstract: Supporting the growing field of space weather forecasting, we propose a framework to analyze ensemble simulations of coronal mass ejections. As the current simulation technique requires manual input, uncertainty is introduced into the simulation pipeline leading to inaccurate predictions. Using our system, the analyst can compare ensemble members against ground truth data (arrival time and geo-effectivity) as well as information derived from satellite imagery. The simulations can be compared globally, based on time-resolved quality measures, and as a 3D volumetric rendering with embedded satellite imagery in a multi-view setup. This flexible framework provides the expert with the tools to increase the knowledge about the, as of yet not fully understood, principles behind the evolution and propagation of coronal mass ejections", "uri": "https://vimeo.com/103146830", "name": "VCMass: A Framework for Verification of Coronal Mass Ejection Ensemble Simulations", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:12:14+00:00", "description": "Authors: Martin R\u00f6hlig, Martin Luboschik, Heidrun Schumann, Markus B\u00f6gl, Bilal Alsallakh, Silvia Miksch\n\nAbstract: Reconstructing processes from measurements of multiple sensors over time is an important task in many application domains. For the reconstruction, these multivariate time-series can be automatically processed. However, the outcomes of automated algorithms often vary in quality and show strong parameter dependencies, making manual inspections and adjustments of the results necessary. We propose a visual analysis approach to support the user in understanding parameters\u2019 influences on these results. With our approach the user can identify and select parameter settings that meet certain quality criteria. The proposed visual and interactive design helps to identify relationships and temporal patterns, supports subsequent decision making, and promotes higher accuracy as well as confidence in the results.", "uri": "https://vimeo.com/103146825", "name": "Analyzing Parameter Influence on Time-Series Segmentation and Labeling", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:12:14+00:00", "description": "Authors: Flavio Gortana, Sebastian Kaim, Martin von Lupin, Till Nagel\n\nAbstract: Isochrone maps are an established method to depict areas of equal travel time, and have been used in transportation planning since the early 20th century. In recent years, interactive isochrone maps allowed users to select areas of interest, or explore temporal mobility patterns for different modes of transport. However, conventional isochrone maps depict one traffic situation at a time. Our visualization approach unifies isochrone maps with time-varying travel data, and instead of showing multiple isolines for different travel times, we show multiple isolines for different times of day in order to reveal time-dependent spatial travel variance. In this paper, we present Isoscope, a web application that provides an interactive map for casual exploration of urban mobility patterns. Through its aesthetic visual form and its simple interface we strive to support people investigating travel time in their own city. We describe our design goals, elaborate on the design and implementation of our prototype, and discuss limitations and future extensions of the system.", "uri": "https://vimeo.com/103146822", "name": "Isoscope - Visualizing temporal mobility variance with isochrone maps", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:12:13+00:00", "description": "Authors: Tiago Etiene, Paulo Pagliosa, Luis Gustavo Nonato\n\nAbstract: Timelines are often used to summarize complex, time-evolving, sequences of events. In this poster, we present the preliminary results of Linea, a tool designed to help users build timelines that summarize content from Wikipedia. We combine Natural Language Processing and the PageRank algorithm to automatically build default timelines of any Wikipedia article, giving users a starting point for tailoring them. Then, users can interact with the event matrix \u201ca collection of histograms of events\u201d by exploring each matrix cell in search for peaks, i.e., proxies of important periods of time. By selecting a peak, the tool shows all events in that period so users are able to add or remove events, thus customizing timelines.", "uri": "https://vimeo.com/103146820", "name": "Linea: Tailoring Timelines by Visual Exploration of Temporal Text", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:07:50+00:00", "description": "Authors: Kazuo Misue, Kiyohisa Taguchi\n\nAbstract: Emotion-weather maps are some kinds of thematic maps to represent emotions. The maps represent spatial distributions of complex emotions of masses. Data of emotions have been extracted from social media. The data have some biases in some aspect of time, space, and categories of emotions. The biases obstructs observation of relatively few emotions. To address such problem, collected data are normalized. As a use case a sets of maps is presented. These maps represent distributions of emotions of an actual day. An earthquake on that day caused fear and surprise in a region in Japan.", "uri": "https://vimeo.com/103146402", "name": "Emotion-Weather Maps: Representation of Spatial Distributions of Mass and Complex Emotions", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:07:50+00:00", "description": "Authors: Hyunjoo Song, Jeongjin Lee, Tae Jung Kim, Bohyoung Kim, Jinwook Seo\n\nAbstract: We present an interactive visual comparison framework (GazeDx) for gaze data from multiple readers, which incorporates important contextual information into the comparative analysis process. A comparative analysis of gaze pattern is essential to understand how radiologists read medical images. However, most prior work on volumetric medical images focused on visualization of gaze patterns, but did not address the need for comparative analyses of multiple readers\u2019 gaze patterns. The GazeDx framework supports qualitative comparison based on interactively coordinated multiple views (spatial view with 3D gaze visualization, enhanced navigation charts, and matrix view), and quantitative comparison of gaze patterns in the similarity view with several similarity measures. It also integrates crucial contextual information such as pupil size, distance to a monitor, or windowing (i.e. adjustment of image contrast and brightness which affects visibility of organs and lesions) into the analysis process.", "uri": "https://vimeo.com/103146398", "name": "Comparative Gaze Analysis Framework for Volumetric Medical Images", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:07:49+00:00", "description": "Authors: Nan-Chen Chen, Laurie Beth Feldman, Judith F. Kroll, Cecilia R. Aragon\n\nAbstract: Socio-emotional communication is a critical determining factor in the cohesiveness of international work teams. In recent years, online text communication (e.g., chat, forums, email) has been widely used in cross-cultural collaborations, and emoticons are often viewed as socio-emotional cues in this type of communication. Therefore, it is important to know how emoticons work in online text communication. One way to investigate this topic is to leverage theories in sociolinguistics to find potential mappings between emoticon use and face-to-face language use. In the present work, we propose a visual analytics tool to explore emoticon use among different groups over time and show how visual analytics can elicit storytelling in studying linguistic alignment of emoticons in a chat log dataset from a four-year scientific collaboration between France and the United States.", "uri": "https://vimeo.com/103146393", "name": "Emoticons and Linguistic Alignment: How Visual Analytics Can Elicit Storytelling", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:07:49+00:00", "description": "Authors: Bertjan Broeksema, Fintan McGee, Magdalena Calusinska, Mohammad Ghoniem\n\nAbstract: Within metagenomics, \"Contig Binning\" is an important step in the process of reconstructing genomes of species in mixed cultures and environmental samples. We present an interactive visual environment which enables a biologist to statistically analyze the multiple dimensions of data that are typically used during binning, and integrate and compare the results of various binning methods. Our system features a web-based parallel coordinate visualization at the front end and a R server back end for analysis and semi-supervised clustering of contig data.", "uri": "https://vimeo.com/103146392", "name": "Interactive Visual Support for Metagenomic Contig Binning", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:07:48+00:00", "description": "Authors: Isenberg Petra, Isenberg Tobias, Michael Sedlmair, Jian Chen, Torsten M\u00f6ller\n\nAbstract: We analyzed visualization paper keywords supplied for 4366 papers accepted to three main visualization conferences. We describe main keywords, topic areas, and 10-year historic trends from author-chosen keywords for papers published in the IEEE Visualization conference series (now called IEEE VIS) since 2004. Furthermore, we present the KeyVis Web application that allows visualization researchers to easily browse the 2600+ keywords used for IEEE VIS papers of the past 10 years, aiming at more informed and, hence, more effective keyword selections for future visualization publications and efficient search for related work.", "uri": "https://vimeo.com/103146391", "name": "Visualization that Bridges the Gap between Visual Communication", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:02:05+00:00", "description": "Authors: Jeremy Boy, Jean-Daniel Fekete\n\nAbstract: We present the design of the CO2 Pollution map, a web-based visualization that uses motion design techniques to assist users in interpreting semantic information about the data. We describe our motivations and inspirations, found in the fields of graphic and motion design, and present how free visual variables can be used to create an impression of what the data are about. We finish by reporting an early evaluation of the impact of the visualization, which we published in the form of a data journalism article.", "uri": "https://vimeo.com/103145836", "name": "The CO2 Pollution Map: Lessons Learned from Designing a Visualization that Bridges the Gap between Visual Communication and Info", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:02:05+00:00", "description": "Authors: Kostiantyn Kucher, Andreas Kerren\n\nAbstract: Text visualization has become a growing and increasingly important subfield of information visualization. Thus, it is getting harder for researchers to look for related work with specific tasks or visual metaphors in mind. In this poster, we present an interactive visual survey of text visualization techniques that can be used for the purposes of search for related work, introduction to the subfield and gaining insight into research trends.", "uri": "https://vimeo.com/103145835", "name": "Text Visualization Browser: A Visual Survey of Text Visualization Techniques", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:02:04+00:00", "description": "Authors: Amal Aboulhassan, Daniel Baum, Markus Hadwiger\n\nAbstract: Current characterization methods for the bulk heterojunction (BHJ) morphology \u201dthe main material of recent organic photovoltaic solar cells\u201d are limited to the analysis of global fabrication parameters. This reduces the efficiency of the BHJ design process, since it misses critical information about local performance indicators. Moreover, these approaches also do not help with intuitive analysis, which is widely adopted by the domain scientists. In this poster, we propose a novel application that fills this gap through visual characterization and exploration of one local performance indicator: charge transport bottlenecks. We propose a new intuition-based geometric model that correlates structural features with performance bottlenecks. Since our goal is to support the BHJ design, we assess the proposed model via its ability to extract correlations between features and bottlenecks. Moreover, we show that our approach can help reduce the BHJ analysis time from days to minutes.", "uri": "https://vimeo.com/103145831", "name": "Intuition-based Visual Modeling of Charges Transport Bottlenecks in Organic Photovoltaic Solar Cells", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:02:04+00:00", "description": "Authors: Martin J. Eppler, Sebastian Kernbach, Benjamin Wiederkehr, Peter Gassner\n\nAbstract: In this paper we propose an interactive knowledge visualization application that enables team members to integrate their knowledge about a project\u2019s parameters (such as budget, scope, staffing etc.) through a visual, interactive configurator that also captures dependencies among these factors visually. The novelty of our approach can be seen in the fact that the diagram encodes knowledge from the participants in the interaction constraints encoded by and later on imposed on the user by the diagram. The dependencies among key factors of the team are encoded as sliders that move together. These dependencies help the team members understand and align their action constraints and allow them to record their experiences for subsequent teams in the diagram, as well as track their on-going project constellation. First focus groups with typical users of the confluence diagram (project managers) show that this is a highly useful and useable property that other diagrams cannot offer as of today.", "uri": "https://vimeo.com/103145829", "name": "The Confluence Diagram: Embedding Knowledge in Interaction Constraints", "year": "2014", "event": ""}, {"created_time": "2014-08-11T16:02:03+00:00", "description": "Authors: Ankit Sharma, Armand Girier, Yeonsoo Yang, Nobuyasu Nakajima\n\nAbstract: We present a novel web browser based interactive visualization application for exploring 3D weather radar data sets delivered by next generation phased array radars. The purpose is to aid in rapid observation of localized weather phenomenon, short term weather forecasting and disaster prevention. Our application allows remote users to access multiple data sets via a web browser application, and the ability to view ray casted weather data volumes atop 3D maps from different viewpoints, animated in time. The interface allows updating intensity threshold and color scale range for comprehending 3D precipitation shape at different intensity levels, vertical cross-sectional views for viewing the internal structure and orthographic projection onto 3D map to clearly locate the affected areas. A demo collated using Osaka city torrential rain data set was presented to domain experts and received encouraging remarks on usability and performance.", "uri": "https://vimeo.com/103145827", "name": "Web based Volume Visualization of Weather Radar Data", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:46:31+00:00", "description": "Authors: Diana Fern\u00e1ndez Prieto, John Alejandro Triana, Juan Camilo Ibarra, Isabel Cristina Arteaga, Jose Tiberio Hernandez, Hans Hagen\n\nAbstract: Determining the location of a new public facility is one of the most critical and challenging tasks for local governments. To analyze the area of influence of a potential facility with respect to the distribution of existing facilities, the capacity of those existing facilities, and the demographic characteristics of the area of influence is of central significance in this planning process. We present an interactive radial visualization that allows planners to analyze the area of influence of the potential facility. The proposed visualization provides an intuitive way to identify the presence or absence of public facilities of different categories as well as it complements the traditional geographic view used by urban planners.", "uri": "https://vimeo.com/103144344", "name": "Radial Visualization for Geo-spatial Categorical Data", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:46:30+00:00", "description": "Authors: Nahid Ferdous, Radu Jianu\n\nAbstract: Changes in human pupil size are known to be correlated with task difficulty. Here we explore the opportunity of using eye tracking to measure task difficulty in the specific context of data visualization. In particular, we use a controlled eye-tracking study to investigate the difference between two types of task difficulty, mental difficulty and visual difficulty, explore the time frames at which pupil size responds to changes in task difficulty, and investigate if pupil size can provide qualitative hints as to which part of a task people find difficult. We found that eye tracker reveals mental difficulty more precisely than visual difficulty. We also found a set of patterns of pupil size changes that are related with mental activity and we show that using pupil size in conjunction with gaze coordinates lets us make inferences about user cognition that would not be possible if looking at gaze coordinates alone.", "uri": "https://vimeo.com/103144342", "name": "Using Pupil Size as an Indicator for Task Difficulty in Data Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:46:30+00:00", "description": "Authors: Wentao Wang, Jiawan Zhang\n\nAbstract: Sports are highly competitive, fast-paced, and teamwork-based. In this article, we introduce a novel approach in analyzing competitive sports based on music metaphor. Our proposed framework MatchVis extracts match information from raw webcast dataset about NBA and incorporates historical models into the investigation, providing a more compact and understandable visual representation of the details and patterns of match, which can consequently aid analysts in performing specific tasks and decision-making.", "uri": "https://vimeo.com/103144339", "name": "MatchVis: A generalized visual multi-scale analysis framework for competitive sports", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:46:29+00:00", "description": "Authors: Chris Kim, Christopher Collins\n\nAbstract: Based on a comprehensive word-color association lexicon, we present Lexichrome: a web-based application that offers its visitors an ability to browse a catalogue of visual relationships between words and colors. Additionally, the application seeks to allow literary scholars, brand managers, and writers to further examine a trend in the popular notion of word-color associations through user-provided texts.", "uri": "https://vimeo.com/103144338", "name": "Lexichrome: Examining Word-Color Associations with Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:46:29+00:00", "description": "Authors: Tanyoung Kim\n\nAbstract: Among many tracking and sensing applications (app) for smart phones, \u201cMoves\u201d\u009d automatically detects location changes and physical activities and enables manual editing of those entries. Harnessing the location data from Moves, we develop a visualization system called \u201cCommute\u201d\u009d that specifically focuses on a user\u2019s home and work places. After authorizing Commute to use her own data, the user can see her commuting patterns including departure/arrival times and duration. In addition to the visualization of location points of commutes, our system presents a visual analysis of the user\u2019s behaviors related to home and work places. Commute enables users to explore mined and analyzed data beyond the raw data from the original app and to discover previously unknown parts of their life style.", "uri": "https://vimeo.com/103144335", "name": "Visual Exploration of Personal Commuting Behaviors", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:39:35+00:00", "description": "Authors: Jay Koven, Enrico Bertini, Nasir Memon, R. Luke Dubois\n\nAbstract: Lawyers and investigators are often presented with large email datasets that contain emails that are not all relevant to any given investigative search. They must often manually comb through information contained within these large datasets in order to find the information they need, expending large amounts of time and money in the process. Our work offers an interactive visual analytic alternative to current methodology. We introduce a method for reducing the number of emails that need to be viewed in a large dataset while also giving the user a quick overview of possible contents and relationships in a set of results.", "uri": "https://vimeo.com/103143683", "name": "I-VEST: Intelligent Visual Email Search and Triage", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:39:34+00:00", "description": "Authors: Sayeed Safayet Alam, Radu Jianu\n\nAbstract: Interest in eye-tracking technology has proliferated in the visualization community in the last decade. Currently, gaze information is analyzed in the image space that gaze coordinates were recorded in, generally with the help of overlays such as heatmaps or scanpaths, or with the help of manually defined areas of interest (AOI). Such analyses require significant manual input and are not feasible for studies involving many subjects, long sessions, and heavily interactive visual stimuli. We propose Data of interest (DOI)-based analysis: a new way of analyzing data provided by eye-tracking devices. Unlike existing point-based and AOI analyses, which focus on the coordinates and areas of the stimuli that users viewed, DOI-based analysis leverages the known structure of an evaluated visualization to detect in real time which individual elements of the visualization are being viewed, and relates those to the base-data from which the visualization was built. We show that DOI-based analysis can be particularly useful for analyzing the use of highly interactive visualizations.", "uri": "https://vimeo.com/103143681", "name": "Data of Interest (DOI)-Based Analysis of Eye-Tracker Data for Interactive Visualizations", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:39:34+00:00", "description": "Authors: Baris Serim, Thanh Tung Vuong, Tuukka Ruotsalo, Luana Micallef, Giulio Jacucci\n\nAbstract: We present the design and implementation of mailVis, an interactive visual interface for email boxes that facilitates re-finding of emails. Email re-finding tasks can be challenging, involving scanning of many emails and modifying the query as the search progresses. We designed mailVis for such tasks in which the user would benefit from having memory clues and multiple options to direct the search. During the design process, we devised a novel interaction technique, filter swipe, that combines filtering and selection into one action for rapidly skimming individual items in a data set.", "uri": "https://vimeo.com/103143680", "name": "mailVis: Visualizing Emailbox for Re-finding Emails", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:39:33+00:00", "description": "Authors: Daeil Seo, Byounghyun Yoo, Heedong Ko\n\nAbstract: It is easy to get lost while browsing and navigating spatiotemporal content, and the problem is exacerbated by the exponential growth of geospatial data with mobile phones and tablets. In this paper, we propose a continuous method of transition visualization with zooming and panning interaction of the maps and timelines on touch-based devices that maintains the spatial and temporal context of spatiotemporal content.", "uri": "https://vimeo.com/103143679", "name": "Visual Interaction for Spatiotemporal Content using Zoom and Pan with Level-of-Detail", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:39:33+00:00", "description": "Authors: Daniel Zielasko, Benjamin Weyers, Bernd Hentschel, Torsten W. Kuhlen\n\nAbstract: Graphs play an important role in data analysis. Especially, graphs with a natural spatial embedding can benefit from a 3D visualization. But even more then in 2D, graphs visualized as intuitively readable 3D node-link diagrams can become very cluttered. This makes graph exploration and data analysis difficult. For this reason, we focus on the challenge of reducing edge clutter by utilizing edge bundling. In this paper we introduce a parallel, edge cluster based accelerator for the force-directed edge bundling algorithm presented in [Holten2009]. This opens up the possibility for user interaction during and after both the clustering and the bundling.", "uri": "https://vimeo.com/103143678", "name": "Interactive 3D Force-Directed Edge Bundling on Clustered Edges", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:31:53+00:00", "description": "Authors: Jonathan C. Roberts, Rick Walker, Lukas Roberts, Robert S. Laramee, Panagiotis D. Ritsos\n\nAbstract: Our goal is to help oceanographers to visualize and navigate their data over several runs. We have been using parallel coordinate plots to display every data value. Through our copy, cut, paste interactions we aim to enable users to drill-down into specific data points and to explore the datasets in a more expressive way. The method allows users to manipulate the PCP on a ZUI canvas, take copies of the current PCP and paste different subset views.", "uri": "https://vimeo.com/103142940", "name": "Exploratory Visualization through Copy, Cut and Paste", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:31:53+00:00", "description": "Authors: Roeland Scheepens, Steffen Michels, Huub van de Wetering, Jarke van Wijk\n\nAbstract: We present a method to visually explain the rationale of a reasoning engine that raises an alarm if a certain situation is reached. As a case study we look at the maritime safety and security domain. Based on evidence and a reasoning structure, the engine concludes with a certain probability that, e.g., the vessel is an environmental hazard. This engine is part of a larger safety and security system manned by an operator who makes decisions based on the output of the reasoning engine. To support decision making we visualize the rationale, an abstraction of the reasoning structure that allows users to understand why the conclusion has been reached, and display the evidence in a color-coded matrix that easily reveals if and where observations contradict.", "uri": "https://vimeo.com/103142939", "name": "Rationale Visualization for Decision Support", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:31:52+00:00", "description": "Authors: Kostiantyn Kucher, Andreas Kerren, Carita Paradis, Magnus Sahlgren\n\nAbstract: Stance in human communication is a linguistic concept relating to expressions of subjectivity such as the speakers' attitudes and emotions. Taking stance is crucial for the social construction of meaning and can be useful for many application fields such as business intelligence, security analytics, or social media monitoring. In order to process large amounts of text data for stance analyses, linguists need interactive tools to explore the textual sources as well as the results of computational linguistics techniques. Both aspects are important for refining the analyses iteratively. In this work, we present a visual analytics tool for online social media text data and corresponding time-series that can be used to investigate stance phenomena and to refine the so-called stance markers collection.", "uri": "https://vimeo.com/103142937", "name": "Visual Analysis of Stance Markers in Online Social Media", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:31:52+00:00", "description": "Authors: Bj\u00f6rn Zimmer, Andreas Kerren\n\nAbstract: The visual analysis of large and complex networks is a challenging task in many fields, such as systems biology or social sciences. Of- ten, various domain experts work together to improve the analysis time or the quality of the analysis results. Collaborative visualiza- tion tools can facilitate this process. We propose a new web-based visualization environment which supports distributed, synchronous and asynchronous collaboration for graphs with up to 10,000 nodes and edges. In addition to standard collaboration features like event tracking or synchronizing, our client/server-based system provides visualization and interaction techniques for better navigation, guid- ance and overview of the overall data set. During asynchronous col- laborations, network changes made by specific analysts or even just visited elements are highlighted on demand by heat maps. These heat map representations are user-sensitive in a sense that the cur- rent analyst is able to perceive which changes were made by others.", "uri": "https://vimeo.com/103142936", "name": "Applying Heat Maps in a Web-Based Collaborative Graph Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:31:51+00:00", "description": "Authors: Sara Johansson Fernstad, Robert C. Glen\n\nAbstract: Missing data are records that are absent from a data set. They are data that were intended to be recorded, but for some reason were not. Missing values are common in data analysis and occur in almost any domain, causing problems such as biased results and reduced statistical rigour. Visual analytics has great potential to provide invaluable support for the investigation of missing data. This poster aims to highlight the importance of analysing missing data and the challenges involved, as well as to emphasize the lack of visualization support in the area and through this encourage visualization scientists to discuss and address this highly relevant issue.", "uri": "https://vimeo.com/103142934", "name": "Visual Analysis of Missing Data - To See What Isn't There", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:29:25+00:00", "description": "Authors: Florence Ying Wang, Arnaud Sallaberry, Karsten Klein, Masahiro Takatsuka\n\nAbstract: Microblogs such as tweets contain useful information for sentiment analysis. In this work, we introduce SentimentClock for visualizing the sentiment of time-varying Twitter data on 2D affective space. To illustrate our visualization design, two case studies are conducted. They demonstrate the effectiveness of SentimentClock in visualizing the temporal sentiment variations of tweets and comparing the sentiment of tweets on different topics. A demo video of our system is available at: http://youtu.be/JvQFAFW-VbI", "uri": "https://vimeo.com/103142695", "name": "Visualizing Time-varying Twitter Data with SentimentClock", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:28:09+00:00", "description": "Authors: Alberto Debiasi, Bruno Sim\u00f5es, Raffaele De Amicis\n\nAbstract: Many geographical datasets can be depicted as a graph layered over a map for motion and spatial relations analyses. In this work we present a novel and real-time exploration system that interactively distorts 3D graph layouts without information loss. The 3D visualisation technique is camera dependent. Therefore, it is affected by all three navigation axes, in contrast to traditional interactions in 2D spaces.", "uri": "https://vimeo.com/103142567", "name": "3DArcLens: An Interactive Technique for the Exploration of Geographical Networks", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:22:33+00:00", "description": "Authors: Maxime Dumas, Michael J. McGuffin, Victoria L. Lemieux\n\nAbstract: We surveyed more than 50 papers that present visualizations of financial data, and made their references available on a website, http://financevis.net, that allows a user to browse thumbnails of the visualizations, and filter them by multiple criteria or perform a plain-text search. We discuss how we classified the visualizations, and outline ways to generate ideas for designing visualizations of financial data.", "uri": "https://vimeo.com/103141989", "name": "Financevis.net : A Visual Survey of Financial Data Visualizations", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:22:32+00:00", "description": "Authors: Brittany Kondo, Hrim Mehta, Christopher Collins\n\nAbstract: We present Glidgets, a combined direct manipulation and visualization technique for exploring and querying changes of elements in dynamic graphs. Traditional approaches provide an indirect time slider and employ visual cues such as global  \\ change highlighting.  Our work merges temporal navigation and the visual  \\ representation of graph dynamics into interactive visual glyphs on nodes and edges.  Our time line glyphs reveal the presence and absence of graph elements,  \\ and node degree. Using sketch-based interaction, the glyphs are used to create queries and navigate time directly on graph nodes and edges.  This enables one-stroke gestures to answer questions such as ``Are these nodes ever connected?'' or ``When is this node present in the network?''  Analysts can directly query changing graph elements and investigate those changes by navigating time, while focusing on the element.", "uri": "https://vimeo.com/103141983", "name": "Glidgets: Interactive Glyphs for Exploring Dynamic Graphs", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:22:31+00:00", "description": "Authors: Jimmy Johansson, Carlo Navarra, Tina Neset, Erik Glaas, Tomasz Opach, Bj\u00f6rn-Ola Linn\u00e9r\n\nAbstract: This poster presents the design and implementation of the web-based visual analytics tool VisAdapt which allows houseowners in the Nordic countries to assess potential climate related risk factors that may have an impact on their living conditions, and to get an overview of existing guidelines of how to adapt to climate change and extreme weather effects.", "uri": "https://vimeo.com/103141982", "name": "VisAdapt-Increasing Nordic Homeowners' Adaptive Capacity to Climate Change", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:15:14+00:00", "description": "Authors: Chihua Ma, Robert V. Kenyon, Tanya Berger-Wolf, Daniel A. Llano\n\nAbstract: Community structure analysis reveals the properties of social networks. The application of this analysis to evolving communities in dynamic networks can help understand how social interactions between individuals dynamically change and how the brain networks grow. Visualizing these communities becomes challenging since the time dimension is an important component in dynamic networks. The problem is still more complex as the biological neural networks have physical structures, and communities are used to explore functional groupings. Our interactive visualization technique is used for exploring the community structure of dynamic mouse brain correlation networks and behaviors of individual nodes in the networks through time.", "uri": "https://vimeo.com/103141284", "name": "Visualizing Communities in Dynamic Mouse Brain Networks", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:15:13+00:00", "description": "Authors: Nicolas Garcia Belmonte\n\nAbstract: During public addresses and debates (like the State of the Union Address) people engage in social networks generating thousands of messages. After the event happened, however, it is difficult to gather past conversations from those platforms and link them to the speech to relive the moment. To address this we developed a visualization technique that leverages the real-time conversational aspect of the Twitter platform to enrich the text of a public presentation with the discussion on Twitter during that speech; creating an annotated visual narrative of an event that can be replayed as the user scans the text of the speech.", "uri": "https://vimeo.com/103141283", "name": "Extracting and Visualizing Insights from Real-Time Conversations Around Public Presentations", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:15:13+00:00", "description": "Authors: Conglei Shi, Siwei Fu, Qing Chen, Huamin Qu\n\nAbstract: Massive Open Online Courses (MOOCs) are becoming increasingly popular and have attracted much research attention. Analyzing clickstreams on MOOC videos poses a special analytical challenge but provides a good opportunity for understanding how students interact with course videos, which in turn can help instructors and educational analysts gain insights into online learning behavior. In this poster,  we develop a visual analytical system, VisMOOC, to help instructors analyze the clickstream data. VisMOOC consists of three main views: the List View to list all course videos for analysts to select the video they are interested in; the Content-based View to show how each type of click actions change along the video timeline, which enables the most viewed sections to be observed and the most interesting patterns to be discovered; The Dashboard View shows the information of the clickstream data in different aspects, including the course information, the geographic distribution, the video temporal information, the video popularity, and the animation. Furthermore, case studies made by the instructors demonstrate the usefulness of VisMOOC and helped them gaining deep insights into learning behavior for MOOCs.", "uri": "https://vimeo.com/103141281", "name": "VisMOOC: Visualizing Video Clickstream Data from Massive Open Online Courses", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:15:12+00:00", "description": "Authors: Holger Stitz, Samuel Gratzl, Stefan Luger, Nils Gehlenborg, Marc Streit\n\nAbstract: Visualizing dynamic graphs is challenging because changing node and edge attributes as well as topological alterations need to be encoded in the visual representation. However, existing approaches such as animation, juxtaposition, and superimposition do not scale well. In this poster we propose a novel layering approach for visualizing dynamic graphs where the graph for each point in time is a single layer and parts of each layer are slightly shifted based on a degree-of-interest (DOI) function. In contrast to 2.5D representations that also use layering, users cannot freely change the viewing perspective but are restricted to the top view, avoiding occlusion and distortion problems. We demonstrate the layering approach by applying the concept to two graph visualizations: a node-link diagram and a radial hierarchy visualization.", "uri": "https://vimeo.com/103141279", "name": "Transparent Layering for Visualizing Dynamic Graphs Using the Flip Book Metaphor", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:15:12+00:00", "description": "Authors: Amy (Zhao) Yu, Kevin Hu, Deepak Jagdish, Cesar Hidalgo\n\nAbstract: We introduce Pantheon, a dataset and visualization platform quantifying cultural accomplishments that have broken the barriers of space, time and language. The Pantheon dataset connects the 11,340 biographies available in more than 25 languages in Wikipedia with a cultural domain, place of birth, and time period. We present this data through an online data visualization platform supporting the exploration of the dataset. In this poster we describe the Pantheon dataset and visualization platform, both of which are available at http://pantheon.media.mit.edu.", "uri": "https://vimeo.com/103141277", "name": "Pantheon: Visualizing Historical Cultural Production", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:09:16+00:00", "description": "Authors: Junpeng Wang, Fei Yang, Yong Cao\n\nAbstract: Most of the GPU-based ray casting algorithms map volume data to 3D texture of the GPU, so that the hardware-accelerated tri-linear interpolation could be taken advantage of. However, in hardware, texture cache is implemented in a 2D fashion. As a result, the viewing direction has a significant effect on the texture cache hit rate. This paper presents a new sampling strategy, i.e. warp marching, for the ray casting algorithm. The new strategy samples volume data in a cache-friendly manner and displays a novel computation-to-core mapping. We apply it to the existing iso-surface volume rendering algorithm and demonstrate significant performance enhancements in certain viewing directions.", "uri": "https://vimeo.com/103140757", "name": "Cache-Aware Iso-Surface Volume Rendering with CUDA", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:09:16+00:00", "description": "Authors: Bryan Bonnet, Keith Kraus, Jose Ramirez-Marquez\n\nAbstract: The purpose of this study is to provide a decision-making tool for visualizing various configurations of optimal placement of public-access Automated External Defibrillators (AEDs) deployed in an urban environment. Use of public-access AEDs has shown promising results in decreasing collapse-to-shock times among Sudden Cardiac Arrest (SCA) patients which is associated with improved patient outcomes. Prior studies have implemented mathematical optimization for placement of public-access AEDs. The novelty of this study lies in the implementation of an interactive tool which allows a decision-maker to change parameters and observe effects on coverage and cost. The approach is deployed and tested for the city of Hoboken, NJ.", "uri": "https://vimeo.com/103140756", "name": "Interactive Visualization for Optimal Placement of Public-Access AEDs", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:09:15+00:00", "description": "Authors: Semih \u00c7elik, Ulu\u011f Bayaz\u0131t\n\nAbstract: Visibility computations are commonly used in computer graphics applications. This paper presents a new view-dependent compression technique for 3D animated meshes. The approach consists of geometry coding of visible parts and connectivity coding of changes in visible regions. The proposed view-dependent predictive compression method yields significant improvement in compression performance over compression method without visibility awareness with gains up to 47 %.", "uri": "https://vimeo.com/103140755", "name": "View-Dependent Coding of 3D Mesh Sequences", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:09:15+00:00", "description": "Authors: Mershack Okoe, Radu Jianu\n\nAbstract: The process of evaluating visualizations can be time-consuming. Here, we present a design aimed at automating the process of performing quantitative and qualitative evaluations of graph visualizations by leveraging crowdsourcing, and a set of predefined evaluation modules based on a graph task taxonomy. Specifically, we allow designers to quickly set up a user study with representative graph tasks, measurable metrics, and evaluation methods. Our system then uses a thin-client architecture to automatically generate a web accessible user study from our desktop visualization, places the study on Mechanical Turk, and uses a statistical package to automatically process incoming results. To evaluate our system, we performed three concrete evaluation studies, all of which were configured and deployed in less than an hour. We discuss how our system can be used for automatic evaluations of interactive graph visualizations, how it can facilitate evaluation of alternative designs during iterative design processes, and how it could be used to find good default configurations for graph visualizations.", "uri": "https://vimeo.com/103140754", "name": "VisUnit: Evaluating Interactive graph Visualization Using CrowdSourcing", "year": "2014", "event": ""}, {"created_time": "2014-08-11T15:09:14+00:00", "description": "Authors: Erk Ediz Akyigit, Tugkan Cengiz, Onur Burak Yildirim, Selim Balcisoy\n\nAbstract: Storyline visualizations are useful to let people explore works of literature. It was first introduced in XKCD's Movie Narrative Charts as a hand-drawn illustration [1]. Tanahashi and Ma proposed some considerations on the design of the storyline visualization and automation became meaningful because of its aesthetic and clear representation style [2]. Shixia Liu and Yingcai Wu's StoryFlow had become the next step in storyline visualization [3]. With the efficient optimization approach on automation, representation of the complex stories became efficient and enabled users to track and understand the story easier. In all cases before generating the visualizations, an input file must be created manually. In this work we propose a semi-automatic exploratory visual tool for preparation of such input files.", "uri": "https://vimeo.com/103140752", "name": "Visual Exploratory Tool for Storyline Generation", "year": "2014", "event": ""}, {"created_time": "2014-08-07T07:02:17+00:00", "description": "Authors: Tuan Dang, Leland Wilkinson\n\nAbstract: Scagnostics (Scatterplot Diagnostics) were developed by Wilkinson et al., based on an idea of Paul and John Tukey, in order to discern meaningful patterns in large collections of scatterplots. The Tukeys' original idea was intended to overcome the impediments involved in examining large scatterplot matrices (multiplicity of plots and lack of detail). Wilkinson's implementation enabled for the first time scagnostics computations on many points as well as many plots. Unfortunately, scagnostics are sensitive to scale transformations. We illustrate the extent of this sensitivity and show how it is possible to pair statistical transformations with scagnostics to enable discovery of hidden structures in data that are not discernible in untransformed visualizations.", "uri": "https://vimeo.com/102809065", "name": "Transforming Scagnostics to Reveal Hidden Features", "year": "2014", "event": ""}, {"created_time": "2014-08-07T07:02:17+00:00", "description": "Authors: Eli T. Brown, Alvitta Ottley, Jieqiong Zhao, Quan Lin, Richard Souvenir, Alex Endert, Remco Chang\n\nAbstract: Visual analytics is inherently a collaboration between human and computer.  However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes.  While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user.  In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data.  Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data.  We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task.  Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism.  Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time.  Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.", "uri": "https://vimeo.com/102809064", "name": "Finding Waldo: Learning about Users from their Interactions", "year": "2014", "event": ""}, {"created_time": "2014-08-07T07:02:16+00:00", "description": "Authors: Charles D. Stolper, Adam Perer, David Gotz\n\nAbstract: As datasets grow and analytic algorithms become more complex, the typical workflow of analysts launching an analytic, waiting for it to complete, inspecting the results, and then re-launching the computation with adjusted parameters is not realistic for many real-world tasks. This paper presents an alternative workflow, progressive visual analytics, which enables an analyst to inspect partial results of an algorithm as they become available and interact with the algorithm to prioritize subspaces of interest. Progressive visual analytics depends on adapting analytical algorithms to produce meaningful partial results and enable analyst intervention without sacrificing computational speed. The paradigm also depends on adapting information visualization techniques to incorporate the constantly refining results without overwhelming analysts and provide interactions to support an analyst directing the analytic. The contributions of this paper include: a description of the progressive visual analytics paradigm; design goals for both the algorithms and visualizations in progressive visual analytics systems; an example progressive visual analytics system (Progressive Insights) for analyzing common patterns in a collection of event sequences; and an evaluation of Progressive Insights and the progressive visual analytics paradigm by clinical researchers analyzing electronic medical records.", "uri": "https://vimeo.com/102809062", "name": "Progressive Visual Analytics: User-Driven Visual Exploration of In-Progress Analytics", "year": "2014", "event": ""}, {"created_time": "2014-08-07T07:02:16+00:00", "description": "Authors: Kre\u0161imir Matkovi\u0107, Denis Gracanin, Rainer Splechtna, Mario Jelovic, Benedikt Stehno, Helwig Hauser, Werner Purgathofer\n\nAbstract: In this paper we propose a novel approach to hybrid visual steering of simulation ensembles. A simulation ensemble is a collection of simulation runs of the same simulation model using different sets of control parameters. Complex engineering systems have very large parameter spaces so a na\u2500\u2592ve sampling can result in prohibitively large simulation ensembles. Interactive steering of simulation ensembles provides the means to select relevant points in a multi-dimensional parameter space (design of experiment). Interactive steering efficiently reduces the number of simulation runs needed by coupling simulation and visualization and allowing a user to request new simulations on the fly. As system complexity grows, a pure interactive solution is not always sufficient. The new approach of hybrid steering combines interactive visual steering with automatic optimization. Hybrid steering allows a domain expert to interactively (in a visualization) select data points in an iterative manner, approximate the values in a continuous region of the simulation space (by regression) and automatically find the \"best\" points in this continuous region based on the specified constraints and objectives (by optimization). We argue that with the full spectrum of optimization options, the steering process can be improved substantially. We describe an integrated system consisting of a simulation, a visualization, and an optimization component. We also \\ describe typical tasks and propose an interactive analysis workflow for complex engineering systems. We demonstrate our approach on a case study from automotive industry, the optimization of a hydraulic circuit in a high pressure common rail Diesel injection system.", "uri": "https://vimeo.com/102809061", "name": "Visual Analytics for Complex Engineering Systems: Hybrid Visual Steering of Simulation Ensembles", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:44:17+00:00", "description": "Authors: Fanny Chevalier, Pierre Dragicevic, Steven Franconeri\n\nAbstract: Interactive visual applications often rely on animation to transition from one display state to another. There are multiple animation techniques to choose from, and it is not always clear which should produce the best visual correspondences between display elements. One major factor is whether the animation relies on staggering\u2014an incremental delay in start times across the moving elements. It has been suggested that staggering may reduce occlusion, while also reducing display complexity and producing less overwhelming animations, though no empirical evidence has demonstrated these advantages. Work in perceptual psychology does show that reducing occlusion, and reducing inter-object proximity (crowding) more generally, improves performance in multiple object tracking. We ran simulations confirming that staggering can in some cases reduce crowding in animated transitions involving dot clouds (as found in, e.g., animated 2D scatterplots). We empirically evaluated the effect of two staggering techniques on tracking tasks, focusing on cases that should most favour staggering. We found that introducing staggering has a negligible, or even negative, impact on multiple object tracking performance. The potential benefits of staggering may be outweighed by strong costs: a loss of common-motion grouping information about which objects travel in similar paths, and less predictability about when any specific object would begin to move. Staggering may be beneficial in some conditions, but they have yet to be demonstrated. The present results are a significant step toward a better understanding of animation pacing, and provide direction for further research.", "uri": "https://vimeo.com/102606411", "name": "The Not-so-Staggering Effect of Staggered Animated Transitions on Visual Tracking", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:44:17+00:00", "description": "Authors: Bahador Saket, Paolo Simonetto, Stephen Kobourov, Katy Borner\n\nAbstract: Effectively showing the relationships between objects in a dataset is one of the main tasks in information visualization. Typically there is a well-defined notion of distance between pairs of objects, and traditional approaches such as principal component analysis or multi-dimensional scaling are used to place the objects as points in 2D space, so that similar objects are close to each other. In another typical setting, the dataset is visualized as a network graph, where related nodes are connected by links. More recently, datasets are also visualized as maps, where in addition to nodes and links, there is an explicit representation of groups and clusters. We consider these three Techniques, characterized by a progressive increase of the amount of encoded information: node diagrams, node-link diagrams and node-link-group diagrams. We assess these three types of diagrams with a controlled experiment that covers nine different tasks falling broadly in three categories: node-based tasks, network-based tasks and group-based tasks. Our findings indicate that adding links, or links and group representations, does not negatively impact performance (time and accuracy) of node-based tasks. Similarly, adding group representations does not negatively impact the performance of network-based tasks. Node-link-group diagrams outperform the others on group-based tasks. These conclusions contradict results in other studies, in similar but subtly different settings. Taken together, however, such results can have significant implications for the design of standard and domain specific visualizations tools.", "uri": "https://vimeo.com/102606410", "name": "Node, Node-Link, and Node-Link-Group Diagrams: An Evaluation", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:44:17+00:00", "description": "Authors: Connor Gramazio, Karen Schloss, David Laidlaw\n\nAbstract: In this paper we make the following contributions: (1) we describe how the grouping, quantity, and size of visual marks affects search time based on the results from two experiments; (2) we report how search performance relates to self-reported difficulty in finding the target for different display types; and (3) we present design guidelines based on our findings to facilitate the design of effective visualizations. Both Experiment 1 and 2 asked participants to search for a unique target in colored visualizations to test how the grouping, quantity, and size of marks affects user performance. In Experiment 1, the target square was embedded in a grid of squares and in Experiment 2 the target was a point in a scatterplot. Search performance was faster when colors were spatially grouped than when they were randomly arranged. The quantity of marks had little effect on search time for grouped displays (\"pop-out\"), but increasing the quantity of marks slowed reaction time for random displays. Regardless of color layout (grouped vs. random), response times were slowest for the smallest mark size and decreased as mark size increased to a point, after which response times plateaued. In addition to these two experiments we also include potential application areas, as well as results from a small case study where we report preliminary findings that size may affect how users infer how visualizations should be used. We conclude with a list of design guidelines that focus on how to best create visualizations based on grouping, quantity, and size of visual marks.", "uri": "https://vimeo.com/102606409", "name": "The relation between visualization size, grouping, and user performance", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:40:11+00:00", "description": "Authors: Ramik Sadana, Timothy Major, Alistair Dove, John Stasko\n\nAbstract: Visualizing sets to reveal relationships between constituent elements is a complex representational problem. Recent research presents several automated placement and grouping techniques to highlight connections between set elements. However, these techniques do not scale well for sets with cardinality greater than one hundred elements. We present OnSet, an interactive, scalable visualization technique for representing large-scale binary set data. The visualization technique defines a single, combined domain of elements for all sets, and models each set by the elements that it both contains and does not contain. OnSet employs direct manipulation interaction and visual highlighting to support easy identification of commonalities and differences as well as membership patterns across different sets of elements. We present case studies to illustrate how the technique can be successfully applied across different domains such as bio-chemical metabolomics and task and event scheduling.", "uri": "https://vimeo.com/102606152", "name": "OnSet: A visualization technique for large-scale binary set data", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:40:09+00:00", "description": "Authors: Manuel Rubio-S\u00e1nchez, Alberto Sanchez\n\nAbstract: Star coordinates is a well-known multivariate visualization method that produces linear dimensionality reduction mappings through a set of radial axes defined by vectors in an observable space. One of its main drawbacks concerns the difficulty to recover attributes of data samples accurately, which typically lie in the [0,1] interval, given the locations of the low-dimensional embeddings and the vectors. In this paper we show that centering the data can considerably increase attribute estimation accuracy, where data values can be read off approximately by projecting embedded points onto calibrated (i.e., labeled) axes, similarly to classical statistical biplots. In addition, this idea can be coupled with a recently developed orthonormalization process on the axis vectors that prevents unnecessary distortions. We demonstrate that the combination of both approaches not only enhances the estimates, but also provides more faithful representations of the data.", "uri": "https://vimeo.com/102606150", "name": "Axis Calibration for Improving Data Attribute Estimation in Star Coordinates Plots", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:40:09+00:00", "description": "Authors: Rita Borgo, Joel Dearden, Mark W. Jones\n\nAbstract: In this paper we introduce Order of Magnitude Markers (OOMMs) as a new technique for number representation. The motivation for this work is that many data sets require the depiction and comparison of numbers that have varying orders of magnitude. Existing techniques for representation use bar charts, plots and colour on linear or logarithmic scales. These all suffer from related problems. There is a limit to the dynamic range available for plotting numbers, and so the required dynamic range of the plot can exceed that of the depiction method. When that occurs, resolving, comparing and relating values across the display becomes problematical or even impossible for the user. With this in mind, we present an empirical study in which we compare logarithmic, linear, scale-stack bars and our new markers for 11 different stimuli grouped into 4 different tasks across all 8 marker types", "uri": "https://vimeo.com/102606149", "name": "Order of Magnitude Markers: An Empirical Study on Large Magnitude Number Detection", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:33:53+00:00", "description": "Authors: Lane Harrison, Fumeng Yang, Steven Franconeri, Remco Chang\n\nAbstract: Despite years of research yielding systems and guidelines to aid visualization design, practitioners still face the challenge of identifying the best visualization for a given dataset and task. One promising approach to circumvent this problem is to leverage perceptual laws to quantitatively evaluate the effectiveness of a visualization design. Following previously established methodologies, we conduct a large scale (n=1687) crowdsourced experiment to investigate whether the perception of correlation in nine commonly used visualizations can be modeled using Weber's law. The results of this experiment contribute to our understanding of information visualization by establishing that: 1) for all tested visualizations, the precision of correlation judgment could be modeled by Weber's law, 2) correlation judgment precision showed striking variation between negatively and positively correlated data, and 3) Weber models provide a concise means to quantify, compare, and rank the perceptual precision afforded by a visualization.", "uri": "https://vimeo.com/102605825", "name": "Ranking Visualization of Correlation Using Weber\u2019s Law", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:33:51+00:00", "description": "Authors: Brittany Kondo, Christopher Collins\n\nAbstract: We introduce a new direct manipulation technique, DimpVis, for interacting with visual items in information visualizations to enable exploration of the time dimension. DimpVis is guided by visual hint paths which indicate how a selected data item changes through the time dimension in a visualization. Temporal navigation is controlled by manipulating any data item along its hint path. All other items are updated to reflect the new time. We demonstrate how the DimpVis technique can be designed to directly manipulate position, colour, and size in familiar visualizations such as bar charts and scatter plots, as a means for temporal navigation. We present results from a comparative evaluation, showing that the DimpVis technique was subjectively preferred and quantitatively competitive with the traditional time slider, and significantly faster than small multiples for a variety of tasks.", "uri": "https://vimeo.com/102605824", "name": "DimpVis: Exploring Time-varying Information Visualizations by Direct Manipulation", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:33:51+00:00", "description": "Authors: Cagatay Turkay, Aidan Slingsby, Helwig Hauser, Jo Wood, Jason Dykes\n\nAbstract: The visual analysis of geographically referenced datasets with a large number of attributes is challenging due to the fact that the characteristics of the attributes are highly dependent upon the locations at which they are focussed, and the scale and time at which they are measured. Specialized interactive visual methods are required to help analysts in understanding the characteristics of the attributes when these multiple aspects are considered concurrently. Here, we develop attribute signatures -- interactively crafted graphics that show the geographic variability of statistics of attributes through which the extent of dependency between the attributes and geography can be visually explored. We compute a number of statistical measures, which can also account for variations in time and scale, and use them as a basis for our visualizations. We then employ different graphical configurations to show and compare both continuous and discrete variation of location and scale. Our methods allow variation in multiple statistical summaries of multiple attributes to be considered concurrently and geographically, as evidenced by examples in which the census geography of London and the wider UK are explored.", "uri": "https://vimeo.com/102605823", "name": "Attribute Signatures: Dynamic Visual Summaries for Analyzing Multivariate Geographical Data", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:33:50+00:00", "description": "Authors: Johannes Fuchs, Petra Isenberg, Anastasia Bezerianos, Fabian Fischer, Enrico Bertini\n\nAbstract: We conducted three experiments to investigate the effects of contours on the detection of data similarity with star glyph variations. A star glyph is a small, compact, data graphic that represents a multi-dimensional data point. Star glyphs are often used in small-multiple settings, to represent data points in tables, on maps, or as overlays on other types of data graphics. In these settings, an important task is the visual comparison of the data points encoded in the star glyph, for example to find other similar data points or outliers.  We hypothesized that for data comparisons, the overall shape of a star glyph---enhanced through contour lines--- would aid the viewer in making accurate similarity judgments. To test this hypothesis, we conducted three experiments.  In our first experiment, we explored how the use of contours influenced how visualization experts and trained novices chose glyphs with similar data values. Our results showed that glyphs without contours make the detection of data similarity easier. Given these results, we conducted a second study to understand intuitive notions of similarity. Star glyphs without contours most intuitively supported the detection of data similarity. In a third experiment, we tested the effect of star glyph reference structures (i.e., tickmarks and gridlines) on the detection of similarity. Surprisingly, our results show that adding reference structures does improve the correctness of similarity judgments for star glyphs with contours, but not for the standard star glyph. As a result of these experiments, we conclude that the simple star glyph without contours performs best under several criteria, reinforcing its practice and popularity in the literature. Contours seem to enhance the detection of other types of similarity, e.g., shape similarity and are distracting when data similarity has to be judged. Based on these findings we provide design considerations regarding the use of contours and reference structures on star glyphs.", "uri": "https://vimeo.com/102605822", "name": "The Influence of Contour on Similarity Perception of Star Glyphs", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:28:13+00:00", "description": "Authors: Rudolf Netzel, Michael Burch, Daniel Weiskopf\n\nAbstract: We present the results of an eye tracking study that compares different visualization methods for long, dense, complex, and piecewise linear spatial trajectories. Typical sources of such data are from temporally discrete measurements of the positions of moving objects, for example, recorded GPS tracks of animals in movement ecology. In the repeated-measures within-subjects user study, four variants of node-link visualization techniques are compared, with the following representations of directed links: standard arrow, tapered, equidistant arrows, and equidistant comets. In addition, we investigate the effect of rendering order for the halo visualization of those links as well as the usefulness of node splatting. All combinations of link visualization techniques are tested for different trajectory density levels. We used three types of tasks: tracing of paths, identification of longest links, and estimation of the density of trajectory clusters. Results are presented in the form of the statistical evaluation of task completion time, task solution accuracy, and two eye tracking metrics. These objective results are complemented by a summary of subjective feedback from the participants. The main result of our study is that tapered links perform very well. However, we discuss that equidistant comets and equidistant arrows are a good option to perceive direction information independent of zoom-level of the display.", "uri": "https://vimeo.com/102605490", "name": "Comparative Eye Tracking Study on Node-Link Visualizations of Trajectories", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:28:12+00:00", "description": "Authors: Pascal Goffin, Wesley Willett, Jean-Daniel Fekete, Petra Isenberg\n\nAbstract: We present an exploration and a design space that characterize the usage and placement of word-scale visualizations within text documents. Word-scale visualizations are a more general version of sparklines\u2013small, word-sized data graphics that allow meta-information to be visually presented in-line with document text. In accordance with Edward Tufte's definition, sparklines are traditionally placed directly before or after words in the text. We describe alternative placements that permit a wider range of word-scale graphics and more flexible integration with text layouts. These alternative placements include positioning visualizations between lines, within additional vertical and horizontal space in the document, and as interactive overlays on top of the text. Each strategy changes the dimensions of the space available to display the visualizations, as well as the degree to which the text must be adjusted or reflowed to accommodate them. We provide an illustrated design space of placement options for word-scale visualizations and identify six important variables that control the placement of the graphics and the level of disruption of the source text. We also contribute a quantitative analysis that highlights the effect of different placements on readability and text disruption. Finally, we use this analysis to propose guidelines to support the design and placement of word-scale visualizations.", "uri": "https://vimeo.com/102605489", "name": "Exploring the Placement and Design of Word-Scale Visualizations", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:28:12+00:00", "description": "Authors: Michael Sedlmair, Christoph Heinzl, Harald Piringer, Stefan Bruckner, Torsten M\u00f6ller\n\nAbstract: Various case studies in different application domains have shown the great potential of visual parameter space analysis to support validating and using simulation models. In order to guide and systematize research endeavors in this area, we provide a conceptual framework for visual parameter space analysis problems. The framework is based on our own experience and a structured analysis of the visualization literature. It contains three major components: (1) a data flow model that helps to abstractly describe visual parameter space analysis problems independent of their application domain; (2) a set of four navigation strategies of how parameter space analysis can be supported by visualization tools; and (3) a characterization of six analysis tasks. Based on our framework, we analyze and classify the current body of literature, and identify three open research gaps in visual parameter space analysis. The framework and its discussion are meant to support visualization designers and researchers in characterizing parameter space analysis problems and to guide their design and evaluation processes.", "uri": "https://vimeo.com/102605488", "name": "Visual Parameter Space Analysis: A Conceptual Framework", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:28:10+00:00", "description": "Authors: Samuel Huron, Yvonne Jansen, Sheelagh Carpendale\n\nAbstract: The accessibility of infovis authoring tools to a wide audience has been identified as a major research challenge. A key task in the authoring process is the development of visual mappings. While the infovis community has long been deeply interested in finding effective visual mappings, comparatively little attention has been placed on how people construct visual mappings. In this paper we present the results of a study designed to shed light on how people spontaneously transform data into visual representations. We asked people to create, update and explain their own information visualizations using simple tangible building blocks. We learned that all participants, most of whom had no experience in visualization, were readily able to create and talk about their own visualizations. Based on our observations, we discuss the actions of our participants in the context of the development of their visual representations and their analytic activities and suggest implications for tool design to enable broader support for infovis authoring.", "uri": "https://vimeo.com/102605485", "name": "Constructing Visual Representations: Investigating the Use of Tangible Tokens", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:28:10+00:00", "description": "Authors: Charles D. Stolper, Minsuk Kahng, Zhiyuan Lin, Florian Foerster, Aakash Goel, John Stasko, Duen Horng Chau\n\nAbstract: The field of graph visualization has produced a wealth of visualization techniques for accomplishing a variety of analysis tasks. Therefore analysts often rely on a suite of different techniques, and visual graph analysis application builders strive to provide this breadth of techniques. To provide a holistic model for specifying network visualization techniques (as opposed to considering each technique in isolation) we present the Graph-Level Operations (GLO) model. We describe a method for identifying GLOs and apply it to identify five classes of GLOs, which can be flexibly combined to re-create six canonical graph visualization techniques. We discuss advantages of the GLO model, including potentially discovering new, effective network visualization techniques and easing the engineering challenges of building multi-technique graph visualization applications. Finally, we implement the GLOs that we identified into the GLO-STIX prototype system that enables an analyst to interactively explore a graph by applying GLOs.", "uri": "https://vimeo.com/102605484", "name": "GLO-STIX: Graph-Level Operations for Specifying Techniques and Interactive eXploration", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:18:29+00:00", "description": "Authors: Tom Polk, Jing Yang, Yueqi Hu, Ye Zhao\n\nAbstract: Existing research efforts into tennis visualization have primarily focused on using ball and player tracking data to enhance professional tennis broadcasts and to aid coaches in helping their students. Gathering and analyzing this data typically requires the use of an array of synchronized cameras, which are expensive for non-professional tennis matches. In this paper, we propose TenniVis, a novel tennis match visualization system that relies entirely on data that can be easily collected, such as score, point outcomes, point lengths, service information, and match videos that can be captured by one consumer-level camera. It provides two new visualizations to allow tennis coaches and players to quickly gain insights into match performance. It also provides rich interactions to support ad hoc hypothesis development and testing. We first demonstrate the usefulness of the system by analyzing the 2007 Australian Open men's singles final. We then validate its usability by two pilot user studies where two college tennis coaches analyzed the matches of their own players. The results indicate that useful insights can quickly be discovered and ad hoc hypotheses based on these insights can conveniently be tested through linked match videos.", "uri": "https://vimeo.com/102604996", "name": "TenniVis: Visualization for Tennis Match Analysis", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:18:29+00:00", "description": "Authors: Katherine Isaacs, Peer-Timo Bremer, Ilir Jusufi, Todd Gamblin, Abhinav Bhatele, Martin Schulz, Bernd Hamann\n\nAbstract: With the continuous rise in complexity of modern supercomputers, optimizing the performance of large-scale parallel programs is becoming increasingly challenging. Simultaneously, the growth in scale magnifies the impact of even minor inefficiencies \u2013 potentially millions of compute hours and megawatts in power consumption can be wasted on avoidable mistakes or sub-optimal algorithms. This makes performance analysis and optimization critical elements in the software development process. One of the most common forms of performance analysis is to study execution traces, which record a history of per-process events and inter-process messages in a parallel application. Trace visualizations allow users to browse this event history and search for insights into the observed performance behavior. However, current visualizations are difficult to understand even for small process counts and do not scale gracefully beyond a few hundred processes. Organizing events in time leads to a virtually unintelligible conglomerate of interleaved events and moderately high process counts overtax even the largest display. As an alternative, we present a new trace visualization approach based on transforming the event history into logical time inferred directly from happened-before relationships. This emphasizes the code's structural behavior, which is much more familiar to the application developer. The original timing data, or other information, is then encoded through color, leading to a more intuitive visualization. Furthermore, we use the discrete nature of logical timelines to cluster processes according to their local behavior leading to a scalable visualization of even long traces on large process counts. We demonstrate our system using two case studies on large-scale parallel codes.", "uri": "https://vimeo.com/102604994", "name": "Combing the Communication Hairball: Visualizing Large-Scale Parallel Execution Traces using Logical Time", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:18:28+00:00", "description": "Authors: Emanuel Zgraggen, Robert Zeleznik, Steven Drucker\n\nAbstract: Interactively exploring multidimensional datasets requires frequent switching among a range of distinct but inter-related tasks (e.g., producing different visuals based on different column sets, calculating new variables, and observing the interactions between sets of data). Existing approaches either target specific different problem domains (e.g., data-transformation or data-presentation) or expose only limited aspects of the general exploratory process; in either case, users are forced to adopt coping strategies (e.g., arranging windows or using undo as a mechanism for comparison instead of using side-by-side displays) to compensate for the lack of an integrated suite of exploratory tools. PanoramicData (PD) addresses these problems by unifying a comprehensive set of tools for visual data exploration into a hybrid pen and touch system designed to exploit the visualization advantages of large interactive displays. PD goes beyond just familiar visualizations by including direct UI support for data transformation and aggregation, filtering and brushing. Leveraging an unbounded whiteboard metaphor, users can combine these tools like building blocks to create detailed interactive visual display networks in which each visualization can act as a filter for others. Further, by operating directly on relational-databases, PD provides an approachable visual language that exposes a broad set of the expressive power of SQL, including functionally complete logic filtering, computation of aggregates and natural table joins. To understand the implications of this novel approach, we conducted a formative user study with both data and visualization experts. The results indicated that the system provided a fluid and natural user experience for probing multi-dimensional data and was able to cover the full range of queries that the users wanted to pose.", "uri": "https://vimeo.com/102604992", "name": "PanoramicData: Data Analysis through Pen & Touch", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:18:27+00:00", "description": "Authors: Juhee Bae, Benjamin Watson\n\nAbstract: In his book Multimedia Learning, Richard Mayer asserts that viewers learn best from imagery that provides them with cues to help them organize new information into the correct knowledge structures. Designers have long been exploiting the Gestalt laws of visual grouping to deliver viewers those cues using visual hierarchy, often communicating structures much more complex than the simple organizations studied in psychological research. Unfortunately, designers are largely practical in their work, and have not paused to build a complex theory of structural communication. If we are to build a tool to help novices create effective and well structured visuals, we need a better understanding of how to create them. Our work takes a first step toward addressing this lack, studying how five of the many grouping cues (proximity, color similarity, common region, connectivity, and alignment) can be effectively combined to communicate structured text and imagery from real world examples. To measure the effectiveness of this structural communication, we applied a digital version of card sorting, a method widely used in anthropology and cognitive science to extract cognitive structures. We then used tree edit distance to measure the difference between perceived and communicated structures. Our most significant findings are: 1) with careful design, complex structure can be communicated clearly; 2) communicating complex structure is best done with multiple reinforcing grouping cues; 3) common region (use of containers such as boxes) is particularly effective at communicating structure; and 4) alignment is a weak structural communicator.", "uri": "https://vimeo.com/102604991", "name": "Reinforcing Visual Grouping Cues to Communicate Complex Informational Structure", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:18:27+00:00", "description": "Authors: Anshul Vikram Pandey, Anjali Manivannan, Oded Nov, Margaret Satterthwaite, Enrico Bertini\n\nAbstract: Data visualization has been used extensively to inform users. However, little research has been done to examine the effects of data visualization in influencing users or in making a message more persuasive. In this study, we present experimental research to fill this gap and present an evidence-based analysis of persuasive visualization. We built on persuasion research from psychology and user interfaces literature in order to explore the persuasive effects of visualization. In this experimental study we define the circumstances under which data visualization can make a message more persuasive, propose hypotheses, and perform quantitative and qualitative analyses on studies conducted to test these hypotheses. We compare visual treatments with data presented through barcharts and linecharts on the one hand, treatments with data presented through tables on the other, and then evaluate their persuasiveness. The findings represent a first step in exploring the effectiveness of persuasive visualization.", "uri": "https://vimeo.com/102604990", "name": "The Persuasive Power of Data Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:09:22+00:00", "description": "Authors: Martijn Tennekes, Edwin de Jonge\n\nAbstract: We present a method to map tree structures to colors from the Hue-Chroma-Luminance color model, which is known for its well balanced perceptual properties. The Tree Colors method can be tuned with several parameters, whose effect on the resulting color schemes is discussed in detail. We provide a free and open source implementation with sensible parameter defaults. Categorical data are very common in statistical graphics, and often these categories form a classification tree. We evaluate applying Tree Colors to tree structured data with a survey on a large group of users from a national statistical institute. Our user study suggests that Tree Colors are useful, not only for improving node-link diagrams, but also for unveiling tree structure in non-hierarchical visualizations.", "uri": "https://vimeo.com/102604515", "name": "Tree Colors: Color Schemes for Tree-Structured Data", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:09:21+00:00", "description": "Authors: Gregorio Palmas, Myroslav Bachynskyi, Antti Oulasvirta, Hans-Peter Seidel, Tino Weinkauf\n\nAbstract: In Human-Computer Interaction (HCI), experts seek to evaluate and compare the performance and ergonomics of user interfaces. Recently, a novel cost-efficient method for estimating physical ergonomics and performance has been introduced to HCI. It is based on optical motion capture and biomechanical simulation. It provides a rich source for analyzing human movements summarized in a multidimensional data set. Existing visualization tools do not sufficiently support the HCI experts in analyzing this data. We identified two shortcomings. First, appropriate visual encodings are missing particularly for the biomechanical aspects of the data. Second, the physical setup of the user interface cannot be incorporated explicitly into existing tools. We present MovExp, a versatile visualization tool that supports the evaluation of user interfaces. In particular, it can be easily adapted by the HCI experts to include the physical setup that is being evaluated, and visualize the data on top of it. Furthermore, it provides a variety of visual encodings to communicate muscular loads, movement directions, and other specifics of HCI studies that employ motion capture and biomechanical simulation. In this design study, we follow a problem-driven research approach. Based on a formalization of the visualization needs and the data structure, we formulate technical requirements for the visualization tool and present novel solutions to the analysis needs of the HCI experts. We show the utility of our tool with four case studies from the daily work of our HCI experts.", "uri": "https://vimeo.com/102604514", "name": "MovExp: A Versatile Visualization Tool for Human-Computer Interaction Studies with 3D Performance and Biomechanical Data", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:09:20+00:00", "description": "Authors: Alexander Lex, Nils Gehlenborg, Hendrik Strobelt, Romain Vuillemot, Hanspeter Pfister\n\nAbstract: Understanding relationships between sets is an important analysis task that has received widespread attention in the visualization community. The major challenge in this context is the combinatorial explosion of the number of set intersections if the number of sets exceeds a trivial threshold. In this paper we introduce UpSet, a novel visualization technique for the quantitative analysis of sets, their intersections, and aggregates of intersections. UpSet is focused on creating task-driven aggregates, communicating the size and properties of aggregates and intersections, and a duality between the visualization of the elements in a dataset and their set membership. UpSet visualizes set intersections in a matrix layout and introduces aggregates based on groupings and queries. The matrix layout enables the effective representation of associated data, such as the number of elements in the aggregates and intersections, as well as additional summary statistics derived from subset or element attributes. Sorting according to various measures enables a task-driven analysis of relevant intersections and aggregates. The elements represented in the sets and their associated attributes are visualized in a separate view. Queries based on containment in specific intersections, aggregates or driven by attribute filters are propagated between both views. We also introduce several advanced visual encodings and interaction methods to overcome the problems of varying scales and to address scalability. UpSet is web-based and open source. We demonstrate its general utility in multiple use cases from various domains.", "uri": "https://vimeo.com/102604513", "name": "UpSet: Visualization of Intersecting Sets", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:09:19+00:00", "description": "Authors: Zhicheng Liu, Jeffrey Heer\n\nAbstract: To support effective exploration, it is often stated that interactive visualizations should provide rapid response times. However, the effects of interactive latency on the process and outcomes of exploratory visual analysis have not been systematically studied. We present an experiment measuring user behavior and knowledge discovery with interactive visualizations under varying latency conditions. We observe that an additional delay of 500ms incurs significant costs, decreasing user activity and data set coverage. Analyzing verbal data from think-aloud protocols, we find that increased latency reduces the rate at which users make observations, draw generalizations and generate hypotheses. Moreover, we note interaction effects in which initial exposure to higher latencies leads to subsequently reduced performance in a low-latency setting. Overall, increased latency causes users to shift exploration strategy, in turn affecting performance. We discuss how these results can inform the design of interactive analysis tools.", "uri": "https://vimeo.com/102604512", "name": "The Effects of Interactive Latency on Exploratory Visual Analysis", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:09:19+00:00", "description": "Authors: Jo Wood, Roger Beecham, Jason Dykes\n\nAbstract: We reflect on a four-year engagement with transport authorities and others involving a large dataset describing the use of a public bicycle-sharing scheme. We describe the role visualization of these data played in fostering engagement with policy makers, transport operators, the transport research community, the museum and gallery sector and the general public. We identify each of these as 'channels' \u2013 evolving relationships between producers and consumers of visualization - where traditional roles of the visualization expert and domain expert are blurred. In each case, we identify the different design decisions that were required to support each of these channels and the role played by the visualization process. Using chauffeured interaction with a flexible visual analytics system we demonstrate how insight was gained by policy makers into gendered spatio-temporal cycle behaviors, how this led to further insight into workplace commuting activity, group cycling behavior and explanations for street navigation choice. We demonstrate how this supported, and was supported by, the seemingly unrelated development of narrative-driven visualization via TEDx, of the creation and the setting of an art installation and the curating of digital and physical artefacts. We assert that existing models of visualization design, of tool/technique development and of insight generation do not adequately capture the richness of parallel engagement via these multiple channels of communication. We argue that developing multiple channels in parallel opens up opportunities for visualization design and analysis by building trust and authority and supporting creativity. This rich, non-sequential approach to visualization design is likely to foster serendipity, deepen insight and increase impact.", "uri": "https://vimeo.com/102604508", "name": "Moving beyond sequential design: Reflections on a rich multi-channel approach to data visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:04:13+00:00", "description": "Authors: Felipe S. L. G. Duarte, Fabio Sikansi, Francisco M. Fatore, Samuel G. Fadel, Fernando V. Paulovich\n\nAbstract: Space-filling techniques seek to use as much as possible the visual space to represent a dataset, splitting it into regions that represent the data elements. Amongst those techniques, Treemaps have received wide attention due to its simplicity, reduced visual complexity, and compact use of the available space. Several different Treemap algorithms have been proposed, however the core idea is the same, to divide the visual space into rectangles with areas proportional to some data attribute or weight. Although pleasant layouts can be effectively produced by the existing techniques, most of them do not take into account relationships that might exist between different data elements when partitioning the visual space. This violates the distance-similarity metaphor, that is, close rectangles do not necessarily represent similar data elements. In this paper, we propose a novel approach, called Neighborhood Treemap (Nmap), that seeks to solve this limitation by employing a slice and scale strategy where the visual space is successively bisected on the horizontal or vertical directions and the bisections are scaled until one rectangle is defined per data element. Compared to the current techniques with the same similarity preservation goal, our approach presents the best results while being two to three orders of magnitude faster. The usefulness of Nmap is shown by two applications involving the organization of document collections and the construction of cartograms, illustrating its effectiveness on different scenarios.", "uri": "https://vimeo.com/102604288", "name": "Nmap: A Novel Neighborhood Preservation Space-filling Algorithm", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:04:12+00:00", "description": "Authors: Jaemin Jo, Jaeseok Huh, Jonghun Park, Bohyoung Kim, Jinwook Seo\n\nAbstract: In this paper, we introduce LiveGantt as a novel interactive schedule visualization tool that helps users explore highly-concurrent large schedules from various perspectives. Although a Gantt chart is the most common approach to illustrate schedules, currently available Gantt chart visualization tools suffer from limited scalability and lack of interactions. LiveGantt is built with newly designed algorithms and interactions to improve conventional charts with better scalability, explorability, and reschedulability. It employs resource reordering and task aggregation to display the schedules in a scalable way. LiveGantt provides four coordinated views and filtering techniques to help users explore and interact with the schedules in more flexible ways. In addition, LiveGantt is equipped with an efficient rescheduler to allow users to instantaneously modify their schedules based on their scheduling experience in the fields. To assess the usefulness of the application of LiveGantt, we conducted a case study on manufacturing schedule data with four industrial engineering researchers. Participants not only grasped an overview of a schedule but also explored the schedule from multiple perspectives to make enhancements.", "uri": "https://vimeo.com/102604286", "name": "LiveGantt: Interactively Visualizing a Large Manufacturing Schedule", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:04:12+00:00", "description": "Authors: Weiwei Cui, Shixia Liu, Zhuofeng Wu, Hao Wei\n\nAbstract: Using a sequence of topic trees to organize documents is a popular way to represent hierarchical and evolving topics in text corpora. However, following evolving topics in the context of topic trees remains difficult for users. To address this issue, we present an interactive visual text analysis approach to allow users to progressively explore and analyze the complex evolution patterns of hierarchical topics. The key idea behind our approach is to exploit a tree cut in approximating each tree and allow users to interactively modify the tree cuts based on their interests. In particular, we propose an incremental evolutionary tree cut algorithm with the goal of balancing 1) the fitness of each tree cut and the smoothness between adjacent tree cuts; 2) the historical and the new information related to user interests. A time-based visualization is designed to illustrate the evolving topics over time. To preserve the mental map, we develop a stable layout algorithm. As a result, our approach can quickly guide users to progressively gain profound insights into evolving hierarchical topics. We evaluate the effectiveness of the proposed method on Amazon's Mechanical Turk and real-world news data. Results show that users are able to successfully analyze evolving topics in text data.", "uri": "https://vimeo.com/102604283", "name": "How Hierarchical Topics Evolve in Large Text Corpora", "year": "2014", "event": ""}, {"created_time": "2014-08-05T08:04:11+00:00", "description": "Authors: Ali Al-Awami, Johanna Beyer, Hendrik Strobelt, Narayanan Kasthuri, Jeff W. Lichtman, Hanspeter Pfister, Markus Hadwiger\n\nAbstract: We present NeuroLines, a novel visualization technique designed for scalable detailed analysis of neuronal connectivity at the nanoscale level. The topology of 3D brain tissue data is abstracted into a multi-scale, relative distance-preserving subway map visualization that allows domain scientists to conduct an interactive analysis of neurons and their connectivity. Nanoscale connectomics aims at reverse-engineering the wiring of the brain. Reconstructing and analyzing the detailed connectivity of neurons and neurites (axons, dendrites) will be crucial to understanding the brain and its development and diseases. However, the enormous scale and complexity of nanoscale neuronal connectivity pose big challenges to existing visualization techniques in terms of scalability. NeuroLines offers a scalable visualization framework that can interactively render thousands of neurites, and that supports the detailed analysis of neuronal structures and their connectivity. We describe and analyze the design of NeuroLines based on two real-world use-cases of our collaborators in developmental neuroscience, and investigate its scalability to large-scale neuronal connectivity data.", "uri": "https://vimeo.com/102604279", "name": "NeuroLines: A Subway Map Metaphor for Visualizing Nanoscale Neuronal Connectivity", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:55:57+00:00", "description": "Authors: Matthew Brehmer, Stephen Ingram, Jonathan Stray, Tamara Munzner\n\nAbstract: For an investigative journalist, a large collection of documents obtained from a Freedom of Information Act request or a leak is both a blessing and a curse: such material may contain multiple newsworthy stories, but it can be difficult and time consuming to find relevant documents. Standard text search is useful, but even if the search target is known it may not be possible to formulate an effective query. In addition, summarization is an important non-search task. We present Overview, an application for the systematic analysis of large document collections based on document clustering, visualization, and tagging. This work contributes to the small set of design studies which evaluate a visualization system \"in the wild\"\u009d, and we report on six case studies where Overview was voluntarily used by self-initiated journalists to produce published stories. We find that the frequently-used language of \"exploring\"\u009d a document collection is both too vague and too narrow to capture how journalists actually used our application. Our iterative process, including multiple rounds of deployment and observations of real world usage, led to a much more specific characterization of tasks. We analyze and justify the visual encoding and interaction techniques used in Overview's design with respect to our final task abstractions, and propose generalizable lessons for visualization design methodology.", "uri": "https://vimeo.com/102603842", "name": "Overview: The Design, Adoption, and Analysis of a Visual Document Mining Tool For Investigative Journalists", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:51:08+00:00", "description": "Authors: Donghao Ren, Tobias H\u00f6llerer, Xiaoru Yuan\n\nAbstract: We present the design, implementation and evaluation of iVisDesigner, a web-based system that enables users to design information visualizations for complex datasets interactively, without the need for textual programming. Our system achieves high interactive expressiveness through conceptual modularity, covering a broad information visualization design space. iVisDesigner supports the interactive design of interactive visualizations, such as provisioning for responsive graph layouts and different types of brushing and linking interactions. We present the system design and implementation, exemplify it through a variety of illustrative visualization designs and discuss its limitations. A performance analysis and an informal user study are presented to evaluate the system.", "uri": "https://vimeo.com/102603569", "name": "iVisDesigner: Expressive Interactive Design of Information Visualizations", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:51:07+00:00", "description": "Authors: Paul van der Corput, Jarke J. van Wijk\n\nAbstract: A common task in visualization is to quickly find interesting items in large sets. When appropriate metadata is missing, automatic queries are impossible and users have to inspect all elements visually. We compared two fundamentally different, but obvious display modes for this task and investigated the difference with respect to effectiveness, efficiency, and satisfaction. The static mode is based on the page metaphor and presents successive pages with a static grid of items. The moving mode is based on the conveyor belt metaphor and lets a grid of items slide though the screen in a continuous flow. In our evaluation, we applied both modes to the common task of browsing images. We performed two experiments where 18 participants had to search for certain target images in a large image collection. The number of shown images per second (pace) was predefined in the first experiment, and under user control in the second one. We conclude that at a fixed pace, the mode has no significant impact on the recall. The perceived pace is generally slower for moving mode, which causes users to systematically choose for a faster real pace than in static mode at the cost of recall, keeping the average number of target images found per second equal for both modes.", "uri": "https://vimeo.com/102603566", "name": "Effects of Presentation Mode and Pace Control on Performance in Image Classification", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:51:06+00:00", "description": "Authors: Stef van den Elzen, Jarke J. van Wijk\n\nAbstract: Network data is ubiquitous; e-mail traffic between persons, telecommunication, transport and financial networks are some examples. Often these networks are large and multivariate, besides the topological structure of the network, multivariate data on the nodes and links is available. Currently, exploration and analysis methods are focused on a single aspect; the network topology or the multivariate data. In addition, tools and techniques are highly domain specific and require expert knowledge. We focus on the non-expert user and propose a novel solution for multivariate network exploration and analysis that tightly couples structural and multivariate analysis. In short, we go from Detail to Overview via Selections and Aggregations (DOSA): users are enabled to gain insights through the creation of selections of interest (manually or automatically), and producing high-level, infographic-style overviews simultaneously. Finally, we present example explorations on real-world datasets that demonstrate the effectiveness of our method for the exploration and understanding of multivariate networks where presentation of findings comes for free.", "uri": "https://vimeo.com/102603562", "name": "Multivariate Network Exploration and Presentation: From Detail to Overview via Selections and Aggregations", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:51:05+00:00", "description": "Authors: Simon Stusak, Aur\u00e9lien Tabard, Franziska Sauka, Rohit Ashok Khot, Andreas Butz\n\nAbstract: Data sculptures are a promising type of visualizations in which data is given a physical form. In the past, they have mostly been used for artistic, communicative or educational purposes, and designers of data sculptures argue that in such situations, physical visualizations can be more enriching than pixel-based visualizations. We present the design of Activity Sculptures: data sculptures of running activity. In a three-week field study we investigated the impact of the sculptures on 14 participants' running activity, the personal and social behaviors generated by the sculptures, as well as participants' experiences when receiving these individual physical tokens generated from the specific data of their runs. The physical rewards generated curiosity and personal experimentation but also social dynamics such as discussion on runs or envy/competition. We argue that such passive (or calm) visualizations can complement nudging and other mechanisms of persuasion with a more playful and reflective look at ones' activity.", "uri": "https://vimeo.com/102603560", "name": "Activity Sculptures: Exploring the Impact of Physical Visualizations on Running Activity", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:49:05+00:00", "description": "Authors: Samuel Gratzl, Nils Gehlenborg, Alexander Lex, Hanspeter Pfister, Marc Streit\n\nAbstract: Answering questions about complex issues often requires analysts to take into account information contained in multiple interconnected datasets. A common strategy in analyzing and visualizing large and heterogeneous data is dividing it into meaningful subsets. Interesting subsets can then be selected and the associated data and the relationships between the subsets visualized. However, neither the extraction and manipulation nor the comparison of subsets is well supported by state-of-the-art techniques. In this paper we present Domino, a novel multiform visualization technique for effectively representing subsets and the relationships between them. By providing comprehensive tools to arrange, combine, and extract subsets, Domino allows users to create both common visualization techniques and advanced visualizations tailored to specific use cases. In addition to the novel technique, we present an implementation that enables analysts to manage the wide range of options that our approach offers. Innovative interactive features such as placeholders and live previews support rapid creation of complex analysis setups. We introduce the technique and the implementation using a simple example and demonstrate scalability and effectiveness in a use case from the field of cancer genomics.", "uri": "https://vimeo.com/102603456", "name": "Domino: Extracting, Comparing, and Manipulating Subsets across Multiple Tabular Datasets", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:49:04+00:00", "description": "Authors: Charles Perin, Pierre Dragicevic, Jean-Daniel Fekete\n\nAbstract: We present BERTIFIER, a web app for rapidly creating tabular visualizations from spreadsheets. BERTIFIER draws from Jacques Bertin's matrix analysis method, whose goal was to \"simplify without destroying\" by encoding cell values visually and grouping similar rows and columns. Although there were several attempts to bring this method to computers, no implementation exists today that is both exhaustive and accessible to a large audience. BERTIFIER remains faithful to Bertin's method while leveraging the power of today's interactive computers. Tables are formatted and manipulated through crossets, a new interaction technique for rapidly applying operations on rows and columns. We also introduce visual reordering, a semi-interactive reordering approach that lets users apply and tune automatic reordering algorithms in a WYSIWYG manner. Sessions with eight users from different backgrounds suggest that BERTIFIER has the potential to bring Bertin's method to a wider audience of both technical and non-technical users, and empower them with data analysis and communication tools that were so far only accessible to a handful of specialists.Sessions with eight users from different backgrounds suggest that BERTIFIER has the potential to bring Bertin's method to a wider audience of both technical and non-technical users, and empower them with data analysis and communication tools that were so far only accessible to a handful of specialists.", "uri": "https://vimeo.com/102603454", "name": "Revisiting Bertin matrices: New Interactions for Crafting Tabular Visualizations", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:49:04+00:00", "description": "Authors: Sean McKenna, Dominika Mazur, James Agutter, Miriah Meyer\n\nAbstract: An important aspect in visualization design is the connection between what a designer does and the decisions the designer makes. Existing design process models, however, do not explicitly link back to models for visualization design decisions. We bridge this gap by introducing the design activity framework, a process model that explicitly connects to the nested model, a well-known visualization design decision model. The framework includes four overlapping activities that characterize the design process, with each activity explicating outcomes related to the nested model. Additionally, we describe and characterize a list of exemplar methods and how they overlap among these activities. The design activity framework is the result of reflective discussions from a collaboration on a visualization redesign project, the details of which we describe to ground the framework in a real-world design process. Lastly, from this redesign project we provide several research outcomes in the domain of cybersecurity, including an extended data abstraction and rich opportunities for future visualization research.", "uri": "https://vimeo.com/102603453", "name": "Design Activity Framework for Visualization Design", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:32:36+00:00", "description": "Authors: Krist Wongsuphasawat, Jimmy Lin\n\nAbstract: Logging user activities is essential to data analysis for internet products and services. Twitter has built a unified logging infrastructure that captures user activities across all clients it owns, making it one of the largest datasets in the organization. This paper describes challenges and opportunities in applying information visualization to log analysis at this massive scale, and shows how various visualization techniques can be adapted to help data scientists extract insights. In particular, we focus on two scenarios: (1) monitoring and exploring a large collection of log events, and (2) performing visual funnel analysis on log data with tens of thousands of event types. Two interactive visualizations were developed for these purposes: we discuss design choices and the implementation of these systems, along with case studies of how they are being used in day-to-day operations at Twitter.", "uri": "https://vimeo.com/102602582", "name": "Using Visualizations to Monitor Changes and Harvest Insights from a Global-Scale Logging Infrastructure at Twitter", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:22:55+00:00", "description": "Authors: Carlos Dietrich, David Koop, Huy Vo, Claudio Silva\n\nAbstract: While many sports use statistics and video to analyze and improve game play, baseball has led the charge throughout its history. With the advent of new technologies that allow all players and the ball to be tracked across the entire field, it is now possible to bring this understanding to another level. From discrete positions across time, we present techniques to reconstruct entire baseball games and visually explore each play. This provides opportunities to not only derive new metrics for the game, but also allow us to investigate existing measures with targeted visualizations. In addition, our techniques allow users to filter on demand so specific situations can be analyzed both in general and according to those situations. We show that gameplay can be accurately reconstructed from the raw position data and discuss how visualization and statistical methods can combine to better inform baseball analyses.", "uri": "https://vimeo.com/102602089", "name": "Baseball4D:  A Tool for Baseball Game Reconstruction & Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:22:55+00:00", "description": "Authors: Artem Konev, J\u00fcrgen Waser, Bernhard Sadransky, Daniel Cornel, Rui A. P. Perdig\u00e3o, Zsolt Horv\u00e1th, Eduard Gr\u00f6ller\n\nAbstract: In this paper, we introduce a simulation-based approach to design protection plans for flood events. Existing solutions require a lot of computation time for an exhaustive search, or demand for a time-consuming expert supervision and steering. We present a faster alternative based on the automated control of multiple parallel simulation runs. Run Watchers are dedicated system components authorized to monitor simulation runs, terminate them, and start new runs originating from existing ones according to domain-specific rules. This approach allows for a more efficient traversal of the search space and overall performance improvements due to a re-use of simulated states and early termination of failed runs. In the course of search, Run Watchers generate large and complex decision trees. We visualize the entire set of decisions made by Run Watchers using interactive, clustered timelines. In addition, we present visualizations to explain the resulting response plans. Run Watchers automatically generate storyboards to convey plan details and to justify the underlying decisions, including those which leave particular buildings unprotected. We evaluate our solution with domain experts.", "uri": "https://vimeo.com/102602088", "name": "Run Watchers: Automatic Simulation-Based Decision Support in Flood Management", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:22:53+00:00", "description": "Authors: Eric Alexander, Joe Kohlmann, Michael Witmore, Robin Valenza, Michael Gleicher\n\nAbstract: Exploration and discovery in a large text corpus requires investigation at multiple levels of abstraction, from a zoomed-out view of the entire corpus down to close-ups of individual passages and words. At each of these levels, there is a wealth of information that can inform inquiry---from statistical models, to metadata, to the researcher's own knowledge and expertise. Joining all this information together can be a challenge, and there are issues of scale to be combatted along the way. In this paper, we describe an approach to text analysis that addresses these challenges of scale and multiple information sources, using probabilistic topic models to structure exploration through multiple levels of inquiry in a way that fosters serendipitous discovery. In implementing this approach into a tool called Serendip, we incorporate topic model data and metadata into a highly reorderable matrix to expose corpus level trends; extend encodings of tagged text to illustrate probabilistic information at a passage level; and introduce a technique for visualizing individual word rankings, along with interaction techniques and new statistical methods to create links between different levels and information types. We describe example uses from both the humanities and visualization research that illustrate the benefits of our approach.", "uri": "https://vimeo.com/102602086", "name": "Serendip: Topic Model-Driven Visual Exploration of Text Corpora", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:22:53+00:00", "description": "Authors: Jorge Poco, Aritra Dasgupta, Yaxing Wei, William Hargrove, Christopher Schwalm, Deborah Huntzinger, Robert Cook, Enrico Bertini, Claudio Silva\n\nAbstract: Visual data analysis often requires grouping of data objects based on their similarity. In many application domains researchers use algorithms and techniques like clustering and multidimensional scaling to extract groupings from data. While extracting these groups using a single similarity criteria is relatively straightforward, comparing alternative criteria poses additional challenges. In this paper we define visual reconciliation as the problem of reconciling multiple alternative similarity spaces through visualization and interaction. We derive this problem from our work on model comparison in climate science where climate modelers are faced with the challenge of making sense of alternative ways to describe their models: one through the output they generate, another through the large set of properties that describe them. Ideally, they want to understand whether groups of models with similar spatio-temporal behaviors share similar sets of criteria or, conversely, whether similar criteria lead to similar behaviors. We propose a visual analytics solution based on linked views, that addresses this problem by allowing the user to dynamically create, modify and observe the interaction among groupings, thereby making the potential explanations apparent. We present case studies that demonstrate the usefulness of our technique in the area of climate science.", "uri": "https://vimeo.com/102602085", "name": "Visual Reconciliation of Alternative Similarity Spaces in Climate Modeling", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:20:54+00:00", "description": "Authors: Patrick K\u00f6thur, Mike Sips, Henryk Dobslaw, Doris Dransch\n\nAbstract: Researchers assess the quality of an ocean model by comparing its output to that of a previous model version or to observations. One objective of the comparison is to detect and to analyze differences and similarities between both data sets regarding geophysical processes, such as particular ocean currents. This task involves the analysis of thousands or hundreds of thousands of geographically referenced temporal profiles in the data. To cope with the amount of data, modelers combine aggregation of temporal profiles to single statistical values with visual comparison. Although this strategy is based on experience and a well-grounded body of expert knowledge, our discussions with domain experts have shown that it has two limitations: (1) using a single statistical measure results in a rather limited scope of the comparison and in significant loss of information, and (2) the decisions modelers have to make in the process may lead to important aspects being overlooked. \\ In this article, we propose a Visual Analytics approach that broadens the scope of the analysis, reduces subjectivity, and facilitates comparison of the two data sets. It comprises three steps: First, it allows modelers to consider many aspects of the temporal behavior of geophysical processes by conducting multiple clusterings of the temporal profiles in each data set. Modelers can choose different features describing the temporal behavior of relevant processes, clustering algorithms, and parameterizations. Second, our approach consolidates the clusterings of one data set into a single clustering via a clustering ensembles approach. The consolidated clustering presents an overview of the geospatial distribution of temporal behavior in a data set. Third, a visual interface allows modelers to compare the two consolidated clusterings. It enables them to detect clusters of temporal profiles that represent geophysical processes and to analyze differences and similarities between two data sets. \\ This work is the result of a close collaboration with ocean modelers. They employed our concept to find aspects of improvement in a new version of the Ocean Model for Circulation and Tides (OMCT).", "uri": "https://vimeo.com/102601981", "name": "Visual Analytics for Comparison of Ocean Model Output with Reference Data: Detecting and Analyzing Geophysical Processes Using C", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:13:33+00:00", "description": "Authors: Josua Krause, Adam Perer, Enrico Bertini\n\nAbstract: Predictive modeling techniques are increasingly being used by data scientists to understand the probability of predicted outcomes. However, for data that is high-dimensional, a critical step in predictive modeling is determining which features should be included in the models. Feature selection algorithms are often used to remove non-informative features from models. However, there are many different classes of feature selection algorithms. Deciding which one to use is problematic as the algorithmic output is often not amenable to user interpretation. This limits the ability for users to utilize their domain expertise during the modeling process. To improve on this limitation, we developed INFUSE, a novel visual analytics system designed to help analysts understand how predictive features are being ranked across feature selection algorithms, cross-validation folds, and classifiers. We demonstrate how our system can lead to important insights in a case study involving clinical researchers predicting patient outcomes from electronic medical records.", "uri": "https://vimeo.com/102601579", "name": "INFUSE: Interactive Feature Selection for Predictive Modeling of High Dimensional Data", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:13:33+00:00", "description": "Authors: Johanna Schmidt, Reinhold Preiner, Thomas Auzinger, Michael Wimmer, Eduard Gr\u00f6ller, Stefan Bruckner\n\nAbstract: Polygonal meshes can be created in several different ways. In this paper we focus on the reconstruction of meshes from point clouds, which are sets of points in 3D. Several algorithms that tackle this task already exist, but they have different benefits and drawbacks, which leads to a large number of possible reconstruction results (i.e., meshes). The evaluation of those techniques requires extensive comparisons between different meshes which is up to now done by either placing images of rendered meshes side-by-side, or by encoding differences by heat maps. A major drawback of both approaches is that they do not scale well with the number of meshes. This paper introduces a new comparative visual analysis technique for 3D meshes which enables the simultaneous comparison of several meshes and allows for the interactive exploration of their differences. Our approach gives an overview of the differences of the input meshes in a 2D view. By selecting certain areas of interest, the user can switch to a 3D representation and explore the spatial differences in detail. To inspect local variations, we provide a magic lens tool in 3D. The location and size of the lens provide further information on the variations of the reconstructions in the selected area. With our comparative visualization approach, differences between several mesh reconstruction algorithms can be easily localized and inspected.", "uri": "https://vimeo.com/102601578", "name": "[VAST 2014] YMCA - Your Mesh Comparison Application", "year": "2014", "event": "VAST"}, {"created_time": "2014-08-05T07:13:32+00:00", "description": "Authors: Zuchao Wang, Tangzhi Ye, Min Lu, Xiaoru Yuan, Huamin Qu, Jacky Yuan, Qianliang Wu\n\nAbstract: In this paper, we present a visual analysis system to explore sparse traffic trajectory data recorded by transportation cells. Such data contains the movements of nearly all moving vehicles on the major roads of a city. Therefore it is very suitable for macro-traffic analysis. However, the vehicle movements are recorded only when they pass through the cells. The exact tracks between two consecutive cells are unknown. To deal with such uncertainties, we first design a local animation, showing the vehicle movements only in the vicinity of cells. Besides, we ignore the micro-behaviors of individual vehicles, and focus on the macro-traffic patterns. We apply existing trajectory aggregation techniques to the dataset, studying cell status pattern and inter-cell flow pattern. Beyond that, we propose to study the correlation between these two patterns with dynamic graph visualization techniques. It allows us to check how traffic congestion on one cell is correlated with traffic flows on neighbouring links, and with route selection in its neighbourhood. Case studies show the effectiveness of our system.", "uri": "https://vimeo.com/102601577", "name": "Visual Exploration of Sparse Traffic Trajectory Data", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:13:32+00:00", "description": "Authors: Christian Partl, Alexander Lex, Marc Streit, Hendrik Strobelt, Anne-Mai Wassermann, Hanspeter Pfister, Dieter Schmalstieg\n\nAbstract: Large scale data analysis is nowadays a crucial part of drug discovery. Biologists and chemists need to quickly explore and evaluate potentially effective yet safe compounds based on many datasets that are in relationship with each other. However, there is a lack of tools that support them in these processes. To remedy this, we developed ConTour, an interactive visual analytics technique that enables the exploration of these complex, multi-relational datasets. At its core ConTour lists all items of each dataset in a column. Relationships between the columns are revealed through interaction: selecting one or multiple items in one column highlights and re-sorts the items in other columns. Filters based on relationships enable drilling down into the large data space. To identify interesting items in the first place, ConTour employs advanced sorting strategies, including strategies based on connectivity strength and uniqueness, as well as sorting based on item attributes. ConTour also introduces interactive nesting of columns, a powerful method to show the related items of a child column for each item in the parent column. Within the columns, ConTour shows rich attribute data about the items as well as information about the connection strengths to other datasets. Finally, ConTour provides a number of detail views, which can show items from multiple datasets and their associated data at the same time. We demonstrate the utility of our system in case studies conducted with a team of chemical biologists, who investigate the effects of chemical compounds on cells and need to understand the underlying mechanisms.", "uri": "https://vimeo.com/102601576", "name": "ConTour: Data-Driven Exploration of Multi-Relational Datasets for Drug Discovery", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:08:13+00:00", "description": "Authors: Bowen Yu, Harish Doraiswamy, Xi Chen, Emily Miraldi, Mario Luis Arrieta-Ortiz, Christoph Hafemeister, Aviv Madar, Richard Bonneau, Claudio Silva\n\nAbstract: Elucidation of transcriptional regulatory networks (TRNs) is a fundamental goal in biology, and one of the most important components of TRNs are transcription factors (TFs), proteins that specifically bind to gene promoter and enhancer regions to alter target gene expression patterns. Advances in genomic technologies as well as advances in computational biology have led to multiple large regulatory network models (directed networks) each with a large corpus of supporting data and gene-annotation. There are multiple possible biological motivations for exploring large regulatory network models, including: validating TF-target gene relationships, figuring out co-regulation patterns, and exploring the coordination of cell processes in response to changes in cell state or environment. Here we focus on queries aimed at validating regulatory network models, and on coordinating visualization of primary data and directed weighted gene regulatory networks. The large size of both the network models and the primary data can make such coordinated queries cumbersome with existing tools and, in particular, inhibits the sharing of results between collaborators. In this work, we develop and demonstrate a web-based framework for coordinating visualization and exploration of expression data (RNA-seq, microarray), network models and gene-binding data (ChIP-seq). Using specialized data structures and multiple coordinated views, we design an efficient querying model to support interactive analysis of the data. Finally, we show the effectiveness of our framework through case studies for the mouse immune system (a dataset focused on a subset of key cellular functions) and a model bacteria (a small genome with high data-completeness). \\", "uri": "https://vimeo.com/102601345", "name": "Genotet: An Interactive Web-based Visual Exploration Framework to Support Validation of Gene Regulatory Networks", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:08:13+00:00", "description": "Authors: Krishna Madhavan, Niklas Elmqvist, Mihaela Vorvoreanu, Xin Chen, Yuet Ling Wong, Hanjun Xian, Zhihua Dong, Aditya Johri\n\nAbstract: We present a design study of the Deep Insights Anywhere, Anytime (DIA2) platform, a web-based visual analytics system that allows program managers and academic staff at the U.S. National Science Foundation to search, view, and analyze their research funding portfolio. The goal of this system is to facilitate users's understanding of both past and currently active research awards in order to make more informed decisions of their future funding. This user group is characterized by high domain expertise yet not necessarily high literacy in visualization and visual analytics\u2014they are essentially casual experts\u2014and thus require careful visual and information design, including adhering to user experience standards, providing a self-instructive interface, and progressively refining visualizations to minimize complexity. We discuss the challenges of designing a system for casual experts and highlight how we addressed this issue by modeling the organizational structure and workflows of the NSF within our system. We discuss each stage of the design process, starting with formative interviews, prototypes, and finally live deployments and evaluation with stakeholders.", "uri": "https://vimeo.com/102601344", "name": "DIA2: Web-based Cyberinfrastructure for Visual Analysis of Funding Portfolio", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:08:10+00:00", "description": "Authors: Jian Zhao, Nan Cao, Zhen Wen, Yale Song, Yu-Ru Lin, Christopher Collins\n\nAbstract: We present FluxFlow, an interactive visual analysis system for revealing and analyzing anomalous information spreading in social media. Everyday, millions of messages are created, commented, and shared by people on social media websites, such as Twitter and Facebook. This provides valuable data for researchers and practitioners in many application domains, such as marketing, to inform decision-making. Distilling valuable social signals from the huge crowd's messages, however, is challenging, due to the heterogeneous and dynamic crowd behaviors. The challenge is rooted in data analysts' capability of discerning the anomalous information behaviors, such as the spreading of rumors or misinformation, from the rest that are more conventional patterns, such as popular topics and newsworthy events, in a timely fashion. FluxFlow incorporates advanced machine learning algorithms to detect anomalies, and offers a set of novel visualization designs for presenting the detected threads for deeper analysis. We evaluated FluxFlow with real datasets containing the Twitter feeds captured during significant events such as Hurricane Sandy. Through quantitative measurements of the algorithmic performance and qualitative interviews with domain experts, the results show that the back-end anomaly detection model is effective in identifying anomalous retweeting threads, and its front-end interactive visualizations are intuitive and useful for analysts to discover insights in data and comprehend the underlying analytical model.", "uri": "https://vimeo.com/102601337", "name": "#FluxFlow: Visual Analysis of Anomalous Information Spreading on Social Media", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:06:46+00:00", "description": "Authors: Wenchao Wu, Yixian Zheng, Huamin Qu, Wei Chen, Eduard Gr\u00f6ller, Lionel Ni\n\nAbstract: Boundary changes exist ubiquitously in our daily life. From the Antarctic ozone hole to the land desertification, and from the territory of a country to the area within one-hour reach from a downtown location, boundaries change over time. With a large number of time-varying boundaries recorded, people often need to analyze the changes, detect their similarities or differences, and find out spatial and temporal patterns of the evolution for various applications. In this paper, we present a comprehensive visual analytics system, BoundarySeer, to help users gain insight into the changes of boundaries. Our system consists of four major viewers: 1) a global viewer to show boundary groups based on their similarity and the distribution of boundary attributes such as smoothness and perimeter; 2) a region viewer to display the regions encircled by the boundaries and how they are affected by boundary changes; 3) a trend viewer to reveal the temporal patterns in the boundary evolution and potential spatio-temporal correlations; 4) a directional change viewer to encode movements of boundary segments in different directions. Quantitative analyses of boundaries (e.g., similarity measurement and adaptive clustering) and intuitive visualizations (e.g., density map and ThemeRiver) are integrated into these viewers, which enable users to explore boundary changes from different aspects and at different scales. Case studies with two real-world datasets have been carried out to demonstrate the effectiveness of our system.", "uri": "https://vimeo.com/102601291", "name": "BoundarySeer: Visual Analysis of 2D Boundary Changes", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:00:56+00:00", "description": "Authors: Conglei Shi, Yingcai Wu, Shixia Liu, Hong Zhou, Huamin Qu\n\nAbstract: The huge amount of user log data collected by search engine providers creates new opportunities to understand user loyalty and defection behavior at an unprecedented scale. However, this also poses a great challenge to analyze the behavior and glean insights into the complex, large data. In this paper, we introduce LoyalTracker, a visual analytics system to track user loyalty and switching behavior towards multiple search engines from the vast amount of user log data. We propose a new interactive visualization technique (flow view) based on a flow metaphor, which conveys a proper visual summary of the dynamics of user loyalty of thousands of users over time. Two other visualization techniques, a density map and a word cloud, are integrated to enable analysts to gain further insights into the patterns identified by the flow view. Case studies and the interview with domain experts are conducted to demonstrate the usefulness of our technique in understanding user loyalty and switching behavior in search engines.", "uri": "https://vimeo.com/102601053", "name": "LoyalTracker: Visualizing Loyalty Dynamics in Search Engines", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:00:55+00:00", "description": "Authors: Michael Behrisch, Fatih Korkmaz, Lin Shao, Tobias Schreck\n\nAbstract: The extraction of relevant and meaningful information from multivariate or high-dimensional data is a challenging problem. One reason for this is that the number of possible representations, which might contain relevant information, grows exponentially with the amount of data dimensions. Also, not all views from a possibly large view space, are potentially relevant to a given analysis task or user. Focus+Context or Semantic Zoom Interfaces can help to some extent to efficiently search for interesting views or data segments, yet they show scalability problems for very large data sets. Accordingly, users are confronted with the problem of identifying interesting views, yet the manual exploration of the entire view space becomes ineffective or even infeasible. While certain quality metrics have been proposed recently to identify potentially interesting views, these often are defined in a heuristic way and do not take into account the application or user context. We introduce a framework for a feedback-driven view exploration, inspired by relevance feedback approaches used in Information Retrieval. Our basic idea is that users iteratively express their notion of interestingness when presented with candidate views. From that expression, a model representing the user\u00d4\u00c7\u00d6s preferences, is trained and used to recommend further interesting view candidates. A decision support system monitors the exploration process and assesses the relevance-driven search process for convergence and stability. We present an instantiation of our framework for exploration of Scatter Plot Spaces based on visual features. We demonstrate the effectiveness of this implementation by a case study on two real-world datasets. We also discuss our framework in light of design alternatives and point out its usefulness for development of user- and context-dependent visual exploration systems.", "uri": "https://vimeo.com/102601050", "name": "Feedback-Driven Interactive Exploration of Large Multidimensional Data Supported by Visual Classifier", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:00:54+00:00", "description": "Authors: Cong Xie, Wei Chen, Xinxin Huang, Yueqi Hu, Scott Barlowe, Jing Yang\n\nAbstract: Previous studies on E-transaction time-series have mainly focused on finding temporal trends of transaction behavior. Interesting transactions that are time-stamped and situation-relevant may easily be obscured in a large amount of information. This paper proposes a visual analytics system, Visual Analysis of E-transaction Time-Series (VAET), that allows the analysts to interactively \\ explore large transaction datasets for insights about time-varying transactions. With a set of analyst-determined training samples, VAET automatically estimates the saliency of each transaction in a large time-series using a probabilistic decision tree learner. It provides an effective time-of-saliency (TOS) map where the analysts can explore a large number of transactions at different time \\ granularities. Interesting transactions are further encoded with KnotLines, a compact visual representation that captures both the temporal variations and the contextual connection of transactions. The analysts can thus explore, select, and investigate knotlines of interest. A case study and user study with a real E-transactions dataset (26 million records) demonstrate the effectiveness of VAET.", "uri": "https://vimeo.com/102601048", "name": "VAET: A Visual Analytics Approach for E-transactions Time-series", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:00:54+00:00", "description": "Authors: Sungahn Ko, Shehzad Afzal, Simon Walton, Yang Yang, Junghoon Chae, Abish Malik, Yun Jang, Min Chen, David Ebert\n\nAbstract: This paper focuses on the integration of a family of visual analytics techniques for analyzing high-dimensional, multivariate network data that features spatial and temporal information, network connections, and a variety of other categorical and numerical data types. Such data types are commonly encountered in transportation, shipping, and logistics industries. Due to the scale and complexity of the data, it is essential to integrate techniques for data \\ analysis, visualization, and exploration. We present new visual representations, \\ Petal and Thread, to effectively present many-to-many network data including multi-attribute vectors. In addition, we deploy an information-theoretic model for anomaly detection across varying dimensions, displaying highlighted anomalies in a visually consistent manner, as well as supporting a managed process of exploration. Lastly, we evaluate the proposed methodology through \\ data exploration and an empirical study.", "uri": "https://vimeo.com/102601047", "name": "Analyzing High-dimensional Multivariate Network Links with Integrated Anomaly Detection, Highlighting and Exploration", "year": "2014", "event": ""}, {"created_time": "2014-08-05T07:00:53+00:00", "description": "Authors: Haidong Chen, Wei Chen, Honghui Mei, Zhiqi Liu, Kun Zhou, Weifeng Chen, Wentao Gu, Kwan-Liu Ma\n\nAbstract: Scatterplots are widely used to visualize scatter dataset for exploring outliers, clusters, local trends, and correlations. Depicting multi-class scattered points within a single scatterplot view, however, may suffer from heavy overdraw, making it inefficient for data analysis. This paper presents a new visual abstraction scheme that employs a hierarchical multi-class sampling technique to show a feature-preserving simplification. To enhance the density contrast, the colors of multiple classes are optimized by taking the multi-class point distributions into account. We design a visual exploration system that supports visual inspection and quantitative analysis from different perspectives. We have applied our system to several challenging datasets, and the results demonstrate the efficiency of our approach.", "uri": "https://vimeo.com/102601045", "name": "Visual Abstraction and Exploration of Multi-class Scatterplots", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:58:53+00:00", "description": "Authors: Ellen Isaacs, Kelly Domico, Shane Ahern, Eugene Bart, Mudita Singhal\n\nAbstract: Searching a large document collection to learn about a broad subject involves the iterative process of figuring out what to ask, filtering the results, identifying useful documents, and deciding when one has covered enough material to stop searching. We are calling this activity \u00d4\u00c7\u00a3discoverage,\u00d4\u00c7\u00d8 discovery of relevant material and tracking coverage of that material. We built a visual analytic tool called Footprints that uses multiple coordinated visualizations to help users navigate through the discoverage process. To support discovery, Footprints displays topics extracted from documents that provide an overview of the search space and are used to construct searches visuospatially. Footprints allows users to triage their search results by assigning a status to each document (To Read, Read, Useful), and those status markings are shown on interactive histograms depicting the user\u00d4\u00c7\u00d6s coverage through the documents across dates, sources, and topics. Coverage histograms help users notice biases in their search and fill any gaps in their analytic process. To create Footprints, we used a highly iterative, user-centered approach in which we conducted many evaluations during both the design and implementation stages and continually modified the design in response to feedback.", "uri": "https://vimeo.com/102600955", "name": "Footprints: A Visual Search Tool that Supports Discovery and Coverage Tracking", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:58:08+00:00", "description": "Authors: Jian Zhao, Liang Gou, Fei Wang, Michelle Zhou\n\nAbstract: Hundreds of millions of people leave digital footprints on social media (e.g., Twitter and Facebook). Such data not only disclose a person's demographics and opinions, but also reveal one's emotional style. Emotional style captures a person's patterns of emotions over time, including his overall emotional volatility and resilience. Understanding one's emotional style can provide great benefits for both individuals and businesses alike, including the support of self-reflection and delivery of individualized customer care. We present PEARL, a timeline-based visual analytic tool that allows users to interactively discover and examine a person's emotional style derived from this person's social media text. Compared to other visual text analytic systems, our work offers three unique contributions. First, it supports multi-dimensional emotion analysis from social media text to automatically detect a person's expressed emotions at different time points and summarize those emotions to reveal the person's emotional style. Second, it effectively visualizes complex, multi-dimensional emotion analysis results to create a visual emotional profile of an individual, which helps users browse and interpret one's emotional style. Third, it supports rich visual interactions that allow users to interactively explore and validate emotion analysis results. We have evaluated our work extensively through a series of studies. The results demonstrate the effectiveness of our tool both in emotion analysis from social media and in support of interactive visualization of the emotion analysis results.", "uri": "https://vimeo.com/102600918", "name": "PEARL: An Interactive Visual Analytic Tool for Understanding Personal Emotion Style Derived from Social Media", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:57:08+00:00", "description": "Authors: Dominik Sacha, Andreas Stoffel, Florian Stoffel, Bum Chul Kwon, Geoffrey Ellis, Daniel Keim\n\nAbstract: Visual analytics enables us to analyze huge information spaces in order to support complex decision making and data exploration. Humans play a central role in generating knowledge from the snippets of evidence emerging from visual data analysis. Although prior research provides frameworks that generalize this process, their scope is often narrowly focused so they do not encompass different perspectives at different levels. This paper proposes a knowledge generation model for visual analytics that ties together these diverse frameworks, yet retains previously developed models (e.g., KDD process) to describe individual segments of the overall visual analytic processes. To test its utility, a real world visual analytics system is compared against the model, demonstrating that the knowledge generation process model provides a useful guideline when developing and evaluating such systems. The model is used to effectively compare different data analysis systems. Furthermore, the model provides a common language and description of visual analytic processes, which can be used for communication between researchers. At the end, our model reflects areas of research that future researchers can embark on.", "uri": "https://vimeo.com/102600865", "name": "Knowledge Generation Model for Visual Analytics", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:50:22+00:00", "description": "Authors: Fei Wang, Wei Chen, Feiran Wu, Ye Zhao, Han Hong, Tianyu Gu, Long Wang, Ronghua Liang, Hujun Bao\n\nAbstract: Transport assessment plays a vital role in urban planning and traffic control, which are influenced by multi-faceted traffic factors involving road infrastructure and traffic flow. Conventional solutions can hardly meet the requirements and expectations of domain experts. In this paper we present a data-driven solution by leveraging a visual analysis system to evaluate the real traffic situations based on taxi trajectory data. A sketch-based visual interface is designed to support dynamic query and visual reasoning of traffic situations within multiple coordinated views. In particular, we propose a novel road-based query model for analysts to interactively conduct evaluation tasks. This model is supported by a bi-directional hash structure, TripHash, which enables real-time responses to the data queries over a huge amount of trajectory data. Case studies with a real taxi GPS trajectory dataset (>30GB) show that our system performs well for on-demand transport assessment and reasoning.", "uri": "https://vimeo.com/102600573", "name": "A Visual Reasoning Approach for Data-driven Transport Assessment on Urban Roads", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:50:21+00:00", "description": "Authors: Yingcai Wu, Shixia Liu, Kai Yan, Mengchen Liu, Fangzhao Wu\n\nAbstract: Analyzing and tracing diffusion of public opinions on social media can find many different applications such as government and business intelligence. However, the rapid propagation and great diversity of public opinions on social media pose great challenges to effective analysis of opinion diffusion. In this paper, we develop a visual analysis system called OpinionFlow to empower analysts to detect opinion propagation patterns and glean insights. Inspired by the information diffusion model and the theory of selective exposure, we develop an opinion diffusion model to approximate opinion propagation among Twitter users. Accordingly, we design an opinion flow visualization that combines a Sankey graph with a tailored density map in one view to visually convey diffusion \\ of opinions among many users. A stacked tree is used to allow analysts to interactively select topics of interest at different levels. The stacked tree is synchronized with the opinion flow visualization to help users examine and compare diffusion patterns across topics. Experiments and case studies on Twitter data demonstrate the effectiveness and usability of OpinionFlow.", "uri": "https://vimeo.com/102600571", "name": "OpinionFlow: Visual Analysis of Opinion Diffusion on Social Media", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:50:20+00:00", "description": "Authors: Jie Li, Kang Zhang, Zhao-Peng Meng\n\nAbstract: We present a new approach to visualizing the climate data of multi-dimensional, time-series, and geo-related characteristics. Our approach integrates three new highly interrelated visualization techniques, and uses the same input data types as in the traditional model-based analysis methods. As the main visualization view, Global Radial Map is used to identify the overall state of climate changes and provide users with a compact and intuitive view for analyzing spatial and temporal patterns at the same time. Other two visualization techniques, providing complementary views, are specialized in analysing time trend and detecting abnormal cases, which are two important analysis tasks in any climate change study. Case studies and expert reviews  have been conducted, through which the effectiveness and scalability of the proposed approach has been confirmed.", "uri": "https://vimeo.com/102600570", "name": "Vismate: Interactive Visual Analysis of Station-Based Observation Data on Climate Changes", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:50:20+00:00", "description": "Authors: Shixia Liu, Xiting Wang, Jianfei Chen, Jun Zhu, Baining Guo\n\nAbstract: We present a visual analytics approach to developing a full picture of relevant topics discussed in multiple sources such as news, blogs, or micro-blogs. The full picture consists of a number of common topics among multiple sources as well as distinctive topics. The key idea behind our approach is to jointly match the topics extracted from each source together in order to interactively and effectively analyze common and distinctive topics. We start by modeling each textual corpus as a topic graph. These graphs are then matched together with a consistent graph matching method. Next, we develop an LOD-based visualization for better understanding and analysis of the matched graph. The major feature of this visualization is that it combines a radially stacked tree visualization with a density-based graph visualization to facilitate the examination of the matched topic graph from multiple perspectives. To compensate for the deficiency of the graph matching algorithm and meet different users\u00d4\u00c7\u00d6 needs, we allow users to interactively modify the graph matching result. We have applied our approach to various data including news, tweets, and blog data. Qualitative evaluation and a real-world case study with domain experts demonstrate the promise of our approach, especially in support of analyzing a topic-graph-based full picture at different levels of detail.", "uri": "https://vimeo.com/102600569", "name": "TopicPanorama: a Full Picture of Relevant Topics", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:50:19+00:00", "description": "Authors: Michael Beham, Wolfgang Herzner, Eduard Gr\u00f6ller, Johannes Kehrer\n\nAbstract: Geometry generators are commonly used in video games and evaluation systems for computer vision to create geometric shapes such as terrains, vegetation or airplanes. The parameters of the generator are often sampled automatically which can lead to many similar or unwanted geometric shapes. In this paper, we propose a novel visual exploration approach that combines the abstract parameter space of the geometry generator with the resulting 3D shapes in a composite visualization. Similar geometric shapes are first grouped using hierarchical clustering and then nested within an illustrative parallel coordinates visualization. This helps the user to study the sensitivity of the generator with respect to its parameter space and to identify invalid parameter settings. Starting from a compact overview representation, the user can iteratively drill-down into local shape differences by clicking on the respective clusters. Additionally, a linked radial tree gives an overview of the cluster hierarchy and enables the user to manually split or merge clusters. We evaluate our approach by exploring the parameter space of a cup generator and provide feedback from domain experts.", "uri": "https://vimeo.com/102600567", "name": "Cupid: Cluster-based Exploration of Geometry Generators with Parallel Coordinates and Radial Trees", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:41:42+00:00", "description": "Authors: Abish Malik, Ross Maciejewski, Sherry Towers, Sean McCullough, David Ebert\n\nAbstract: In this paper, we present a visual analytics approach that provides decision makers with a proactive and predictive environment in order to assist them in making effective resource allocation and deployment decisions. The challenges involved with such predictive analytics processes include end-users\u00d4\u00c7\u00d6 understanding, and the application of the underlying statistical algorithms at the right spatiotemporal granularity levels so that good prediction estimates can be established. In our approach, we provide analysts with a suite of natural scale templates and methods that enable them to focus and drill down to appropriate geospatial and temporal resolution levels. Our forecasting technique is based on the Seasonal Trend decomposition based on Loess (STL) method, which we apply in a spatiotemporal visual analytics context to provide analysts with predicted levels of future activity. We also present a novel kernel density estimation technique we have developed, in which the prediction process is influenced by the spatial correlation of recent incidents at nearby locations. We demonstrate our techniques by applying our methodology to Criminal, Traffic and Civil (CTC) incident datasets.", "uri": "https://vimeo.com/102600217", "name": "Proactive Spatiotemporal Resource Allocation and Predictive Visual Analytics for Community Policing and Law Enforcement", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:41:11+00:00", "description": "Authors: Johannes Landstorfer, Ivo Herrmann, Jan-Erik Stange, Marian D\u00f6rk, Reto Wettach\n\nAbstract: We created a pixel map for multivariate data based on an analysis of the needs of network security engineers. Parameters of a log record are shown as pixels and these pixels are stacked to represent a record. This allows a broad view of a data set on one screen while staying very close to the raw data and to expose common and rare patterns of user behavior through the visualization itself (the \u00d4\u00c7\u00a3Carpet\u00d4\u00c7\u00d8). Visualizations that immediately point to areas of suspicious activity without requiring extensive filtering, help network engineers investigating unknown computer security incidents. Most of them, however, have limited knowledge of advanced visualization techniques, while many designers and data scientists are unfamiliar with computer security topics. To bridge this gap, we developed visualizations together with engineers, following a co-creative process. We will show how we explored the scope of the engineers' tasks and how we jointly developed ideas and designs. Our expert evaluation indicates that this visualization helps to scan large parts of log files quickly and to define areas of interest for closer inspection.", "uri": "https://vimeo.com/102600193", "name": "Weaving a Carpet from Log Entries: a Network Security Visualization Built with Co-Creation", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:41:10+00:00", "description": "Authors: Guodao Sun, Yingcai Wu, Shixia Liu, Tai-Quan Peng, Jonathan Zhu, Ronghua Liang\n\nAbstract: Cooperation and competition (jointly called \"coopetition\") are two modes of interactions among a set of concurrent topics on social media. How do topics cooperate or compete with each other to gain public attention? Which topics tend to cooperate or compete with one another? Who plays the key role in coopetition-related interactions? We answer these intricate questions by proposing a visual analytics system that facilitates the in-depth analysis of topic coopetition on social media. We model the complex interactions among topics as a combination of carry-over, coopetition recruitment, and coopetition distraction effects. This model provides a close functional approximation of the cooperation process by depicting how different groups of influential users (i.e., \"topic leaders\") affect coopetition. We also design EvoRiver, a time-based visualization, that allows users to explore coopetition-related interactions and to detect dynamically evolving patterns, as well as their major causes. We test our model and demonstrate the usefulness of our system based on two Twitter data sets (social topics data and business topics data).", "uri": "https://vimeo.com/102600192", "name": "EvoRiver: Visual Analysis of Topic Coopetition on Social Media", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:41:10+00:00", "description": "Authors: Yafeng Lu, Robert Krueger, Dennis Thom, Feng Wang, Steffen Koch, Thomas Ertl, Ross Maciejewski\n\nAbstract: A key analytical task across many domains is model building and exploration for predictive analysis. Data is collected, parsed and analyzed for relationships, and features are selected and mapped to estimate the response of a system under exploration. As social media data has grown more abundant, data can be captured that may potentially represent behavioral patterns in society. In turn, this unstructured social media data can be parsed and integrated as a key factor for predictive intelligence. In this paper, we present a framework for the development of predictive models utilizing social media data. We combine feature selection mechanisms, similarity comparisons and model cross-validation through a variety of interactive visualizations to support analysts in model building and prediction. In order to explore how predictions might be performed in such a framework, we present results from a user study focusing on social media data as a predictor for movie box-office success.", "uri": "https://vimeo.com/102600191", "name": "Integrating Predictive Analytics and Social Media", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:41:09+00:00", "description": "Authors: Pierre Accorsi, Micka\u00ebl Fabr\u00e8gue, Arnaud Sallaberry, Flavie Cernesson, Nathalie Lalande, Agn\u00e8s Braud, Sandra Bringay, Florence Le Ber, Pascal Poncelet, Maguelonne Teisseire\n\nAbstract: Economic development based on industrialization, intensive agriculture expansion and population growth places greater pressure on water resources through increased water abstraction and water quality degradation. River pollution is now a visible issue, with emblematic ecological disasters following industrial accidents such as the pollution of the Rhine river in 1986. River water quality is a pivotal public health and environmental issue that has prompted governments to plan initiatives for preserving or restoring aquatic ecosystems and water resources. Water managers require operational tools to help interpret the complex range of information available on river water quality functioning. Tools based on statistical approaches often fail to resolve some tasks due to the sparse nature of the data. Here we describe HydroQual, a tool to facilitate visual analysis of river water quality. This tool combines spatiotemporal data mining and visualization techniques to perform tasks defined by water experts. We illustrate the approach with a case study that illustrates how the tool helps experts analyze water quality. We also perform a qualitative evaluation with these experts.", "uri": "https://vimeo.com/102600189", "name": "HydroQual: Visual Analysis of River Water Quality", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:32:12+00:00", "description": "Authors: Wei Zeng, Chi-Wing Fu, Stefan M\u00fcller Arisona, Alexander Erath, Huamin Qu\n\nAbstract: Public transportation systems (PTSs) play an important role in modern cities, providing shared/massive transportation services that are essential for the general public. However, due to their increasing complexity, designing effective methods to visualize and explore PTS is highly challenging. Most existing techniques employ network visualization methods and focus on showing the network topology across stops while ignoring various mobility-related factors such as riding time, transfer time, waiting time, and round-the-clock patterns. This work aims to visualize and explore passenger mobility in a PTS with a family of analytical tasks based on inputs from transportation researchers. After exploring different design alternatives, we come up with an integrated solution with three visualization modules: isochrone map view for geographical information, isotime flow map view for effective temporal information comparison and manipulation, and OD-pair journey view for detailed visual analysis of mobility factors along routes between specific origin-destination pairs. The isotime flow map linearizes a flow map into a parallel isoline representation, maximizing the visualization of mobility information along the horizontal time axis while presenting clear and smooth pathways from origin to destinations. Moreover, we devise several interactive visual query methods for users to easily explore the dynamics of PTS mobility over space and time. Lastly, we also construct a PTS mobility model from millions of real passenger trajectories, and evaluate our visualization techniques with assorted case studies with the transportation researchers.", "uri": "https://vimeo.com/102599786", "name": "Visualizing Mobility of Public Transportation System", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:32:11+00:00", "description": "Authors: Sungahn Ko, Jieqiong Zhao, Jing Xia, Shehzad Afzal, Xiaoyu Wang, Greg Abram, Niklas Elmqvist, Len Kne, David Riper, Kelly Gaither, Shaun Kennedy, William Tolone, William Ribarsky, David Ebert\n\nAbstract: We present VASA, a visual analytics platform consisting of a desktop application, a component model, and a suite of distributed simulation components for modeling the impact of societal threats such as weather, food contamination, and traffic on critical infrastructure such as supply chains, road networks, and power grids. Each component encapsulates a high-fidelity simulation model that together form an asynchronous simulation pipeline: a system of systems of individual simulations with a common data and parameter exchange format. At the heart of VASA is the Workbench, a visual analytics application providing three distinct features: (1) low-fidelity approximations of the distributed simulation components using local simulation proxies to enable analysts to interactively configure a simulation run; (2) computational steering mechanisms to manage the execution of individual simulation components; and (3) spatiotemporal and interactive methods to explore the combined results of a simulation run. We showcase the utility of the platform using examples involving supply chains during a hurricane as well as food contamination in a fast food restaurant chain.", "uri": "https://vimeo.com/102599784", "name": "VASA: Interactive Computational Steering of Large Asynchronous Simulation Pipelines for Societal Infrastructure", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:32:11+00:00", "description": "Authors: David Gotz, Harry Stavropoulos\n\nAbstract: Temporal event sequence data is increasingly commonplace, with applications ranging from electronic medical records to financial transactions to social media activity.  Previously developed techniques have focused on low-dimensional datasets (e.g., with less than 20 distinct event types).  Real-world datasets are often far more complex.  This paper describes DecisionFlow, a visual analysis technique designed to support the analysis of high-dimensional temporal event sequence data (e.g., thousands of event types).  DecisionFlow combines a scalable and dynamic temporal event data structure with interactive multi-view visualizations and ad hoc statistical analytics.  We provide a detailed review of our methods, and present the results from a 12-person user study.  The study results demonstrate that DecisionFlow enables the quick and accurate completion of a range of sequence analysis tasks for datasets containing thousands of event types and millions of individual events.", "uri": "https://vimeo.com/102599783", "name": "DecisionFlow: Visual Analytics for High-Dimensional Temporal Event Sequence Data", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:32:10+00:00", "description": "Authors: Olav Lenz, Frank Keul, Sebastian Bremm, Kay Hamacher, Tatiana von Landesberger\n\nAbstract: Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As \\ each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.", "uri": "https://vimeo.com/102599781", "name": "Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:26:32+00:00", "description": "Authors: Steven Gomez, Hua Guo, Caroline Ziemkiewicz, David Laidlaw\n\nAbstract: We present a method for evaluating visualizations using both tasks and exploration, and demonstrate this method in a study of spatiotemporal network designs for a visual analytics system. The method is well suited for studying visual analytics applications in which users perform both targeted data searches and analyses of broader patterns. In such applications, an effective visualization design is one that helps users complete tasks accurately and efficiently, and supports hypothesis generation during open-ended exploration. To evaluate both of these aims in a single study, we developed an approach called layered insight- and task-based evaluation (LITE) that interposes several prompts for observations about the data model between sequences of predefined search tasks. We demonstrate the evaluation method in a user study of four network visualizations for spatiotemporal data in a visual analytics application. Results include findings that might have been difficult to obtain in a single experiment using a different methodology. For example, with one dataset we studied, we found that on average participants were faster on search tasks using a force-directed layout than using our other designs; at the same time, participants found this design least helpful in understanding the data. Our contributions include a novel evaluation method that combines well-defined tasks with exploration and observation, an evaluation of network visualization designs for spatiotemporal visual analytics, and guidelines for using this evaluation method.", "uri": "https://vimeo.com/102599535", "name": "An Insight- and Task-based Methodology for Evaluating Spatiotemporal Visual Analytics", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:26:32+00:00", "description": "Authors: Thomas M\u00fchlbacher, Harald Piringer, Samuel Gratzl, Michael Sedlmair, Marc Streit\n\nAbstract: An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.", "uri": "https://vimeo.com/102599534", "name": "Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:26:31+00:00", "description": "Authors: Lauren Bradel, Chris North, Leanna House, Scotland Leman\n\nAbstract: Semantic interaction offers an intuitive communication mechanism between human users and complex statistical models. By shielding the users from manipulating model parameters, they focus instead on directly manipulating the spatialization, thus remaining in their cognitive zone. However, this technique is not inherently scalable past hundreds of text documents. To remedy this, we present the concept of multi-model semantic interaction, where semantic interactions can be used to steer multiple models at multiple levels of data scale, enabling users to tackle larger data problems. We also present an updated visualization pipeline model for generalized multi-model semantic interaction. To demonstrate multi-model semantic interaction, we introduce StarSPIRE, a visual text analytics prototype that transforms user interactions on documents into both small-scale display layout updates as well as large-scale relevancy-based document selection.", "uri": "https://vimeo.com/102599532", "name": "Multi-Model Semantic Interaction for Text Analytics", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:26:31+00:00", "description": "Authors: Maoyuan Sun, Chris North, Naren Ramakrishnan\n\nAbstract: Analysts often need to explore and identify coordinated relationships (e.g., four people who visited the same five cities on the same set of days) within some large datasets for sensemaking. Biclusters provide a potential solution to ease this process, because each computed bicluster bundles individual relationships into coordinated sets. By understanding such computed, structural, relations within biclusters, analysts can leverage their domain knowledge and intuition to determine the importance and relevance of the extracted relationships for making hypotheses. However, due to the lack of systematic design guidelines, it is still a challenge to design effective and usable visualizations of biclusters to enhance their perceptibility and interactivity for exploring coordinated relationships. In this paper, we present a five-level design framework for bicluster visualizations, with a survey of the state-of-the-art design considerations and applications that are related or that can be applied to bicluster visualizations. We summarize pros and cons of these design options to support user tasks at each of the five-level relationships. Finally, we discuss future research challenges for bicluster visualizations and their incorporation into visual analytics tools.", "uri": "https://vimeo.com/102599529", "name": "A Five-Level Design Framework for Bicluster Visualizations", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:17:33+00:00", "description": "Author: Matthew Brehmer", "uri": "https://vimeo.com/102599193", "name": "Visualization Task Abstraction from Multiple Perspectives", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:17:33+00:00", "description": "Author: Mai El-Shehaly", "uri": "https://vimeo.com/102599192", "name": "StreamProbe: GPU-based Selection and Visualization of Multi-Volume Time-Variant Scienti\ufb01c Data", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:17:33+00:00", "description": "Author: Peter Bodesinsky", "uri": "https://vimeo.com/102599191", "name": "Visual Process Mining: Event Data Exploration and Analysis", "year": "2014", "event": ""}, {"created_time": "2014-08-05T06:17:32+00:00", "description": "Author: Sriram Karthik Badam", "uri": "https://vimeo.com/102599190", "name": "Multimodal Interaction Design for Collaborative Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-04T14:15:43+00:00", "description": "Moderator: Moritz Stefaner\n\nPanelists: Greg McInerny, Scott David, Matteo Maggiore, and Tariq Khokhar\n\nAbstract: We experience an increasing demand for data visualization in the context of NGOs, policy making and data democratization. What are the specific challenges that organizations like the World Bank, OECD, or the World Economic Forum faces when publishing reports and data sets? What is the role that data visualization can play in these contexts? And how can we improve on making an actual difference in important questions, with data visualization?", "uri": "https://vimeo.com/102528451", "name": "Panel: Data with a cause: Visualization for policy change", "year": "2014", "event": "PANEL"}, {"created_time": "2014-08-04T14:09:58+00:00", "description": "Authors: Sung-Hee Kim, Jeremy Boy, Sukwon Lee, Ji Soo Yi, and Niklas Elmqvist\n\nAbstract: We propose a hands-on workshop where participants will learn about and discuss visualization literacy by actually designing and evaluating questions for a visualization literacy measure. Though the value of information visualization is becoming apparent to a broad audience, visualization researchers often acknowledge that people have different levels of understanding of visualization techniques. In other words, our understanding of how users interpret visualizations has not caught up with design and technical developments, and even the concept of visualization literacy is still debated. Different domains of research, such as mathematics education, cognitive science, and psychology, have been approaching this problem within their domain. We believe that researchers in information visualization and visual analytics should lead the effort in de\ufb01ning the concept, and in creating valid and practical measurement tools. The goal of our workshop is to take a step in this direction by developing a better understanding of visualization literacy, identifying possible metrics for evaluation, and raising new questions for future research through the design and evaluation of visualization literacy tests. The outcome of our workshop will be a participatory web-platform for collectively created visualization literacy tests and questionnaires that can directly be used by researchers in our community.", "uri": "https://vimeo.com/102527957", "name": "Workshop: Towards An Open Visualization Literacy Testing Platform", "year": "2014", "event": "WORKSHOP"}, {"created_time": "2014-08-04T14:07:26+00:00", "description": "Organisers: Kai Xu, Simon Attfield, T.J. Jankun-Kelly\nWebsite: http://visualizationliteracy.org \n\nAbstract: During complex sensemaking and analysis task, it can be valuable to maintain a history of the processes and transformations involved - referred to as \u2018provenance\u2019 information. Provenance information can be a resource for \"re\ufb02ection-in-action\" during analyses, for supporting planning and reframing of objectives and scope. It can also be a resource after the event, supporting the interpretation of claims, audit, accountability or training. There has been considerable work on capturing and visualizing of \u2018data provenance\u2019, which focuses on data collection and computation, and \u2018analytic provenance\u2019, which captures the interactive data exploration process. However, there is limited work of utilizing these provenance information to support sensemaking, in terms of improving its ef\ufb01cacy and avoid pitfalls such as data quality issue and human bias. This workshop aims to bring together researchers involved in visual analytics and various aspects of sensemaking to consider emerging positions, questions, and \ufb01ndings related to the capture, processing, representation and use of provenance information to support complex sensemaking tasks. The emphasis is on discussion and collaboration, with a goal to produce a paper describing the state-of-the-art of provenance for sensemaking after the workshop.", "uri": "https://vimeo.com/102527734", "name": "Workshop: Provenance for Sensemaking", "year": "2014", "event": "WORKSHOP"}, {"created_time": "2014-08-04T14:07:26+00:00", "description": "Organisers: Catherine Plaisant, Silvia Miksch, Theresia Gschwandtner, Sana Malik\nWebsite: http://www.cs.umd.edu/hcil/parisehrvis/ \n\nAbstract: Electronic Health Record (EHR) databases contain millions of patient records including events such as diagnoses, test results, or medication prescriptions. These records are an invaluable data source for clinical research and improvement of clinical quality, as they provide longitudinal health information about patient populations. The use of EHR databases could be dramatically improved if easy-to-use interfaces allowed clinical researchers and quality improvement analysts to explore complex patterns in order to build and test hypotheses regarding the benefits, risks, and appropriateness of treatments or medication regimens. Novel strategies in information visualization and visual analytics are needed. The interest in this topic is growing at very rapid pace and is very interdisciplinary by nature, both in term of field (medicine and computer science) but also research environment (academic research as well as industry and government agencies). Because of the European location of the conference, we have a unique opportunity to create bridges and explore new collaborations between groups that would have never met otherwise.", "uri": "https://vimeo.com/102527732", "name": "Workshop: Visualizing Electronic Health Record Data", "year": "2014", "event": "WORKSHOP"}, {"created_time": "2014-08-04T14:07:25+00:00", "description": "Organisers: Yvonne Jansen, Petra Isenberg, Jason Dykes, Sheelagh Carpendale, Sriram Subramanian, Daniel Keefe\nWebsite: http://beyond.wallviz.dk/\n\nAbstract: The Desktop computer is dead. Monitors sit on desks, unplugged - hosting layers of Post-It notes or gathering dust as a retro emergency low-light mirror. Visualization is colourful, big, tangible, nosy, interactive, compelling and everywhere. It supports all sorts of creative activity and is key to problem solving in education, science, government and industry. But how? What is your 'imagined future' for visualization? We will be exploring possible visualization scenarios with short but rich scenarios in which designers, practitioners and researchers creatively explore opportunities for 'beyond-the-desktop' visualization. We will be discussing these and using them to develop the community\u2019s perspective on the future of VIS.", "uri": "https://vimeo.com/102527731", "name": "Workshop: Death of the Desktop \u2013 Envisioning Visualization without Desktop Computing", "year": "2014", "event": "WORKSHOP"}, {"created_time": "2014-08-04T14:01:57+00:00", "description": "Authors: Asmund Birkeland, Cagatay Turkay, and Ivan Viola\n\nAbstract: Flow data is often visualized by animated particles inserted into a flow field. The velocity of a particle on the screen is typically linearly scaled by the velocities in the data. However, the perception of velocity magnitude in animated particles is not necessarily linear. We present a study on how different parameters affect relative motion perception. We have investigated the impact of four parameters. The parameters consist of speed multiplier, direction, contrast type and the global velocity scale. In addition, we investigated if multiple motion cues, and point distribution, affect the speed estimation. Several studies were executed to investigate the impact of each parameter. In the initial results, we noticed trends in scale and multiplier. Using the trends for the significant parameters, we designed a compensation model, which adjusts the particle speed to compensate for the effect of the parameters. We then performed a second study to investigate the performance of the compensation model. From the second study we detected a constant estimation error, which we adjusted for in the last study. In addition, we connect our work to established theories in psychophysics by comparing our model to a model based on Stevens\u2019 Power Law.", "uri": "https://vimeo.com/102527262", "name": "Perceptually Uniform Motion Space", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:59:01+00:00", "description": "Authors: Radu Jianu, Adrian Rusu, Yifan Hu, Douglas Taggart\n\nAbstract: We present the results of evaluating four techniques for displaying group or cluster information overlaid on node-link diagrams: node coloring, GMap, BubbleSets, and LineSets. The contributions of the paper are three fold. First, we present quantitative results and statistical analyses of data from an online study in which approximately 800 subjects performed ten types of group and network tasks in the four evaluated visualizations. Specifically, we show that BubbleSets is the best alternative for tasks involving group membership assessment; that visually encoding group information over basic node-link diagrams incurs an accuracy penalty of about 25% in solving network tasks; and that GMap\u2019s use of prominent group labels improves memorability. We also show that GMap\u2019s visual metaphor can be slightly altered to outperform BubbleSets in group membership assessment. Second, we discuss visual characteristics that can explain the observed quantitative differences in the four visualizations and suggest design recommendations. This discussion is supported by a small scale eye-tracking study and previous results from the visualization literature. Third, we present an easily extensible user study methodology.", "uri": "https://vimeo.com/102526993", "name": "How to Display Group Information on Node-Link Diagrams: an Evaluation", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:57:58+00:00", "description": "Authors: Abon Chaudhuri, Teng-Yok Lee, Han-Wei Shen, Rephael Wenger\n\nAbstract: Large scale scientific simulations frequently use streamline based techniques to visualize flow fields. As the shape of a streamline is often related to some underlying property of the field, it is important to identify streamlines (or their parts) with unique geometric features. In this paper, we introduce a metric, called the box counting ratio, which measures the geometric complexity of streamlines by measuring their space-filling capacity at different scales.We propose a novel interactive visualization framework which utilizes this metric to extract, organize and visualize features of varying density and complexity hidden in large numbers of streamlines. The proposed framework extracts complex regions of varying density from the streamlines, and organizes and presents them on an interactive 2D information space, allowing user selection and visualization of streamlines. We also extend this framework to support exploration using an ensemble of measures including box counting ratio. Our framework allows the user to easily visualize and interact with features otherwise hidden in large vector field data. We strengthen our claims with case studies using combustion and climate simulation data sets.", "uri": "https://vimeo.com/102526895", "name": "Exploring Flow Fields Using Space-filling Analysis of Streamlines", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:54:57+00:00", "description": "Authors: Benjamin Bach, Emmanuel Pietriga, Jean-Daniel Fekete\n\nAbstract: Identifying, tracking and understanding changes in dynamic networks are complex and cognitively demanding tasks. We present GraphDiaries, a visual interface designed to improve support for these tasks in any node-link based graph visualization system. GraphDiaries relies on animated transitions that highlight changes in the network between time steps, thus helping users identify and understand those changes. To better understand the tasks related to the exploration of dynamic networks, we first introduce a task taxonomy, that informs the design of GraphDiaries, presented afterwards. We then report on a user study, based on representative tasks identified through the taxonomy, and that compares GraphDiaries to existing techniques for temporal navigation in dynamic networks, showing that it outperforms them in terms of both task time and errors for several of these tasks.", "uri": "https://vimeo.com/102526646", "name": "GraphDiaries: Animated Transitions and Temporal Navigation for Dynamic Networks", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:53:22+00:00", "description": "Authors: Steffen Oeltze, Dirk J. Lehmann, Alexander Kuhn, G\u00e1bor Janiga, Holger Theisel, Bernhard Preim\n\nAbstract: Understanding the hemodynamics of blood flow in vascular pathologies such as intracranial aneurysms is essential for both their diagnosis and treatment. Computational fluid dynamics (CFD) simulations of blood flow based on patient-individual data are performed to better understand aneurysm initiation and progression and more recently, for predicting treatment success. In virtual stenting, a flow-diverting mesh tube (stent) is modeled inside the reconstructed vasculature and integrated in the simulation. We focus on steady-state simulation and the resulting complex multiparameter data. The blood flow pattern captured therein is assumed to be related to the success of stenting. It is often visualized by a dense and cluttered set of streamlines.We present a fully automatic approach for reducing visual clutter and exposing characteristic flow structures by clustering streamlines and computing cluster representatives. While individual clustering techniques have been applied before to streamlines in 3D flow fields, we contribute a general quantitative and a domain-specific qualitative evaluation of three state-of-the-art techniques. We show that clustering based on streamline geometry as well as on domain-specific streamline attributes contributes to comparing and evaluating different virtual stenting strategies. With our work, we aim at supporting CFD engineers and interventional neuroradiologists.", "uri": "https://vimeo.com/102526517", "name": "Blood Flow Clustering and Applications in Virtual Stenting of Intracranial Aneurysms", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:43:40+00:00", "description": "Authors: Harsh Bhatia, Valerio Pascucci, and Peer-Timo Bremer\n\nAbstract: The Helmholtz-Hodge decomposition (HHD) describes a flow as the sum of an incompressible, an irrotational, and a harmonic flow, and is a fundamental tool for simulation and analysis. Unfortunately, for bounded domains, the HHD is not uniquely defined, and traditionally, boundary conditions are imposed to obtain a unique solution. However, in general, the boundary conditions used during the simulation may not be known and many simulations use open boundary conditions. In these cases, the flow imposed by traditional boundary conditions may not be compatible with the given data, which leads to sometimes drastic artifacts and distortions in all three components, hence producing unphysical results. Instead, this paper proposes the natural HHD, which is defined by separating the flow into internal and external components. Using a completely data-driven approach, the proposed technique obtains uniqueness without assuming boundary conditions a priori. As a result, it enables a reliable and artifact-free analysis for flows with open boundaries or unknown boundary conditions. Furthermore, our approach computes the HHD on a point-wise basis in contrast to the existing global techniques, and thus supports computing inexpensive local approximations for any subset of the domain. Finally, the technique is easy to implement for a variety of spatial discretizations and interpolated fields in both two and three dimensions.", "uri": "https://vimeo.com/102525668", "name": "The Natural Helmholtz-Hodge Decomposition For Open-Boundary Flow Analysis", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:43:38+00:00", "description": "Authors: Sedat Ozer, Deborah Silver, Karen Bemis, Pino Martin\n\nAbstract: For large-scale simulations, the data sets are so massive that it is sometimes not feasible to view the data with basic visualization methods, let alone explore all time steps in detail. Automated tools are necessary for knowledge discovery, i.e., to help sift through the data and isolate specific time steps that can then be further explored. Scientists study patterns and interactions and want to know when and where interesting things happen. Activity detection, the detection of specific interactions of objects which span a limited duration of time, has been an active research area in the computer vision community. In this paper, we introduce activity detection to scientific simulations and show how it can be utilized in scientific visualization. We show how activity detection allows a scientist to model an activity and can then validate their hypothesis on the underlying processes. Three case studies are presented.", "uri": "https://vimeo.com/102525663", "name": "Activity Detection in Scientific Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:39:26+00:00", "description": "Authors: Brian Duffy, Jeyarajan Thiyagalingam, Simon Walton, David J. Smith, Anne Trefethen, Jackson C. Kirkman-Brown, Eamonn A. Gaffney and Min Chen\n\nAbstract: Existing efforts in computer assisted semen analysis have been focused on high speed imaging and automated image analysis of sperm motility. This results in a large amount of data, and is extremely challenging for clinical scientists and researchers to interpret, compare and correlate the multidimensional and time-varying measurements captured from video data. We use glyphs to encode a collection of numerical measurements taken at regular intervals and summarize spatio-temporal motion characteristics using static visual representations. The design of the glyphs addresses the needs for (a) encoding 20 variables using separable visual channels, (b) supporting scientific observation of interrelationships between different measurements and comparison between different sperm cells and their flagella, and (c) facilitating learning of encoding scheme by making use of appropriate visual abstractions and metaphors. We focus this work on video visualization for computer-aided semen analysis, which has a broad impact on both biological sciences and medical healthcare. We demonstrate glyph-based visualization can serve as a means of external memorization of video data as well as an overview of a large set of spatiotemporal measurements. It enables domain scientists to make observations in a cost-effective manner by reducing the burden of viewing videos repeatedly, while providing a new visual representation for conveying semen statistics.", "uri": "https://vimeo.com/102525333", "name": "Glyph-Based Video Visualization for Semen Analysis", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:39:26+00:00", "description": "Authors: Erick Gomez-Nieto, Frizzi San Roman, Paulo Pagliosa, Wallace Casaca, Elias S. Helou, Maria Cristina Ferreira de Oliveira, Luis Gustavo Nonato\n\nAbstract: Internet users are very familiar with the results of a search query displayed as a ranked list of snippets. Each textual snippet shows a content summary of the referred document and a link to it. This display has many advantages, e.g., it affords easy navigation and is straightforward to interpret. Several search tasks would be easier if users were shown an overview of the returned documents, organized so as to reflect how related they are, content-wise, being that the main goal of the visualization method proposed in this work. Call ProjSnippet, the proposed method combines the neighborhood preservation capability of multidimensional projections with the familiar snippet-based representation. The multidimensional projection ensures that similar snippets are neighbors in the visual space, but not avoiding overlaps. Overlapping is handled by defining an energy functional that considers both the amount snippets overlap each other and the preservation of the neighborhood structure as given in the projected layout. The resulting visualization conveys a global view of the query results while highlighting visual groupings of related results.", "uri": "https://vimeo.com/102525332", "name": "Similarity Preserving Snippet-Based Visualization of Web Search Results", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:39:25+00:00", "description": "Authors: James Abello, Steffen Hadlak, Heidrun Schumann, and Hans-J\u00f6rg Schulz\n\nAbstract: Large dynamic networks are targets of analysis in many fields. Tracking temporal changes at scale in these networks is challenging due in part to the fact that small changes can be missed or drowned-out by the rest of the network. For static networks, current approaches allow the identification of specific network elements within their context. However, in the case of dynamic networks, the user is left alone with finding salient local network elements and tracking them over time. In this work, we introduce a modular DoI specification to flexibly define what salient changes are and to assign them a measure of their importance in a time-varying setting. The specification takes into account neighborhood structure information, numerical attributes of nodes/edges, and their temporal evolution. A tailored visualization of the DoI specification complements our approach. Alongside a traditional node-link view of the dynamic network, it serves as an interface for the interactive definition of a DoI function. By using it to successively refine and investigate the captured details, it supports the analysis of dynamic networks from an initial view until pinpointing a user's analysis goal. We report on applying our approach to scientific co-authorship networks and give concrete results for the DBLP dataset.", "uri": "https://vimeo.com/102525330", "name": "A Modular Degree-of-Interest Specification for the Visual Analysis of Large Dynamic Networks", "year": "2014", "event": ""}, {"created_time": "2014-08-04T13:39:24+00:00", "description": "Authors: Jun Tao, Chaoli Wang, Ching-Kuang Shene, Seung Hyun Kim\n\nAbstract: Striking a careful balance among coverage, occlusion, and complexity is a resounding theme in the visual understanding of large and complex three-dimensional flow fields. In this paper, we present a novel deformation framework for focus+context streamline visualization that reduces occlusion and clutter around the focal regions while compacting the context region in a full view. Unlike existing techniques that vary streamline densities, we advocate a different approach that manipulates streamline positions. This is achieved by partitioning the flow field's volume space into blocks and deforming the blocks to guide streamline repositioning. We formulate block expansion and block smoothing into energy terms and solve for a deformed grid that minimizes the objective function under the volume boundary and edge flipping constraints. Leveraging a GPU linear system solver, we demonstrate interactive focus+context visualization with 3D flow field data of various characteristics. Compared to the fisheye focus+context technique, our method can magnify multiple streamlines of focus in different regions simultaneously while minimizing the distortion through optimized deformation. Both automatic and manual feature specifications are provided for flexible focus selection and effective visualization.", "uri": "https://vimeo.com/102525329", "name": "A Deformation Framework for Focus+Context Flow Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:44:36+00:00", "description": "Authors: Harish Doraiswamy, Nivan Ferreira, Theodoros Damoulas, Juliana Freire, Claudio Silva\n\nAbstract: The explosion in the volume of data about urban environments has opened up opportunities to inform both policy and administration and thereby help governments improve the lives of their citizens, increase the efficiency of public services, and reduce the environmental harms of development. However, cities are complex systems and exploring the data they generate is challenging. The interaction between the various components in a city creates complex dynamics where interesting facts occur at multiple scales, requiring users to inspect a large number of data slices over time and space. Manual exploration of these slices is ineffective, time consuming, and in many cases impractical. In this paper, we propose a technique that supports event-guided exploration of large, spatio-temporal urban data. We model the data as time-varying scalar functions and use computational topology to automatically identify events in different data slices. To handle a potentially large number of events, we develop an algorithm to group and index them, thus allowing users to interactively explore and query event patterns on the fly. A visual exploration interface helps guide users towards data slices that display interesting events and trends. We demonstrate the effectiveness of our technique on two different data sets from New York City (NYC): data about taxi trips and subway service. We also report on the feedback we received from analysts at different NYC agencies.", "uri": "https://vimeo.com/102510557", "name": "Using Topological Analysis to Support Event-Guided Exploration in Urban Data", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:34:19+00:00", "description": "Authors: Jiaxi Hu, Guangyu Jeff Zou, Jing Hua\n\nAbstract: In order to visualize and analyze complex collective data, complicated geometric structure of each data is desired to be mapped onto a canonical domain to enable map-based visual exploration. This paper proposes a novel volume-preserving mapping and registration method which facilitates effective collective data visualization. Given two 3-manifolds with the same topology, there exists a mapping between them to preserve each local volume element. Starting from an initial mapping, a volume restoring diffeomorphic flow is constructed as a compressible flow based on the volume forms at the manifold. Such a flow yields equality of each local volume element between the original manifold and the target at its final state. Furthermore, the salient features can be used to register the manifold to a reference template by an incompressible flow guided by a divergence-free vector field within the manifold. The process can retain the equality of local volume elements while registering the manifold to a template at the same time. An efficient and practical algorithm is also presented to generate a volume-preserving mapping and a salient feature registration on discrete 3D volumes which are represented with tetrahedral meshes embedded in 3D space. This method can be applied to comparative analysis and visualization of volumetric medical imaging data across subjects. We demonstrate an example application in multimodal neuroimaging data analysis and collective data visualization.", "uri": "https://vimeo.com/102509842", "name": "Volume-Preserving Mapping and Registration for Collective Data Visualization", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:29:22+00:00", "description": "Authors: Stefan Lindholm, Daniel J\u00f6nsson, Charles Hansen, Anders Ynnerman\n\nAbstract: In visualization, the combined role of data reconstruction and its classification plays a crucial role. In this paper we propose a novel approach that improves classification of different materials and their boundaries by combining information from the classifiers at the reconstruction stage. Our approach estimates the targeted materials\u00e2\u20ac\u2122 local support before performing multiple material-specific reconstructions that prevent much of the misclassification traditionally associated with transitional regions and transfer function (TF) design. With respect to previously published methods our approach offers a number of improvements and advantages. For one, it does not rely on TFs acting on derivative expressions, therefore it is less sensitive to noisy data and the classification of a single material does not depend on specialized TF widgets or specifying regions in a multidimensional TF . Additionally, improved classification is attained without increasing TF dimensionality, which promotes scalability to multivariate data. These aspects are also key in maintaining low interaction complexity. The results are simple-to-achieve visualizations that better comply with the user\u00e2\u20ac\u2122s understanding of discrete features within the studied object.", "uri": "https://vimeo.com/102509550", "name": "Boundary Aware Reconstruction of Scalar Fields", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:29:22+00:00", "description": "Authors: Fan Hong, Chufan Lai, Hanqi Guo, Xiaoru Yuan, Enya Shen, Sikun Li\n\nAbstract: In this paper, we present a novel feature extraction approach called FLDA for unsteady flow fields based on Latent Dirichlet allocation (LDA) model. Analogous to topic modeling in text analysis, in our approach, pathlines and features in a given flow field are defined as documents and words respectively. Flow topics are then extracted based on Latent Dirichlet allocation. Different from other feature extraction methods, our approach clusters pathlines with probabilistic assignment, and aggregates features to meaningful topics at the same time. We build a prototype system to support exploration of unsteady flow field with our proposed LDA-based method. Interactive techniques are also developed to explore the extracted topics and to gain insight from the data. We conduct case studies to demonstrate the effectiveness of our proposed approach.", "uri": "https://vimeo.com/102509549", "name": "FLDA: Latent Dirichlet Allocation Based Unsteady Flow Analysis", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:19:36+00:00", "description": "Authors: Manuela Waldner, mathieu le muzic, Matthias Bernhard, Werner Purgathofer, Ivan Viola\n\nAbstract: Focus+context techniques provide visual guidance in visualizations by giving strong visual prominence to elements of interest while the context is suppressed. However, finding a visual feature to enhance for the focus to pop out from its context in a large dynamic scene, while leading to minimal visual deformation and subjective disturbance, is challenging. This paper proposes Attractive Flicker, a novel technique for visual guidance in dynamic narrative visualizations. We first show that flicker is a strong visual attractor in the entire visual field, without distorting, suppressing, or adding any scene elements. The novel aspect of our Attractive Flicker technique is that it consists of two signal stages: The first \"orientation stage\" is a short but intensive flicker stimulus to attract the attention to elements of interest.  \\ Subsequently, the intensive flicker is reduced to a minimally disturbing luminance oscillation (\"engagement stage\") as visual support to keep track of the focus elements. To find a good trade-off between attraction effectiveness and subjective annoyance caused by flicker, we conducted two perceptual studies to find suitable signal parameters. We showcase Attractive Flicker with the parameters obtained from the perceptual statistics in a study of molecular interactions. With Attractive Flicker, users were able to easily follow the narrative of the visualization on a large display, while the flickering of focus elements was not disturbing when observing the context.", "uri": "https://vimeo.com/102508934", "name": "Attractive Flicker: Guiding Attention in Dynamic Narrative Visualizations", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:19:35+00:00", "description": "Authors: Attila Gyulassy, David Guenther, Joshua A. Levine, Julien Tierny, Valerio Pascucci\n\nAbstract: Morse-Smale (MS) complexes have been gaining popularity as a tool for feature-driven data analysis and visualization. However, the quality of their geometric embedding and the sole dependence on the input scalar field data can limit their applicability when expressing application-dependent features. In this paper we introduce a new combinatorial technique to compute an MS complex that conforms to both an input scalar field and an additional, prior segmentation of the domain. The segmentation constrains the MS complex computation guaranteeing that boundaries in the segmentation are captured as separatrices of the MS complex. We demonstrate the utility and versatility of our approach with two applications. First, we use streamline integration to determine numerically computed basins/mountains and use the resulting segmentation as an input to our algorithm. This strategy enables the incorporation of prior flow path knowledge, effectively resulting in an MS complex that is as geometrically accurate as the employed numerical integration. Our second use case is motivated by the observation that often the data itself does not explicitly contain features known to be present by a domain expert. We introduce edit operations for MS complexes so that a user can directly modify their features while maintaining all the advantages of a robust topology-based representation.", "uri": "https://vimeo.com/102508931", "name": "Conforming Morse-Smale Complexes", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:19:34+00:00", "description": "Authors: David Schroeder, Fedor Korsakov, Carissa Mai-Ping Knipe, Lauren Thorson, Arin M. Ellingson, David Nuckley, John Carlis, Daniel F. Keefe\n\nAbstract: In biomechanics studies, researchers collect, via experiments or simulations, datasets with hundreds or thousands of trials, each describing the same type of motion (e.g., a neck flexion-extension exercise) but under different conditions (e.g., different patients, different disease states, pre- and post-treatment). Analyzing similarities and differences across all of the motions in these collections is a major challenge. Visualizing a single trial at a time does not work, and the typical alternative to juxtaposing multiple trials in a single visual display leads to complex, difficult-to-interpret visualizations. We address this problem via a new strategy that organizes the analysis around motion trends rather than trials. This new strategy is significant because it matches the cognitive approach that scientists would like to take when analyzing motion collections. We introduce several technical innovations required to make trend- centric motion visualization possible. The first is an algorithm for detecting trends in a motion collection via time-dependent clustering. The second is a 2D graphical technique for visualizing all the trends in a motion collection, including how trends come and go over time. The third is a 3D graphical technique for visualizing the biomechanics of the set of motions within each trend using a median 3D motion and a visual variance indicator. These innovations are combined to create an interactive exploratory visualization tool, which we designed through an iterative process in collaboration with both domain scientists and a traditionally-trained graphic designer. We report on insights generated during this design process and demonstrate the tool\u00e2\u20ac\u2122s effectiveness via: (1) a validation study with synthetic data, and (2) feedback from expert musculoskeletal biomechanics researchers who used the tool to analyze the effects of disc degeneration on human spinal kinematics.", "uri": "https://vimeo.com/102508928", "name": "Trend-Centric Motion Visualization: Designing and Applying a new Strategy for Analyzing Scientific Motion Collections", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:19:34+00:00", "description": "Authors: Tao Ju, Minxin Cheng, Xu Wang, Ye Duan\n\nAbstract: Parallel vectors (PV), the loci where two vector fields are parallel, are commonly used to represent curvilinear features in 3D for data visualization. Methods for extracting PV usually operate on a 3D grid and start with detecting seed points on a cell face. We propose, to the best of our knowledge, the first provably correct test that determines the parity of the number of PV points on a cell face. The test only needs to sample along the face boundary and works for any choice of the two vector fields. A discretization of the test is described, validated, and compared with existing tests that are also based on boundary sampling. The test can guide PV-extraction algorithms to ensure closed curves wherever the input fields are continuous, which we exemplify in extracting ridges and valleys of scalar functions.", "uri": "https://vimeo.com/102508926", "name": "A Robust Parity Test for Extracting Parallel Vectors in 3D", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:17:17+00:00", "description": "Authors: Lars Huettenberger, Christian Heine, Christoph Garth\n\nAbstract: Topological and structural analysis of multivariate data is aimed at improving the understanding and usage of such data through identification of intrinsic features and structural relationships among multiple variables. We present two novel methods for simplifying so-called Pareto sets that describe such structural relationships. Such simplification is a precondition for meaningful visualization of structurally rich or noisy data. As a framework for simplification operations, we introduce a decomposition of the data domain into regions of equivalent structural behavior and the reachability graph that describes global connectivity of Pareto extrema. Simplification is then performed as a sequence of edge collapses in this graph; to determine a suitable sequence of such operations, we describe and utilitze a comparison measure that reflects the changes to the data that each operation represents. We demonstrate and evaluate our methods on synthetic and real-world examples.", "uri": "https://vimeo.com/102508778", "name": "Decomposition and Segmentation of Multivariate Data using Pareto Set", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:12:16+00:00", "description": "Authors: Gustavo Mello Machado, Filip Sadlo, Thomas M\u00fcller, Thomas Ertl\n\nAbstract: We present a technique to visualize the streamline-based mapping between the boundary of a simply-connected subregion of arbitrary 3D vector fields. While the streamlines are seeded on one part of the boundary, the remaining part serves as escape border. Hence, the seeding part of the boundary represents a map of streamline behavior, indicating if streamlines reach the escape border or not. Since the resulting maps typically exhibit a very fine and complex structure and are thus not amenable to direct sampling, our approach instead aims at topologically consistent extraction of their boundary. We show that isocline surfaces of the projected vector field provide a robust basis for streamsurface-based extraction of these boundaries. The utility of our technique is demonstrated in the context of transport processes using vector field data from different domains. \\", "uri": "https://vimeo.com/102508486", "name": "Escape Maps", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:12:16+00:00", "description": "Authors: Steffen Frey, Filip Sadlo, Kwan-Liu Ma, Thomas Ertl\n\nAbstract: We present a novel scheme for progressive rendering in interactive visualization. Static settings with respect to a certain \\ image quality or frame rate are inherently incapable of delivering both high frame rates for rapid changes and high image quality for \\ detailed investigation. Our novel technique flexibly adapts by steering the visualization process in three major degrees of freedom: \\ when to terminate the refinement of a frame in the background and start a new one, when to display a frame currently computed, and \\ how much resources to consume. We base these decisions on the correlation of the errors due to insufficient sampling and response \\ delay, which we estimate separately using fast yet expressive heuristics. To automate the configuration of the steering behavior, we \\ employ offline video quality analysis. We provide an efficient implementation of our scheme for the application of volume raycasting, \\ featuring integrated GPU-accelerated image reconstruction and error estimation. Our implementation performs an integral handling \\ of the changes due to camera transforms, transfer function adaptations, as well as the progression of the data to in time. Finally, the \\ overall technique is evaluated with an expert study.", "uri": "https://vimeo.com/102508484", "name": "Interactive Progressive Visualization with Space-Time Error Control", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:12:15+00:00", "description": "Authors: Franz Sauer, Hongfeng Yu, Kwan-Liu Ma\n\nAbstract: Studying the dynamic evolution of time-varying volumetric data is essential in countless scientific endeavors. The ability to isolate and track features of interest allows domain scientists to better manage large complex datasets both in terms of visual understanding and computational efficiency. This work presents a new trajectory-based feature tracking technique for use in joint particle/volume datasets. While traditional feature tracking approaches generally require a high temporal resolution, this method utilizes the indexed trajectories of corresponding Lagrangian particle data to efficiently track features over large jumps in time. Such a technique is especially useful for situations where the volume dataset is either temporally sparse or too large to efficiently track a feature through all intermediate timesteps. In addition, this paper presents a few other applications of this approach, such as the ability to efficiently track the internal properties of volumetric features using variables from the particle data. We demonstrate the effectiveness of this technique using real world combustion and atmospheric datasets and compare it to existing tracking methods to justify its advantages and accuracy.", "uri": "https://vimeo.com/102508482", "name": "Trajectory-based Flow Feature Tracking in Joint Particle/Volume Datasets", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:12:15+00:00", "description": "Authors: Jan Kretschmer, Grzegorz Soza, Christian Tietjen, Michael Suehling, Bernhard Preim, Marc Stamminger\n\nAbstract: Dedicated visualization methods are among the most important tools of modern computer-aided medical applications.Reformation methods such as Multiplanar Reformation or Curved Planar Reformation have evolved as useful tools that facilitate diagnostic and therapeutic work. In this paper, we present a novel approach that can be seen as a generalization of Multiplanar Reformation to curved surfaces. The main concept is to generate reformatted medical volumes driven by the individual anatomical geometry of a specific patient. This allows to provide flat views of anatomical structures that facilitate many tasks such as diagnosis, navigation and annotation. Our reformation framework is based on a non-linear as-rigid-as-possible volumetric deformation scheme that uses generic triangular surface meshes as input. To manage inevitable distortions during reformation, we introduce importance maps which allow to control error distribution and to improve the overall visual quality in areas of elevated interest. Our method seamlessly integrates with well-established concepts such as the slice-based inspection of medical datasets and we believe it can \\ improve the overall efficiency of many medical workflows. To demonstrate this, we additionally present an integrated visualization system and discuss several use cases that substantiate its benefits.", "uri": "/channels/721847https://vimeo.com/102508480", "name": "ADR - Anatomy-Driven Reformation", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:12:14+00:00", "description": "Authors: HyungSuk Choi, Woohyuk Choi, Tran Minh Quan, David Hildebrand, Hanspeter Pfister, Won-Ki Jeong\n\nAbstract: As the size of image data from microscopes and telescopes increases, the need for high-throughput processing and visualization of large volumetric data has become more pressing. At the same time, many-core processors and GPU accelerators are commonplace, making high-performance distributed heterogeneous computing systems affordable. However, effectively utilizing GPU clusters is difficult for novice programmers, and even experienced programmers often fail to fully leverage the computing power of new parallel architectures due to their steep learning curve and programming complexity. In this paper, we propose Vivaldi, a new domain-specific language for volume processing and visualization on distributed heterogeneous computing systems. Vivaldi's Python-like grammar and parallel processing abstractions provide flexible programming tools for non-experts to easily write high-performance parallel computing code. Vivaldi provides commonly used functions and numerical operators for customized visualization and high-throughput image processing applications. We demonstrate the performance and usability of Vivaldi on several examples ranging from volume rendering to image segmentation.", "uri": "https://vimeo.com/102508479", "name": "Vivaldi: A Domain-Specific Language for Volume Processing and Visualization on Distributed Heterogeneous Systems", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:04:34+00:00", "description": "Authors: Norbert Lindow, Daniel Baum, Hans-Christian Hege\n\nAbstract: The most popular molecular surface in molecular visualization is the solvent excluded surface (SES). It provides information about the accessibility of a biomolecule for a solvent molecule that is geometrically approximated by a sphere. During a period of almost four decades, the SES has served for many purposes \u00e2\u20ac\u201c including visualization, analysis of molecular interactions and the study of cavities in molecular structures. However, if one is interested in the surface that is accessible to a molecule whose shape differs significantly from a sphere, a different concept is necessary. To address this problem, we generalize the definition of the SES by replacing the probe sphere with the full geometry of the ligand defined by the arrangement of its van der Waals spheres. We call the new surface ligand excluded surface (LES) and present an efficient, grid-based algorithm for its computation. Furthermore, we show that this algorithm can also be used to compute molecular cavities that could host the ligand molecule. We provide a detailed description of its implementation on CPU and GPU. Furthermore, we present a performance and convergence analysis and compare the LES for several molecules, using as ligands either water or small organic molecules.", "uri": "https://vimeo.com/102508041", "name": "Ligand Excluded Surface : A New Type of Molecular Surface", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:04:33+00:00", "description": "Authors: Tobias G\u00fcnther, Holger Theisel\n\nAbstract: The cores of massless, swirling particle motion are an indicator for vortex-like behavior in vector fields and to this end, a number of coreline extractors have been proposed in the literature. Though, many practical applications go beyond the study of the vector field. Instead, engineers seek to understand the behavior of inertial particles moving therein, for instance in sediment transport, helicopter brownout and pulverized coal combustion. In this paper, we present two strategies for the extraction of the corelines that inertial particles swirl around, which depend on particle density, particle diameter, fluid viscosity and gravity. The first is to deduce the local swirling behavior from the autonomous inertial motion ODE, which eventually reduces to a parallel vectors operation. For the second strategy, we use a particle density estimation to locate inertial attractors. With this, we are able to extract the cores of swirling inertial particle motion for both steady and unsteady 3D vector fields. We demonstrate our techniques in a number of benchmark data sets, and elaborate on the relation to traditional massless corelines.", "uri": "https://vimeo.com/102508040", "name": "Vortex Cores of Inertial Particles", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:04:33+00:00", "description": "Authors: David Guenther, Alec Jacobson, Jan Reininghaus, Hans-Peter Seidel, Olga Sorkine-Hornung, Tino Weinkauf\n\nAbstract: Data acquisition, numerical inaccuracies, and sampling often introduce noise in measurements and simulations. Removing this noise is often necessary for efficient analysis and visualization of this data, yet many denoising techniques change the minima and maxima of a scalar field. For example, the extrema can appear or disappear, spatially move, and change their value. This can lead to wrong interpretations of the data, e.g., when the maximum temperature over an area is falsely reported being a few degrees cooler because the denoising method is unaware of these features. Recently, a topological denoising technique based on a global energy optimization was proposed, which allows the topology-controlled denoising of 2D scalar fields. While this method preserves the minima and maxima, it is constrained by the size of the data. We extend this work to large 2D data and medium-sized 3D data by introducing a novel domain decomposition approach. It allows processing small patches of the domain independently while still avoiding the introduction of new critical points. Furthermore, we propose an iterative refinement of the solution, which decreases the optimization energy compared to the previous approach and therefore gives smoother results that are closer to the input. We illustrate our technique on synthetic and real-world 2D and 3D data sets that highlight potential applications.", "uri": "https://vimeo.com/102508038", "name": "Fast and Memory-Efficient Topological Denoising of 2D and 3D Scalar Fields", "year": "2014", "event": ""}, {"created_time": "2014-08-04T09:01:12+00:00", "description": "Authors: Dilip Thomas, Vijay Natarajan\n\nAbstract: The complexity in visualizing volumetric data often limits the scope of direct exploration of scalar fields. Isocontour extraction is a popular method for exploring scalar fields because of its simplicity in presenting features in the data. In this paper, we present a novel representation of contours with the aim of studying the similarity relationship between the contours. The representation maps contours to points in a high-dimensional transformation-invariant descriptor space. We leverage the power of this representation to \\ design a clustering based algorithm for detecting symmetric regions in a scalar field. Symmetry detection is a challenging problem because it demands both segmentation of the data and identification of transformation invariant segments. While the former task can be addressed using topological analysis of scalar fields, the latter requires geometry based solutions. Our approach combines the two by utilizing the contour tree for segmenting the data and the descriptor space for determining transformation invariance. We discuss two applications, query driven exploration and asymmetry visualization, that demonstrate the effectiveness of the approach.", "uri": "https://vimeo.com/102507835", "name": "Multiscale Symmetry Detection in Scalar Fields by Clustering Contours", "year": "2014", "event": ""}, {"created_time": "2014-08-04T08:55:05+00:00", "description": "Authors: Christian Schulte zu Berge, Maximilian Baust, Nassir Navab, Ankur Kapoor\n\nAbstract: Direct volume visualization techniques offer powerful insight into volumetric medical images and are part of the clinical routine for many applications.  \\ Up to now, however, their use is mostly limited to tomographic imaging modalities such as CT or MRI.  \\ With very few exceptions, such as fetal ultrasound, classic volume rendering using one-dimensional intensity-based transfer functions fails to yield satisfying results in case of ultrasound volumes.  \\ This is particularly due its gradient-like nature, a high amount of noise and speckle, and the fact that individual tissue types are rather characterized by a similar texture than by similar intensity values.  \\ Therefore, clinicians still prefer to look at 2D slices extracted from the ultrasound volume.  \\ In this work, we present an entirely novel approach to the classification and compositing stage of the volume rendering pipeline, specifically designed for use with ultrasonic images.  \\ We introduce point predicates as a generic formulation for integrating the evaluation of not only low-level information like local intensity or gradient, but also of high-level information, such as non-local image features or even anatomical models.  \\ Thus, we can successfully filter clinically relevant from non-relevant information.  \\ In order to effectively reduce the potentially high dimensionality of the predicate configuration space, we propose the predicate histogram as an intuitive user interface. \\ This is augmented by a scribble technique to provide a comfortable metaphor for selecting predicates of interest.  \\ Assigning importance factors to the predicates allows for focus-and-context visualization that ensures to always show important (focus) regions of the data while maintaining as much context information as possible.  \\ Our method naturally integrates into standard ray casting algorithms and yields superior results in comparison to traditional methods in terms of visualizing a specific target anatomy in ultrasound volumes.  \\", "uri": "https://vimeo.com/102507483", "name": "Predicate-based Focus-and-Context Visualization for 3D Ultrasound", "year": "2014", "event": ""}, {"created_time": "2014-08-04T08:55:05+00:00", "description": "Authors: Marco Ament, Filip Sadlo, Carsten Dachsbacher, Daniel Weiskopf\nAbstract: We present a novel and efficient method to compute volumetric soft shadows for interactive direct volume visualization to improve the perception of spatial depth. By direct control of the softness of volumetric shadows, disturbing visual patterns due to hard shadows can be avoided and users can adapt the illumination to their personal and application-specific requirements. We compute the shadowing of a point in the data set by employing spatial filtering of the optical depth over a finite area patch pointing toward each light source. Conceptually, the area patch spans a volumetric region that is sampled with shadow rays; afterward, the resulting optical depth values are convolved with a low-pass filter on the patch. In the numerical computation, however, to avoid expensive shadow ray marching, we show how to align and set up summed area tables for both directional and point light sources. Once computed, the summed area tables enable efficient evaluation of soft shadows for each point in constant time without shadow ray marching and the softness of the shadows can be controlled interactively. We integrated our method in a GPU-based volume renderer with ray casting from the camera, which offers interactive control of the transfer function, light source positions, and viewpoint, for both static and time-dependent data sets. Our results demonstrate the benefit of soft shadows for visualization to achieve user-controlled illumination with many-point lighting setups for improved perception combined with high rendering speed.", "uri": "https://vimeo.com/102507481", "name": "Low-Pass Filtered Volumetric Shadows", "year": "2014", "event": ""}, {"created_time": "2014-08-04T08:55:04+00:00", "description": "Authors: Silvia Born, Simon S\u00fcndermann, Christoph Russ, Raoul Hopf, Carlos Ruiz, Volkmar Falk, Michael Gessat\n\nAbstract: Transcatheter aortic valve implantation (TAVI) is a minimally-invasive method for the treatment of aortic valve stenosis in patients with high surgical risk.  \\ Despite the success of TAVI, side effects such as paravalvular leakages can occur postoperatively. The goal of this project is to quantitatively analyze the co-occurrence of this complication and several potential risk factors such as stent shape after implantation, implantation height, amount and distribution of calcifications, and contact forces between stent and surrounding structure. In this paper, we present a two-dimensional visualization (stent maps), which allows (1) to comprehensively display all these aspects from CT data and mechanical simulation results and (2) to compare different datasets to identify patterns that are typical for adverse effects. The area of a stent map represents the surface area of the implanted stent -- virtually straightened and uncoiled. Several properties of interest, like radial forces or stent compression, are displayed in this stent map in a heatmap-like fashion. Important anatomical landmarks and calcifications are plotted to show their spatial relation to the stent and possible correlations with the color-coded parameters. To provide comparability, the maps of different patient datasets are spatially adjusted according to a corresponding anatomical landmark. Also, stent maps summarizing the characteristics of different populations (e.g. with or without side effects) can be generated. Up to this point several interesting patterns have been observed with our technique, which remained hidden when examining the raw CT data or 3D visualizations of the same data. One example are obvious radial force maxima between the right and non-coronary valve leaflet occurring mainly in cases without leakages. These observations confirm the usefulness of our approach and give starting points for new hypotheses and further analyses. Because of its reduced dimensionality, the stent map data is an appropriate input for statistical group evaluation and machine learning methods.", "uri": "https://vimeo.com/102507478", "name": "Stent maps \u2013 Comparative visualization for the prediction of adverse events of transcatheter aortic valve implantations", "year": "2014", "event": ""}, {"created_time": "2014-08-04T08:55:03+00:00", "description": "Authors: Hui Zhang, Jianguang Weng, Guangchen Ruan\n\nAbstract: In this paper, we present a mathematical visualization paradigm for exploring curves embedded in 3D and surfaces in 4D mathematical world. The basic problem is that, 3D figures of 4D mathematical entities often twist, turn, and fold back on themselves, leaving important properties behind the surface sheets. We propose an interactive system to visualize the topological features of the original 4D surface by slicing its 3D figure into a series of feature diagram. A novel 4D visualization interface is designed to allow users to control 4D topological shapes via the collection of diagram handles using the established curve manipulation mechanism. Our system can support rich mathematical interaction of 4D mathematical objects which is very difficult with any existing approach. We further demonstrate the effectiveness of the proposed visualization tool using various experimental results and cases studies.", "uri": "https://vimeo.com/102507477", "name": "Visualizing 2-dimensional manifolds with curve handles in 4D", "year": "2014", "event": ""}, {"created_time": "2014-08-04T08:51:36+00:00", "description": "Authors: David Guenther, Roberto Alvarez Boto, Julia Contreras Garcia, Jean-Philip Piquemal, Julien Tierny\n\nAbstract: Interactions between atoms have a major influence on the chemical properties of molecular systems. While covalent interactions impose the structural integrity of molecules, noncovalent interactions govern more subtle phenomena such as protein folding, bonding or self assembly. The understanding of these types of interactions is necessary for the interpretation of many biological processes and chemical design tasks. While traditionally the electron density is analyzed to interpret the quantum chemistry of a molecular system, noncovalent interactions are characterized by low electron densities and only slight variations of them -- challenging their extraction and characterization. Recently, the signed electron density and the reduced gradient, two scalar fields derived from the electron density, have drawn much attention in quantum chemistry since they enable a qualitative visualization of these interactions \\ even in complex molecular systems and experimental measurements. In this work, we present the first combinatorial algorithm for the automated extraction and characterization of covalent and noncovalent interactions in molecular systems. The proposed algorithm is based on a joint topological analysis of the signed electron density and the reduced gradient. Combining the connectivity information of the critical points of these two scalar fields enables to visualize, enumerate, classify and investigate molecular interactions in a \\ robust manner. Experiments on a variety of molecular systems, from simple dimers to proteins or DNA, demonstrate the ability of our technique to robustly extract these interactions and to reveal their structural relations to the atoms and bonds forming the molecules. \\ For simple systems, our analysis corroborates the observations made by the chemists while it provides new visual and quantitative insights on chemical interactions for larger molecular systems.", "uri": "https://vimeo.com/102507315", "name": "Characterizing Molecular Interactions in Chemical Systems", "year": "2014", "event": ""}, {"created_time": "2014-08-04T08:47:13+00:00", "description": "Authors: Ronell Sicat, Jens Krueger, Torsten M\u00f6ller, Markus Hadwiger\n\nAbstract: This paper presents a new multi-resolution volume representation called sparse pdf volumes, which enables consistent multi-resolution volume rendering based on probability density functions (pdfs) of voxel neighborhoods. These pdfs are defined in the 4D domain jointly comprising the 3D volume and its 1D intensity range. Crucially, the computation of sparse pdf volumes exploits data coherence in 4D, resulting in a sparse representation with surprisingly low storage requirements. At run time, we dynamically apply transfer functions to the pdfs using simple and fast convolutions. Whereas standard low-pass filtering and down-sampling incur visible differences between resolution levels, the use of pdfs facilitates consistent results independent of the resolution level used. We \\ describe the efficient out-of-core computation of large-scale sparse pdf volumes, using a novel iterative simplification procedure of a mixture of 4D Gaussians. Finally, our data structure is optimized to facilitate interactive multi-resolution volume rendering on GPUs.", "uri": "https://vimeo.com/102507091", "name": "Sparse PDF Volumes for Consistent Multi-Resolution Volume Rendering", "year": "2014", "event": ""}, {"created_time": "2014-08-04T08:42:22+00:00", "description": "Authors: Ismail Demir, Christian Dick, R\u00fcdiger Westermann\n\nAbstract: A comparative visualization of multiple volume data sets is challenging due to the inherent occlusion effects, yet it is important to effectively reveal uncertainties, correlations and reliable trends in 3D ensemble fields. In this paper we present bidirectional linking of multi-charts and volume visualization as a means to analyze visually 3D scalar ensemble fields at the data level. Multi-charts are an extension of conventional bar and line charts: They linearize the 3D data points along a space-filling curve and draw them as multiple charts in the same plot area. The bar charts encode statistical information on ensemble members, such as histograms and probability densities, and line charts are overlayed to allow comparing members against the ensemble. Alternative linearizations based on histogram similarities or ensemble variation allow clustering of spatial locations depending on data distribution. Multi-charts organize the data at multiple scales to quickly provide overviews and enable users to select regions exhibiting interesting behavior interactively. They are further put into a spatial context by allowing the user to brush or query value intervals and specific distributions, and to simultaneously visualize the corresponding spatial points via volume rendering. By providing a picking mechanism in 3D and instantly highlighting the corresponding data points in the chart, the user can go back and forth between the abstract and the 3D view to focus the analysis.", "uri": "https://vimeo.com/102506845", "name": "Multi-Charts for Comparative 3D Ensemble Visualization", "year": "2014", "event": ""}, {"created_time": "2013-10-04T00:44:05+00:00", "description": null, "uri": "https://vimeo.com/76107208", "name": "VisWeek 2010 Capstone", "year": "2013", "event": "CAPSTONE"}, {"created_time": "2013-09-22T05:31:55+00:00", "description": "Authors: Tobias Isenberg, Petra Isenberg, Jian Chen, Michael Sedlmair, Torsten M\u00f6ller\nAbstract: We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80\u201390% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.", "uri": "https://vimeo.com/75127918", "name": "A Systematic Review on the Practice of Evaluating Visualization", "year": "2013", "event": ""}, {"created_time": "2013-09-12T20:54:34+00:00", "description": "Authors: Bongshin Lee, Rubaiat Habib Kazi, Greg Smith\n\nAbstract: Presenting and communicating insights to an audience\u2014telling a story\u2014is one of the main goals of data exploration. Even though visualization as a storytelling medium has recently begun to gain attention, storytelling is still underexplored in information visualization and little research has been done to help people tell their stories with data. To create a new, more engaging form of storytelling with data, we leverage and extend the narrative storytelling attributes of whiteboard animation with pen and touch interactions. We present SketchStory, a data-enabled digital whiteboard that facilitates the creation of personalized and expressive data charts quickly and easily. SketchStory recognizes a small set of sketch gestures for chart invocation, and automatically completes charts by synthesizing the visuals from the presenter-provided example icon and binding them to the underlying data. Furthermore, SketchStory allows the presenter to move and resize the completed data charts with touch, and filter the underlying data to facilitate interactive exploration. We conducted a controlled experiment for both audiences and presenters to compare SketchStory with a traditional presentation system, Microsoft PowerPoint. Results show that the audience is more engaged by presentations done with SketchStory than PowerPoint. Eighteen out of 24 audience participants preferred SketchStory to PowerPoint. Four out of five presenter participants also favored SketchStory despite the extra effort required for presentation.", "uri": "https://vimeo.com/74413615", "name": "VIS 2013: 30 Second Video: SketchStory: Telling More Engaging Stories with Data through Freeform Sketching", "year": "2013", "event": "PREVIEW"}]